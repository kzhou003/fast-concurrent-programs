

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction to Triton &mdash; Fast Concurrent Programming Guide 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1aac1d93" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/sidebar-fix.js?v=6c2f6f50"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Overview" href="../gpu-tutorials/01-vector-add.html" />
    <link rel="prev" title="Step 1: Profile First" href="performance-optimization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Fast Concurrent Programming Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">CPU Concurrency</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#parallelism">Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#visual-comparison">Visual Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#threading-concurrent-futures-threadpoolexecutor">Threading (concurrent.futures.ThreadPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#multiprocessing-concurrent-futures-processpoolexecutor">Multiprocessing (concurrent.futures.ProcessPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#when-to-use-what">When to Use What</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#what-is-the-gil">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-points">Key Points:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#impact-on-performance">Impact on Performance:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#observing-the-gil-from-script-06">Observing the GIL (from script 06):</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop">Event Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#how-it-works-from-script-07">How It Works (from script 07):</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop-lifecycle">Event Loop Lifecycle:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#modern-vs-old-patterns">Modern vs Old Patterns:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#what-are-coroutines">What are Coroutines?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#defining-coroutines">Defining Coroutines:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-features">Key Features:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#example-from-script-08">Example from Script 08:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#execution-flow">Execution Flow:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#important-rules">Important Rules:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#tasks">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#task-characteristics">Task Characteristics:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#example-from-script-09">Example from Script 09:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#futures">Futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#waiting-for-multiple-tasks">Waiting for Multiple Tasks:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#comparison-table">Comparison Table:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#hybrid-workloads">Hybrid Workloads:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#time-measurement-time-clock-time-perf-counter">1. Time Measurement (<code class="docutils literal notranslate"><span class="pre">time.clock()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#coroutine-syntax-asyncio-coroutine-async-def">2. Coroutine Syntax (<code class="docutils literal notranslate"><span class="pre">&#64;asyncio.coroutine</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#task-creation-asyncio-task-asyncio-create-task">3. Task Creation (<code class="docutils literal notranslate"><span class="pre">asyncio.Task()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">asyncio.create_task()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop-management">4. Event Loop Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#future-callbacks-callbacks-await">5. Future Callbacks (Callbacks -&gt; <code class="docutils literal notranslate"><span class="pre">await</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#blocking-calls-in-async-code">6. Blocking Calls in Async Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#string-formatting-f-strings">7. String Formatting (<code class="docutils literal notranslate"><span class="pre">%</span></code> -&gt; f-strings)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#migration-checklist">Migration Checklist</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#compatibility">Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#quick-reference-guide">Quick Reference Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#further-reading">Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html">Physical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#logical-cores">Logical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#the-concept">The Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#how-it-works">How It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#technical-implementation">Technical Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#hyperthreading-limitations">Hyperthreading Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#checking-hyperthreading-status">Checking Hyperthreading Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#the-fundamental-constraint">The Fundamental Constraint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#optimal-worker-count">Optimal Worker Count</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#real-world-example">Real-World Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#cpu-vs-gpu-different-design-philosophies">CPU vs GPU: Different Design Philosophies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#key-differences">Key Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#simd-and-gpu-architecture">SIMD and GPU Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#why-gpus-excel-at-compute-intensive-tasks">Why GPUs Excel at Compute-Intensive Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#what-gpus-are-good-at">What GPUs Are Good At</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#silicon-real-estate-comparison">Silicon Real Estate Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#detailed-benchmark-results">Detailed Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#decision-matrix">Decision Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#practical-guidelines">Practical Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-1-image-processing">Example 1: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-2-monte-carlo-simulation">Example 2: Monte Carlo Simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-3-neural-network-training">Example 3: Neural Network Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#core-principles">Core Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html">Key Points About start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#example">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#key-points-about-join">Key Points About join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#id1">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#without-join-danger">WITHOUT join() - DANGER!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#with-join-correct">WITH join() - CORRECT!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-1-calling-the-function-directly-instead-of-start">Mistake 1: Calling the function directly instead of start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-2-forgetting-join">Mistake 2: Forgetting join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-3-thinking-threads-share-data-automatically">Mistake 3: Thinking threads share data automatically</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_event_loop.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_event_loop.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html#use-cases">Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_and_futures.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_and_futures.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_task_manipulation.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_task_manipulation.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/concurrent_futures_pooling.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/concurrent_futures_pooling.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#internal-structure">Internal Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#automatic-locking">Automatic Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#put-item"><code class="docutils literal notranslate"><span class="pre">put(item)</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#problem-with-manual-locks">Problem with Manual Locks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#how-queue-does-locking">How Queue Does Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#locking-benefits">Locking Benefits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#execution-flow">Execution Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#thread-safe-data-structure">1. <strong>Thread-Safe Data Structure</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#blocks-correctly">2. <strong>Blocks Correctly</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#no-busy-waiting">3. <strong>No Busy-Waiting</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#task-tracking">4. <strong>Task Tracking</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#safe-for-multiple-producers-consumers">5. <strong>Safe for Multiple Producers/Consumers</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#step-by-step-what-happens-in-put">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">put()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#step-by-step-what-happens-in-get">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">get()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-1-forgetting-lock">Mistake 1: Forgetting Lock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-2-busy-waiting">Mistake 2: Busy-Waiting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-3-race-condition">Mistake 3: Race Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_internal_mechanics.html">The Answer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_internal_mechanics.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html">Without task_done() - Can’t Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#internal-counter-system">Internal Counter System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#visual-timeline">Visual Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#code-simplified">Code (Simplified)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#two-operations">Two Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#timeline-all-three-conditions">Timeline: All Three Conditions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#put">put()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#get">get()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#task-done">task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#join">join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#scenario-1-producer-1-consumer">Scenario: 1 Producer, 1 Consumer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-1-put-increment-counter">Step 1: put() - Increment Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#diagram-tracking-one-task">Diagram: Tracking One Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#diagram-multiple-tasks">Diagram: Multiple Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#counter">Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#condition-variable">Condition Variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#together">Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#without-all-tasks-done-condition">Without all_tasks*done Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#queue-join-without-task-done">Queue.join() Without task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-it-blocks-forever">Why It Blocks Forever</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#queue-join-with-task-done">Queue.join() With task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-it-works">Why It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-1-put-item">Step 1: Put Item</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-2-get-and-process">Step 2: Get and Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-3-mark-done">Step 3: Mark Done</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#flow-diagram">Flow Diagram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#without-task-done-problematic">Without task_done() - PROBLEMATIC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-1-verify-all-work-complete">Use Case 1: Verify All Work Complete</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-2-track-progress">Use Case 2: Track Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-3-batch-processing">Use Case 3: Batch Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#scenario-main-thread-needs-to-know-when-workers-finish">Scenario: Main thread needs to know when workers finish</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#timeline">Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#better-code-pattern">Better Code Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-we-need-task-done">Why We Need task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#the-pattern">The Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html">Regular Lock vs RLock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#why-rlock-is-needed-here">Why RLock is Needed Here</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#use-regular-lock-when">Use Regular Lock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#use-rlock-when">Use RLock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#regular-lock-would-deadlock">Regular Lock - Would Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#counting-semaphore-counter-1">1. Counting Semaphore (Counter &gt; 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#binary-semaphore-counter-0-or-1">2. Binary Semaphore (Counter = 0 or 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#execution-timeline">Execution Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#acquire"><code class="docutils literal notranslate"><span class="pre">acquire()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#counting-semaphore-3-spots-available">Counting Semaphore (3 spots available)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#example-1-swimming-pool-with-limited-capacity">Example 1: Swimming Pool with Limited Capacity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#limiting-concurrent-access">1. <strong>Limiting Concurrent Access</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#synchronizing-multiple-threads">3. <strong>Synchronizing Multiple Threads</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#lock-threading-lock">Lock (<code class="docutils literal notranslate"><span class="pre">threading.Lock</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#semaphore-counting">Semaphore (Counting)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-does-python-have-a-gil">Why Does Python Have a GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-the-gil-works">How the GIL Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#gil-behavior-with-different-operations">GIL Behavior with Different Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-critical-difference">The Critical Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#real-world-analogy">Real-World Analogy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-with-threading-for-cpu-bound">The Problem with Threading for CPU-bound</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-solution-multiprocessing">The Solution: Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-multiprocessing-bypasses-the-gil">How Multiprocessing Bypasses the GIL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#trade-offs-of-multiprocessing">Trade-offs of Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#when-the-trade-off-is-worth-it">When the Trade-off is Worth It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-wasted-time">The Problem: Wasted Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-solution-threading">The Solution: Threading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-threading-works-for-i-o">Why Threading Works for I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-the-os-helps">How the OS Helps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-not-multiprocessing-for-i-o">Why Not Multiprocessing for I/O?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-with-threading-overhead">The Problem with Threading: Overhead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#asyncio-cooperative-multitasking">Asyncio: Cooperative Multitasking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-asyncio-works">How Asyncio Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#event-loop-visualization">Event Loop Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#asyncio-vs-threading-detailed-comparison">Asyncio vs Threading: Detailed Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#when-asyncio-shines">When Asyncio Shines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#cpu-bound-with-threading-the-gil-dance">CPU-bound with Threading: The GIL Dance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-cpu-bound-task-computing-pi">Benchmark: CPU-bound Task (Computing pi)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-i-o-bound-task-web-requests">Benchmark: I/O-bound Task (Web Requests)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-mixed-workload">Benchmark: Mixed Workload</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#quick-reference-table">Quick Reference Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#code-templates">Code Templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-gil-controls-everything">1. The GIL Controls Everything</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#resource-usage-matters">2. Resource Usage Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#know-your-workload">4. Know Your Workload</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Concepts</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gpu-fundamentals.html">Key Differences: CPU vs GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-fundamentals.html#streaming-multiprocessors-sms">Streaming Multiprocessors (SMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-fundamentals.html#thread-organization">Thread Organization</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-fundamentals.html#example-visualization">Example Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory-hierarchy.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory-hierarchy.html#l2-cache">L2 Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory-hierarchy.html#registers">Registers</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory-hierarchy.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html">Warps and SIMD Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#thread-divergence">Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#what-is-occupancy">What is Occupancy?</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#factors-limiting-occupancy">Factors Limiting Occupancy</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#example-calculation">Example Calculation</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#why-occupancy-matters">Why Occupancy Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#the-occupancy-sweet-spot">The Occupancy Sweet Spot</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#grid-and-block-dimensions">Grid and Block Dimensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#choosing-block-size">Choosing Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#within-a-block">Within a Block</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#between-blocks">Between Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#warp-level-reductions">Warp-Level Reductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#overlap-compute-and-memory">Overlap Compute and Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#traditional-approach">Traditional Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#persistent-approach">Persistent Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#key-factors">Key Factors</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html#profiling-tools">Profiling Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html">Step 1: Profile First</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#step-2-identify-bottleneck">Step 2: Identify Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-1-kernel-fusion">Strategy 1: Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-2-tiling">Strategy 2: Tiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-3-vectorized-loads">Strategy 3: Vectorized Loads</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-4-memory-coalescing">Strategy 4: Memory Coalescing</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-1-use-tensor-cores">Strategy 1: Use Tensor Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-2-increase-arithmetic-intensity">Strategy 2: Increase Arithmetic Intensity</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-3-minimize-thread-divergence">Strategy 3: Minimize Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-4-optimize-loop-structure">Strategy 4: Optimize Loop Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-1-reduce-register-usage">Strategy 1: Reduce Register Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-2-tune-shared-memory">Strategy 2: Tune Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#strategy-3-adjust-block-size">Strategy 3: Adjust Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#why-auto-tune">Why Auto-Tune?</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#triton-auto-tuning">Triton Auto-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#persistent-kernels">Persistent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#recomputation">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#pattern-reduction">Pattern: Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#pattern-element-wise">Pattern: Element-wise</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#pattern-matrix-multiply">Pattern: Matrix Multiply</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#issue-low-bandwidth">Issue: Low Bandwidth</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#issue-low-compute-utilization">Issue: Low Compute Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#issue-lower-than-pytorch">Issue: Lower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#before-you-optimize">Before You Optimize</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#memory-optimizations">Memory Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#compute-optimizations">Compute Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#occupancy-optimization">Occupancy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance-optimization.html#advanced">Advanced</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction to Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="#vector-addition-example">Vector Addition Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-grid-syntax-sugar-python-s-getitem">The <cite>[grid]</cite> Syntax Sugar: Python’s <cite>__getitem__</cite></a></li>
<li class="toctree-l1"><a class="reference internal" href="#implementation-details">Implementation Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-complete-flow">The Complete Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="#example-with-vector-addition">Example with Vector Addition</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-ceiling-division">Why Ceiling Division?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#flow-diagram">Flow Diagram</a></li>
<li class="toctree-l1"><a class="reference internal" href="#manual-synchronization">Manual Synchronization</a></li>
<li class="toctree-l1"><a class="reference internal" href="#advanced-topics">Advanced Topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="#performance-considerations">Performance Considerations</a></li>
<li class="toctree-l1"><a class="reference internal" href="#comparison-with-other-optimization-techniques">Comparison with Other Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="#references">References</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#next-steps">Next Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#extensions">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html">Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html">Low Memory Dropout</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html">Layer Norm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html">Fused Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html">Extern Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#summary">Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton Compiler</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-1-python-ast-parsing">Stage 1: Python AST Parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-2-code-generation-ttir">Stage 2: Code Generation (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-3-triton-gpu-ir-ttgir">Stage 3: Triton GPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-4-llvm-ir">Stage 4: LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#block-based-programming-model">Block-based Programming Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#jit-compilation">JIT Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#mlir-infrastructure">MLIR Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#python-components">Python Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#c-components">C++ Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#backend-components">Backend Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html">CodeGenerator Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ast-visitor-pattern">AST Visitor Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#type-inference">Type Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#compilation-orchestration">Compilation Orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ttgir-llvm-ir">TTGIR -&gt; LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#llvm-ir-ptx">LLVM IR -&gt; PTX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-key-components">Cache Key Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-directory-structure">Cache Directory Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html">The NVCC Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#the-llvm-path">The LLVM Path</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#llvm-ir-stage">LLVM IR Stage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-generated-by-triton">PTX Generated by Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#same-tools-same-artifacts">Same Tools, Same Artifacts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#source-language">Source Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compiler-stack">Compiler Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-time">Compilation Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#can-triton-and-cuda-c-work-together">Can Triton and CUDA C++ Work Together?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#advantages-of-llvm-backend">Advantages of LLVM Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#why-not-use-nvcc">Why Not Use NVCC?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#trade-offs">Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#file-types">File Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#example-directory-structures">Example Directory Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-inspection">PTX Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-paths-compared">Compilation Paths Compared</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html">Traditional Compiler Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#example-matrix-multiplication">Example: Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#mlir-philosophy">MLIR Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#dialects">1. Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-triton-uses-mlir">Why Triton Uses MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-s-mlir-dialects">Triton’s MLIR Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#python-source">Python Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-2-tritongpu-ir-ttgir">Stage 2: TritonGPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-3-llvm-dialect">Stage 3: LLVM Dialect</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-4-llvm-ir-actual">Stage 4: LLVM IR (Actual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#command-line-tools">Command-Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#debugging-mlir">Debugging MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#official-documentation">Official Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tutorials">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-specific">Triton-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#key-concepts-recap">Key Concepts Recap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-mlir-matters-for-triton">Why MLIR Matters for Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#the-big-picture">The Big Picture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../learning-paths.html">Learning Paths</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html">CUDA Out of Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#out-of-shared-memory">Out of Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-results">Wrong Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nan-or-inf-values">NaN or Inf Values</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slower-than-pytorch">Slower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#low-gpu-utilization">Low GPU Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#compilation-errors">Compilation Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slow-compilation">Slow Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nvidia-specific">NVIDIA-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#amd-specific">AMD-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-gpu-selected">Wrong GPU Selected</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#print-debugging">Print Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#profiling">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#assertions">Assertions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#unit-testing">Unit Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#when-stuck">When Stuck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#cuda">CUDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#rocm-amd">ROCm (AMD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#flash-attention">Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#normalization">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#optimization-techniques">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#id1">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#gpu-programming">GPU Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#deep-learning">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-tools">NVIDIA Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-tools">AMD Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#tutorials-and-courses">Tutorials and Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#community">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#blogs-and-articles">Blogs and Articles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#triton-examples">Triton Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#production-usage">Production Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-gpus">NVIDIA GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-gpus">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#subscribe-to">Subscribe To</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#conferences">Conferences</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Fast Concurrent Programming Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction to Triton</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/gpu-concepts/triton-concepts.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Triton GPU Programming Concepts</p>
<p>This document explains the core concepts of Triton GPU programming, including kernel launching, grid specifications, program indexing, and how parallel execution works on GPUs.</p>
<nav class="contents local" id="table-of-contents" role="doc-toc">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#introduction-to-triton" id="id1">Introduction to Triton</a></p></li>
<li><p><a class="reference internal" href="#vector-addition-example" id="id2">Vector Addition Example</a></p></li>
<li><p><a class="reference internal" href="#the-grid-syntax-sugar-python-s-getitem" id="id3">The <cite>[grid]</cite> Syntax Sugar: Python’s <cite>__getitem__</cite></a></p>
<ul>
<li><p><a class="reference internal" href="#how-it-works" id="id4">How It Works</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#implementation-details" id="id5">Implementation Details</a></p></li>
<li><p><a class="reference internal" href="#the-complete-flow" id="id6">The Complete Flow</a></p>
<ul>
<li><p><a class="reference internal" href="#purpose" id="id7">Purpose</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#example-with-vector-addition" id="id8">Example with Vector Addition</a></p></li>
<li><p><a class="reference internal" href="#why-ceiling-division" id="id9">Why Ceiling Division?</a></p>
<ul>
<li><p><a class="reference internal" href="#the-spmd-model" id="id10">The SPMD Model</a></p></li>
<li><p><a class="reference internal" href="#program-id-assignment" id="id11">Program ID Assignment</a></p></li>
<li><p><a class="reference internal" href="#implementation-stack" id="id12">Implementation Stack</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#flow-diagram" id="id13">Flow Diagram</a></p>
<ul>
<li><p><a class="reference internal" href="#is-kernel-launch-synchronous-or-asynchronous" id="id14">Is Kernel Launch Synchronous or Asynchronous?</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#manual-synchronization" id="id15">Manual Synchronization</a></p></li>
<li><p><a class="reference internal" href="#advanced-topics" id="id16">Advanced Topics</a></p>
<ul>
<li><p><a class="reference internal" href="#multi-dimensional-grids" id="id17">Multi-Dimensional Grids</a></p></li>
<li><p><a class="reference internal" href="#kernel-caching" id="id18">Kernel Caching</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#performance-considerations" id="id19">Performance Considerations</a></p>
<ul>
<li><p><a class="reference internal" href="#grid-size-selection" id="id20">Grid Size Selection</a></p></li>
<li><p><a class="reference internal" href="#memory-coalescing" id="id21">Memory Coalescing</a></p></li>
<li><p><a class="reference internal" href="#is-hip-detect-amd-gpu-backend" id="id22">is_hip() - Detect AMD GPU Backend</a></p></li>
<li><p><a class="reference internal" href="#what-is-tl-assume" id="id23">What is tl.assume()?</a></p></li>
<li><p><a class="reference internal" href="#why-assumptions-help-optimization" id="id24">Why Assumptions Help Optimization</a></p></li>
<li><p><a class="reference internal" href="#matrix-multiplication-example" id="id25">Matrix Multiplication Example</a></p></li>
<li><p><a class="reference internal" href="#when-to-use-tl-assume" id="id26">When to Use <cite>tl.assume()</cite></a></p></li>
<li><p><a class="reference internal" href="#how-compiler-uses-assumptions" id="id27">How Compiler Uses Assumptions</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#comparison-with-other-optimization-techniques" id="id28">Comparison with Other Optimization Techniques</a></p></li>
<li><p><a class="reference internal" href="#references" id="id29">References</a></p></li>
</ul>
</nav>
<section id="introduction-to-triton">
<h1><a class="toc-backref" href="#id1" role="doc-backlink">Introduction to Triton</a><a class="headerlink" href="#introduction-to-triton" title="Link to this heading"></a></h1>
<p>Triton is a Python-based GPU programming language that allows you to write high-performance GPU kernels without needing to learn CUDA or other low-level languages. It provides a SPMD (Single Program, Multiple Data) execution model where the same kernel code runs on multiple GPU cores in parallel.</p>
<p>Key advantages:
- Write GPU code in Python
- Automatic performance optimization
- No manual memory management
- Abstract away hardware-specific details</p>
</section>
<section id="vector-addition-example">
<h1><a class="toc-backref" href="#id2" role="doc-backlink">Vector Addition Example</a><a class="headerlink" href="#vector-addition-example" title="Link to this heading"></a></h1>
<p>Throughout this document, we’ll use a simple vector addition kernel as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add_kernel</span><span class="p">(</span><span class="n">x_ptr</span><span class="p">,</span> <span class="n">y_ptr</span><span class="p">,</span> <span class="n">output_ptr</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">block_start</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">block_start</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">offsets</span> <span class="o">&lt;</span> <span class="n">n_elements</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">x_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">y_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">output_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>When you launch a Triton kernel, you use this syntax:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">meta</span><span class="p">:</span> <span class="p">(</span><span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">n_elements</span><span class="p">,</span> <span class="n">meta</span><span class="p">[</span><span class="s1">&#39;BLOCK_SIZE&#39;</span><span class="p">]),</span> <span class="p">)</span>
<span class="n">add_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p><strong>Grid specification</strong>: Defines how many parallel programs to launch</p></li>
<li><p><strong>Indexing syntax</strong> <cite>[grid]</cite>: Uses Python’s <cite>__getitem__</cite> magic method</p></li>
<li><p><strong>Kernel arguments</strong>: Data passed to each program</p></li>
<li><p><strong>Synchronization</strong>: Optional explicit synchronization with <cite>torch.cuda.synchronize()</cite></p></li>
</ol>
</section>
<section id="the-grid-syntax-sugar-python-s-getitem">
<h1><a class="toc-backref" href="#id3" role="doc-backlink">The <cite>[grid]</cite> Syntax Sugar: Python’s <cite>__getitem__</cite></a><a class="headerlink" href="#the-grid-syntax-sugar-python-s-getitem" title="Link to this heading"></a></h1>
<section id="how-it-works">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">How It Works</a><a class="headerlink" href="#how-it-works" title="Link to this heading"></a></h2>
<p>The <cite>add_kernel[grid]</cite> syntax is <strong>syntactic sugar</strong> for launching a kernel with a specific grid configuration. Under the hood, it uses Python’s <cite>__getitem__</cite> magic method.</p>
</section>
</section>
<section id="implementation-details">
<h1><a class="toc-backref" href="#id5" role="doc-backlink">Implementation Details</a><a class="headerlink" href="#implementation-details" title="Link to this heading"></a></h1>
<p>When you write:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">add_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">add_kernel</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">grid</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">KernelInterface</span><span class="p">(</span><span class="n">Generic</span><span class="p">[</span><span class="n">T</span><span class="p">]):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A JIT function is launched with: fn[grid](*args, **kwargs).</span>
<span class="sd">        Hence JITFunction.__getitem__ returns a callable proxy that</span>
<span class="sd">        memorizes the grid.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><cite>__getitem__</cite> receives the <cite>grid</cite> parameter</p></li>
<li><p>Returns a <strong>lambda function</strong> that captures the grid</p></li>
<li><p>The lambda calls <cite>self.run()</cite> with the stored grid when invoked</p></li>
<li><p><cite>self.run()</cite> handles actual kernel compilation and GPU launch</p></li>
</ul>
</section>
<section id="the-complete-flow">
<h1><a class="toc-backref" href="#id6" role="doc-backlink">The Complete Flow</a><a class="headerlink" href="#the-complete-flow" title="Link to this heading"></a></h1>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>1. Create kernel instance
   add_kernel = JITFunction(add_kernel_fn)

2. Index with grid (calls __getitem__)
   callable_with_grid = add_kernel[grid]
   # Returns: lambda *args, **kwargs: add_kernel.run(grid=grid, warmup=False, *args, **kwargs)

3. Call with arguments (invokes the lambda)
   add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
   # Calls: add_kernel.run(grid=(97,), warmup=False, x, y, output, n_elements, BLOCK_SIZE=1024)

4. Inside run()
   - Specializes kernel based on argument types
   - Compiles if not cached
   - Extracts grid dimensions
   - Launches GPU kernel
</pre></div>
</div>
<hr class="docutils" />
<section id="purpose">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Purpose</a><a class="headerlink" href="#purpose" title="Link to this heading"></a></h2>
<p><cite>triton.cdiv(a, b)</cite> computes the <strong>ceiling division</strong> of <cite>a</cite> by <cite>b</cite>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>triton.cdiv(a, b) = ceil(a / b) = (a + b - 1) // b
</pre></div>
</div>
</section>
</section>
<section id="example-with-vector-addition">
<h1><a class="toc-backref" href="#id8" role="doc-backlink">Example with Vector Addition</a><a class="headerlink" href="#example-with-vector-addition" title="Link to this heading"></a></h1>
<p>For 98,432 elements and BLOCK_SIZE=1024:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">meta</span><span class="p">:</span> <span class="p">(</span><span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="mi">98432</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="p">)</span>
<span class="c1"># Evaluates to: (97,)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>97 programs</strong> will be launched</p></li>
<li><p>Each program processes <strong>1,024 elements</strong></p></li>
<li><p>Last program (pid=96) processes remaining 432 elements</p></li>
<li><p>Total coverage: 97 * 1024 = 99,328 &gt; 98,432 [OK]</p></li>
</ul>
</section>
<section id="why-ceiling-division">
<h1><a class="toc-backref" href="#id9" role="doc-backlink">Why Ceiling Division?</a><a class="headerlink" href="#why-ceiling-division" title="Link to this heading"></a></h1>
<p>If you used regular division:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid_wrong</span> <span class="o">=</span> <span class="mi">98432</span> <span class="o">//</span> <span class="mi">1024</span>  <span class="c1"># = 96 programs</span>
<span class="c1"># This would miss the last 432 elements!</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid_correct</span> <span class="o">=</span> <span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="mi">98432</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>  <span class="c1"># = 97 programs</span>
<span class="c1"># All elements are covered</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">offsets</span> <span class="o">&lt;</span> <span class="n">n_elements</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">x_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># Masked load prevents out-of-bounds access</span>
</pre></div>
</div>
<hr class="docutils" />
<section id="the-spmd-model">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">The SPMD Model</a><a class="headerlink" href="#the-spmd-model" title="Link to this heading"></a></h2>
<p>Triton uses SPMD (Single Program, Multiple Data):
- <strong>One program definition</strong>: The same kernel code
- <strong>Multiple data partitions</strong>: Each program processes different data
- <strong>Parallel execution</strong>: All programs run simultaneously on different GPU cores</p>
</section>
<section id="program-id-assignment">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Program ID Assignment</a><a class="headerlink" href="#program-id-assignment" title="Link to this heading"></a></h2>
<p>When the GPU launches a kernel with grid size <cite>(97,)</cite>, the GPU runtime automatically assigns each program a unique <strong>program ID</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inside the kernel</span>
<span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># pid values across all programs:</span>
<span class="c1"># Program 0: pid = 0</span>
<span class="c1"># Program 1: pid = 1</span>
<span class="c1"># Program 2: pid = 2</span>
<span class="c1"># ...</span>
<span class="c1"># Program 96: pid = 96</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Each program calculates which data elements it should process:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Line 44-45 in vector-add.py</span>
<span class="n">block_start</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span>
<span class="n">offsets</span> <span class="o">=</span> <span class="n">block_start</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>For BLOCK_SIZE=1024:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Program | pid | block_start | offsets
======= === ========== ========================================
0       0   0          [0, 1, 2, ..., 1023]
1       1   1024       [1024, 1025, 1026, ..., 2047]
2       2   2048       [2048, 2049, 2050, ..., 3071]
96      96  98304      [98304, 98305, ..., 99327]
</pre></div>
</div>
<hr class="docutils" />
<p>Each program loads and processes different tensor slices:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Line 50-54</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">x_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># Load program&#39;s slice</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">y_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>                            <span class="c1"># Element-wise addition</span>
<span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">output_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># Store results</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Program 0:  x[0:1024] + y[0:1024]     -&gt; output[0:1024]
Program 1:  x[1024:2048] + y[1024:2048] -&gt; output[1024:2048]
Program 2:  x[2048:3072] + y[2048:3072] -&gt; output[2048:3072]
...
Program 96: x[98304:99328] + y[98304:99328] -&gt; output[98304:99328]
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="implementation-stack">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Implementation Stack</a><a class="headerlink" href="#implementation-stack" title="Link to this heading"></a></h2>
<p><cite>tl.program_id()</cite> is implemented across multiple layers:</p>
<p><strong>1. User Code (core.py:1605)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">program_id</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">_semantic</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_semantic</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">program_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorTy</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;program_id axis must be 0, 1, or 2 but got </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">create_get_program_id</span><span class="p">(</span><span class="n">axis</span><span class="p">),</span>  <span class="c1"># Generate LLVM IR</span>
        <span class="n">tl</span><span class="o">.</span><span class="n">int32</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>The <cite>create_get_program_id(axis)</cite> method generates LLVM Intermediate Representation code that retrieves the program ID from the GPU.</p>
<p><strong>4. Hardware-Specific Code Generation</strong></p>
<p>LLVM compiles to GPU assembly:</p>
<p><strong>NVIDIA PTX (CUDA):</strong></p>
<div class="highlight-ptx notranslate"><div class="highlight"><pre><span></span><span class="k">mov</span><span class="kt">.u32</span><span class="w"> </span><span class="nv">%r0</span><span class="o">,</span><span class="w"> </span><span class="nv">%ctaid.x</span><span class="w">  </span><span class="c">// Move block ID to register</span>
</pre></div>
</div>
<div class="highlight-amdgpu notranslate"><div class="highlight"><pre><span></span><span class="k">s_get_workgroup_id_x</span><span class="w"> </span><span class="nv">s0</span><span class="w">  </span><span class="c1">// Get block ID in SGPR</span>
</pre></div>
</div>
<p>At runtime, the GPU provides the program ID via special registers:
- NVIDIA: <cite>blockIdx.x</cite>, <cite>blockIdx.y</cite>, <cite>blockIdx.z</cite>
- AMD: Workgroup ID registers</p>
<p>The GPU’s thread scheduler automatically assigns each block a unique ID when launching the kernel.</p>
</section>
</section>
<section id="flow-diagram">
<h1><a class="toc-backref" href="#id13" role="doc-backlink">Flow Diagram</a><a class="headerlink" href="#flow-diagram" title="Link to this heading"></a></h1>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Host (Python):
    grid = (97,)
    kernel[grid](...)
    |
    v
Triton Compiler:
    tl.program_id(axis=0)
    |
    v
LLVM IR Generation:
    builder.create_get_program_id(0)
    |
    v
GPU Code Generation:
    PTX: mov.u32 %r0, %ctaid.x
    AMDGPU: s_get_workgroup_id_x
    |
    v
GPU Runtime (CUDA/HIP Driver):
    Launch 97 blocks with blockIdx = 0..96
    |
    v
GPU Hardware:
    Each block reads its blockIdx from register
    Program 0: blockIdx.x = 0
    Program 1: blockIdx.x = 1
    ...
    Program 96: blockIdx.x = 96
</pre></div>
</div>
<hr class="docutils" />
<section id="is-kernel-launch-synchronous-or-asynchronous">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">Is Kernel Launch Synchronous or Asynchronous?</a><a class="headerlink" href="#is-kernel-launch-synchronous-or-asynchronous" title="Link to this heading"></a></h2>
<p><strong>The kernel launch is ASYNCHRONOUS</strong> by default. When you call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">add_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="manual-synchronization">
<h1><a class="toc-backref" href="#id15" role="doc-backlink">Manual Synchronization</a><a class="headerlink" href="#manual-synchronization" title="Link to this heading"></a></h1>
<p>You only need <cite>torch.cuda.synchronize()</cite> if you need to measure execution time:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Without synchronization (asynchronous)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">add_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
<span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>  <span class="c1"># Measures launch overhead only, not execution</span>

<span class="c1"># With synchronization (measures actual execution)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">add_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>  <span class="c1"># Wait for GPU to finish</span>
<span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>  <span class="c1"># Measures actual kernel execution</span>
</pre></div>
</div>
<hr class="docutils" />
<p>PyTorch automatically synchronizes when you use the tensor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output_triton</span> <span class="o">=</span> <span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Kernel launches asynchronously</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_triton</span><span class="p">)</span>       <span class="c1"># Synchronizes before printing</span>

<span class="c1"># Or when moving to CPU</span>
<span class="n">output_cpu</span> <span class="o">=</span> <span class="n">output_triton</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>  <span class="c1"># Synchronizes before copying</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output_torch</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>                      <span class="c1"># CPU-side, returns immediately</span>
<span class="n">output_triton</span> <span class="o">=</span> <span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>                 <span class="c1"># GPU kernel, launches asynchronously</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_torch</span><span class="p">)</span>                       <span class="c1"># Synchronizes implicitly</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_triton</span><span class="p">)</span>                      <span class="c1"># Synchronizes implicitly</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Max difference: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">output_torch</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">output_triton</span><span class="p">))</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="c1"># By here, both are ready and comparison works</span>
</pre></div>
</div>
<hr class="docutils" />
<p>This design allows:
1. <strong>Overlapping computation</strong>: CPU can queue multiple kernels while GPU executes
2. <strong>Better resource utilization</strong>: No blocking waits
3. <strong>Higher throughput</strong>: Multiple kernels can run concurrently on different GPU streams</p>
</section>
<section id="advanced-topics">
<h1><a class="toc-backref" href="#id16" role="doc-backlink">Advanced Topics</a><a class="headerlink" href="#advanced-topics" title="Link to this heading"></a></h1>
<section id="multi-dimensional-grids">
<h2><a class="toc-backref" href="#id17" role="doc-backlink">Multi-Dimensional Grids</a><a class="headerlink" href="#multi-dimensional-grids" title="Link to this heading"></a></h2>
<p>For 2D operations, you can use multi-dimensional grids:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">matrix_op_kernel</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span> <span class="n">M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="n">pid_m</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Row block ID</span>
    <span class="n">pid_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Column block ID</span>
    <span class="c1"># ... process matrix block (pid_m, pid_n)</span>

<span class="c1"># Launch 2D grid</span>
<span class="n">grid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">meta</span><span class="p">:</span> <span class="p">(</span><span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
</pre></div>
</div>
<hr class="docutils" />
<p>The <cite>tl.constexpr</cite> annotation marks parameters as compile-time constants:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">add_kernel</span><span class="p">(</span><span class="n">x_ptr</span><span class="p">,</span> <span class="n">y_ptr</span><span class="p">,</span> <span class="n">output_ptr</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="c1"># BLOCK_SIZE is a compile-time constant</span>
    <span class="c1"># Triton specializes the kernel for each unique BLOCK_SIZE value</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>  <span class="c1"># Unrolled at compile time</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Unroll loops</p></li>
<li><p>Optimize memory access patterns</p></li>
<li><p>Generate specialized code per BLOCK_SIZE</p></li>
</ul>
</section>
<section id="kernel-caching">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">Kernel Caching</a><a class="headerlink" href="#kernel-caching" title="Link to this heading"></a></h2>
<p>Triton caches compiled kernels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">add_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>  <span class="c1"># Compiles</span>
<span class="n">add_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>  <span class="c1"># Cached!</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Kernel signature</p></li>
<li><p>Argument specialization</p></li>
<li><p>Compilation options</p></li>
</ul>
</section>
</section>
<section id="performance-considerations">
<h1><a class="toc-backref" href="#id19" role="doc-backlink">Performance Considerations</a><a class="headerlink" href="#performance-considerations" title="Link to this heading"></a></h1>
<section id="grid-size-selection">
<h2><a class="toc-backref" href="#id20" role="doc-backlink">Grid Size Selection</a><a class="headerlink" href="#grid-size-selection" title="Link to this heading"></a></h2>
<p>Choose BLOCK_SIZE based on GPU architecture:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Good choices</span>
<span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="mi">256</span>   <span class="c1"># Common for NVIDIA</span>
<span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="c1"># Compute grid size</span>
<span class="n">grid_size</span> <span class="o">=</span> <span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Always mask out-of-bounds accesses:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">offsets</span> <span class="o">&lt;</span> <span class="n">n_elements</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">x_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Segmentation faults</p></li>
<li><p>Invalid memory access</p></li>
<li><p>Undefined behavior</p></li>
</ul>
</section>
<section id="memory-coalescing">
<h2><a class="toc-backref" href="#id21" role="doc-backlink">Memory Coalescing</a><a class="headerlink" href="#memory-coalescing" title="Link to this heading"></a></h2>
<p>Triton optimizes memory access patterns automatically, but contiguous access is still important:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Good: contiguous memory access</span>
<span class="n">offsets</span> <span class="o">=</span> <span class="n">block_start</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">x_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>

<span class="c1"># Bad: strided memory access</span>
<span class="n">offsets</span> <span class="o">=</span> <span class="p">(</span><span class="n">block_start</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">))</span> <span class="o">*</span> <span class="n">stride</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">x_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>When optimizing kernels for specific GPU architectures, you often need to:
1. Compile the kernel ahead of time
2. Extract resource usage information
3. Calculate optimal grid size based on GPU properties
4. Load the binary on the GPU</p>
<p>This section explains the functions that enable this workflow.</p>
</section>
<section id="is-hip-detect-amd-gpu-backend">
<h2><a class="toc-backref" href="#id22" role="doc-backlink">is_hip() - Detect AMD GPU Backend</a><a class="headerlink" href="#is-hip-detect-amd-gpu-backend" title="Link to this heading"></a></h2>
<p><strong>Purpose</strong>: Check if the kernel is running on an AMD GPU (HIP backend) vs NVIDIA (CUDA).</p>
<p><strong>Definition</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">is_hip</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">triton</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">active</span><span class="o">.</span><span class="n">get_current_target</span><span class="p">()</span><span class="o">.</span><span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;hip&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if backend is HIP (AMD’s Heterogeneous-Interface for Portability)</p></li>
<li><p>Returns <code class="docutils literal notranslate"><span class="pre">False</span></code> if backend is CUDA (NVIDIA)</p></li>
</ul>
<p><strong>Why it matters:</strong></p>
<p>AMD and NVIDIA GPUs have fundamentally different architectures:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>NVIDIA Architecture          AMD RDNA/CDNA Architecture
├── CUDA cores              ├── Stream Processors (SPs)
├── Warps (32 threads)      ├── Waves (64 threads)
├── Registers: ~256 per warp ├── VGPRs: 256 per wave
└── Occupancy formula       └── Different occupancy formula
   based on register usage      (includes register pools)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">is_hip</span><span class="p">():</span>
    <span class="c1"># AMD-specific calculation</span>
    <span class="n">occupancy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">NUM_GPRS</span> <span class="o">//</span> <span class="n">WARP_SIZE</span> <span class="o">//</span> <span class="n">n_regs</span><span class="p">,</span> <span class="n">max_num_waves</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_warps</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># NVIDIA-specific calculation</span>
    <span class="n">occupancy</span> <span class="o">=</span> <span class="n">NUM_REGS</span> <span class="o">//</span> <span class="p">(</span><span class="n">n_regs</span> <span class="o">*</span> <span class="n">WARP_SIZE</span> <span class="o">*</span> <span class="n">num_warps</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p><strong>Purpose</strong>: Check if the AMD GPU is CDNA architecture (data center) vs RDNA (gaming).</p>
<p><strong>Definition</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">is_cdna</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">is_hip</span><span class="p">()</span> <span class="ow">and</span> <span class="n">triton</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">active</span><span class="o">.</span><span class="n">get_current_target</span><span class="p">()</span><span class="o">.</span><span class="n">arch</span> <span class="ow">in</span> <span class="p">(</span>
        <span class="s1">&#39;gfx940&#39;</span><span class="p">,</span> <span class="s1">&#39;gfx941&#39;</span><span class="p">,</span> <span class="s1">&#39;gfx942&#39;</span><span class="p">,</span>  <span class="c1"># CDNA 3 (MI300)</span>
        <span class="s1">&#39;gfx90a&#39;</span><span class="p">,</span> <span class="s1">&#39;gfx908&#39;</span>              <span class="c1"># CDNA 1-2 (MI200/MI100)</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>AMD GPUs
├── RDNA (Gaming/Consumer)
│   ├── RX 6800, 6900, 7000 series
│   └── Single register pool (256 VGPRs per wave)
│
└── CDNA (Data Center)
    ├── MI100 (CDNA 1)
    │   └── arch: gfx908
    ├── MI200 (CDNA 2)
    │   └── arch: gfx90a
    └── MI300 (CDNA 3)
        ├── arch: gfx940, gfx941, gfx942
        └── Dual register pools: 512 total VGPRs
</pre></div>
</div>
<p>CDNA architecture has a unique register organization for matrix operations:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CDNA Register Layout
├── Regular VGPRs: 256 registers per wave
├── Accumulation VGPRs: 256 registers per wave
└── Total: 512 registers available per wave
</pre></div>
</div>
<p><strong>Example usage from fused softmax:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">is_hip</span><span class="p">():</span>
    <span class="n">NUM_GPRS</span> <span class="o">=</span> <span class="n">NUM_REGS</span>  <span class="c1"># Start with regular registers</span>
    <span class="k">if</span> <span class="n">is_cdna</span><span class="p">():</span>
        <span class="n">NUM_GPRS</span> <span class="o">=</span> <span class="n">NUM_REGS</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># CDNA can use 2x registers</span>
    <span class="n">occupancy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">NUM_GPRS</span> <span class="o">//</span> <span class="n">WARP_SIZE</span> <span class="o">//</span> <span class="n">n_regs</span><span class="p">,</span> <span class="n">max_num_waves</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_warps</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># NVIDIA doesn&#39;t have dual pools</span>
    <span class="n">occupancy</span> <span class="o">=</span> <span class="n">NUM_REGS</span> <span class="o">//</span> <span class="p">(</span><span class="n">n_regs</span> <span class="o">*</span> <span class="n">WARP_SIZE</span> <span class="o">*</span> <span class="n">num_warps</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p><strong>Purpose</strong>: Compile the kernel and extract resource metadata (registers, shared memory) without actually running it on the GPU.</p>
<p><strong>Signature</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">warmup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">MockTensor</span><span class="o">.</span><span class="n">wrap_dtype</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p><strong>Compiles</strong> the kernel using the Triton compiler pipeline
- Python source code → LLVM IR → GPU assembly (PTX/AMDGPU)</p></li>
<li><p><strong>Analyzes</strong> the compiled binary to extract resource usage</p></li>
<li><p><strong>Returns</strong> a kernel object with metadata properties</p></li>
<li><p><strong>Does NOT execute</strong> on the GPU (warmup=True flag)</p></li>
</ol>
<p><strong>Resource information extracted:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="n">softmax_kernel</span><span class="o">.</span><span class="n">warmup</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                               <span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span>
                               <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                               <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                               <span class="n">num_warps</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                               <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>

<span class="c1"># After warmup, these properties are available:</span>
<span class="n">n_regs</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">n_regs</span>                    <span class="c1"># Registers per thread</span>
<span class="n">size_smem</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">shared</span>        <span class="c1"># Shared memory in bytes</span>
<span class="n">n_spills</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">n_spills</span>                <span class="c1"># Spilled registers to memory</span>
<span class="n">n_max_threads</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">n_max_threads</span>      <span class="c1"># Max concurrent threads</span>
</pre></div>
</div>
<p>Occupancy calculations require <strong>actual compiled kernel properties</strong>, not just source code:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Without warmup:
    Source code → ?? registers needed ??
    Can&#39;t calculate occupancy accurately

With warmup:
    Source code → Compiler → Analyze binary
    |
    ├── n_regs = 64 (per thread)
    ├── metadata.shared = 2048 bytes
    └── Can now calculate:
        occupancy = NUM_REGS / (64 * WARP_SIZE * num_warps)
        grid_size = NUM_SM * occupancy
</pre></div>
</div>
<p>The warmup function respects constexpr specialization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Each unique constexpr combination requires separate warmup</span>
<span class="n">kernel_256</span> <span class="o">=</span> <span class="n">softmax_kernel</span><span class="o">.</span><span class="n">warmup</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="c1"># kernel_256.n_regs = 32 (optimized for 256 elements)</span>

<span class="n">kernel_512</span> <span class="o">=</span> <span class="n">softmax_kernel</span><span class="o">.</span><span class="n">warmup</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="c1"># kernel_512.n_regs = 64 (needs more registers for 512 elements)</span>

<span class="c1"># Different specializations have different resource usage!</span>
</pre></div>
</div>
<hr class="docutils" />
<p><strong>Purpose</strong>: Load the compiled kernel binary on the GPU and initialize runtime handles.</p>
<p><strong>Signature</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_init_handles</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>  <span class="c1"># Already initialized</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">active</span><span class="o">.</span><span class="n">get_current_device</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_run</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">active</span><span class="o">.</span><span class="n">launcher_cls</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span><span class="p">)</span>

    <span class="c1"># Validate shared memory</span>
    <span class="n">max_shared</span> <span class="o">=</span> <span class="n">max_shared_mem</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">shared</span> <span class="o">&gt;</span> <span class="n">max_shared</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">OutOfResources</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="c1"># Load binary and extract register info from the loaded module</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_regs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_spills</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_max_threads</span> <span class="o">=</span> \
        <span class="n">driver</span><span class="o">.</span><span class="n">active</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_binary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">shared</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

    <span class="c1"># Validate thread resources</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_warps</span> <span class="o">*</span> <span class="n">warp_size</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_max_threads</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">OutOfResources</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p><strong>Loads</strong> the binary on the current GPU device</p></li>
<li><p><strong>Validates</strong> that kernel resources fit within GPU limits</p></li>
<li><p><strong>Extracts</strong> register and thread information from the loaded module</p></li>
<li><p><strong>Initializes</strong> GPU-specific launcher and function pointers</p></li>
<li><p><strong>Raises errors</strong> if resources exceed GPU capabilities</p></li>
</ol>
<p><strong>Resource validation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check 1: Shared memory</span>
<span class="k">if</span> <span class="n">kernel</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">shared</span> <span class="o">&gt;</span> <span class="n">max_shared_mem</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">OutOfResources</span><span class="p">(</span><span class="s2">&quot;shared memory&quot;</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">limit</span><span class="p">)</span>

<span class="c1"># Check 2: Tensor memory (Blackwell)</span>
<span class="k">if</span> <span class="n">kernel</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">tmem_size</span> <span class="o">&gt;</span> <span class="mi">512</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">OutOfResources</span><span class="p">(</span><span class="s2">&quot;tensor memory&quot;</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">limit</span><span class="p">)</span>

<span class="c1"># Check 3: Thread count</span>
<span class="n">max_threads</span> <span class="o">=</span> <span class="n">num_warps</span> <span class="o">*</span> <span class="n">warp_size</span>
<span class="k">if</span> <span class="n">max_threads</span> <span class="o">&gt;</span> <span class="n">max_threads_per_sm</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">OutOfResources</span><span class="p">(</span><span class="s2">&quot;threads&quot;</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">limit</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">_init_handles()</span></code> is called lazily (on demand) for several reasons:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lazy initialization pattern:</span>
<span class="nd">@property</span>
<span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_handles</span><span class="p">()</span>  <span class="c1"># Only when first accessed</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run</span>

<span class="c1"># Benefits:</span>
<span class="c1"># 1. Speed: Don&#39;t load binaries until actually needed</span>
<span class="c1"># 2. Device switching: Can switch GPU devices before first run</span>
<span class="c1"># 3. Memory efficiency: Defer loading expensive binaries</span>
</pre></div>
</div>
<p>You explicitly call <code class="docutils literal notranslate"><span class="pre">_init_handles()</span></code> when you need resource information <strong>before</strong> launching:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Warmup: compile and get basic metadata</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">softmax_kernel</span><span class="o">.</span><span class="n">warmup</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

<span class="c1"># Initialize GPU handles to confirm resource usage</span>
<span class="n">kernel</span><span class="o">.</span><span class="n">_init_handles</span><span class="p">()</span>

<span class="c1"># Now safely access n_regs (confirmed from loaded binary)</span>
<span class="n">n_regs</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">n_regs</span>

<span class="c1"># Calculate occupancy with verified register count</span>
<span class="n">occupancy</span> <span class="o">=</span> <span class="n">NUM_REGS</span> <span class="o">//</span> <span class="p">(</span><span class="n">n_regs</span> <span class="o">*</span> <span class="n">WARP_SIZE</span> <span class="o">*</span> <span class="n">num_warps</span><span class="p">)</span>

<span class="c1"># Calculate grid size</span>
<span class="n">num_programs</span> <span class="o">=</span> <span class="n">NUM_SM</span> <span class="o">*</span> <span class="n">occupancy</span>

<span class="c1"># Launch with optimized grid</span>
<span class="n">kernel</span><span class="p">[(</span><span class="n">num_programs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)](</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Here’s how all these functions work together in the fused softmax kernel:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># 1. Calculate basic parameters</span>
    <span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="n">triton</span><span class="o">.</span><span class="n">next_power_of_2</span><span class="p">(</span><span class="n">n_cols</span><span class="p">)</span>
    <span class="n">num_warps</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">num_stages</span> <span class="o">=</span> <span class="mi">4</span>

    <span class="c1"># 2. WARMUP: Compile kernel and get resource usage</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">softmax_kernel</span><span class="o">.</span><span class="n">warmup</span><span class="p">(</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span>
        <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="n">BLOCK_SIZE</span><span class="p">,</span>
        <span class="n">num_stages</span><span class="o">=</span><span class="n">num_stages</span><span class="p">,</span>
        <span class="n">num_warps</span><span class="o">=</span><span class="n">num_warps</span><span class="p">,</span>
        <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span>  <span class="c1"># Dummy grid for warmup</span>
    <span class="p">)</span>

    <span class="c1"># 3. INIT_HANDLES: Load binary on GPU and validate</span>
    <span class="n">kernel</span><span class="o">.</span><span class="n">_init_handles</span><span class="p">()</span>

    <span class="c1"># 4. EXTRACT METADATA</span>
    <span class="n">n_regs</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">n_regs</span>          <span class="c1"># Now confirmed from loaded binary</span>
    <span class="n">size_smem</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">shared</span>

    <span class="c1"># 5. DETECT GPU ARCHITECTURE</span>
    <span class="k">if</span> <span class="n">is_hip</span><span class="p">():</span>
        <span class="n">NUM_GPRS</span> <span class="o">=</span> <span class="n">NUM_REGS</span>
        <span class="k">if</span> <span class="n">is_cdna</span><span class="p">():</span>
            <span class="n">NUM_GPRS</span> <span class="o">=</span> <span class="n">NUM_REGS</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># CDNA has dual register pools</span>

        <span class="n">max_num_waves</span> <span class="o">=</span> <span class="n">MAX_NUM_THREADS</span> <span class="o">//</span> <span class="n">WARP_SIZE</span>
        <span class="n">occupancy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">NUM_GPRS</span> <span class="o">//</span> <span class="n">WARP_SIZE</span> <span class="o">//</span> <span class="n">n_regs</span><span class="p">,</span>
                       <span class="n">max_num_waves</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_warps</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># NVIDIA GPU</span>
        <span class="n">occupancy</span> <span class="o">=</span> <span class="n">NUM_REGS</span> <span class="o">//</span> <span class="p">(</span><span class="n">n_regs</span> <span class="o">*</span> <span class="n">WARP_SIZE</span> <span class="o">*</span> <span class="n">num_warps</span><span class="p">)</span>

    <span class="c1"># 6. APPLY CONSTRAINTS</span>
    <span class="n">occupancy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">occupancy</span><span class="p">,</span> <span class="n">SIZE_SMEM</span> <span class="o">//</span> <span class="n">size_smem</span><span class="p">)</span>

    <span class="c1"># 7. CALCULATE GRID</span>
    <span class="n">num_programs</span> <span class="o">=</span> <span class="n">NUM_SM</span> <span class="o">*</span> <span class="n">occupancy</span>
    <span class="n">num_programs</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_programs</span><span class="p">,</span> <span class="n">n_rows</span><span class="p">)</span>

    <span class="c1"># 8. LAUNCH with optimized grid</span>
    <span class="n">kernel</span><span class="p">[(</span><span class="n">num_programs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)](</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span>
        <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_stages</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
<hr class="docutils" />
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Source Code (02-fused-softmax.py)
└─ softmax_kernel (JIT function)
   │
   └─ warmup()  [Step 2]
      ├─ Compile: Python → LLVM → GPU Assembly
      ├─ Extract: n_regs = 64, shared = 2048
      └─ Return: kernel object

      kernel._init_handles()  [Step 3]
      ├─ Load binary on GPU
      ├─ Validate resources
      ├─ Confirm: n_regs = 64 ✓

      is_hip()  [Step 5a]
      └─ Check: backend == &quot;hip&quot; → True

      is_cdna()  [Step 5b]
      └─ Check: arch in CDNA list → False

      Occupancy Calculation  [Step 5-7]
      ├─ occupancy = NUM_REGS // (64 * WARP_SIZE * 8)
      ├─ occupancy = 65536 // (64 * 32 * 8) = 4
      └─ num_programs = 160 SM * 4 = 640

      Kernel Launch  [Step 8]
      └─ kernel[(min(640, 1823), 1, 1)](...)
         └─ Launch 1823 blocks (one per row)
</pre></div>
</div>
<hr class="docutils" />
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Function        | Type    | Purpose                        | Returns
=============== ========= ================================ ==========================
is_hip()        | Check   | Detect AMD GPU backend         | True/False
is_cdna()       | Check   | Detect AMD CDNA arch           | True/False
warmup()        | Compile | Pre-compile + extract metadata | Kernel object with n_regs, shared
_init_handles() | Init    | Load binary on GPU + validate  | (Initializes internal state)
</pre></div>
</div>
<hr class="docutils" />
<p>When writing high-performance GPU kernels, you often know certain conditions will always be true at runtime. Rather than letting the compiler generate defensive code to handle all cases, you can use <cite>tl.assume()</cite> to tell the compiler these guarantees, enabling aggressive optimizations.</p>
</section>
<section id="what-is-tl-assume">
<h2><a class="toc-backref" href="#id23" role="doc-backlink">What is tl.assume()?</a><a class="headerlink" href="#what-is-tl-assume" title="Link to this heading"></a></h2>
<p><strong>Purpose</strong>: Provide compile-time hints to the Triton compiler that a condition is guaranteed to be true.</p>
<p><strong>Definition</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">assume</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">_semantic</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Allow compiler to assume the :code:`cond` is True.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">_semantic</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">_semantic</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">cond</span><span class="p">))</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Tells the compiler: “This condition will always be true at runtime”</p></li>
<li><p>Allows the compiler to:
1. Eliminate redundant bounds checks
2. Simplify pointer arithmetic
3. Remove impossible code branches
4. Generate smaller, faster code
5. Enable more aggressive optimizations</p></li>
</ul>
<p><strong>Important caveat</strong>: If your assumption is false, the compiler generates incorrect code and you get undefined behavior!</p>
</section>
<section id="why-assumptions-help-optimization">
<h2><a class="toc-backref" href="#id24" role="doc-backlink">Why Assumptions Help Optimization</a><a class="headerlink" href="#why-assumptions-help-optimization" title="Link to this heading"></a></h2>
<p>Consider address calculation in matrix multiplication:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Without assumption:</span>
<span class="n">a_ptr</span> <span class="o">=</span> <span class="n">base_ptr</span> <span class="o">+</span> <span class="p">(</span><span class="n">offset</span> <span class="o">*</span> <span class="n">stride</span><span class="p">)</span>
<span class="c1"># Compiler must check:</span>
<span class="c1"># - Is stride positive or negative?</span>
<span class="c1"># - Could offset * stride overflow?</span>
<span class="c1"># - Is pointer within valid bounds?</span>
<span class="c1"># Generated code: ~5-7 instructions</span>

<span class="c1"># With assumption:</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">a_ptr</span> <span class="o">=</span> <span class="n">base_ptr</span> <span class="o">+</span> <span class="p">(</span><span class="n">offset</span> <span class="o">*</span> <span class="n">stride</span><span class="p">)</span>
<span class="c1"># Compiler knows stride is positive:</span>
<span class="c1"># - offset * stride always non-negative</span>
<span class="c1"># - No overflow for positive values</span>
<span class="c1"># - Can simplify calculations</span>
<span class="c1"># Generated code: ~2-3 instructions</span>
</pre></div>
</div>
</section>
<section id="matrix-multiplication-example">
<h2><a class="toc-backref" href="#id25" role="doc-backlink">Matrix Multiplication Example</a><a class="headerlink" href="#matrix-multiplication-example" title="Link to this heading"></a></h2>
<p>In <cite>03-matrix-multiplication.py</cite> (lines 267-277), the kernel makes the following assumptions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add some integer bound assumptions.</span>
<span class="c1"># This helps to guide integer analysis in the backend to optimize</span>
<span class="c1"># load/store offset address calculation</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">pid_m</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">pid_n</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride_am</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride_ak</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride_bn</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride_bk</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride_cm</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride_cn</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Program IDs are non-negative:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">pid_m</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">pid_n</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Strides are positive:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride_am</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># stride from tensor.stride(0)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride_ak</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># stride from tensor.stride(1)</span>
<span class="c1"># ... and so on</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Skip overflow checks on stride multiplication</p></li>
<li><p>Avoid handling backward (negative stride) cases</p></li>
<li><p>Simplify bounds validation</p></li>
<li><p>Generate optimal addressing code</p></li>
</ul>
<p><strong>Real-world impact:</strong></p>
<p>After these assumptions, the kernel calculates:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pointer arithmetic for matrix blocks</span>
<span class="n">offs_am</span> <span class="o">=</span> <span class="p">(</span><span class="n">pid_m</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_M</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE_M</span><span class="p">))</span> <span class="o">%</span> <span class="n">M</span>
<span class="n">offs_bn</span> <span class="o">=</span> <span class="p">(</span><span class="n">pid_n</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_N</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">))</span> <span class="o">%</span> <span class="n">N</span>
<span class="n">offs_k</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">)</span>

<span class="c1"># With positive stride assumptions, these simplify significantly:</span>
<span class="n">a_ptrs</span> <span class="o">=</span> <span class="n">a_ptr</span> <span class="o">+</span> <span class="p">(</span><span class="n">offs_am</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_am</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_ak</span><span class="p">)</span>
<span class="n">b_ptrs</span> <span class="o">=</span> <span class="n">b_ptr</span> <span class="o">+</span> <span class="p">(</span><span class="n">offs_k</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_bk</span> <span class="o">+</span> <span class="n">offs_bn</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_bn</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="when-to-use-tl-assume">
<h2><a class="toc-backref" href="#id26" role="doc-backlink">When to Use <cite>tl.assume()</cite></a><a class="headerlink" href="#when-to-use-tl-assume" title="Link to this heading"></a></h2>
<p>Use <cite>tl.assume()</cite> when you know a condition is <strong>guaranteed</strong> because:</p>
<ol class="arabic simple">
<li><p><strong>GPU runtime guarantees it</strong></p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">pid</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Always true from GPU runtime</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stride</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># True for contiguous tensors</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_blocks</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">num_blocks</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># cdiv always returns &gt;= 1</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># In Python wrapper:</span>
<span class="k">assert</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Tensor must be contiguous&quot;</span>

<span class="c1"># In kernel:</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Safe because Python layer checked it</span>
</pre></div>
</div>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WRONG - Makes false assumption</span>
<span class="n">stride</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Could be negative!</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># DANGER: False assumption!</span>

<span class="c1"># Compiler generates code assuming stride &gt; 0</span>
<span class="c1"># But if stride is actually negative, UNDEFINED BEHAVIOR!</span>
<span class="c1"># Results in memory corruption, crashes, wrong answers</span>
</pre></div>
</div>
<hr class="docutils" />
<p><cite>tl.assume()</cite> is similar to <cite>tl.static_assert()</cite> but with important differences:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Feature              | tl.assume()              | tl.static_assert()
==================== ========================= ======================
When evaluated       | Compile time (hint)      | Compile time (check)
If condition false   | Undefined behavior       | Compilation error
Purpose              | Optimization hint        | Safety validation
Use case             | Known truths             | Verify preconditions
Performance impact   | Enables optimizations    | None (fails to compile)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use static_assert to verify kernel preconditions</span>
<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kernel</span><span class="p">(</span><span class="n">x_ptr</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="c1"># Check: Is BLOCK_SIZE valid at compile time?</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">static_assert</span><span class="p">(</span><span class="n">BLOCK_SIZE</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Error if BLOCK_SIZE &lt; 1</span>

    <span class="c1"># Assume: stride will always be positive at runtime</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Hint for optimization</span>
</pre></div>
</div>
<hr class="docutils" />
<p>The <cite>tl.assume()</cite> calls in matrix multiplication are critical for performance:</p>
<dl class="simple">
<dt>Without assumptions:</dt><dd><ul class="simple">
<li><p>Compiler generates defensive code</p></li>
<li><p>Includes bounds checks and overflow validation</p></li>
<li><p>Handles negative strides and offsets</p></li>
<li><p>~50% more instructions in address calculation loop</p></li>
<li><p>Lower throughput: ~200 TFLOPS</p></li>
</ul>
</dd>
<dt>With assumptions:</dt><dd><ul class="simple">
<li><p>Compiler generates optimal code</p></li>
<li><p>Eliminates impossible cases</p></li>
<li><p>Simplifies stride multiplication</p></li>
<li><p>~33% fewer instructions</p></li>
<li><p>Higher throughput: ~240-260 TFLOPS</p></li>
</ul>
</dd>
</dl>
<p><strong>This 20-30% performance difference comes purely from compiler optimization enabled by assumptions!</strong></p>
</section>
<section id="how-compiler-uses-assumptions">
<h2><a class="toc-backref" href="#id27" role="doc-backlink">How Compiler Uses Assumptions</a><a class="headerlink" href="#how-compiler-uses-assumptions" title="Link to this heading"></a></h2>
<p>The Triton compiler’s integer analysis backend uses <cite>tl.assume()</cite> to:</p>
<ol class="arabic simple">
<li><p><strong>Range analysis</strong>: Determine possible value ranges</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">pid_m</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># Compiler knows: pid_m ∈ [0, ∞)</span>
<span class="c1"># Can eliminate negative checks</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">offset</span> <span class="o">&lt;</span> <span class="n">MAX_INT</span><span class="p">)</span>
<span class="c1"># Compiler knows: stride * offset won&#39;t overflow</span>
<span class="c1"># Can skip overflow checks</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">ptr</span> <span class="o">&gt;=</span> <span class="n">base_ptr</span><span class="p">)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">ptr</span> <span class="o">&lt;</span> <span class="n">end_ptr</span><span class="p">)</span>
<span class="c1"># Compiler knows: ptr is always in bounds</span>
<span class="c1"># Can eliminate bounds checking</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tl</span><span class="o">.</span><span class="n">assume</span><span class="p">(</span><span class="n">condition</span><span class="p">)</span>
<span class="k">if</span> <span class="n">condition</span><span class="p">:</span>
    <span class="c1"># Path taken</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Dead code - compiler eliminates it</span>
    <span class="k">pass</span>
</pre></div>
</div>
<hr class="docutils" />
<p><strong>Key takeaways:</strong></p>
<ul class="simple">
<li><p><strong>Purpose</strong>: Optimization hint telling compiler about guaranteed conditions</p></li>
<li><p><strong>Use when</strong>: You know a condition is absolutely true</p></li>
<li><p><strong>Benefits</strong>: 2-3x fewer instructions for pointer arithmetic</p></li>
<li><p><strong>Risk</strong>: False assumptions cause undefined behavior</p></li>
<li><p><strong>Best practice</strong>: Only assume things guaranteed by runtime/library/preconditions</p></li>
<li><p><strong>Performance impact</strong>: 10-30% improvement in pointer-heavy kernels</p></li>
</ul>
</section>
</section>
<section id="comparison-with-other-optimization-techniques">
<h1><a class="toc-backref" href="#id28" role="doc-backlink">Comparison with Other Optimization Techniques</a><a class="headerlink" href="#comparison-with-other-optimization-techniques" title="Link to this heading"></a></h1>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Technique           | Purpose                | When to Use
=================== ======================== =========================
tl.assume()         | Compiler optimization  | Known guaranteed conditions
tl.static_assert()  | Validation/safety      | Verify kernel preconditions
tl.static_print()   | Debugging              | Print compile-time values
Constexpr params    | Specialization         | Compile-time constants
Unrolling hints     | Loop optimization      | Control loop unrolling
</pre></div>
</div>
<hr class="docutils" />
<p>Key Concepts Recap:</p>
<ol class="arabic simple">
<li><p><strong>Grid Specification</strong>: <cite>triton.cdiv()</cite> calculates how many programs to launch</p></li>
<li><p><strong>Syntax Sugar</strong>: <cite>kernel[grid]()</cite> uses Python’s <cite>__getitem__</cite> to create a callable proxy</p></li>
<li><p><strong>Program Identification</strong>: <cite>tl.program_id()</cite> retrieves the program’s unique ID from GPU hardware</p></li>
<li><p><strong>Data Partitioning</strong>: Each program uses <cite>pid * BLOCK_SIZE</cite> to partition data</p></li>
<li><p><strong>SPMD Model</strong>: All programs run the same code on different data concurrently</p></li>
<li><p><strong>Asynchronous Execution</strong>: Kernels launch asynchronously; use <cite>torch.cuda.synchronize()</cite> or PyTorch operations to wait</p></li>
<li><p><strong>GPU Hardware</strong>: <cite>program_id()</cite> maps to hardware block indices (blockIdx in CUDA)</p></li>
</ol>
<p>The beauty of Triton is that it handles all the low-level details (LLVM IR generation, memory optimization, GPU-specific code generation) automatically, letting you write high-performance GPU code in Python.</p>
</section>
<section id="references">
<h1><a class="toc-backref" href="#id29" role="doc-backlink">References</a><a class="headerlink" href="#references" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p>Triton Documentation: <a class="reference external" href="https://triton-lang.org/">https://triton-lang.org/</a></p></li>
<li><p>NVIDIA CUDA Programming Guide: <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a></p></li>
<li><p>Vector Addition Example: <cite>/triton_cuda/triton_practice/01-vector-add.py</cite></p></li>
<li><p>Triton Runtime JIT: <cite>/triton/python/triton/runtime/jit.py</cite></p></li>
<li><p>Triton Language Semantic: <cite>/triton/python/triton/language/semantic.py</cite></p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="performance-optimization.html" class="btn btn-neutral float-left" title="Step 1: Profile First" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../gpu-tutorials/01-vector-add.html" class="btn btn-neutral float-right" title="Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Concurrent Programs.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>