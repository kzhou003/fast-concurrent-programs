

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performance Optimization Strategies &mdash; Triton GPU Programming Guide 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Vector Addition in Triton" href="../tutorials/01-vector-add.html" />
    <link rel="prev" title="GPU Execution Model" href="execution-model.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Triton GPU Programming Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gpu-fundamentals.html">GPU Fundamentals</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gpu-fundamentals.html#understanding-gpu-architecture">Understanding GPU Architecture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="gpu-fundamentals.html#key-differences-cpu-vs-gpu">Key Differences: CPU vs GPU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="gpu-fundamentals.html#gpu-hierarchy">GPU Hierarchy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="gpu-fundamentals.html#streaming-multiprocessors-sms">Streaming Multiprocessors (SMs)</a></li>
<li class="toctree-l3"><a class="reference internal" href="gpu-fundamentals.html#thread-organization">Thread Organization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="gpu-fundamentals.html#spmd-programming-model">SPMD Programming Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="gpu-fundamentals.html#example-visualization">Example Visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="gpu-fundamentals.html#compute-capabilities">Compute Capabilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpu-fundamentals.html#key-concepts-summary">Key Concepts Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpu-fundamentals.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="memory-hierarchy.html">GPU Memory Hierarchy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="memory-hierarchy.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#the-performance-pyramid">The Performance Pyramid</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="memory-hierarchy.html#global-memory-hbm-dram">Global Memory (HBM/DRAM)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#memory-coalescing">Memory Coalescing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="memory-hierarchy.html#l2-cache">L2 Cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="memory-hierarchy.html#l1-cache-shared-memory-sram">L1 Cache / Shared Memory (SRAM)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#example-matrix-multiplication">Example: Matrix Multiplication</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="memory-hierarchy.html#registers">Registers</a></li>
<li class="toctree-l2"><a class="reference internal" href="memory-hierarchy.html#memory-access-patterns">Memory Access Patterns</a><ul>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#pattern-1-streaming-bandwidth-bound">Pattern 1: Streaming (Bandwidth-Bound)</a></li>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#pattern-2-staged-computation-compute-bound">Pattern 2: Staged Computation (Compute-Bound)</a></li>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#pattern-3-reduction-mixed">Pattern 3: Reduction (Mixed)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="memory-hierarchy.html#optimizing-for-memory-hierarchy">Optimizing for Memory Hierarchy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#the-golden-rules">The Golden Rules</a></li>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#example-naive-vs-optimized-softmax">Example: Naive vs Optimized Softmax</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="memory-hierarchy.html#measuring-memory-performance">Measuring Memory Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#key-metrics">Key Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="memory-hierarchy.html#tools-for-profiling">Tools for Profiling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="memory-hierarchy.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="execution-model.html">GPU Execution Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#thread-execution">Thread Execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#warps-and-simd-execution">Warps and SIMD Execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#thread-divergence">Thread Divergence</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#occupancy">Occupancy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#what-is-occupancy">What is Occupancy?</a></li>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#factors-limiting-occupancy">Factors Limiting Occupancy</a></li>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#example-calculation">Example Calculation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#latency-hiding">Latency Hiding</a><ul>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#why-occupancy-matters">Why Occupancy Matters</a></li>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#the-occupancy-sweet-spot">The Occupancy Sweet Spot</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#kernel-launch-configuration">Kernel Launch Configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#grid-and-block-dimensions">Grid and Block Dimensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#choosing-block-size">Choosing Block Size</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#synchronization">Synchronization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#within-a-block">Within a Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#between-blocks">Between Blocks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#warp-level-operations">Warp-Level Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#warp-level-reductions">Warp-Level Reductions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#software-pipelining">Software Pipelining</a><ul>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#overlap-compute-and-memory">Overlap Compute and Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#persistent-kernels">Persistent Kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#traditional-approach">Traditional Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#persistent-approach">Persistent Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#performance-considerations">Performance Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#key-factors">Key Factors</a></li>
<li class="toctree-l3"><a class="reference internal" href="execution-model.html#profiling-tools">Profiling Tools</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="execution-model.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Performance Optimization Strategies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-optimization-process">The Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-profile-first">Step 1: Profile First</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-identify-bottleneck">Step 2: Identify Bottleneck</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#memory-optimization">Memory Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#strategy-1-kernel-fusion">Strategy 1: Kernel Fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strategy-2-tiling">Strategy 2: Tiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strategy-3-vectorized-loads">Strategy 3: Vectorized Loads</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strategy-4-memory-coalescing">Strategy 4: Memory Coalescing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#compute-optimization">Compute Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#strategy-1-use-tensor-cores">Strategy 1: Use Tensor Cores</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strategy-2-increase-arithmetic-intensity">Strategy 2: Increase Arithmetic Intensity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strategy-3-minimize-thread-divergence">Strategy 3: Minimize Thread Divergence</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strategy-4-optimize-loop-structure">Strategy 4: Optimize Loop Structure</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#occupancy-optimization">Occupancy Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#strategy-1-reduce-register-usage">Strategy 1: Reduce Register Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strategy-2-tune-shared-memory">Strategy 2: Tune Shared Memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strategy-3-adjust-block-size">Strategy 3: Adjust Block Size</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#auto-tuning">Auto-Tuning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#why-auto-tune">Why Auto-Tune?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#triton-auto-tuning">Triton Auto-Tuning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-techniques">Advanced Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#persistent-kernels">Persistent Kernels</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recomputation">Recomputation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#common-patterns">Common Patterns</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pattern-reduction">Pattern: Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pattern-element-wise">Pattern: Element-wise</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pattern-matrix-multiply">Pattern: Matrix Multiply</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#debugging-performance-issues">Debugging Performance Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#issue-low-bandwidth">Issue: Low Bandwidth</a></li>
<li class="toctree-l3"><a class="reference internal" href="#issue-low-compute-utilization">Issue: Low Compute Utilization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#issue-lower-than-pytorch">Issue: Lower Than PyTorch</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performance-checklist">Performance Checklist</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#before-you-optimize">Before You Optimize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memory-optimizations">Memory Optimizations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compute-optimizations">Compute Optimizations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Occupancy Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced">Advanced</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#next-steps">Next Steps</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Beginner Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/01-vector-add.html">Vector Addition in Triton</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#what-you-ll-learn">What You’ll Learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#gpu-cuda-concepts">GPU/CUDA Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#spmd-single-program-multiple-data">SPMD (Single Program, Multiple Data)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#program-id-and-block-processing">Program ID and Block Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#grid-size">Grid Size</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#memory-hierarchy">Memory Hierarchy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#memory-coalescing">Memory Coalescing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#masking-for-boundary-conditions">Masking for Boundary Conditions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#how-the-kernel-works">How the Kernel Works</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#step-by-step-execution">Step-by-Step Execution</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#performance-characteristics">Performance Characteristics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#bandwidth-bound-operation">Bandwidth-Bound Operation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#arithmetic-intensity">Arithmetic Intensity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#theoretical-performance">Theoretical Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#triton-specific-features">Triton-Specific Features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#triton-jit-decorator"><code class="docutils literal notranslate"><span class="pre">&#64;triton.jit</span></code> Decorator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#constexpr-for-compile-time-constants"><code class="docutils literal notranslate"><span class="pre">constexpr</span></code> for Compile-Time Constants</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#launch-grid-syntax">Launch Grid Syntax</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#benchmarking-insights">Benchmarking Insights</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#expected-results">Expected Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#common-patterns-you-ll-see">Common Patterns You’ll See</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#pointer-arithmetic">1. Pointer Arithmetic</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#vectorized-operations">2. Vectorized Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/01-vector-add.html#masked-memory-operations">3. Masked Memory Operations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#comparison-to-cuda">Comparison to CUDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01-vector-add.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/02-fused-softmax.html">Fused Softmax in Triton</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#what-you-ll-learn">What You’ll Learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#the-problem-with-naive-softmax">The Problem with Naive Softmax</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#standard-softmax-formula">Standard Softmax Formula</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#naive-pytorch-implementation">Naive PyTorch Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#gpu-memory-hierarchy">GPU Memory Hierarchy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#dram-global-memory">DRAM (Global Memory)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#sram-shared-memory-l1-cache">SRAM (Shared Memory / L1 Cache)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#the-key-insight">The Key Insight</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#how-the-fused-kernel-works">How the Fused Kernel Works</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#block-level-processing">Block-Level Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#step-1-load-row-into-sram">Step 1: Load Row Into SRAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#step-2-compute-max-reduction">Step 2: Compute Max (Reduction)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#step-3-exponentiation">Step 3: Exponentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#step-4-compute-sum-another-reduction">Step 4: Compute Sum (Another Reduction)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#step-5-normalize-and-write-back">Step 5: Normalize and Write Back</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#power-of-two-block-sizes">Power-of-Two Block Sizes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#why-must-block-size-be-power-of-2">Why Must BLOCK*SIZE Be Power of 2?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#occupancy-and-performance-tuning">Occupancy and Performance Tuning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#number-of-warps">Number of Warps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#number-of-pipeline-stages">Number of Pipeline Stages</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#computing-occupancy">Computing Occupancy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#persistent-kernels">Persistent Kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#the-pattern">The Pattern</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#numerical-stability">Numerical Stability</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#why-subtract-max">Why Subtract Max?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#padding-with-inf">Padding with -inf</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#memory-bandwidth-analysis">Memory Bandwidth Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#theoretical-speedup">Theoretical Speedup</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#actual-performance">Actual Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#key-cuda-triton-concepts">Key CUDA/Triton Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#reduction-operations">Reduction Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#block-vs-thread">Block vs Thread</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#common-pitfalls">Common Pitfalls</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#row-too-large-for-sram">1. Row Too Large for SRAM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#numerical-precision">2. Numerical Precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#masking-errors">3. Masking Errors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#performance-tips">Performance Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#comparison-to-other-approaches">Comparison to Other Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#jit-fusion-torch-jit-script">JIT Fusion (torch.jit.script)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#manual-cuda">Manual CUDA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/02-fused-softmax.html#cudnn-cublas">CuDNN/CuBLAS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#extensions">Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02-fused-softmax.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Intermediate Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html">Matrix Multiplication in Triton</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#what-you-ll-learn">What You’ll Learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#the-matrix-multiplication-problem">The Matrix Multiplication Problem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#basic-algorithm">Basic Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#why-it-s-hard-to-optimize">Why It’s Hard to Optimize</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#gpu-matrix-multiplication-strategy">GPU Matrix Multiplication Strategy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#blocked-algorithm">Blocked Algorithm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#pseudocode-for-our-kernel">Pseudocode for our kernel</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#pointer-arithmetic-in-2d">Pointer Arithmetic in 2D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#understanding-strides">Understanding Strides</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#block-pointers">Block Pointers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#broadcasting-multiplication">Broadcasting multiplication:</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#l2-cache-optimization">L2 Cache Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#the-problem-with-row-major-ordering">The Problem with Row-Major Ordering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#grouped-ordering-swizzling">Grouped Ordering (Swizzling)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#auto-tuning">Auto-Tuning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#the-configuration-space">The Configuration Space</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#triton-s-auto-tuner">Triton’s Auto-Tuner</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#id3">)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#good-configurations">Good Configurations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#the-kernel-implementation">The Kernel Implementation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#step-1-compute-program-ids">Step 1: Compute Program IDs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#step-2-initialize-pointers">Step 2: Initialize Pointers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#step-3-accumulation-loop">Step 3: Accumulation Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#step-4-apply-activation-optional">Step 4: Apply Activation (Optional)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#step-5-store-result">Step 5: Store Result</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#tensor-cores">Tensor Cores</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#what-are-tensor-cores">What Are Tensor Cores?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#how-tensor-cores-work">How Tensor Cores Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#triton-and-tensor-cores">Triton and Tensor Cores</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#performance-analysis">Performance Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#arithmetic-intensity">Arithmetic Intensity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#roofline-model">Roofline Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#expected-performance">Expected Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#advanced-optimizations">Advanced Optimizations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#software-pipelining">Software Pipelining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#loop-unrolling">Loop Unrolling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#register-pressure">Register Pressure</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#common-pitfalls">Common Pitfalls</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#non-contiguous-tensors">1. Non-Contiguous Tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#wrong-stride-calculation">2. Wrong Stride Calculation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#boundary-conditions">3. Boundary Conditions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#numerical-precision">4. Numerical Precision</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#benchmarking-tips">Benchmarking Tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#warm-up">1. Warm-up</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#measure-tflops">2. Measure TFLOPS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#compare-against-cublas-rocblas">3. Compare Against cuBLAS/rocBLAS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#test-different-sizes">4. Test Different Sizes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03-matrix-multiplication.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html">Low-Memory Dropout in Triton</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#what-you-ll-learn">What You’ll Learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#what-is-dropout">What is Dropout?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#purpose">Purpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#mathematical-definition">Mathematical Definition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#id1">}</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#scaling-factor">Scaling Factor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#naive-implementation-problems">Naive Implementation Problems</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#standard-pytorch-dropout">Standard PyTorch Dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#the-backward-pass-problem">The Backward Pass Problem</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#forward">Forward</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#backward-later">Backward (later)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#backward-pass-uses-different-mask-wrong-gradients">Backward pass uses different mask → wrong gradients!</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#seeded-dropout-solution">Seeded Dropout Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#key-insight">Key Insight</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#the-triton-implementation">The Triton Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#parallel-random-number-generation">Parallel Random Number Generation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#the-challenge">The Challenge</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#the-solution-counter-based-prng">The Solution: Counter-Based PRNG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#the-philox-algorithm">The Philox Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#using-philox-in-triton">Using Philox in Triton</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#call-again-with-same-seed-and-offsets">Call again with same seed and offsets:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#different-seed">Different seed:</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#memory-and-performance-trade-offs">Memory and Performance Trade-offs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#memory-comparison">Memory Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#computational-cost">Computational Cost</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#when-to-use-seeded-dropout">When to Use Seeded Dropout</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#reproducibility-and-determinism">Reproducibility and Determinism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#ensuring-same-random-numbers">Ensuring Same Random Numbers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#forward-pass">Forward pass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#backward-pass-later">Backward pass (later)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#must-use-same-seed-to-get-same-mask">Must use SAME seed to get SAME mask</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#implementation-details">Implementation Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#the-tl-where-function">The <code class="docutils literal notranslate"><span class="pre">tl.where</span></code> Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#masking-for-boundary-conditions">Masking for Boundary Conditions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#random-number-distribution">Random Number Distribution</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#advanced-considerations">Advanced Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#different-random-distributions">Different Random Distributions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#generate-normal-distribution">Generate normal distribution</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#performance-benchmarks">Performance Benchmarks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#expected-performance">Expected Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#bottleneck-analysis">Bottleneck Analysis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#practical-usage">Practical Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#integration-with-pytorch">Integration with PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#seed-generation-strategies">Seed Generation Strategies</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#ensure-different-seeds-on-different-gpus">Ensure different seeds on different GPUs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04-low-memory-dropout.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/05-layer-norm.html">Layer Normalization in Triton</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#what-you-ll-learn">What You’ll Learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#what-is-layer-normalization">What is Layer Normalization?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#the-formula">The Formula</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#step-by-step-math">Step-by-Step Math</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#why-layer-normalization">Why Layer Normalization?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#batch-norm-vs-layer-norm">Batch Norm vs Layer Norm</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#the-forward-pass">The Forward Pass</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#kernel-structure">Kernel Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#computing-the-mean">Computing the Mean</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#computing-the-variance">Computing the Variance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#normalization-and-transformation">Normalization and Transformation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#the-backward-pass">The Backward Pass</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#gradient-mathematics">Gradient Mathematics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#gradient-for-biases">Gradient for Biases</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#gradient-for-weights">Gradient for Weights</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#gradient-for-input-complex">Gradient for Input (Complex!)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#parallel-reduction-strategy">Parallel Reduction Strategy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#the-challenge">The Challenge</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#two-stage-reduction">Two-Stage Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#group-assignment">Group Assignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#using-locks">Using Locks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/05-layer-norm.html#in-kernel">In kernel:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/05-layer-norm.html#acquire-lock">Acquire lock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/05-layer-norm.html#critical-section-update-dw-and-db">Critical section: Update DW and DB</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/05-layer-norm.html#release-lock">Release lock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/05-layer-norm.html#lock-acquired-we-set-it-to-1">Lock acquired (we set it to 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/05-layer-norm.html#do-work">Do work…</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#triton-implementation-details">Triton Implementation Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#memory-layout">Memory Layout</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#stride-usage">Stride Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#why-use-float32-for-accumulation">Why Use float32 for Accumulation?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#performance-characteristics">Performance Characteristics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#computational-complexity">Computational Complexity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#memory-bandwidth">Memory Bandwidth</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#optimization-opportunities">Optimization Opportunities</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#common-pitfalls">Common Pitfalls</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#numerical-stability">1. Numerical Stability</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#dimension-confusion">2. Dimension Confusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#gradient-accumulation-race-conditions">3. Gradient Accumulation Race Conditions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/05-layer-norm.html#use-locks-or-separate-buffers">Use locks or separate buffers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#advanced-concepts">Advanced Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#rmsnorm-simpler-variant">RMSNorm (Simpler Variant)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#groupnorm">GroupNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#fp8-and-mixed-precision">FP8 and Mixed Precision</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#comparison-to-pytorch">Comparison to PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#pytorch-implementation">PyTorch Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#triton-advantages">Triton Advantages</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/05-layer-norm.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05-layer-norm.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/06-fused-attention.html">Fused Attention (Flash Attention) in Triton</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#what-you-ll-learn">What You’ll Learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#background-the-attention-mechanism">Background: The Attention Mechanism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#standard-attention-formula">Standard Attention Formula</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#step-by-step-computation">Step-by-Step Computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#the-memory-problem">The Memory Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#flash-attention-the-key-insights">Flash Attention: The Key Insights</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#insight-1-we-don-t-need-to-store-s-and-p">Insight 1: We Don’t Need to Store S and P</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#insight-2-online-softmax">Insight 2: Online Softmax</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/06-fused-attention.html#pass-1-find-max">Pass 1: Find max</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/06-fused-attention.html#pass-2-compute-softmax">Pass 2: Compute softmax</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#the-online-softmax-algorithm">The Online Softmax Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#standard-softmax">Standard Softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#block-wise-computation">Block-wise Computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#updating-the-output">Updating the Output</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#triton-implementation-details">Triton Implementation Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#the-inner-loop">The Inner Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#causal-masking">Causal Masking</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#stages-for-causal-attention">Stages for Causal Attention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#memory-optimizations">Memory Optimizations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#tensor-descriptors">Tensor Descriptors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#reduced-shared-memory">Reduced Shared Memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#fp8-support">FP8 Support</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#backward-pass">Backward Pass</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#preprocess-step">Preprocess Step</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#computing-dk-and-dv">Computing dK and dV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#computing-dq">Computing dQ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#why-recomputation">Why Recomputation?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#performance-characteristics">Performance Characteristics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#memory-complexity">Memory Complexity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#compute-complexity">Compute Complexity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#arithmetic-intensity">Arithmetic Intensity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#auto-tuning-configurations">Auto-Tuning Configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#common-pitfalls">Common Pitfalls</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#sram-overflow">1. SRAM Overflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#numerical-instability">2. Numerical Instability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/06-fused-attention.html#wrong">Wrong</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/06-fused-attention.html#right">Right</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/06-fused-attention.html#wrong-for-causal-token-can-t-attend-to-itself">Wrong for causal (token can’t attend to itself!)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/06-fused-attention.html#id9">Right</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/06-fused-attention.html#must-update-both-m-i-and-l-i">Must update both m*i and l*i!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/06-fused-attention.html#and-correct-accumulator">And correct accumulator</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#extensions-and-variants">Extensions and Variants</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#multi-query-attention-mqa">Multi-Query Attention (MQA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#grouped-query-attention-gqa">Grouped Query Attention (GQA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#sliding-window-attention">Sliding Window Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/06-fused-attention.html#flash-attention-3">Flash Attention 3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#comparison-to-standard-attention">Comparison to Standard Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/06-fused-attention.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html">Using External Functions (libdevice) in Triton</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#what-you-ll-learn">What You’ll Learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#what-is-libdevice">What is libdevice?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#nvidia-s-libdevice">NVIDIA’s libdevice</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#amd-s-device-libraries">AMD’s Device Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#what-functions-are-available">What Functions Are Available?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#why-use-external-functions">Why Use External Functions?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#triton-built-in-math">Triton Built-in Math</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#when-you-need-more">When You Need More</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#using-libdevice-in-triton">Using libdevice in Triton</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#the-simple-way-default-path">The Simple Way (Default Path)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#the-custom-path-way">The Custom Path Way</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#find-libdevice-in-triton-installation">Find libdevice in Triton installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#pass-to-kernel">Pass to kernel</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#how-it-works-under-the-hood">How It Works Under the Hood</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#compilation-process">Compilation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#type-dispatch">Type Dispatch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#calling-convention">Calling Convention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#available-libdevice-functions-in-triton">Available libdevice Functions in Triton</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#triton-s-libdevice-wrapper">Triton’s libdevice Wrapper</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#trigonometric">Trigonometric</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#hyperbolic">Hyperbolic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#exponential-logarithmic">Exponential/Logarithmic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#special-functions">Special functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#and-many-more">And many more!</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#example-arc-sine-asin">Example: Arc Sine (asin)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#the-math">The Math</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#why-use-libdevice">Why Use libdevice?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#full-example">Full Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#test">Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#should-match">Should match!</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#performance-considerations">Performance Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#libdevice-performance">Libdevice Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#when-to-use-vs-triton-intrinsics">When to Use vs Triton Intrinsics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#linking-multiple-libraries-amd-example">Linking Multiple Libraries (AMD Example)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#debugging-linking-issues">Debugging Linking Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#common-errors">Common Errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#verifying-libraries">Verifying Libraries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#nvidia">NVIDIA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/07-extern-functions.html#amd">AMD</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#advanced-usage">Advanced Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#calling-custom-external-functions">Calling Custom External Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorials/07-extern-functions.html#id5">}</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#mixing-multiple-external-libraries">Mixing Multiple External Libraries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#portability-considerations">Portability Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#nvidia-vs-amd">NVIDIA vs AMD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#function-name-differences">Function Name Differences</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#comparison-to-cuda">Comparison to CUDA</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#in-cuda">In CUDA</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorials/07-extern-functions.html#id6">}</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/07-extern-functions.html#in-triton">In Triton</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/07-extern-functions.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../learning-paths.html">Learning Paths</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#path-1-fast-track-essentials">Path 1: Fast Track (Essentials)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#path-2-deep-understanding-comprehensive">Path 2: Deep Understanding (Comprehensive)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#path-3-transformer-focus-for-llm-nlp">Path 3: Transformer Focus (For LLM/NLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#path-4-computer-vision-focus">Path 4: Computer Vision Focus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#path-5-performance-engineering">Path 5: Performance Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#by-topic">By Topic</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../learning-paths.html#memory-optimization">Memory Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../learning-paths.html#compute-optimization">Compute Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../learning-paths.html#backward-pass-training">Backward Pass / Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../learning-paths.html#advanced-techniques">Advanced Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#learning-tips">Learning Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#assessment-checkpoints">Assessment Checkpoints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../learning-paths.html#after-path-1-fast-track">After Path 1 (Fast Track)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../learning-paths.html#after-path-2-comprehensive">After Path 2 (Comprehensive)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../learning-paths.html#after-path-3-transformer">After Path 3 (Transformer)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#next-steps-after-completing-a-path">Next Steps After Completing a Path</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#resources-for-continued-learning">Resources for Continued Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#choose-your-path">Choose Your Path</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html">Troubleshooting Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#out-of-memory-errors">Out of Memory Errors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#cuda-out-of-memory">CUDA Out of Memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#out-of-shared-memory">Out of Shared Memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#correctness-issues">Correctness Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#wrong-results">Wrong Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#nan-or-inf-values">NaN or Inf Values</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#performance-issues">Performance Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#slower-than-pytorch">Slower Than PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#low-gpu-utilization">Low GPU Utilization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#compilation-issues">Compilation Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#compilation-errors">Compilation Errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#slow-compilation">Slow Compilation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#platform-specific-issues">Platform-Specific Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#nvidia-specific">NVIDIA-Specific</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#amd-specific">AMD-Specific</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#multi-gpu-issues">Multi-GPU Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#wrong-gpu-selected">Wrong GPU Selected</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#debugging-techniques">Debugging Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#print-debugging">Print Debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#profiling">Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#assertions">Assertions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#unit-testing">Unit Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#common-error-messages">Common Error Messages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#getting-help">Getting Help</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting.html#when-stuck">When Stuck</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#best-practices-for-avoiding-issues">Best Practices for Avoiding Issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#prevention-checklist">Prevention Checklist</a></li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References and Resources</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../references.html#official-documentation">Official Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references.html#triton">Triton</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#cuda">CUDA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#rocm-amd">ROCm (AMD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#pytorch">PyTorch</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#research-papers">Research Papers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references.html#flash-attention">Flash Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#normalization">Normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#optimization-techniques">Optimization Techniques</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#id1">Triton</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#books">Books</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references.html#gpu-programming">GPU Programming</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#deep-learning">Deep Learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#tools-and-profilers">Tools and Profilers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references.html#nvidia-tools">NVIDIA Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#amd-tools">AMD Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#benchmarking">Benchmarking</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#online-resources">Online Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references.html#tutorials-and-courses">Tutorials and Courses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#community">Community</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#blogs-and-articles">Blogs and Articles</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#example-repositories">Example Repositories</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references.html#triton-examples">Triton Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#production-usage">Production Usage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#hardware-documentation">Hardware Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references.html#nvidia-gpus">NVIDIA GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#amd-gpus">AMD GPUs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#performance-databases">Performance Databases</a></li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#keeping-up-to-date">Keeping Up to Date</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references.html#subscribe-to">Subscribe To</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references.html#conferences">Conferences</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#academic-courses">Academic Courses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#contributing">Contributing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#citation">Citation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../references.html#quick-reference">Quick Reference</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Triton GPU Programming Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Performance Optimization Strategies</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/concepts/performance-optimization.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="performance-optimization-strategies">
<h1>Performance Optimization Strategies<a class="headerlink" href="#performance-optimization-strategies" title="Link to this heading"></a></h1>
<p>A practical guide to making your GPU kernels fast.</p>
<section id="the-optimization-process">
<h2>The Optimization Process<a class="headerlink" href="#the-optimization-process" title="Link to this heading"></a></h2>
<section id="step-1-profile-first">
<h3>Step 1: Profile First<a class="headerlink" href="#step-1-profile-first" title="Link to this heading"></a></h3>
<p><strong>Never optimize blindly!</strong></p>
<ol class="arabic simple">
<li><p>Measure baseline performance</p></li>
<li><p>Identify bottleneck (memory vs compute)</p></li>
<li><p>Target the bottleneck</p></li>
<li><p>Measure improvement</p></li>
<li><p>Repeat</p></li>
</ol>
<p>Tools:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># NVIDIA</span>
<span class="n">nsys</span> <span class="n">profile</span> <span class="o">--</span><span class="n">stats</span><span class="o">=</span><span class="n">true</span> <span class="n">python</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span>
<span class="n">ncu</span> <span class="o">--</span><span class="nb">set</span> <span class="n">full</span> <span class="n">python</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span>

<span class="c1"># AMD</span>
<span class="n">rocprof</span> <span class="o">--</span><span class="n">stats</span> <span class="n">python</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
<section id="step-2-identify-bottleneck">
<h3>Step 2: Identify Bottleneck<a class="headerlink" href="#step-2-identify-bottleneck" title="Link to this heading"></a></h3>
<p><strong>Memory-Bound</strong>: Low arithmetic intensity</p>
<ul class="simple">
<li><p>Achieved bandwidth &gt; 60% of peak → Good!</p></li>
<li><p>Achieved bandwidth &lt; 30% → Optimization needed</p></li>
<li><p>Focus on: Reducing memory traffic, coalescing</p></li>
</ul>
<p><strong>Compute-Bound</strong>: High arithmetic intensity</p>
<ul class="simple">
<li><p>Compute utilization &gt; 80% → Good!</p></li>
<li><p>Compute utilization &lt; 50% → Optimization needed</p></li>
<li><p>Focus on: Using Tensor Cores, increasing parallelism</p></li>
</ul>
</section>
</section>
<section id="memory-optimization">
<h2>Memory Optimization<a class="headerlink" href="#memory-optimization" title="Link to this heading"></a></h2>
<section id="strategy-1-kernel-fusion">
<h3>Strategy 1: Kernel Fusion<a class="headerlink" href="#strategy-1-kernel-fusion" title="Link to this heading"></a></h3>
<p>Combine multiple operations to reduce memory traffic.</p>
<p><strong>Before</strong> (3 separate kernels):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Kernel 1: Read X, compute Y = exp(X), write Y</span>
<span class="c1"># Kernel 2: Read Y, compute Z = Y / sum(Y), write Z</span>
<span class="c1"># Kernel 3: Read Z, compute W = Z * scale, write W</span>
<span class="c1"># Total: 3 reads + 3 writes = 6 memory operations</span>
</pre></div>
</div>
<p><strong>After</strong> (1 fused kernel):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">fused_kernel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>      <span class="c1"># Read once</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">/</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>      <span class="c1"># Write once</span>
<span class="c1"># Total: 1 read + 1 write = 2 memory operations</span>
<span class="c1"># 3x reduction in memory traffic!</span>
</pre></div>
</div>
<p><strong>Examples in this guide</strong>:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../tutorials/02-fused-softmax.html"><span class="doc">Fused Softmax in Triton</span></a></p></li>
<li><p><a class="reference internal" href="../tutorials/06-fused-attention.html"><span class="doc">Fused Attention (Flash Attention) in Triton</span></a></p></li>
</ul>
</section>
<section id="strategy-2-tiling">
<h3>Strategy 2: Tiling<a class="headerlink" href="#strategy-2-tiling" title="Link to this heading"></a></h3>
<p>Load data into fast SRAM, reuse multiple times.</p>
<p><strong>Example</strong>: Matrix multiplication:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load A block to SRAM (reused for many B blocks)</span>
<span class="c1"># Load B block to SRAM (reused for many A blocks)</span>
<span class="c1"># Compute in SRAM (fast!)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="../tutorials/03-matrix-multiplication.html"><span class="doc">Matrix Multiplication in Triton</span></a> for details.</p>
</section>
<section id="strategy-3-vectorized-loads">
<h3>Strategy 3: Vectorized Loads<a class="headerlink" href="#strategy-3-vectorized-loads" title="Link to this heading"></a></h3>
<p>Load multiple elements per instruction:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bad: Load 1 element at a time (slow)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">ptr</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>

<span class="c1"># Good: Load BLOCK_SIZE elements at once</span>
<span class="n">offsets</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">)</span>  <span class="c1"># Vectorized!</span>
</pre></div>
</div>
<p>Triton does this automatically for you.</p>
</section>
<section id="strategy-4-memory-coalescing">
<h3>Strategy 4: Memory Coalescing<a class="headerlink" href="#strategy-4-memory-coalescing" title="Link to this heading"></a></h3>
<p>Ensure adjacent threads access adjacent memory.</p>
<p><strong>Coalesced</strong> (fast):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Thread i accesses address base + i</span>
<span class="n">offset</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">thread_id</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">ptr</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Uncoalesced</strong> (slow):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Random access pattern</span>
<span class="n">offset</span> <span class="o">=</span> <span class="n">random_indices</span><span class="p">[</span><span class="n">thread_id</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">ptr</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span>
</pre></div>
</div>
<p>Triton’s built-in patterns are usually coalesced.</p>
</section>
</section>
<section id="compute-optimization">
<h2>Compute Optimization<a class="headerlink" href="#compute-optimization" title="Link to this heading"></a></h2>
<section id="strategy-1-use-tensor-cores">
<h3>Strategy 1: Use Tensor Cores<a class="headerlink" href="#strategy-1-use-tensor-cores" title="Link to this heading"></a></h3>
<p>For matrix operations, Tensor Cores provide 10-100x speedup!</p>
<p><strong>Automatically used by</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">accumulator</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">accumulator</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Requirements</strong>:</p>
<ul class="simple">
<li><p>FP16, BF16, TF32, FP8, or INT8 inputs</p></li>
<li><p>Block sizes multiple of 16</p></li>
<li><p>Contiguous memory layout</p></li>
</ul>
<p><strong>Performance</strong>:</p>
<ul class="simple">
<li><p>Regular cores: ~30 TFLOPS</p></li>
<li><p>With Tensor Cores: ~300 TFLOPS</p></li>
</ul>
</section>
<section id="strategy-2-increase-arithmetic-intensity">
<h3>Strategy 2: Increase Arithmetic Intensity<a class="headerlink" href="#strategy-2-increase-arithmetic-intensity" title="Link to this heading"></a></h3>
<p>Do more work per byte loaded:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Low AI: Load data, do simple operation</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># AI = 1 FLOP / 8 bytes = 0.125</span>

<span class="c1"># High AI: Load data, do many operations</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="o">^</span><span class="mi">3</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span>
<span class="c1"># AI = 7 FLOPs / 8 bytes = 0.875</span>
</pre></div>
</div>
<p>Tiling naturally increases AI by reusing data.</p>
</section>
<section id="strategy-3-minimize-thread-divergence">
<h3>Strategy 3: Minimize Thread Divergence<a class="headerlink" href="#strategy-3-minimize-thread-divergence" title="Link to this heading"></a></h3>
<p><strong>Use predication</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bad: Branching</span>
<span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">expensive_computation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Good: Predication</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">expensive_computation</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Note</strong>: For expensive computations, short-circuit evaluation helps.</p>
</section>
<section id="strategy-4-optimize-loop-structure">
<h3>Strategy 4: Optimize Loop Structure<a class="headerlink" href="#strategy-4-optimize-loop-structure" title="Link to this heading"></a></h3>
<p><strong>Unroll loops when possible</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compiler can unroll for better instruction-level parallelism</span>
<span class="c1">#pragma unroll</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">BLOCK_K</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Triton auto-unrolls when beneficial.</p>
</section>
</section>
<section id="occupancy-optimization">
<h2>Occupancy Optimization<a class="headerlink" href="#occupancy-optimization" title="Link to this heading"></a></h2>
<section id="strategy-1-reduce-register-usage">
<h3>Strategy 1: Reduce Register Usage<a class="headerlink" href="#strategy-1-reduce-register-usage" title="Link to this heading"></a></h3>
<p><strong>Monitor</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ncu</span> <span class="o">--</span><span class="n">metrics</span> <span class="n">sm__sass_inst_executed_per_thread</span><span class="o">.</span><span class="n">avg</span>
</pre></div>
</div>
<p><strong>Reduce by</strong>:</p>
<ul class="simple">
<li><p>Smaller local arrays</p></li>
<li><p>Recompute instead of store</p></li>
<li><p>Use shared memory for large temporaries</p></li>
</ul>
</section>
<section id="strategy-2-tune-shared-memory">
<h3>Strategy 2: Tune Shared Memory<a class="headerlink" href="#strategy-2-tune-shared-memory" title="Link to this heading"></a></h3>
<p><strong>Too much</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Only 2 blocks fit per SM</span>
<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kernel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">shared</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">])</span>  <span class="c1"># Too big!</span>
</pre></div>
</div>
<p><strong>Balanced</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kernel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">shared</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">])</span>
    <span class="c1"># BLOCK_SIZE=128 → 64KB, allows 4 blocks per SM</span>
</pre></div>
</div>
</section>
<section id="strategy-3-adjust-block-size">
<h3>Strategy 3: Adjust Block Size<a class="headerlink" href="#strategy-3-adjust-block-size" title="Link to this heading"></a></h3>
<p><strong>Trade-offs</strong>:</p>
<ul class="simple">
<li><p>Smaller blocks: Higher occupancy, less data reuse</p></li>
<li><p>Larger blocks: Lower occupancy, more data reuse</p></li>
</ul>
<p><strong>Find sweet spot with auto-tuning!</strong></p>
</section>
</section>
<section id="auto-tuning">
<h2>Auto-Tuning<a class="headerlink" href="#auto-tuning" title="Link to this heading"></a></h2>
<section id="why-auto-tune">
<h3>Why Auto-Tune?<a class="headerlink" href="#why-auto-tune" title="Link to this heading"></a></h3>
<p>Optimal configuration varies with:</p>
<ul class="simple">
<li><p>Problem size (M, N, K)</p></li>
<li><p>Hardware (A100 vs H100)</p></li>
<li><p>Data type (FP16 vs FP8)</p></li>
</ul>
<p><strong>Solution</strong>: Try multiple configs, pick the fastest.</p>
</section>
<section id="triton-auto-tuning">
<h3>Triton Auto-Tuning<a class="headerlink" href="#triton-auto-tuning" title="Link to this heading"></a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@triton</span><span class="o">.</span><span class="n">autotune</span><span class="p">(</span>
    <span class="n">configs</span><span class="o">=</span><span class="p">[</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_M&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;BLOCK_N&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_M&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s1">&#39;BLOCK_N&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_warps</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_M&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;BLOCK_N&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_warps</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span>
        <span class="c1"># ... more configs</span>
    <span class="p">],</span>
    <span class="n">key</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;K&#39;</span><span class="p">],</span>  <span class="c1"># Cache best config for these values</span>
<span class="p">)</span>
<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kernel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p><strong>Process</strong>:</p>
<ol class="arabic simple">
<li><p>First call: Triton benchmarks all configs</p></li>
<li><p>Caches best config for (M, N, K)</p></li>
<li><p>Subsequent calls: Uses cached best config</p></li>
</ol>
<p><strong>Config Parameters</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">BLOCK_M,</span> <span class="pre">BLOCK_N,</span> <span class="pre">BLOCK_K</span></code>: Tile sizes</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_stages</span></code>: Software pipelining depth (2-5)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_warps</span></code>: Warps per block (2, 4, 8, 16)</p></li>
</ul>
</section>
</section>
<section id="advanced-techniques">
<h2>Advanced Techniques<a class="headerlink" href="#advanced-techniques" title="Link to this heading"></a></h2>
<section id="warp-specialization">
<h3>Warp Specialization<a class="headerlink" href="#warp-specialization" title="Link to this heading"></a></h3>
<p>Different warps do different tasks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Warp 0-3: Load data from memory</span>
<span class="c1"># Warp 4-7: Compute on data</span>
</pre></div>
</div>
<p>Benefits:</p>
<ul class="simple">
<li><p>Overlap memory and compute</p></li>
<li><p>Better resource utilization</p></li>
</ul>
<p>Available on Hopper/Blackwell GPUs.</p>
</section>
<section id="persistent-kernels">
<h3>Persistent Kernels<a class="headerlink" href="#persistent-kernels" title="Link to this heading"></a></h3>
<p>Each block processes multiple work items:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kernel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="n">process</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
<p>Benefits:</p>
<ul class="simple">
<li><p>Amortize launch overhead</p></li>
<li><p>Better L2 cache utilization</p></li>
<li><p>Flexible load balancing</p></li>
</ul>
</section>
<section id="recomputation">
<h3>Recomputation<a class="headerlink" href="#recomputation" title="Link to this heading"></a></h3>
<p>Trade compute for memory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Standard: Store intermediate</span>
<span class="n">intermediate</span> <span class="o">=</span> <span class="n">expensive_compute</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">save</span><span class="p">(</span><span class="n">intermediate</span><span class="p">)</span>  <span class="c1"># Memory cost</span>
<span class="n">later_use</span><span class="p">(</span><span class="n">intermediate</span><span class="p">)</span>

<span class="c1"># Optimized: Recompute</span>
<span class="n">later_use</span><span class="p">(</span><span class="n">expensive_compute</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Compute cost, no memory</span>
</pre></div>
</div>
<p><strong>When useful</strong>: Memory-bound kernels with spare compute.</p>
<p><strong>Example</strong>: Flash Attention recomputes attention scores in backward pass.</p>
</section>
</section>
<section id="common-patterns">
<h2>Common Patterns<a class="headerlink" href="#common-patterns" title="Link to this heading"></a></h2>
<section id="pattern-reduction">
<h3>Pattern: Reduction<a class="headerlink" href="#pattern-reduction" title="Link to this heading"></a></h3>
<p>Sum/max/min across elements:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">reduce_kernel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="c1"># Load data</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="c1"># Reduce</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Efficient reduction</span>

    <span class="c1"># Store</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Key</strong>: Triton uses warp shuffles and shared memory efficiently.</p>
</section>
<section id="pattern-element-wise">
<h3>Pattern: Element-wise<a class="headerlink" href="#pattern-element-wise" title="Link to this heading"></a></h3>
<p>Independent operation on each element:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">elementwise_kernel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">x_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Any function</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">y_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Optimization</strong>: Fuse multiple element-wise ops.</p>
</section>
<section id="pattern-matrix-multiply">
<h3>Pattern: Matrix Multiply<a class="headerlink" href="#pattern-matrix-multiply" title="Link to this heading"></a></h3>
<p>Blocked algorithm with tiling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">BLOCK_K</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">a_ptrs</span><span class="p">)</span>  <span class="c1"># Load A block</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">b_ptrs</span><span class="p">)</span>  <span class="c1"># Load B block</span>
    <span class="n">acc</span> <span class="o">+=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># Accumulate</span>
</pre></div>
</div>
<p><strong>Key</strong>: Data reuse in SRAM, Tensor Core usage.</p>
</section>
</section>
<section id="debugging-performance-issues">
<h2>Debugging Performance Issues<a class="headerlink" href="#debugging-performance-issues" title="Link to this heading"></a></h2>
<section id="issue-low-bandwidth">
<h3>Issue: Low Bandwidth<a class="headerlink" href="#issue-low-bandwidth" title="Link to this heading"></a></h3>
<p><strong>Symptoms</strong>:</p>
<ul class="simple">
<li><p>Achieved bandwidth &lt;&lt; peak bandwidth</p></li>
<li><p>Memory-bound operation</p></li>
</ul>
<p><strong>Check</strong>:</p>
<ol class="arabic simple">
<li><p>Coalescing: Use <code class="docutils literal notranslate"><span class="pre">ncu</span> <span class="pre">--metrics</span> <span class="pre">l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum</span></code></p></li>
<li><p>L2 hit rate: Might be hitting cache</p></li>
<li><p>Occupancy: Too low?</p></li>
</ol>
</section>
<section id="issue-low-compute-utilization">
<h3>Issue: Low Compute Utilization<a class="headerlink" href="#issue-low-compute-utilization" title="Link to this heading"></a></h3>
<p><strong>Symptoms</strong>:</p>
<ul class="simple">
<li><p>Low % of peak FLOPs</p></li>
<li><p>Compute-bound operation</p></li>
</ul>
<p><strong>Check</strong>:</p>
<ol class="arabic simple">
<li><p>Tensor Core usage: Are they being used?</p></li>
<li><p>Thread divergence: Branching killing performance?</p></li>
<li><p>Occupancy: Need more warps?</p></li>
</ol>
</section>
<section id="issue-lower-than-pytorch">
<h3>Issue: Lower Than PyTorch<a class="headerlink" href="#issue-lower-than-pytorch" title="Link to this heading"></a></h3>
<p><strong>Possible causes</strong>:</p>
<ol class="arabic simple">
<li><p>Not using Tensor Cores (PyTorch does)</p></li>
<li><p>Sub-optimal config (need auto-tuning)</p></li>
<li><p>Missing optimizations (fusion, tiling)</p></li>
<li><p>Data layout issues (non-contiguous)</p></li>
</ol>
<p><strong>Debug</strong>:</p>
<ul class="simple">
<li><p>Profile both with <code class="docutils literal notranslate"><span class="pre">ncu</span></code></p></li>
<li><p>Compare metrics (bandwidth, compute, occupancy)</p></li>
<li><p>Check if PyTorch uses vendor lib (cuBLAS, cuDNN)</p></li>
</ul>
</section>
</section>
<section id="performance-checklist">
<h2>Performance Checklist<a class="headerlink" href="#performance-checklist" title="Link to this heading"></a></h2>
<section id="before-you-optimize">
<h3>Before You Optimize<a class="headerlink" href="#before-you-optimize" title="Link to this heading"></a></h3>
<p>☐ Profile to identify bottleneck
☐ Measure baseline performance
☐ Set performance target (realistic!)</p>
</section>
<section id="memory-optimizations">
<h3>Memory Optimizations<a class="headerlink" href="#memory-optimizations" title="Link to this heading"></a></h3>
<p>☐ Fuse operations to reduce memory traffic
☐ Use tiling to maximize SRAM reuse
☐ Ensure coalesced memory accesses
☐ Minimize global memory accesses</p>
</section>
<section id="compute-optimizations">
<h3>Compute Optimizations<a class="headerlink" href="#compute-optimizations" title="Link to this heading"></a></h3>
<p>☐ Use Tensor Cores for matmul
☐ Minimize thread divergence
☐ Maximize arithmetic intensity
☐ Use appropriate data types (FP16, BF16)</p>
</section>
<section id="id1">
<h3>Occupancy Optimization<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>☐ Check register usage
☐ Check shared memory usage
☐ Tune block size
☐ Measure actual occupancy</p>
</section>
<section id="advanced">
<h3>Advanced<a class="headerlink" href="#advanced" title="Link to this heading"></a></h3>
<p>☐ Auto-tune configurations
☐ Use software pipelining
☐ Consider persistent kernels
☐ Profile with vendor tools</p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<p><strong>Optimization hierarchy</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Algorithm</strong>: Choose efficient algorithm</p></li>
<li><p><strong>Memory</strong>: Reduce traffic, maximize reuse</p></li>
<li><p><strong>Compute</strong>: Use specialized hardware (Tensor Cores)</p></li>
<li><p><strong>Parallelism</strong>: Balance resources for good occupancy</p></li>
<li><p><strong>Tuning</strong>: Auto-tune for specific hardware and problem sizes</p></li>
</ol>
<p><strong>Remember</strong>: Profile → Optimize → Measure → Repeat</p>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading"></a></h2>
<p>Apply these concepts in practice:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../tutorials/02-fused-softmax.html"><span class="doc">Fused Softmax in Triton</span></a> - Memory optimization through fusion</p></li>
<li><p><a class="reference internal" href="../tutorials/03-matrix-multiplication.html"><span class="doc">Matrix Multiplication in Triton</span></a> - Compute optimization with Tensor Cores</p></li>
<li><p><a class="reference internal" href="../tutorials/06-fused-attention.html"><span class="doc">Fused Attention (Flash Attention) in Triton</span></a> - Advanced optimization combining all techniques</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="execution-model.html" class="btn btn-neutral float-left" title="GPU Execution Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../tutorials/01-vector-add.html" class="btn btn-neutral float-right" title="Vector Addition in Triton" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Concurrent Programs.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>