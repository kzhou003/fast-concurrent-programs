

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The NVCC Compiler &mdash; Fast Concurrent Programming Guide 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1aac1d93" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/sidebar-fix.js?v=6c2f6f50"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Traditional Compiler Limitations" href="05-mlir-concepts.html" />
    <link rel="prev" title="CodeGenerator Class" href="03-compilation-pipeline.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Fast Concurrent Programming Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">CPU Concurrency</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#parallelism">Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#visual-comparison">Visual Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#threading-concurrent-futures-threadpoolexecutor">Threading (concurrent.futures.ThreadPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#multiprocessing-concurrent-futures-processpoolexecutor">Multiprocessing (concurrent.futures.ProcessPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#when-to-use-what">When to Use What</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#what-is-the-gil">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-points">Key Points:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#impact-on-performance">Impact on Performance:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#observing-the-gil-from-script-06">Observing the GIL (from script 06):</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop">Event Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#how-it-works-from-script-07">How It Works (from script 07):</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop-lifecycle">Event Loop Lifecycle:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#modern-vs-old-patterns">Modern vs Old Patterns:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#what-are-coroutines">What are Coroutines?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#defining-coroutines">Defining Coroutines:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-features">Key Features:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#example-from-script-08">Example from Script 08:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#execution-flow">Execution Flow:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#important-rules">Important Rules:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#tasks">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#task-characteristics">Task Characteristics:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#example-from-script-09">Example from Script 09:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#futures">Futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#waiting-for-multiple-tasks">Waiting for Multiple Tasks:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#comparison-table">Comparison Table:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#hybrid-workloads">Hybrid Workloads:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#time-measurement-time-clock-time-perf-counter">1. Time Measurement (<code class="docutils literal notranslate"><span class="pre">time.clock()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#coroutine-syntax-asyncio-coroutine-async-def">2. Coroutine Syntax (<code class="docutils literal notranslate"><span class="pre">&#64;asyncio.coroutine</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#task-creation-asyncio-task-asyncio-create-task">3. Task Creation (<code class="docutils literal notranslate"><span class="pre">asyncio.Task()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">asyncio.create_task()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop-management">4. Event Loop Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#future-callbacks-callbacks-await">5. Future Callbacks (Callbacks -&gt; <code class="docutils literal notranslate"><span class="pre">await</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#blocking-calls-in-async-code">6. Blocking Calls in Async Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#string-formatting-f-strings">7. String Formatting (<code class="docutils literal notranslate"><span class="pre">%</span></code> -&gt; f-strings)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#migration-checklist">Migration Checklist</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#compatibility">Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#quick-reference-guide">Quick Reference Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#further-reading">Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html">Physical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#logical-cores">Logical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#the-concept">The Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#how-it-works">How It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#technical-implementation">Technical Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#hyperthreading-limitations">Hyperthreading Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#checking-hyperthreading-status">Checking Hyperthreading Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#the-fundamental-constraint">The Fundamental Constraint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#why-more-threads-more-speed">Why More Threads != More Speed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#optimal-worker-count">Optimal Worker Count</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#real-world-example">Real-World Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#cpu-vs-gpu-different-design-philosophies">CPU vs GPU: Different Design Philosophies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#key-differences">Key Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#simd-and-gpu-architecture">SIMD and GPU Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#why-gpus-excel-at-compute-intensive-tasks">Why GPUs Excel at Compute-Intensive Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#what-gpus-are-good-at">What GPUs Are Good At</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#silicon-real-estate-comparison">Silicon Real Estate Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#detailed-benchmark-results">Detailed Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#decision-matrix">Decision Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#practical-guidelines">Practical Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-1-image-processing">Example 1: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-2-monte-carlo-simulation">Example 2: Monte Carlo Simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-3-neural-network-training">Example 3: Neural Network Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#core-principles">Core Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html">Key Points About start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#example">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#key-points-about-join">Key Points About join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#id1">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#without-join-danger">WITHOUT join() - DANGER!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#with-join-correct">WITH join() - CORRECT!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-1-calling-the-function-directly-instead-of-start">Mistake 1: Calling the function directly instead of start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-2-forgetting-join">Mistake 2: Forgetting join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-3-thinking-threads-share-data-automatically">Mistake 3: Thinking threads share data automatically</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_event_loop.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_event_loop.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html#use-cases">Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_and_futures.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_and_futures.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_and_futures.html#examples">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_task_manipulation.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_task_manipulation.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/concurrent_futures_pooling.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/concurrent_futures_pooling.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#internal-structure">Internal Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#automatic-locking">Automatic Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#put-item"><code class="docutils literal notranslate"><span class="pre">put(item)</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#problem-with-manual-locks">Problem with Manual Locks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#how-queue-does-locking">How Queue Does Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#locking-benefits">Locking Benefits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#execution-flow">Execution Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#thread-safe-data-structure">1. <strong>Thread-Safe Data Structure</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#blocks-correctly">2. <strong>Blocks Correctly</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#no-busy-waiting">3. <strong>No Busy-Waiting</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#task-tracking">4. <strong>Task Tracking</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#safe-for-multiple-producers-consumers">5. <strong>Safe for Multiple Producers/Consumers</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#step-by-step-what-happens-in-put">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">put()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#step-by-step-what-happens-in-get">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">get()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-1-forgetting-lock">Mistake 1: Forgetting Lock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-2-busy-waiting">Mistake 2: Busy-Waiting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-3-race-condition">Mistake 3: Race Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_internal_mechanics.html">The Answer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_internal_mechanics.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html">Without task_done() - Canâ€™t Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#with-task-done-can-track-completion">With task_done() - Can Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#internal-counter-system">Internal Counter System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#visual-timeline">Visual Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#code-simplified">Code (Simplified)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#two-operations">Two Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#timeline-all-three-conditions">Timeline: All Three Conditions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#put">put()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#get">get()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#task-done">task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#join">join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#scenario-1-producer-1-consumer">Scenario: 1 Producer, 1 Consumer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-1-put-increment-counter">Step 1: put() - Increment Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-2-get-item-removed-counter-unchanged">Step 2: get() - Item Removed, Counter Unchanged</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-3-task-done-decrement-counter">Step 3: task_done() - Decrement Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-4-join-wait-then-return">Step 4: join() - Wait, Then Return</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#diagram-tracking-one-task">Diagram: Tracking One Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#diagram-multiple-tasks">Diagram: Multiple Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#counter">Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#condition-variable">Condition Variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#together">Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#without-all-tasks-done-condition">Without all_tasks*done Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#queue-join-without-task-done">Queue.join() Without task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-it-blocks-forever">Why It Blocks Forever</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#queue-join-with-task-done">Queue.join() With task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-it-works">Why It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-1-put-item">Step 1: Put Item</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-2-get-and-process">Step 2: Get and Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-3-mark-done">Step 3: Mark Done</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#flow-diagram">Flow Diagram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#without-task-done-problematic">Without task_done() - PROBLEMATIC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#with-task-done-correct">With task_done() - CORRECT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-1-verify-all-work-complete">Use Case 1: Verify All Work Complete</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-2-track-progress">Use Case 2: Track Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-3-batch-processing">Use Case 3: Batch Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#scenario-main-thread-needs-to-know-when-workers-finish">Scenario: Main thread needs to know when workers finish</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#timeline">Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#better-code-pattern">Better Code Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-we-need-task-done">Why We Need task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#the-pattern">The Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html">Regular Lock vs RLock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#why-rlock-is-needed-here">Why RLock is Needed Here</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#use-regular-lock-when">Use Regular Lock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#use-rlock-when">Use RLock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#regular-lock-would-deadlock">Regular Lock - Would Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#rlock-no-deadlock">RLock - No Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#counting-semaphore-counter-1">1. Counting Semaphore (Counter &gt; 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#binary-semaphore-counter-0-or-1">2. Binary Semaphore (Counter = 0 or 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#execution-timeline">Execution Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#acquire"><code class="docutils literal notranslate"><span class="pre">acquire()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#counting-semaphore-3-spots-available">Counting Semaphore (3 spots available)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#binary-semaphore-producer-consumer">Binary Semaphore (Producer-Consumer)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#example-1-swimming-pool-with-limited-capacity">Example 1: Swimming Pool with Limited Capacity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#example-2-producer-consumer-like-the-code">Example 2: Producer-Consumer (Like the Code)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#limiting-concurrent-access">1. <strong>Limiting Concurrent Access</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#producer-consumer-communication">2. <strong>Producer-Consumer Communication</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#synchronizing-multiple-threads">3. <strong>Synchronizing Multiple Threads</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#lock-threading-lock">Lock (<code class="docutils literal notranslate"><span class="pre">threading.Lock</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#semaphore-counting">Semaphore (Counting)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#semaphore-binary-used-as-signal">Semaphore (Binary - Used as Signal)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-does-python-have-a-gil">Why Does Python Have a GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-the-gil-works">How the GIL Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#gil-behavior-with-different-operations">GIL Behavior with Different Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-critical-difference">The Critical Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#real-world-analogy">Real-World Analogy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-with-threading-for-cpu-bound">The Problem with Threading for CPU-bound</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-solution-multiprocessing">The Solution: Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-multiprocessing-bypasses-the-gil">How Multiprocessing Bypasses the GIL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#trade-offs-of-multiprocessing">Trade-offs of Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#when-the-trade-off-is-worth-it">When the Trade-off is Worth It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-wasted-time">The Problem: Wasted Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-solution-threading">The Solution: Threading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-threading-works-for-i-o">Why Threading Works for I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-the-os-helps">How the OS Helps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-not-multiprocessing-for-i-o">Why Not Multiprocessing for I/O?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#threading-trade-offs">Threading Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-with-threading-overhead">The Problem with Threading: Overhead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#asyncio-cooperative-multitasking">Asyncio: Cooperative Multitasking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-asyncio-works">How Asyncio Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#event-loop-visualization">Event Loop Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#asyncio-vs-threading-detailed-comparison">Asyncio vs Threading: Detailed Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#when-asyncio-shines">When Asyncio Shines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#asyncio-trade-offs">Asyncio Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#cpu-bound-with-threading-the-gil-dance">CPU-bound with Threading: The GIL Dance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#cpu-bound-with-multiprocessing-true-parallel">CPU-bound with Multiprocessing: True Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#i-o-bound-with-threading-gil-released">I/O-bound with Threading: GIL Released</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#i-o-bound-with-asyncio-event-loop-magic">I/O-bound with Asyncio: Event Loop Magic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-cpu-bound-task-computing-pi">Benchmark: CPU-bound Task (Computing pi)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-i-o-bound-task-web-requests">Benchmark: I/O-bound Task (Web Requests)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-mixed-workload">Benchmark: Mixed Workload</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#quick-reference-table">Quick Reference Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#code-templates">Code Templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-gil-controls-everything">1. The GIL Controls Everything</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#resource-usage-matters">2. Resource Usage Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#trade-offs-are-real">3. Trade-offs are Real</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#know-your-workload">4. Know Your Workload</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html">Key Differences: CPU vs GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#streaming-multiprocessors-sms">Streaming Multiprocessors (SMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#thread-organization">Thread Organization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#example-visualization">Example Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#l2-cache">L2 Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#registers">Registers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html">Warps and SIMD Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#thread-divergence">Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#what-is-occupancy">What is Occupancy?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#factors-limiting-occupancy">Factors Limiting Occupancy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#example-calculation">Example Calculation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#why-occupancy-matters">Why Occupancy Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#the-occupancy-sweet-spot">The Occupancy Sweet Spot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#grid-and-block-dimensions">Grid and Block Dimensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#choosing-block-size">Choosing Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#within-a-block">Within a Block</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#between-blocks">Between Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-level-reductions">Warp-Level Reductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#overlap-compute-and-memory">Overlap Compute and Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#traditional-approach">Traditional Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#persistent-approach">Persistent Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#key-factors">Key Factors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#profiling-tools">Profiling Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html">Step 1: Profile First</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#step-2-identify-bottleneck">Step 2: Identify Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-kernel-fusion">Strategy 1: Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tiling">Strategy 2: Tiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-vectorized-loads">Strategy 3: Vectorized Loads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-memory-coalescing">Strategy 4: Memory Coalescing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-use-tensor-cores">Strategy 1: Use Tensor Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-increase-arithmetic-intensity">Strategy 2: Increase Arithmetic Intensity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-minimize-thread-divergence">Strategy 3: Minimize Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-optimize-loop-structure">Strategy 4: Optimize Loop Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-reduce-register-usage">Strategy 1: Reduce Register Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tune-shared-memory">Strategy 2: Tune Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-adjust-block-size">Strategy 3: Adjust Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#why-auto-tune">Why Auto-Tune?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#triton-auto-tuning">Triton Auto-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#persistent-kernels">Persistent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#recomputation">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-reduction">Pattern: Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-element-wise">Pattern: Element-wise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-matrix-multiply">Pattern: Matrix Multiply</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-bandwidth">Issue: Low Bandwidth</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-compute-utilization">Issue: Low Compute Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-lower-than-pytorch">Issue: Lower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#before-you-optimize">Before You Optimize</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#memory-optimizations">Memory Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#compute-optimizations">Compute Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#occupancy-optimization">Occupancy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#advanced">Advanced</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/triton-concepts.html">Triton Concepts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#next-steps">Next Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#extensions">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html">Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html">Low Memory Dropout</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html">Layer Norm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html">Fused Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html">Extern Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#summary">Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton Compiler</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="01-overview.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#stage-1-python-ast-parsing">Stage 1: Python AST Parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#stage-2-code-generation-ttir">Stage 2: Code Generation (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#stage-3-triton-gpu-ir-ttgir">Stage 3: Triton GPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#stage-4-llvm-ir">Stage 4: LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#stage-5-ptx-amdgcn">Stage 5: PTX / AMDGCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#stage-6-binary-cubin-hsaco">Stage 6: Binary (CUBIN / HSACO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#block-based-programming-model">Block-based Programming Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#jit-compilation">JIT Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#mlir-infrastructure">MLIR Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#python-components">Python Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#c-components">C++ Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-overview.html#backend-components">Backend Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-jit-decorator.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-jit-decorator.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html">CodeGenerator Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#ast-visitor-pattern">AST Visitor Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#triton-language-primitives">Triton Language Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#type-inference">Type Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#example-ttir-output">Example TTIR Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#compilation-orchestration">Compilation Orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#ttir-ttgir-transformation">TTIR -&gt; TTGIR Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#ttgir-llvm-ir">TTGIR -&gt; LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#llvm-ir-ptx">LLVM IR -&gt; PTX</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#ptx-cubin">PTX -&gt; CUBIN</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#cache-key-components">Cache Key Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#cache-directory-structure">Cache Directory Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-compilation-pipeline.html#cache-lookup">Cache Lookup</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">The NVCC Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="#nvcc-compilation-stages">NVCC Compilation Stages</a></li>
<li class="toctree-l1"><a class="reference internal" href="#ptx-assembly-output">PTX Assembly Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-llvm-path">The LLVM Path</a></li>
<li class="toctree-l1"><a class="reference internal" href="#llvm-ir-stage">LLVM IR Stage</a></li>
<li class="toctree-l1"><a class="reference internal" href="#ptx-generated-by-triton">PTX Generated by Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="#same-tools-same-artifacts">Same Tools, Same Artifacts</a></li>
<li class="toctree-l1"><a class="reference internal" href="#source-language">Source Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="#compiler-stack">Compiler Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="#compilation-time">Compilation Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="#optimization-levels">Optimization Levels</a></li>
<li class="toctree-l1"><a class="reference internal" href="#can-triton-and-cuda-c-work-together">Can Triton and CUDA C++ Work Together?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#advantages-of-llvm-backend">Advantages of LLVM Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-not-use-nvcc">Why Not Use NVCC?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#trade-offs">Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="#file-types">File Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="#example-directory-structures">Example Directory Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="#ptx-inspection">PTX Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="#cubin-inspection">CUBIN Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="#compilation-paths-compared">Compilation Paths Compared</a></li>
<li class="toctree-l1"><a class="reference internal" href="#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html">Traditional Compiler Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#example-matrix-multiplication">Example: Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#mlir-philosophy">MLIR Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#dialects">1. Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#operations">2. Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#regions-and-blocks">3. Regions and Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#types">4. Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#attributes">5. Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#passes">6. Passes</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#why-triton-uses-mlir">Why Triton Uses MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#triton-s-mlir-dialects">Tritonâ€™s MLIR Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#python-source">Python Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#stage-1-triton-ir-ttir">Stage 1: Triton IR (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#stage-2-tritongpu-ir-ttgir">Stage 2: TritonGPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#stage-3-llvm-dialect">Stage 3: LLVM Dialect</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#stage-4-llvm-ir-actual">Stage 4: LLVM IR (Actual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#command-line-tools">Command-Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#tablegen-for-defining-operations">TableGen for Defining Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#debugging-mlir">Debugging MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#official-documentation">Official Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#tutorials">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#triton-specific">Triton-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#key-concepts-recap">Key Concepts Recap</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#why-mlir-matters-for-triton">Why MLIR Matters for Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-mlir-concepts.html#the-big-picture">The Big Picture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../learning-paths.html">Learning Paths</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html">CUDA Out of Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#out-of-shared-memory">Out of Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-results">Wrong Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nan-or-inf-values">NaN or Inf Values</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slower-than-pytorch">Slower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#low-gpu-utilization">Low GPU Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#compilation-errors">Compilation Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slow-compilation">Slow Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nvidia-specific">NVIDIA-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#amd-specific">AMD-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-gpu-selected">Wrong GPU Selected</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#print-debugging">Print Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#profiling">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#assertions">Assertions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#unit-testing">Unit Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#when-stuck">When Stuck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#cuda">CUDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#rocm-amd">ROCm (AMD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#flash-attention">Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#normalization">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#optimization-techniques">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#id1">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#gpu-programming">GPU Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#deep-learning">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-tools">NVIDIA Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-tools">AMD Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#tutorials-and-courses">Tutorials and Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#community">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#blogs-and-articles">Blogs and Articles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#triton-examples">Triton Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#production-usage">Production Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-gpus">NVIDIA GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-gpus">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#subscribe-to">Subscribe To</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#conferences">Conferences</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Fast Concurrent Programming Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">The NVCC Compiler</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/triton-compiler/04-cuda-comparison.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Triton vs CUDA C++: Compilation Comparison</p>
<p>This document explains how Tritonâ€™s compilation pipeline relates to traditional CUDA C++ compilation, showing where they differ and where they converge.</p>
<p>Overview: Two Paths, Same Destination</p>
<p>Both Triton and CUDA C++ ultimately produce the same binary artifacts that run on NVIDIA GPUs:</p>
<ul class="simple">
<li><p><strong>PTX (Parallel Thread Execution)</strong> - GPU assembly language</p></li>
<li><p><strong>CUBIN (CUDA Binary)</strong> - GPU machine code</p></li>
</ul>
<p>However, they take <strong>completely different paths</strong> to get there:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CUDA C++ Path:
.cu file (C++ with CUDA extensions)
       down
[nvcc frontend] - Parse C++ and CUDA syntax
       down
CUDA C++ IR (NVIDIA proprietary)
       down
[cicc compiler] - NVIDIA&#39;s internal compiler
       down
PTX assembly
       down
[ptxas assembler]
       down
CUBIN binary

Triton Path:
Python function (@triton.jit)
       down
Python AST (standard Python parser)
       down
Triton IR (TTIR) - MLIR-based
       down
Triton GPU IR (TTGIR) - MLIR-based
       down
LLVM IR - Standard LLVM
       down
[LLVM NVPTX backend]
       down
PTX assembly
       down
[ptxas assembler] &lt;- Same tool as CUDA!
       down
CUBIN binary
</pre></div>
</div>
<p><strong>Key insight:</strong> Triton and CUDA C++ converge at PTX. From PTX onward, they use the <strong>same NVIDIA tools</strong> (ptxas).</p>
<p>Traditional CUDA C++ Compilation</p>
<section id="the-nvcc-compiler">
<h1>The NVCC Compiler<a class="headerlink" href="#the-nvcc-compiler" title="Link to this heading">ïƒ</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">nvcc</span></code> is NVIDIAâ€™s proprietary compiler for CUDA C++.</p>
<p><strong>Example CUDA C++ kernel:</strong></p>
<div class="highlight-cuda notranslate"><div class="highlight"><pre><span></span><span class="c1">// add.cu</span>
<span class="kr">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">add_kernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">out</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">out</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Compilation command:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nvcc<span class="w"> </span>-arch<span class="o">=</span>sm_80<span class="w"> </span>add.cu<span class="w"> </span>-o<span class="w"> </span>add.o
</pre></div>
</div>
</section>
<section id="nvcc-compilation-stages">
<h1>NVCC Compilation Stages<a class="headerlink" href="#nvcc-compilation-stages" title="Link to this heading">ïƒ</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">nvcc</span></code> is actually a <strong>driver</strong> that orchestrates multiple tools:</p>
<ol class="arabic">
<li><p><strong>Preprocessing</strong> - Handle <code class="docutils literal notranslate"><span class="pre">#include</span></code>, <code class="docutils literal notranslate"><span class="pre">#define</span></code>, etc.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Expand macros and includes</span>
nvcc<span class="w"> </span>-E<span class="w"> </span>add.cu<span class="w"> </span>&gt;<span class="w"> </span>add.i
</pre></div>
</div>
</li>
<li><p><strong>Frontend (cudafe++)</strong> - Parse CUDA C++ syntax</p>
<ul class="simple">
<li><p>Separates device code (<code class="docutils literal notranslate"><span class="pre">__global__</span></code>, <code class="docutils literal notranslate"><span class="pre">__device__</span></code>) from host code</p></li>
<li><p>Generates CUDA C++ IR (proprietary format)</p></li>
</ul>
</li>
<li><p><strong>Device compiler (cicc)</strong> - NVIDIAâ€™s internal compiler</p>
<ul class="simple">
<li><p>Optimizes device code</p></li>
<li><p>Generates PTX assembly</p></li>
</ul>
</li>
<li><p><strong>PTX assembler (ptxas)</strong> - Assembles PTX to CUBIN</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ptxas<span class="w"> </span>-arch<span class="o">=</span>sm_80<span class="w"> </span>kernel.ptx<span class="w"> </span>-o<span class="w"> </span>kernel.cubin
</pre></div>
</div>
</li>
<li><p><strong>Host compiler (g++/cl)</strong> - Compiles CPU code</p>
<ul class="simple">
<li><p>Links with CUDA runtime library (<code class="docutils literal notranslate"><span class="pre">cudart</span></code>)</p></li>
</ul>
</li>
</ol>
<p><strong>Full pipeline visualization:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>add.cu
  down
[nvcc driver]
        down
      host.o
        down
[linker] Combine host + device
        down
    add.exe/add.out
</pre></div>
</div>
</section>
<section id="ptx-assembly-output">
<h1>PTX Assembly Output<a class="headerlink" href="#ptx-assembly-output" title="Link to this heading">ïƒ</a></h1>
<p>For our CUDA C++ kernel, <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> generates PTX like this:</p>
<div class="highlight-ptx notranslate"><div class="highlight"><pre><span></span><span class="kr">.version</span><span class="w"> </span><span class="m">8.0</span>
<span class="kr">.target</span><span class="w"> </span><span class="nv">sm_80</span>
<span class="kr">.address_size</span><span class="w"> </span><span class="m">64</span>

<span class="kr">.visible</span><span class="w"> </span><span class="kr">.entry</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel</span><span class="p">(</span>
<span class="w">    </span><span class="kp">.param</span><span class="w"> </span><span class="kt">.u64</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel_param_0</span><span class="o">,</span><span class="w">  </span><span class="c">// float* x</span>
<span class="w">    </span><span class="kp">.param</span><span class="w"> </span><span class="kt">.u64</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel_param_1</span><span class="o">,</span><span class="w">  </span><span class="c">// float* y</span>
<span class="w">    </span><span class="kp">.param</span><span class="w"> </span><span class="kt">.u64</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel_param_2</span><span class="o">,</span><span class="w">  </span><span class="c">// float* out</span>
<span class="w">    </span><span class="kp">.param</span><span class="w"> </span><span class="kt">.u32</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel_param_3</span><span class="w">   </span><span class="c">// int n</span>
<span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="kr">.reg</span><span class="w"> </span><span class="kt">.pred</span><span class="w"> </span><span class="nv">%p</span><span class="p">&lt;</span><span class="m">2</span><span class="p">&gt;;</span>
<span class="w">    </span><span class="kr">.reg</span><span class="w"> </span><span class="kt">.f32</span><span class="w"> </span><span class="nv">%f</span><span class="p">&lt;</span><span class="m">3</span><span class="p">&gt;;</span>
<span class="w">    </span><span class="kr">.reg</span><span class="w"> </span><span class="kt">.b32</span><span class="w"> </span><span class="nv">%r</span><span class="p">&lt;</span><span class="m">5</span><span class="p">&gt;;</span>
<span class="w">    </span><span class="kr">.reg</span><span class="w"> </span><span class="kt">.b64</span><span class="w"> </span><span class="nv">%rd</span><span class="p">&lt;</span><span class="m">7</span><span class="p">&gt;;</span>

<span class="w">    </span><span class="c">// Load parameters</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.param</span><span class="kt">.u64</span><span class="w"> </span><span class="nv">%rd1</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="k">add</span><span class="nv">_kernel_param_0</span><span class="p">];</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.param</span><span class="kt">.u64</span><span class="w"> </span><span class="nv">%rd2</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="k">add</span><span class="nv">_kernel_param_1</span><span class="p">];</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.param</span><span class="kt">.u64</span><span class="w"> </span><span class="nv">%rd3</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="k">add</span><span class="nv">_kernel_param_2</span><span class="p">];</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.param</span><span class="kt">.u32</span><span class="w"> </span><span class="nv">%r1</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="k">add</span><span class="nv">_kernel_param_3</span><span class="p">];</span>

<span class="w">    </span><span class="c">// Calculate thread index</span>
<span class="w">    </span><span class="k">mov</span><span class="kt">.u32</span><span class="w"> </span><span class="nv">%r2</span><span class="o">,</span><span class="w"> </span><span class="nv">%tid.x</span><span class="p">;</span>
<span class="w">    </span><span class="k">mov</span><span class="kt">.u32</span><span class="w"> </span><span class="nv">%r3</span><span class="o">,</span><span class="w"> </span><span class="nv">%ctaid.x</span><span class="p">;</span>
<span class="w">    </span><span class="k">mov</span><span class="kt">.u32</span><span class="w"> </span><span class="nv">%r4</span><span class="o">,</span><span class="w"> </span><span class="nv">%ntid.x</span><span class="p">;</span>
<span class="w">    </span><span class="k">mad</span><span class="nv">.lo.s32</span><span class="w"> </span><span class="nv">%r2</span><span class="o">,</span><span class="w"> </span><span class="nv">%r3</span><span class="o">,</span><span class="w"> </span><span class="nv">%r4</span><span class="o">,</span><span class="w"> </span><span class="nv">%r2</span><span class="p">;</span><span class="w">  </span><span class="c">// idx = blockIdx.x * blockDim.x + threadIdx.x</span>

<span class="w">    </span><span class="c">// Bounds check</span>
<span class="w">    </span><span class="k">setp</span><span class="nv">.ge.s32</span><span class="w"> </span><span class="nv">%p1</span><span class="o">,</span><span class="w"> </span><span class="nv">%r2</span><span class="o">,</span><span class="w"> </span><span class="nv">%r1</span><span class="p">;</span>
<span class="w">    </span><span class="err">@</span><span class="nv">%p1</span><span class="w"> </span><span class="k">bra</span><span class="w"> </span><span class="nv">EXIT</span><span class="p">;</span>

<span class="w">    </span><span class="c">// Pointer arithmetic and load</span>
<span class="w">    </span><span class="k">mul</span><span class="kp">.wide</span><span class="kt">.s32</span><span class="w"> </span><span class="nv">%rd4</span><span class="o">,</span><span class="w"> </span><span class="nv">%r2</span><span class="o">,</span><span class="w"> </span><span class="m">4</span><span class="p">;</span>
<span class="w">    </span><span class="k">add</span><span class="kt">.s64</span><span class="w"> </span><span class="nv">%rd5</span><span class="o">,</span><span class="w"> </span><span class="nv">%rd1</span><span class="o">,</span><span class="w"> </span><span class="nv">%rd4</span><span class="p">;</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.global</span><span class="kt">.f32</span><span class="w"> </span><span class="nv">%f1</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="nv">%rd5</span><span class="p">];</span><span class="w">  </span><span class="c">// x[idx]</span>

<span class="w">    </span><span class="k">add</span><span class="kt">.s64</span><span class="w"> </span><span class="nv">%rd6</span><span class="o">,</span><span class="w"> </span><span class="nv">%rd2</span><span class="o">,</span><span class="w"> </span><span class="nv">%rd4</span><span class="p">;</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.global</span><span class="kt">.f32</span><span class="w"> </span><span class="nv">%f2</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="nv">%rd6</span><span class="p">];</span><span class="w">  </span><span class="c">// y[idx]</span>

<span class="w">    </span><span class="c">// Addition</span>
<span class="w">    </span><span class="k">add</span><span class="kt">.f32</span><span class="w"> </span><span class="nv">%f3</span><span class="o">,</span><span class="w"> </span><span class="nv">%f1</span><span class="o">,</span><span class="w"> </span><span class="nv">%f2</span><span class="p">;</span>

<span class="w">    </span><span class="c">// Store result</span>
<span class="w">    </span><span class="k">add</span><span class="kt">.s64</span><span class="w"> </span><span class="nv">%rd7</span><span class="o">,</span><span class="w"> </span><span class="nv">%rd3</span><span class="o">,</span><span class="w"> </span><span class="nv">%rd4</span><span class="p">;</span>
<span class="w">    </span><span class="k">st</span><span class="kp">.global</span><span class="kt">.f32</span><span class="w"> </span><span class="p">[</span><span class="nv">%rd7</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">%f3</span><span class="p">;</span><span class="w">  </span><span class="c">// out[idx] = x[idx] + y[idx]</span>

<span class="nl">EXIT:</span>
<span class="w">    </span><span class="k">ret</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Triton Compilation Revisited</p>
</section>
<section id="the-llvm-path">
<h1>The LLVM Path<a class="headerlink" href="#the-llvm-path" title="Link to this heading">ïƒ</a></h1>
<p>Triton uses <strong>open-source LLVM</strong> instead of NVIDIAâ€™s proprietary compiler.</p>
<p><strong>Equivalent Triton kernel:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">triton</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">triton.language</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tl</span>

<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add_kernel</span><span class="p">(</span><span class="n">x_ptr</span><span class="p">,</span> <span class="n">y_ptr</span><span class="p">,</span> <span class="n">out_ptr</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">offs</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">offs</span> <span class="o">&lt;</span> <span class="n">n_elements</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">x_ptr</span> <span class="o">+</span> <span class="n">offs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">y_ptr</span> <span class="o">+</span> <span class="n">offs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">out_ptr</span> <span class="o">+</span> <span class="n">offs</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Compilation stages:</strong></p>
<ol class="arabic simple">
<li><p><strong>Python AST parsing</strong> - Standard Python <code class="docutils literal notranslate"><span class="pre">ast.parse()</span></code></p></li>
<li><p><strong>Code generation</strong> - Python AST -&gt; Triton IR (MLIR)</p></li>
<li><p><strong>GPU lowering</strong> - TTIR -&gt; TTGIR (add layouts, shared memory)</p></li>
<li><p><strong>LLVM lowering</strong> - TTGIR -&gt; LLVM IR</p></li>
<li><p><strong>LLVM backend</strong> - LLVM IR -&gt; PTX (using NVPTX backend)</p></li>
<li><p><strong>PTX assembler</strong> - PTX -&gt; CUBIN (using <strong>same ptxas</strong> as CUDA!)</p></li>
</ol>
</section>
<section id="llvm-ir-stage">
<h1>LLVM IR Stage<a class="headerlink" href="#llvm-ir-stage" title="Link to this heading">ïƒ</a></h1>
<p>Before generating PTX, Triton produces LLVM IR:</p>
<div class="highlight-llvm notranslate"><div class="highlight"><pre><span></span><span class="c">; LLVM IR for Triton add_kernel (simplified)</span>
<span class="k">define</span><span class="w"> </span><span class="k">void</span><span class="w"> </span><span class="vg">@add_kernel</span><span class="p">(</span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="nv">%x_ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="nv">%y_ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="nv">%out_ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="nv">%n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="nl">entry:</span>
<span class="w">  </span><span class="c">; Get thread/block IDs using NVVM intrinsics</span>
<span class="w">  </span><span class="nv">%tid</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">call</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="vg">@llvm.nvvm.read.ptx.sreg.tid.x</span><span class="p">()</span>
<span class="w">  </span><span class="nv">%bid</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">call</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="vg">@llvm.nvvm.read.ptx.sreg.ctaid.x</span><span class="p">()</span>

<span class="w">  </span><span class="c">; Calculate offset</span>
<span class="w">  </span><span class="nv">%block_start</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">mul</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="nv">%bid</span><span class="p">,</span><span class="w"> </span><span class="m">128</span>
<span class="w">  </span><span class="nv">%offset</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">add</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="nv">%block_start</span><span class="p">,</span><span class="w"> </span><span class="nv">%tid</span>

<span class="w">  </span><span class="c">; Bounds check</span>
<span class="w">  </span><span class="nv">%in_bounds</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">icmp</span><span class="w"> </span><span class="k">slt</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="nv">%offset</span><span class="p">,</span><span class="w"> </span><span class="nv">%n</span>
<span class="w">  </span><span class="k">br</span><span class="w"> </span><span class="kt">i1</span><span class="w"> </span><span class="nv">%in_bounds</span><span class="p">,</span><span class="w"> </span><span class="kt">label</span><span class="w"> </span><span class="nv">%load</span><span class="p">,</span><span class="w"> </span><span class="kt">label</span><span class="w"> </span><span class="nv">%exit</span>

<span class="nl">load:</span>
<span class="w">  </span><span class="c">; Pointer arithmetic</span>
<span class="w">  </span><span class="nv">%x_gep</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">getelementptr</span><span class="w"> </span><span class="kt">float</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="nv">%x_ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="nv">%offset</span>
<span class="w">  </span><span class="nv">%y_gep</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">getelementptr</span><span class="w"> </span><span class="kt">float</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="nv">%y_ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="nv">%offset</span>

<span class="w">  </span><span class="c">; Load values</span>
<span class="w">  </span><span class="nv">%x_val</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">load</span><span class="w"> </span><span class="kt">float</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="nv">%x_gep</span><span class="p">,</span><span class="w"> </span><span class="k">align</span><span class="w"> </span><span class="m">4</span>
<span class="w">  </span><span class="nv">%y_val</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">load</span><span class="w"> </span><span class="kt">float</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="nv">%y_gep</span><span class="p">,</span><span class="w"> </span><span class="k">align</span><span class="w"> </span><span class="m">4</span>

<span class="w">  </span><span class="c">; Compute</span>
<span class="w">  </span><span class="nv">%sum</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">fadd</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="nv">%x_val</span><span class="p">,</span><span class="w"> </span><span class="nv">%y_val</span>

<span class="w">  </span><span class="c">; Store</span>
<span class="w">  </span><span class="nv">%out_gep</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">getelementptr</span><span class="w"> </span><span class="kt">float</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="nv">%out_ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="nv">%offset</span>
<span class="w">  </span><span class="k">store</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="nv">%sum</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="nv">%out_gep</span><span class="p">,</span><span class="w"> </span><span class="k">align</span><span class="w"> </span><span class="m">4</span>
<span class="w">  </span><span class="k">br</span><span class="w"> </span><span class="kt">label</span><span class="w"> </span><span class="nv">%exit</span>

<span class="nl">exit:</span>
<span class="w">  </span><span class="k">ret</span><span class="w"> </span><span class="k">void</span>
<span class="p">}</span>

<span class="c">; NVVM intrinsics for GPU built-ins</span>
<span class="k">declare</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="vg">@llvm.nvvm.read.ptx.sreg.tid.x</span><span class="p">()</span>
<span class="k">declare</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="vg">@llvm.nvvm.read.ptx.sreg.ctaid.x</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>LLVM NVPTX Backend</strong> - Converts LLVM IR to PTX</p>
<p>The NVPTX backend is part of open-source LLVM. It knows how to:</p>
<ul class="simple">
<li><p>Translate LLVM instructions to PTX instructions</p></li>
<li><p>Map NVVM intrinsics (<code class="docutils literal notranslate"><span class="pre">llvm.nvvm.*</span></code>) to PTX special registers</p></li>
<li><p>Generate PTX directives (<code class="docutils literal notranslate"><span class="pre">.version</span></code>, <code class="docutils literal notranslate"><span class="pre">.target</span></code>, etc.)</p></li>
</ul>
</section>
<section id="ptx-generated-by-triton">
<h1>PTX Generated by Triton<a class="headerlink" href="#ptx-generated-by-triton" title="Link to this heading">ïƒ</a></h1>
<p>Tritonâ€™s LLVM backend generates PTX that looks <strong>very similar</strong> to CUDA C++:</p>
<div class="highlight-ptx notranslate"><div class="highlight"><pre><span></span><span class="kr">.version</span><span class="w"> </span><span class="m">8.0</span>
<span class="kr">.target</span><span class="w"> </span><span class="nv">sm_80</span>
<span class="kr">.address_size</span><span class="w"> </span><span class="m">64</span>

<span class="kr">.visible</span><span class="w"> </span><span class="kr">.entry</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel</span><span class="p">(</span>
<span class="w">    </span><span class="kp">.param</span><span class="w"> </span><span class="kt">.u64</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel_param_0</span><span class="o">,</span>
<span class="w">    </span><span class="kp">.param</span><span class="w"> </span><span class="kt">.u64</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel_param_1</span><span class="o">,</span>
<span class="w">    </span><span class="kp">.param</span><span class="w"> </span><span class="kt">.u64</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel_param_2</span><span class="o">,</span>
<span class="w">    </span><span class="kp">.param</span><span class="w"> </span><span class="kt">.u32</span><span class="w"> </span><span class="k">add</span><span class="nv">_kernel_param_3</span>
<span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="kr">.reg</span><span class="w"> </span><span class="kt">.pred</span><span class="w"> </span><span class="nv">%p</span><span class="p">&lt;</span><span class="m">2</span><span class="p">&gt;;</span>
<span class="w">    </span><span class="kr">.reg</span><span class="w"> </span><span class="kt">.f32</span><span class="w"> </span><span class="nv">%f</span><span class="p">&lt;</span><span class="m">3</span><span class="p">&gt;;</span>
<span class="w">    </span><span class="kr">.reg</span><span class="w"> </span><span class="kt">.b32</span><span class="w"> </span><span class="nv">%r</span><span class="p">&lt;</span><span class="m">5</span><span class="p">&gt;;</span>
<span class="w">    </span><span class="kr">.reg</span><span class="w"> </span><span class="kt">.b64</span><span class="w"> </span><span class="nv">%rd</span><span class="p">&lt;</span><span class="m">7</span><span class="p">&gt;;</span>

<span class="w">    </span><span class="c">// Nearly identical to CUDA C++ PTX!</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.param</span><span class="kt">.u64</span><span class="w"> </span><span class="nv">%rd1</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="k">add</span><span class="nv">_kernel_param_0</span><span class="p">];</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.param</span><span class="kt">.u64</span><span class="w"> </span><span class="nv">%rd2</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="k">add</span><span class="nv">_kernel_param_1</span><span class="p">];</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.param</span><span class="kt">.u64</span><span class="w"> </span><span class="nv">%rd3</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="k">add</span><span class="nv">_kernel_param_2</span><span class="p">];</span>
<span class="w">    </span><span class="k">ld</span><span class="kp">.param</span><span class="kt">.u32</span><span class="w"> </span><span class="nv">%r1</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="k">add</span><span class="nv">_kernel_param_3</span><span class="p">];</span>

<span class="w">    </span><span class="k">mov</span><span class="kt">.u32</span><span class="w"> </span><span class="nv">%r2</span><span class="o">,</span><span class="w"> </span><span class="nv">%tid.x</span><span class="p">;</span>
<span class="w">    </span><span class="k">mov</span><span class="kt">.u32</span><span class="w"> </span><span class="nv">%r3</span><span class="o">,</span><span class="w"> </span><span class="nv">%ctaid.x</span><span class="p">;</span>
<span class="w">    </span><span class="c">// ... rest is similar ...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Convergence Point: PTX and CUBIN</p>
</section>
<section id="same-tools-same-artifacts">
<h1>Same Tools, Same Artifacts<a class="headerlink" href="#same-tools-same-artifacts" title="Link to this heading">ïƒ</a></h1>
<p><strong>PTX Assembler (ptxas)</strong></p>
<p>Both CUDA C++ and Triton use <strong>the same tool</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># CUDA C++ (called by nvcc)</span>
ptxas<span class="w"> </span>-arch<span class="o">=</span>sm_80<span class="w"> </span>cuda_kernel.ptx<span class="w"> </span>-o<span class="w"> </span>cuda_kernel.cubin

<span class="c1"># Triton (called by Triton compiler)</span>
ptxas<span class="w"> </span>-arch<span class="o">=</span>sm_80<span class="w"> </span>triton_kernel.ptx<span class="w"> </span>-o<span class="w"> </span>triton_kernel.cubin
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">ptxas</span></code> is NVIDIAâ€™s proprietary assembler, distributed with CUDA Toolkit.</p>
<p><strong>CUBIN Binary Format</strong></p>
<p>The output <code class="docutils literal notranslate"><span class="pre">.cubin</span></code> files have the <strong>identical format</strong> regardless of source:</p>
<ul class="simple">
<li><p>ELF binary format</p></li>
<li><p>GPU machine code for specific architecture (e.g., sm_80)</p></li>
<li><p>Metadata (register usage, shared memory, etc.)</p></li>
<li><p>Relocatable or executable sections</p></li>
</ul>
<p><strong>Binary Equivalence:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inspect CUBIN from CUDA C++</span>
cuobjdump<span class="w"> </span>-sass<span class="w"> </span>cuda_kernel.cubin

<span class="c1"># Inspect CUBIN from Triton</span>
cuobjdump<span class="w"> </span>-sass<span class="w"> </span>triton_kernel.cubin
</pre></div>
</div>
<p>Both show the same SASS (low-level GPU assembly).</p>
<p>Key Differences</p>
</section>
<section id="source-language">
<h1>Source Language<a class="headerlink" href="#source-language" title="Link to this heading">ïƒ</a></h1>
<p>Aspect              CUDA C++                  Triton
Language            C++ with extensions       Python DSL
Syntax              <code class="docutils literal notranslate"><span class="pre">__global__</span></code>, etc.      <code class="docutils literal notranslate"><span class="pre">&#64;triton.jit</span></code>
Type System         C++ static types          Python + inference
Memory Mgmt         Manual (<code class="docutils literal notranslate"><span class="pre">__shared__</span></code>)   Automatic
Threading Model     SIMT (per-thread)         Block-level SPMD</p>
</section>
<section id="compiler-stack">
<h1>Compiler Stack<a class="headerlink" href="#compiler-stack" title="Link to this heading">ïƒ</a></h1>
<p>Stage               CUDA C++                  Triton
Frontend            cudafe++ (proprietary)    Python AST (open)
Mid-level IR        CUDA IR (proprietary)     MLIR (open)
Low-level IR        NVIDIA internal           LLVM IR (open)
Backend             cicc (proprietary)        LLVM NVPTX (open)
Assembler           ptxas (NVIDIA)            ptxas (NVIDIA)</p>
<p><strong>Open vs Proprietary:</strong></p>
<ul class="simple">
<li><p>CUDA C++: Most of the stack is <strong>closed-source</strong></p></li>
<li><p>Triton: Everything up to PTX is <strong>open-source</strong> (except ptxas)</p></li>
</ul>
</section>
<section id="compilation-time">
<h1>Compilation Time<a class="headerlink" href="#compilation-time" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CUDA C++ (nvcc):
- First compile: 1-5 seconds (C++ parsing overhead)
- Incremental: Fast with proper build system
- JIT (NVRTC): 100-500ms runtime compilation

Triton:
-------
- First compile: 250-1000ms (MLIR + LLVM overhead)
- Cached: &lt; 1ms (hash-based cache)
- Always JIT: No separate compilation step
</pre></div>
</div>
</section>
<section id="optimization-levels">
<h1>Optimization Levels<a class="headerlink" href="#optimization-levels" title="Link to this heading">ïƒ</a></h1>
<p><strong>CUDA C++ (nvcc):</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nvcc<span class="w"> </span>-O0<span class="w">  </span><span class="c1"># No optimization</span>
nvcc<span class="w"> </span>-O1<span class="w">  </span><span class="c1"># Basic optimization</span>
nvcc<span class="w"> </span>-O2<span class="w">  </span><span class="c1"># Default</span>
nvcc<span class="w"> </span>-O3<span class="w">  </span><span class="c1"># Aggressive (default for device code)</span>
</pre></div>
</div>
<p><strong>Triton:</strong></p>
<p>Triton doesnâ€™t expose optimization levels. Instead:</p>
<ul class="simple">
<li><p>MLIR passes always run (coalescing, layout optimization)</p></li>
<li><p>LLVM optimization level is fixed (usually -O3)</p></li>
<li><p>User controls performance through kernel design (block size, etc.)</p></li>
</ul>
<p>Interoperability</p>
</section>
<section id="can-triton-and-cuda-c-work-together">
<h1>Can Triton and CUDA C++ Work Together?<a class="headerlink" href="#can-triton-and-cuda-c-work-together" title="Link to this heading">ïƒ</a></h1>
<p><strong>Yes!</strong> Because they produce the same artifacts (CUBIN), you can:</p>
<ol class="arabic">
<li><p><strong>Mix kernels in the same application</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch example</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.cpp_extension</span><span class="w"> </span><span class="kn">import</span> <span class="n">load</span>

<span class="c1"># Load CUDA C++ kernel</span>
<span class="n">cuda_kernel</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;cuda_add&quot;</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;add.cu&quot;</span><span class="p">])</span>

<span class="c1"># Use Triton kernel</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">triton</span>

<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">triton_add</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="c1"># Use both in the same program!</span>
<span class="n">cuda_kernel</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="n">triton_add</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Link compiled objects</strong></p>
<p>Triton kernels compile to <code class="docutils literal notranslate"><span class="pre">.cubin</span></code> files that can be loaded by CUDA runtime:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// C++ code loading Triton-compiled kernel</span>
<span class="n">CUmodule</span><span class="w"> </span><span class="k">module</span><span class="p">;</span>
<span class="n">cuModuleLoad</span><span class="p">(</span><span class="o">&amp;</span><span class="k">module</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;triton_kernel.cubin&quot;</span><span class="p">);</span>

<span class="n">CUfunction</span><span class="w"> </span><span class="n">kernel</span><span class="p">;</span>
<span class="n">cuModuleGetFunction</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="k">module</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;add_kernel&quot;</span><span class="p">);</span>

<span class="c1">// Launch Triton kernel from C++!</span>
<span class="n">cuLaunchKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">grid_x</span><span class="p">,</span><span class="w"> </span><span class="n">grid_y</span><span class="p">,</span><span class="w"> </span><span class="n">grid_z</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>
</pre></div>
</div>
</li>
<li><p><strong>Share memory between kernels</strong></p>
<p>Both use CUDA unified memory or device pointers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Allocate with PyTorch</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Use with CUDA C++ kernel</span>
<span class="n">cuda_kernel</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Use with Triton kernel</span>
<span class="n">triton_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>Why Triton Chose This Path</p>
</section>
<section id="advantages-of-llvm-backend">
<h1>Advantages of LLVM Backend<a class="headerlink" href="#advantages-of-llvm-backend" title="Link to this heading">ïƒ</a></h1>
<ol class="arabic simple">
<li><p><strong>Open Source</strong> - Entire stack (except ptxas) is open and modifiable</p></li>
<li><p><strong>Portable</strong> - LLVM supports AMD (AMDGCN), Intel, ARM GPUs</p></li>
<li><p><strong>Modern</strong> - MLIR provides better optimization infrastructure</p></li>
<li><p><strong>Ecosystem</strong> - Reuse LLVM tools (opt, llc, llvm-dis)</p></li>
<li><p><strong>Research-Friendly</strong> - Easy to experiment with new passes</p></li>
</ol>
</section>
<section id="why-not-use-nvcc">
<h1>Why Not Use NVCC?<a class="headerlink" href="#why-not-use-nvcc" title="Link to this heading">ïƒ</a></h1>
<ul class="simple">
<li><p>NVCC is <strong>closed-source</strong> - Canâ€™t modify internals</p></li>
<li><p>NVCC is <strong>NVIDIA-only</strong> - Canâ€™t target AMD, Intel</p></li>
<li><p>NVCC requires <strong>C++ parsing</strong> - Complex language frontend</p></li>
<li><p>NVCC is <strong>hard to extend</strong> - Adding new IR passes is difficult</p></li>
</ul>
</section>
<section id="trade-offs">
<h1>Trade-offs<a class="headerlink" href="#trade-offs" title="Link to this heading">ïƒ</a></h1>
<p><strong>Advantages of Tritonâ€™s approach:</strong></p>
<p>[[OK]] Full control over compilation pipeline
[[OK]] Easy to add new optimizations (MLIR passes)
[[OK]] Multi-vendor GPU support (NVIDIA, AMD)
[[OK]] Python-friendly (no C++ build complexity)
[[OK]] Reproducible builds (open toolchain)</p>
<p><strong>Disadvantages:</strong></p>
<p>[[FAIL]] Dependency on LLVM version
[[FAIL]] Canâ€™t use some NVIDIA-specific optimizations (in cicc)
[[FAIL]] Still depends on proprietary ptxas
[[FAIL]] Compilation overhead from LLVM</p>
<p>Compilation Artifacts Comparison</p>
</section>
<section id="file-types">
<h1>File Types<a class="headerlink" href="#file-types" title="Link to this heading">ïƒ</a></h1>
<p>Artifact            CUDA C++             Triton                 Shared?
Source              <code class="docutils literal notranslate"><span class="pre">.cu</span></code>              <code class="docutils literal notranslate"><span class="pre">.py</span></code>                No
IR (high-level)     CUDA IR              TTIR/TTGIR (MLIR)      No
IR (low-level)      NVIDIA internal      LLVM IR                No
Assembly            <code class="docutils literal notranslate"><span class="pre">.ptx</span></code>             <code class="docutils literal notranslate"><span class="pre">.ptx</span></code>               <strong>Yes</strong>
Binary              <code class="docutils literal notranslate"><span class="pre">.cubin</span></code>           <code class="docutils literal notranslate"><span class="pre">.cubin</span></code>             <strong>Yes</strong>
Fat binary          <code class="docutils literal notranslate"><span class="pre">.fatbin</span></code>          N/A                    No
Object              <code class="docutils literal notranslate"><span class="pre">.o</span></code>               N/A                    No
Executable          <code class="docutils literal notranslate"><span class="pre">.exe/.out</span></code>        N/A (Python runtime)   No</p>
</section>
<section id="example-directory-structures">
<h1>Example Directory Structures<a class="headerlink" href="#example-directory-structures" title="Link to this heading">ïƒ</a></h1>
<p><strong>CUDA C++ project:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cuda_project/
</pre></div>
</div>
<p><strong>Triton project:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>triton_project/
</pre></div>
</div>
<p><strong>Notice:</strong> Triton caches <strong>all intermediate representations</strong>, while CUDA C++ only keeps final artifacts by default.</p>
<p>Inspecting Compilation Artifacts</p>
</section>
<section id="ptx-inspection">
<h1>PTX Inspection<a class="headerlink" href="#ptx-inspection" title="Link to this heading">ïƒ</a></h1>
<p><strong>From CUDA C++:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate PTX</span>
nvcc<span class="w"> </span>-ptx<span class="w"> </span>-arch<span class="o">=</span>sm_80<span class="w"> </span>add.cu<span class="w"> </span>-o<span class="w"> </span>add.ptx

<span class="c1"># View PTX</span>
cat<span class="w"> </span>add.ptx
</pre></div>
</div>
<p><strong>From Triton:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find cached PTX</span>
find<span class="w"> </span>~/.triton/cache<span class="w"> </span>-name<span class="w"> </span><span class="s2">&quot;*.ptx&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>head<span class="w"> </span>-1

<span class="c1"># View PTX</span>
cat<span class="w"> </span>~/.triton/cache/7a3f2e1b.../add_kernel.ptx
</pre></div>
</div>
</section>
<section id="cubin-inspection">
<h1>CUBIN Inspection<a class="headerlink" href="#cubin-inspection" title="Link to this heading">ïƒ</a></h1>
<p><strong>Disassemble CUBIN to SASS:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Works for both CUDA and Triton!</span>
cuobjdump<span class="w"> </span>-sass<span class="w"> </span>kernel.cubin
</pre></div>
</div>
<p><strong>View metadata:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cuobjdump<span class="w"> </span>-elf<span class="w"> </span>kernel.cubin
</pre></div>
</div>
<p><strong>Compare binaries:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate from both sources</span>
nvcc<span class="w"> </span>-cubin<span class="w"> </span>add.cu<span class="w"> </span>-o<span class="w"> </span>cuda.cubin
<span class="c1"># (Triton generates triton.cubin)</span>

<span class="c1"># Compare SASS</span>
diff<span class="w"> </span>&lt;<span class="o">(</span>cuobjdump<span class="w"> </span>-sass<span class="w"> </span>cuda.cubin<span class="o">)</span><span class="w"> </span>&lt;<span class="o">(</span>cuobjdump<span class="w"> </span>-sass<span class="w"> </span>triton.cubin<span class="o">)</span>
</pre></div>
</div>
<p>Often, the SASS is <strong>nearly identical</strong> for simple kernels!</p>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">ïƒ</a></h2>
</section>
</section>
<section id="compilation-paths-compared">
<h1>Compilation Paths Compared<a class="headerlink" href="#compilation-paths-compared" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CUDA C++:  .cu -&gt; [nvcc] -&gt; CUDA IR -&gt; [cicc] -&gt; PTX -&gt; [ptxas] -&gt; CUBIN
                            up                         up           up
                       Proprietary              Proprietary   Shared

Triton:    .py -&gt; [AST] -&gt; TTIR -&gt; TTGIR -&gt; LLVM IR -&gt; [NVPTX] -&gt; PTX -&gt; [ptxas] -&gt; CUBIN
                  up       up       up        up         up        up       up        up
                Open    Open    Open     Open      Open    Shared  Propr.   Shared
</pre></div>
</div>
<p><strong>Convergence:</strong> Both paths produce <strong>identical PTX and CUBIN formats</strong>.</p>
</section>
<section id="key-takeaways">
<h1>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading">ïƒ</a></h1>
<ol class="arabic simple">
<li><p><strong>Same destination, different routes</strong></p>
<ul class="simple">
<li><p>CUDA C++: Proprietary NVIDIA toolchain</p></li>
<li><p>Triton: Open-source LLVM toolchain</p></li>
</ul>
</li>
<li><p><strong>Binary compatibility</strong></p>
<ul class="simple">
<li><p>Both produce standard PTX and CUBIN</p></li>
<li><p>Can mix kernels from both sources</p></li>
<li><p>Use same CUDA runtime APIs</p></li>
</ul>
</li>
<li><p><strong>Trade-offs</strong></p>
<ul class="simple">
<li><p>CUDA C++: Mature, heavily optimized, NVIDIA-only</p></li>
<li><p>Triton: Flexible, portable, easier to extend</p></li>
</ul>
</li>
<li><p><strong>Shared infrastructure</strong></p>
<ul class="simple">
<li><p>Both use <code class="docutils literal notranslate"><span class="pre">ptxas</span></code> for final assembly</p></li>
<li><p>Both run on same CUDA driver</p></li>
<li><p>Both produce GPU binaries with identical format</p></li>
</ul>
</li>
<li><p><strong>Inspection and debugging</strong></p>
<ul class="simple">
<li><p>Same tools work for both (<code class="docutils literal notranslate"><span class="pre">cuobjdump</span></code>, <code class="docutils literal notranslate"><span class="pre">nvprof</span></code>, <code class="docutils literal notranslate"><span class="pre">nsight</span></code>)</p></li>
<li><p>PTX is human-readable assembly</p></li>
<li><p>CUBIN contains actual machine code</p></li>
</ul>
</li>
</ol>
<p>Further Reading</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/">PTX ISA Reference</a> - Official PTX documentation</p></li>
<li><p><a class="reference external" href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/">NVCC Documentation</a> - NVIDIAâ€™s compiler guide</p></li>
<li><p><a class="reference external" href="https://llvm.org/docs/NVPTXUsage.html">LLVM NVPTX Backend</a> - LLVMâ€™s PTX generator</p></li>
<li><p><a class="reference external" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/">CUDA Binary Utilities</a> - cuobjdump and friends</p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="03-compilation-pipeline.html" class="btn btn-neutral float-left" title="CodeGenerator Class" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="05-mlir-concepts.html" class="btn btn-neutral float-right" title="Traditional Compiler Limitations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Concurrent Programs.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>