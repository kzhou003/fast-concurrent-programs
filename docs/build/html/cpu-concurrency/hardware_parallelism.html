

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Physical Cores &mdash; Fast Concurrent Programming Guide 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1aac1d93" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/sidebar-fix.js?v=6c2f6f50"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Key Points About start()" href="threading_basics.html" />
    <link rel="prev" title="Concurrency" href="key_concepts.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Fast Concurrent Programming Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">CPU Concurrency</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#parallelism">Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#visual-comparison">Visual Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#threading-concurrent-futures-threadpoolexecutor">Threading (concurrent.futures.ThreadPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#multiprocessing-concurrent-futures-processpoolexecutor">Multiprocessing (concurrent.futures.ProcessPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#when-to-use-what">When to Use What</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#what-is-the-gil">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#key-points">Key Points:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#impact-on-performance">Impact on Performance:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#observing-the-gil-from-script-06">Observing the GIL (from script 06):</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#event-loop">Event Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#how-it-works-from-script-07">How It Works (from script 07):</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#event-loop-lifecycle">Event Loop Lifecycle:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#modern-vs-old-patterns">Modern vs Old Patterns:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#what-are-coroutines">What are Coroutines?</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#defining-coroutines">Defining Coroutines:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#key-features">Key Features:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#example-from-script-08">Example from Script 08:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#execution-flow">Execution Flow:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#important-rules">Important Rules:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#tasks">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#task-characteristics">Task Characteristics:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#example-from-script-09">Example from Script 09:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#futures">Futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#waiting-for-multiple-tasks">Waiting for Multiple Tasks:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#comparison-table">Comparison Table:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#hybrid-workloads">Hybrid Workloads:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#time-measurement-time-clock-time-perf-counter">1. Time Measurement (<code class="docutils literal notranslate"><span class="pre">time.clock()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#coroutine-syntax-asyncio-coroutine-async-def">2. Coroutine Syntax (<code class="docutils literal notranslate"><span class="pre">&#64;asyncio.coroutine</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#task-creation-asyncio-task-asyncio-create-task">3. Task Creation (<code class="docutils literal notranslate"><span class="pre">asyncio.Task()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">asyncio.create_task()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#event-loop-management">4. Event Loop Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#future-callbacks-callbacks-await">5. Future Callbacks (Callbacks -&gt; <code class="docutils literal notranslate"><span class="pre">await</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#blocking-calls-in-async-code">6. Blocking Calls in Async Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#string-formatting-f-strings">7. String Formatting (<code class="docutils literal notranslate"><span class="pre">%</span></code> -&gt; f-strings)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#migration-checklist">Migration Checklist</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#compatibility">Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#quick-reference-guide">Quick Reference Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#further-reading">Further Reading</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Physical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="#logical-cores">Logical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-concept">The Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-it-works">How It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="#technical-implementation">Technical Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="#hyperthreading-limitations">Hyperthreading Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="#checking-hyperthreading-status">Checking Hyperthreading Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-fundamental-constraint">The Fundamental Constraint</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-more-threads-more-speed">Why More Threads != More Speed</a></li>
<li class="toctree-l1"><a class="reference internal" href="#optimal-worker-count">Optimal Worker Count</a></li>
<li class="toctree-l1"><a class="reference internal" href="#real-world-example">Real-World Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="#cpu-vs-gpu-different-design-philosophies">CPU vs GPU: Different Design Philosophies</a></li>
<li class="toctree-l1"><a class="reference internal" href="#key-differences">Key Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="#simd-and-gpu-architecture">SIMD and GPU Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-gpus-excel-at-compute-intensive-tasks">Why GPUs Excel at Compute-Intensive Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="#what-gpus-are-good-at">What GPUs Are Good At</a></li>
<li class="toctree-l1"><a class="reference internal" href="#silicon-real-estate-comparison">Silicon Real Estate Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="#detailed-benchmark-results">Detailed Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="#decision-matrix">Decision Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="#practical-guidelines">Practical Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="#example-1-image-processing">Example 1: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="#example-2-monte-carlo-simulation">Example 2: Monte Carlo Simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="#example-3-neural-network-training">Example 3: Neural Network Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="#core-principles">Core Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="#quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html">Key Points About start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#example">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#key-points-about-join">Key Points About join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#id1">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#without-join-danger">WITHOUT join() - DANGER!</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#with-join-correct">WITH join() - CORRECT!</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#mistake-1-calling-the-function-directly-instead-of-start">Mistake 1: Calling the function directly instead of start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#mistake-2-forgetting-join">Mistake 2: Forgetting join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#mistake-3-thinking-threads-share-data-automatically">Mistake 3: Thinking threads share data automatically</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_event_loop.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_event_loop.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_coroutine.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_coroutine.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_coroutine.html#use-cases">Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_and_futures.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_and_futures.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_and_futures.html#examples">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_task_manipulation.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_task_manipulation.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrent_futures_pooling.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrent_futures_pooling.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#internal-structure">Internal Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#automatic-locking">Automatic Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#put-item"><code class="docutils literal notranslate"><span class="pre">put(item)</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#problem-with-manual-locks">Problem with Manual Locks</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#how-queue-does-locking">How Queue Does Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#locking-benefits">Locking Benefits</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#execution-flow">Execution Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#thread-safe-data-structure">1. <strong>Thread-Safe Data Structure</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#blocks-correctly">2. <strong>Blocks Correctly</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#no-busy-waiting">3. <strong>No Busy-Waiting</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#task-tracking">4. <strong>Task Tracking</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#safe-for-multiple-producers-consumers">5. <strong>Safe for Multiple Producers/Consumers</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#step-by-step-what-happens-in-put">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">put()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#step-by-step-what-happens-in-get">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">get()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#mistake-1-forgetting-lock">Mistake 1: Forgetting Lock</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#mistake-2-busy-waiting">Mistake 2: Busy-Waiting</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#mistake-3-race-condition">Mistake 3: Race Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_internal_mechanics.html">The Answer</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_internal_mechanics.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html">Without task_done() - Canâ€™t Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#with-task-done-can-track-completion">With task_done() - Can Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#internal-counter-system">Internal Counter System</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#visual-timeline">Visual Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#code-simplified">Code (Simplified)</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#two-operations">Two Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#timeline-all-three-conditions">Timeline: All Three Conditions</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#put">put()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#get">get()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#task-done">task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#join">join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#scenario-1-producer-1-consumer">Scenario: 1 Producer, 1 Consumer</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-1-put-increment-counter">Step 1: put() - Increment Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-2-get-item-removed-counter-unchanged">Step 2: get() - Item Removed, Counter Unchanged</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-3-task-done-decrement-counter">Step 3: task_done() - Decrement Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-4-join-wait-then-return">Step 4: join() - Wait, Then Return</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#diagram-tracking-one-task">Diagram: Tracking One Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#diagram-multiple-tasks">Diagram: Multiple Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#counter">Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#condition-variable">Condition Variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#together">Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#without-all-tasks-done-condition">Without all_tasks*done Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#queue-join-without-task-done">Queue.join() Without task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#why-it-blocks-forever">Why It Blocks Forever</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#queue-join-with-task-done">Queue.join() With task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#why-it-works">Why It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-1-put-item">Step 1: Put Item</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-2-get-and-process">Step 2: Get and Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-3-mark-done">Step 3: Mark Done</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#flow-diagram">Flow Diagram</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#without-task-done-problematic">Without task_done() - PROBLEMATIC</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#with-task-done-correct">With task_done() - CORRECT</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#use-case-1-verify-all-work-complete">Use Case 1: Verify All Work Complete</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#use-case-2-track-progress">Use Case 2: Track Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#use-case-3-batch-processing">Use Case 3: Batch Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#scenario-main-thread-needs-to-know-when-workers-finish">Scenario: Main thread needs to know when workers finish</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#timeline">Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#better-code-pattern">Better Code Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#why-we-need-task-done">Why We Need task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#the-pattern">The Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html">Regular Lock vs RLock</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#why-rlock-is-needed-here">Why RLock is Needed Here</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#use-regular-lock-when">Use Regular Lock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#use-rlock-when">Use RLock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#regular-lock-would-deadlock">Regular Lock - Would Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#rlock-no-deadlock">RLock - No Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#counting-semaphore-counter-1">1. Counting Semaphore (Counter &gt; 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#binary-semaphore-counter-0-or-1">2. Binary Semaphore (Counter = 0 or 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#execution-timeline">Execution Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#acquire"><code class="docutils literal notranslate"><span class="pre">acquire()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#counting-semaphore-3-spots-available">Counting Semaphore (3 spots available)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#binary-semaphore-producer-consumer">Binary Semaphore (Producer-Consumer)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#example-1-swimming-pool-with-limited-capacity">Example 1: Swimming Pool with Limited Capacity</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#example-2-producer-consumer-like-the-code">Example 2: Producer-Consumer (Like the Code)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#limiting-concurrent-access">1. <strong>Limiting Concurrent Access</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#producer-consumer-communication">2. <strong>Producer-Consumer Communication</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#synchronizing-multiple-threads">3. <strong>Synchronizing Multiple Threads</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#lock-threading-lock">Lock (<code class="docutils literal notranslate"><span class="pre">threading.Lock</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#semaphore-counting">Semaphore (Counting)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#semaphore-binary-used-as-signal">Semaphore (Binary - Used as Signal)</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#why-does-python-have-a-gil">Why Does Python Have a GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#how-the-gil-works">How the GIL Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#gil-behavior-with-different-operations">GIL Behavior with Different Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#the-critical-difference">The Critical Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#real-world-analogy">Real-World Analogy</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#the-problem-with-threading-for-cpu-bound">The Problem with Threading for CPU-bound</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#the-solution-multiprocessing">The Solution: Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#how-multiprocessing-bypasses-the-gil">How Multiprocessing Bypasses the GIL</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#trade-offs-of-multiprocessing">Trade-offs of Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#when-the-trade-off-is-worth-it">When the Trade-off is Worth It</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#the-problem-wasted-time">The Problem: Wasted Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#the-solution-threading">The Solution: Threading</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#why-threading-works-for-i-o">Why Threading Works for I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#how-the-os-helps">How the OS Helps</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#why-not-multiprocessing-for-i-o">Why Not Multiprocessing for I/O?</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#threading-trade-offs">Threading Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#the-problem-with-threading-overhead">The Problem with Threading: Overhead</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#asyncio-cooperative-multitasking">Asyncio: Cooperative Multitasking</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#how-asyncio-works">How Asyncio Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#event-loop-visualization">Event Loop Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#asyncio-vs-threading-detailed-comparison">Asyncio vs Threading: Detailed Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#when-asyncio-shines">When Asyncio Shines</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#asyncio-trade-offs">Asyncio Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#cpu-bound-with-threading-the-gil-dance">CPU-bound with Threading: The GIL Dance</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#cpu-bound-with-multiprocessing-true-parallel">CPU-bound with Multiprocessing: True Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#i-o-bound-with-threading-gil-released">I/O-bound with Threading: GIL Released</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#i-o-bound-with-asyncio-event-loop-magic">I/O-bound with Asyncio: Event Loop Magic</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#benchmark-cpu-bound-task-computing-pi">Benchmark: CPU-bound Task (Computing pi)</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#benchmark-i-o-bound-task-web-requests">Benchmark: I/O-bound Task (Web Requests)</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#benchmark-mixed-workload">Benchmark: Mixed Workload</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#quick-reference-table">Quick Reference Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#code-templates">Code Templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#the-gil-controls-everything">1. The GIL Controls Everything</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#resource-usage-matters">2. Resource Usage Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#trade-offs-are-real">3. Trade-offs are Real</a></li>
<li class="toctree-l1"><a class="reference internal" href="patterns_problems_mapping.html#know-your-workload">4. Know Your Workload</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html">Key Differences: CPU vs GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#streaming-multiprocessors-sms">Streaming Multiprocessors (SMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#thread-organization">Thread Organization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#example-visualization">Example Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#l2-cache">L2 Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#registers">Registers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html">Warps and SIMD Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#thread-divergence">Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#what-is-occupancy">What is Occupancy?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#factors-limiting-occupancy">Factors Limiting Occupancy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#example-calculation">Example Calculation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#why-occupancy-matters">Why Occupancy Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#the-occupancy-sweet-spot">The Occupancy Sweet Spot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#grid-and-block-dimensions">Grid and Block Dimensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#choosing-block-size">Choosing Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#within-a-block">Within a Block</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#between-blocks">Between Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-level-reductions">Warp-Level Reductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#overlap-compute-and-memory">Overlap Compute and Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#traditional-approach">Traditional Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#persistent-approach">Persistent Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#key-factors">Key Factors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#profiling-tools">Profiling Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html">Step 1: Profile First</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#step-2-identify-bottleneck">Step 2: Identify Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-kernel-fusion">Strategy 1: Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tiling">Strategy 2: Tiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-vectorized-loads">Strategy 3: Vectorized Loads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-memory-coalescing">Strategy 4: Memory Coalescing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-use-tensor-cores">Strategy 1: Use Tensor Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-increase-arithmetic-intensity">Strategy 2: Increase Arithmetic Intensity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-minimize-thread-divergence">Strategy 3: Minimize Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-optimize-loop-structure">Strategy 4: Optimize Loop Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-reduce-register-usage">Strategy 1: Reduce Register Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tune-shared-memory">Strategy 2: Tune Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-adjust-block-size">Strategy 3: Adjust Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#why-auto-tune">Why Auto-Tune?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#triton-auto-tuning">Triton Auto-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#persistent-kernels">Persistent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#recomputation">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-reduction">Pattern: Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-element-wise">Pattern: Element-wise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-matrix-multiply">Pattern: Matrix Multiply</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-bandwidth">Issue: Low Bandwidth</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-compute-utilization">Issue: Low Compute Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-lower-than-pytorch">Issue: Lower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#before-you-optimize">Before You Optimize</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#memory-optimizations">Memory Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#compute-optimizations">Compute Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#occupancy-optimization">Occupancy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#advanced">Advanced</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/triton-concepts.html">Triton Concepts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#next-steps">Next Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#extensions">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html">Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html">Low Memory Dropout</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html">Layer Norm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html">Fused Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html">Extern Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#summary">Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton Compiler</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-1-python-ast-parsing">Stage 1: Python AST Parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-2-code-generation-ttir">Stage 2: Code Generation (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-3-triton-gpu-ir-ttgir">Stage 3: Triton GPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-4-llvm-ir">Stage 4: LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-5-ptx-amdgcn">Stage 5: PTX / AMDGCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-6-binary-cubin-hsaco">Stage 6: Binary (CUBIN / HSACO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#block-based-programming-model">Block-based Programming Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#jit-compilation">JIT Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#mlir-infrastructure">MLIR Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#python-components">Python Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#c-components">C++ Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#backend-components">Backend Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html">CodeGenerator Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ast-visitor-pattern">AST Visitor Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#triton-language-primitives">Triton Language Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#type-inference">Type Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#example-ttir-output">Example TTIR Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#compilation-orchestration">Compilation Orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ttir-ttgir-transformation">TTIR -&gt; TTGIR Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ttgir-llvm-ir">TTGIR -&gt; LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#llvm-ir-ptx">LLVM IR -&gt; PTX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ptx-cubin">PTX -&gt; CUBIN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-key-components">Cache Key Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-directory-structure">Cache Directory Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-lookup">Cache Lookup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html">The NVCC Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#nvcc-compilation-stages">NVCC Compilation Stages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-assembly-output">PTX Assembly Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#the-llvm-path">The LLVM Path</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#llvm-ir-stage">LLVM IR Stage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-generated-by-triton">PTX Generated by Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#same-tools-same-artifacts">Same Tools, Same Artifacts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#source-language">Source Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compiler-stack">Compiler Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-time">Compilation Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#optimization-levels">Optimization Levels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#can-triton-and-cuda-c-work-together">Can Triton and CUDA C++ Work Together?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#advantages-of-llvm-backend">Advantages of LLVM Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#why-not-use-nvcc">Why Not Use NVCC?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#trade-offs">Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#file-types">File Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#example-directory-structures">Example Directory Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-inspection">PTX Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#cubin-inspection">CUBIN Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-paths-compared">Compilation Paths Compared</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html">Traditional Compiler Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#example-matrix-multiplication">Example: Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#mlir-philosophy">MLIR Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#dialects">1. Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#operations">2. Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#regions-and-blocks">3. Regions and Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#types">4. Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#attributes">5. Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#passes">6. Passes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-triton-uses-mlir">Why Triton Uses MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-s-mlir-dialects">Tritonâ€™s MLIR Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#python-source">Python Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-1-triton-ir-ttir">Stage 1: Triton IR (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-2-tritongpu-ir-ttgir">Stage 2: TritonGPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-3-llvm-dialect">Stage 3: LLVM Dialect</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-4-llvm-ir-actual">Stage 4: LLVM IR (Actual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#command-line-tools">Command-Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tablegen-for-defining-operations">TableGen for Defining Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#debugging-mlir">Debugging MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#official-documentation">Official Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tutorials">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-specific">Triton-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#key-concepts-recap">Key Concepts Recap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-mlir-matters-for-triton">Why MLIR Matters for Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#the-big-picture">The Big Picture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../learning-paths.html">Learning Paths</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html">CUDA Out of Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#out-of-shared-memory">Out of Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-results">Wrong Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nan-or-inf-values">NaN or Inf Values</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slower-than-pytorch">Slower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#low-gpu-utilization">Low GPU Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#compilation-errors">Compilation Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slow-compilation">Slow Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nvidia-specific">NVIDIA-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#amd-specific">AMD-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-gpu-selected">Wrong GPU Selected</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#print-debugging">Print Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#profiling">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#assertions">Assertions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#unit-testing">Unit Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#when-stuck">When Stuck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#cuda">CUDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#rocm-amd">ROCm (AMD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#flash-attention">Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#normalization">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#optimization-techniques">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#id1">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#gpu-programming">GPU Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#deep-learning">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-tools">NVIDIA Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-tools">AMD Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#tutorials-and-courses">Tutorials and Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#community">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#blogs-and-articles">Blogs and Articles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#triton-examples">Triton Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#production-usage">Production Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-gpus">NVIDIA GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-gpus">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#subscribe-to">Subscribe To</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#conferences">Conferences</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Fast Concurrent Programming Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Physical Cores</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/cpu-concurrency/hardware_parallelism.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Hardware Parallelism: Cores, Hyperthreading, and GPUs</p>
<p>A comprehensive guide to understanding physical cores, hyperthreading, and why GPUs excel at compute-intensive parallel tasks.</p>
<p>Table of Contents
1. <a class="reference external" href="#physical-cores-vs-logical-cores">Physical Cores vs Logical Cores</a>
2. <a class="reference external" href="#what-is-hyperthreading">What is Hyperthreading?</a>
3. <a class="reference external" href="#how-threading-is-bounded-by-physical-cores">How Threading is Bounded by Physical Cores</a>
4. <a class="reference external" href="#why-gpus-excel-at-parallel-computing">Why GPUs Excel at Parallel Computing</a>
5. <a class="reference external" href="#cpu-vs-gpu-architecture">CPU vs GPU Architecture</a>
6. <a class="reference external" href="#when-to-use-what">When to Use What</a>
7. <a class="reference external" href="#practical-examples">Practical Examples</a></p>
<p>â€”</p>
<p>Physical Cores vs Logical Cores</p>
<section id="physical-cores">
<h1>Physical Cores<a class="headerlink" href="#physical-cores" title="Link to this heading">ïƒ</a></h1>
<p><strong>Definition</strong>: An actual, independent processing unit on the CPU chip with its own:
- Arithmetic Logic Unit (ALU)
- Floating Point Unit (FPU)
- L1 and L2 cache
- Execution units</p>
<p>Physical CPU Chip:
|  |  Core 0  |  |  Core 1  |  |  Core 2  |   â€¦   |
|  | | ALU  | |  | | ALU  | |  | | ALU  | |         |
|  | | FPU  | |  | | FPU  | |  | | FPU  | |         |
|  | | L1/L2| |  | | L1/L2| |  | | L1/L2| |         |
|              Shared L3 Cache                        |</p>
<p><strong>Characteristics</strong>:
- [[OK]] True parallel execution
- [[OK]] Independent computation streams
- [[OK]] Each can execute different instructions simultaneously
- [[OK]] Maximum performance for CPU-bound tasks</p>
</section>
<section id="logical-cores">
<h1>Logical Cores<a class="headerlink" href="#logical-cores" title="Link to this heading">ïƒ</a></h1>
<p><strong>Definition</strong>: Virtual cores created by technologies like Intelâ€™s Hyperthreading or AMDâ€™s Simultaneous Multithreading (SMT).</p>
<p><strong>Example</strong>:
- CPU: Intel Core i7 with 4 physical cores
- With Hyperthreading: Shows as 8 logical cores
- Ratio: 2 logical cores per 1 physical core</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import os</p>
<p>Check on your system
logical_cores = os.cpu_count()  # e.g., 8
Physical cores require platform-specific code:
On Linux: check /proc/cpuinfo
On macOS: sysctl hw.physicalcpu
Typically: physical_cores = logical_cores / 2 (if HT enabled)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">---</span>
</pre></div>
</div>
<p>What is Hyperthreading?</p>
</section>
<section id="the-concept">
<h1>The Concept<a class="headerlink" href="#the-concept" title="Link to this heading">ïƒ</a></h1>
<p><strong>Hyperthreading (HT)</strong> allows a single physical core to execute two instruction streams (threads) simultaneously by sharing the coreâ€™s execution resources.</p>
</section>
<section id="how-it-works">
<h1>How It Works<a class="headerlink" href="#how-it-works" title="Link to this heading">ïƒ</a></h1>
<p>A CPU core has multiple execution units but theyâ€™re not always all in use:</p>
<p>Without Hyperthreading (one thread per core):
Time -&gt;
Core execution units: [ALU][FPU][Load][Store][Branch]</p>
<blockquote>
<div><p>down    down     down     down      down</p>
</div></blockquote>
<dl>
<dt>Thread A:             [[#]]  [ ]   [[#]]   [ ]    [[#]]   &lt;- Only 60% utilized</dt><dd><blockquote>
<div><p>up         up            up</p>
</div></blockquote>
<p>Used units  (unused)   Used units</p>
</dd>
</dl>
<p>Wasted capacity: 40%</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">::</span>
</pre></div>
</div>
<p>With Hyperthreading (two threads per core):
Time -&gt;
Core execution units: [ALU][FPU][Load][Store][Branch]</p>
<blockquote>
<div><p>down    down     down     down      down</p>
</div></blockquote>
<p>Thread A:             [[#]]  [ ]   [[#]]   [ ]    [[#]]
Thread B:             [ ]  [[#]]   [ ]   [[#]]    [ ]   &lt;- Fills the gaps!</p>
<blockquote>
<div><p>up    up     up     up      up</p>
</div></blockquote>
<p>Combined utilization: [[#]]  [[#]]   [[#]]   [[#]]    [[#]]   &lt;- ~85% utilized</p>
<p>Better resource usage!</p>
</section>
<section id="technical-implementation">
<h1>Technical Implementation<a class="headerlink" href="#technical-implementation" title="Link to this heading">ïƒ</a></h1>
<p>Each physical core with HT has:</p>
<p>Physical Core with Hyperthreading:
|  Duplicated (per thread):              |
|  | Thread 1 |      | Thread 2 |        |
|  | * PC     |      | * PC     |        |  PC = Program Counter
|  | * Regs   |      | * Regs   |        |  Regs = Registers
|  | * State  |      | * State  |        |
|  Shared (between both threads):        |
|  | * ALU (Arithmetic Logic Unit)    |  |
|  | * FPU (Floating Point Unit)      |  |
|  | * L1/L2 Cache                    |  |
|  | * Execution Units                |  |
|  | * Load/Store Units               |  |</p>
<p><strong>Key Insight</strong>: Two threads share the same execution hardware but have separate architectural state (registers, program counter).</p>
</section>
<section id="performance-characteristics">
<h1>Performance Characteristics<a class="headerlink" href="#performance-characteristics" title="Link to this heading">ïƒ</a></h1>
<p><strong>Best Case</strong> (threads use different execution units):</p>
<p>Thread A: Integer operations (uses ALU)
Thread B: Floating-point operations (uses FPU)
Result: ~70-80% better performance than single thread</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Worst</span> <span class="n">Case</span><span class="o">**</span> <span class="p">(</span><span class="n">threads</span> <span class="n">compete</span> <span class="k">for</span> <span class="n">same</span> <span class="n">resources</span><span class="p">):</span>
</pre></div>
</div>
<p>Thread A: Integer operations (needs ALU)
Thread B: Integer operations (also needs ALU)
Result: ~10-20% better performance (mostly from hiding latency)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Reality</span><span class="o">**</span> <span class="p">(</span><span class="n">typical</span> <span class="n">workloads</span><span class="p">):</span>
</pre></div>
</div>
<p>Hyperthreading improvement: 20-40% on average
Still much less than true dual-core: 100% improvement</p>
</section>
<section id="hyperthreading-limitations">
<h1>Hyperthreading Limitations<a class="headerlink" href="#hyperthreading-limitations" title="Link to this heading">ïƒ</a></h1>
<ol class="arabic">
<li><p><strong>Not True Parallelism</strong></p>
<p>1 physical core + HT = 1.3x performance (not 2x)
2 physical cores = 2x performance</p>
</li>
<li><p><strong>Shared Resources Create Contention</strong></p>
<p>Both threads need cache -&gt; cache thrashing
Both threads need FPU -&gt; one waits
Both threads need memory -&gt; bandwidth split</p>
</li>
<li><p><strong>Can Hurt Performance in Some Cases</strong>
.. code-block:: python</p>
<p># CPU-intensive Python code with GIL
# 4 physical cores, 8 logical cores</p>
<p># Using 4 workers (physical cores): 3.8x speedup [[OK]]
# Using 8 workers (logical cores): 3.2x speedup [[FAIL]] (worse!)</p>
<p># Why? OS scheduling overhead + resource contention</p>
</li>
</ol>
</section>
<section id="checking-hyperthreading-status">
<h1>Checking Hyperthreading Status<a class="headerlink" href="#checking-hyperthreading-status" title="Link to this heading">ïƒ</a></h1>
<p><strong>Linux</strong>:
.. code-block:: bash</p>
<p>Check if HT is enabled
lscpu | grep â€œThread(s) per coreâ€
Output: Thread(s) per core: 2  &lt;- HT enabled
Output: Thread(s) per core: 1  &lt;- HT disabled</p>
<p>Or check CPU info
grep -E â€œsiblings|cpu coresâ€ /proc/cpuinfo | head -2
siblings = logical cores per physical CPU
cpu cores = physical cores per physical CPU</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">macOS</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Logical cores
sysctl hw.logicalcpu
Output: hw.logicalcpu: 8</p>
<p>Physical cores
sysctl hw.physicalcpu
Output: hw.physicalcpu: 4</p>
<p>If logical &gt; physical, HT is enabled</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Python</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import os
import subprocess</p>
<p>logical_cores = os.cpu_count()</p>
<p>Platform-specific physical core detection
import platform
if platform.system() == â€˜Darwinâ€™:  # macOS</p>
<blockquote>
<div><dl class="simple">
<dt>result = subprocess.run([â€˜sysctlâ€™, â€˜-nâ€™, â€˜hw.physicalcpuâ€™],</dt><dd><p>capture_output=True, text=True)</p>
</dd>
</dl>
<p>physical_cores = int(result.stdout.strip())</p>
</div></blockquote>
<dl>
<dt>elif platform.system() == â€˜Linuxâ€™:</dt><dd><p># Count unique physical IDs
with open(â€˜/proc/cpuinfoâ€™) as f:</p>
<blockquote>
<div><dl class="simple">
<dt>physical_cores = len(set(</dt><dd><p>line.split(â€˜:â€™)[1].strip()
for line in f
if line.startswith(â€˜physical idâ€™)</p>
</dd>
</dl>
<p>))</p>
</div></blockquote>
</dd>
<dt>else:  # Windows</dt><dd><p>physical_cores = logical_cores // 2  # Approximation</p>
</dd>
</dl>
<p>print(fâ€Logical cores: {logical_cores}â€)
print(fâ€Physical cores: {physical_cores}â€)
print(fâ€Hyperthreading: {â€˜Enabledâ€™ if logical_cores &gt; physical_cores else â€˜Disabledâ€™}â€)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">---</span>
</pre></div>
</div>
<p>How Threading is Bounded by Physical Cores</p>
</section>
<section id="the-fundamental-constraint">
<h1>The Fundamental Constraint<a class="headerlink" href="#the-fundamental-constraint" title="Link to this heading">ïƒ</a></h1>
<p><strong>No matter how many threads you create, true parallel execution is limited by physical cores.</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>System: 4 physical cores (8 logical with HT)

Scenario 1: 4 CPU-intensive threads
|Thread 1 | |Thread 2 | |Thread 3 | |Thread 4 |
|  100%   | |  100%   | |  100%   | |  100%   |
     down           down           down           down
| Core 0  | | Core 1  | | Core 2  | | Core 3  |
|  100%   | |  100%   | |  100%   | |  100%   |
Result: Perfect utilization, 4x speedup [OK]

Scenario 2: 8 CPU-intensive threads
| T1 || T2 || T3 || T4 || T5 || T6 || T7 || T8 |
     down      down      down      down      down      down      down
| Core 0  | | Core 1  | | Core 2  | | Core 3  |
| T1 + T5 | | T2 + T6 | | T3 + T7 | | T4 + T8 |
| compete | | compete | | compete | | compete |
Result: ~4.5x speedup (not 8x!) [warning]

Scenario 3: 16 CPU-intensive threads
|T1||T2||T3||T4||T5||T6||T7||T8||T9||10||11||12||13||14||15||16|
                          down
| Core 0  | | Core 1  | | Core 2  | | Core 3  |
| 4 threads| | 4 threads| | 4 threads| | 4 threads|
|time-slice| |time-slice| |time-slice| |time-slice|
Result: ~4x speedup (same as 4 threads!) + overhead [FAIL]
</pre></div>
</div>
</section>
<section id="why-more-threads-more-speed">
<h1>Why More Threads != More Speed<a class="headerlink" href="#why-more-threads-more-speed" title="Link to this heading">ïƒ</a></h1>
<p><strong>CPU-bound tasks</strong> are limited by actual computation capacity:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Benchmark: Computing sum of squares</p>
<p>import time
from concurrent.futures import ProcessPoolExecutor</p>
<dl>
<dt>def compute(n):</dt><dd><p>return sum(i_i for i in range(n))</p>
</dd>
<dt>def benchmark(num_workers):</dt><dd><p>start = time.perf_counter()
with ProcessPoolExecutor(max_workers=num_workers) as executor:</p>
<blockquote>
<div><p>tasks = [10*000*000] * num_workers
list(executor.map(compute, tasks))</p>
</div></blockquote>
<p>return time.perf_counter() - start</p>
</dd>
</dl>
<p>Results on 4-core CPU:
1 worker:  10.0s  (baseline)
2 workers:  5.1s  (1.96x speedup) [[OK]]
4 workers:  2.6s  (3.85x speedup) [[OK]]
8 workers:  2.8s  (3.57x speedup) [warning] (worse than 4!)
16 workers: 3.2s  (3.13x speedup) [[FAIL]] (much worse!)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Why</span> <span class="n">performance</span> <span class="n">degrades</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ol class="arabic">
<li><p><strong>Context Switching Overhead</strong></p>
<p>OS must constantly switch between threads:
- Save thread state (registers, PC, stack pointer)
- Load next thread state
- Flush CPU caches
- Update memory mappings</p>
<p>Cost: ~1-10 microseconds per switch
With many threads: Spends more time switching than computing!</p>
</li>
<li><p><strong>Cache Thrashing</strong></p>
<p>Each thread loads its data into cache:
Thread A: Loads data -&gt; Evicts Thread Bâ€™s cache
Thread B: Loads data -&gt; Evicts Thread Câ€™s cache
Thread C: Loads data -&gt; Evicts Thread Aâ€™s cache
Thread A: Needs data again -&gt; Cache miss! (must reload)</p>
<p>Result: More memory access, slower execution</p>
</li>
<li><p><strong>Resource Contention</strong></p>
<p>Multiple threads compete for:
- Memory bandwidth
- Cache space
- TLB entries
- Execution units</p>
<p>More threads = More contention = Slower per-thread progress</p>
</li>
</ol>
</section>
<section id="optimal-worker-count">
<h1>Optimal Worker Count<a class="headerlink" href="#optimal-worker-count" title="Link to this heading">ïƒ</a></h1>
<p><strong>For CPU-bound tasks</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import os</p>
<p>Best practice:
physical_cores = os.cpu_count() // 2  # Approximate physical cores
optimal_workers = physical_cores</p>
<p>Conservative (recommended for production):
optimal_workers = max(1, physical_cores - 1)  # Leave one core for OS</p>
<p>Or detect actual physical cores:
import psutil  # pip install psutil
optimal_workers = psutil.cpu_count(logical=False)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Rule</span> <span class="n">of</span> <span class="n">thumb</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<p>CPU-bound tasks:</p>
<p>I/O-bound tasks:</p>
</section>
<section id="real-world-example">
<h1>Real-World Example<a class="headerlink" href="#real-world-example" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Matrix multiplication (CPU-intensive)</p>
<p>import numpy as np
from concurrent.futures import ProcessPoolExecutor
import time</p>
<dl>
<dt>def multiply_matrices(size):</dt><dd><p>A = np.random.rand(size, size)
B = np.random.rand(size, size)
return np.dot(A, B)</p>
</dd>
<dt>def benchmark(num_workers, num_tasks=12):</dt><dd><p>â€œâ€â€
Multiply 12 matrices of size 1000x1000
Each multiplication takes ~1 second on one core
â€œâ€â€
start = time.perf_counter()</p>
<dl>
<dt>with ProcessPoolExecutor(max_workers=num_workers) as executor:</dt><dd><dl class="simple">
<dt>futures = [executor.submit(multiply_matrices, 1000)</dt><dd><p>for * in range(num_tasks)]</p>
</dd>
</dl>
<p>results = [f.result() for f in futures]</p>
</dd>
</dl>
<p>elapsed = time.perf_counter() - start
speedup = (num_tasks * 1.0) / elapsed
efficiency = speedup / num_workers * 100</p>
<p>return elapsed, speedup, efficiency</p>
</dd>
</dl>
<p>Test on 4-core CPU:
print(â€œWorkers | Time  | Speedup | Efficiencyâ€)
print(â€â€”â€”â€“<a href="#id9"><span class="problematic" id="id10">|-------|</span></a>â€”â€”â€”<a href="#id1"><span class="problematic" id="id2">|</span></a>â€”â€”â€”â€”â€œ)
for workers in [1, 2, 4, 6, 8]:</p>
<blockquote>
<div><p>time, speedup, eff = benchmark(workers)
print(fâ€{workers:7} | {time:5.1f}s | {speedup:5.2f}x  | {eff:6.1f}%â€)</p>
</div></blockquote>
<p>Typical output:
Workers | Time  | Speedup | Efficiency</p>
<blockquote>
<div><p>1 | 12.0s |  1.00x  |  100.0%
2 |  6.1s |  1.97x  |   98.5%  &lt;- Near perfect
4 |  3.1s |  3.87x  |   96.8%  &lt;- Near perfect
6 |  2.8s |  4.29x  |   71.5%  &lt;- Diminishing returns
8 |  2.9s |  4.14x  |   51.8%  &lt;- Getting worse</p>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Analysis</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>1-4 workers</strong>: Nearly linear speedup (limited by 4 physical cores)</p></li>
<li><p><strong>6 workers</strong>: Still faster, but efficiency drops (HT helps a bit)</p></li>
<li><p><strong>8 workers</strong>: Performance plateau or degradation (overhead dominates)</p></li>
</ul>
<p>â€”</p>
<p>Why GPUs Excel at Parallel Computing</p>
</section>
<section id="cpu-vs-gpu-different-design-philosophies">
<h1>CPU vs GPU: Different Design Philosophies<a class="headerlink" href="#cpu-vs-gpu-different-design-philosophies" title="Link to this heading">ïƒ</a></h1>
<p>CPU (Latency-Optimized):
Goal: Execute single thread as fast as possible</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|</span>  <span class="n">Few</span> <span class="n">cores</span> <span class="p">(</span><span class="mi">4</span><span class="o">-</span><span class="mi">64</span><span class="p">),</span> <span class="n">each</span> <span class="n">very</span> <span class="n">powerful</span>  <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span> <span class="n">Core</span> <span class="mi">0</span>  <span class="o">|</span>  <span class="o">|</span> <span class="n">Core</span> <span class="mi">1</span>  <span class="o">|</span>             <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">ALU</span><span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">ALU</span><span class="o">|</span>  <span class="o">|</span>  <span class="o">...</span>        <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">FPU</span><span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">FPU</span><span class="o">|</span>  <span class="o">|</span>             <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">Big</span><span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">Big</span><span class="o">|</span>  <span class="o">|</span>             <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">L1</span> <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">L1</span> <span class="o">|</span>  <span class="o">|</span>             <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">L2</span> <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span><span class="n">L2</span> <span class="o">|</span>  <span class="o">|</span>             <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span> <span class="n">Complex</span> <span class="o">|</span>  <span class="o">|</span> <span class="n">Complex</span> <span class="o">|</span>             <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span> <span class="n">Control</span> <span class="o">|</span>  <span class="o">|</span> <span class="n">Control</span> <span class="o">|</span>             <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span><span class="n">Out</span><span class="o">-</span><span class="n">order</span><span class="o">|</span>  <span class="o">|</span><span class="n">Out</span><span class="o">-</span><span class="n">order</span><span class="o">|</span>             <span class="o">|</span>
<span class="o">|</span>  <span class="o">|</span> <span class="n">Exec</span>    <span class="o">|</span>  <span class="o">|</span> <span class="n">Exec</span>    <span class="o">|</span>             <span class="o">|</span>
<span class="o">|</span>      <span class="n">Huge</span> <span class="n">L3</span> <span class="n">Cache</span> <span class="p">(</span><span class="mi">32</span><span class="o">+</span> <span class="n">MB</span><span class="p">)</span>            <span class="o">|</span>
<span class="o">|</span>      <span class="n">Complex</span> <span class="n">Branch</span> <span class="n">Prediction</span>         <span class="o">|</span>
<span class="o">|</span>      <span class="n">Speculative</span> <span class="n">Execution</span>             <span class="o">|</span>
</pre></div>
</div>
<p>GPU (Throughput-Optimized):
Goal: Execute many threads simultaneously</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|</span>  <span class="n">Thousands</span> <span class="n">of</span> <span class="n">tiny</span> <span class="n">cores</span>               <span class="o">|</span>
<span class="o">|</span> <span class="o">|</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">|</span>        <span class="o">|</span>
<span class="o">|</span> <span class="o">|</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">|</span>  <span class="o">...</span>   <span class="o">|</span>
<span class="o">|</span> <span class="o">|</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">||</span><span class="n">C</span><span class="o">|</span>        <span class="o">|</span>
<span class="o">|</span>  <span class="o">...</span> <span class="p">(</span><span class="n">thousands</span> <span class="n">more</span><span class="p">)</span> <span class="o">...</span>              <span class="o">|</span>
<span class="o">|</span>  <span class="n">Tiny</span> <span class="n">L1</span> <span class="n">caches</span>                        <span class="o">|</span>
<span class="o">|</span>  <span class="n">Simple</span> <span class="n">control</span> <span class="n">logic</span>                  <span class="o">|</span>
<span class="o">|</span>  <span class="n">No</span> <span class="n">branch</span> <span class="n">prediction</span>                  <span class="o">|</span>
<span class="o">|</span>  <span class="n">No</span> <span class="n">out</span><span class="o">-</span><span class="n">of</span><span class="o">-</span><span class="n">order</span> <span class="n">execution</span>             <span class="o">|</span>
<span class="o">|</span>  <span class="n">SIMD</span><span class="p">:</span> <span class="n">Same</span> <span class="n">instruction</span><span class="p">,</span> <span class="nb">all</span> <span class="n">cores</span>     <span class="o">|</span>
</pre></div>
</div>
</section>
<section id="key-differences">
<h1>Key Differences<a class="headerlink" href="#key-differences" title="Link to this heading">ïƒ</a></h1>
<div class="line-block">
<div class="line">Aspect | CPU | GPU |</div>
<div class="line"><strong>Core Count</strong> | 4-64 cores | 1,000s-10,000s of cores |</div>
<div class="line"><strong>Core Speed</strong> | 3-5 GHz | 1-2 GHz |</div>
<div class="line"><strong>Core Complexity</strong> | Very complex | Very simple |</div>
<div class="line"><strong>Cache per Core</strong> | 256KB-2MB L2 | 16-128KB L1 |</div>
<div class="line"><strong>Control Logic</strong> | 40% of die | 5% of die |</div>
<div class="line"><strong>Compute Units</strong> | 60% of die | 95% of die |</div>
<div class="line"><strong>Best For</strong> | Complex, branching code | Simple, repetitive operations |</div>
<div class="line"><strong>Parallelism</strong> | 4-64 tasks | 1,000-10,000+ tasks |</div>
</div>
</section>
<section id="simd-and-gpu-architecture">
<h1>SIMD and GPU Architecture<a class="headerlink" href="#simd-and-gpu-architecture" title="Link to this heading">ïƒ</a></h1>
<p><strong>SIMD</strong>: Single Instruction, Multiple Data</p>
<p>CPU executing 4 additions sequentially:
Time -&gt;
Core 1: [A+B] -&gt; [C+D] -&gt; [E+F] -&gt; [G+H]  (4 time units)</p>
<p>GPU executing 4 additions in parallel (SIMD):
Time -&gt;
Core 1: [A+B]
Core 2: [C+D]  } All execute simultaneously
Core 3: [E+F]    (1 time unit)
Core 4: [G+H]</p>
<p>Speedup: 4x for this simple case</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">GPU</span> <span class="n">Organization</span><span class="o">**</span> <span class="p">(</span><span class="n">NVIDIA</span> <span class="n">example</span><span class="p">):</span>
</pre></div>
</div>
<p>GPU (e.g., NVIDIA RTX 4090):
|  Streaming Multiprocessors (SMs): 128 units         |
|  Each SM contains:                                  |
|  <a href="#id3"><span class="problematic" id="id4">|</span></a>- 128 CUDA cores (simple ALUs)                   |
|  <a href="#id5"><span class="problematic" id="id6">|</span></a>- 4 Tensor cores (matrix operations)             |
|  <a href="#id7"><span class="problematic" id="id8">|</span></a>- Shared memory (64-128 KB)                      |
|  +- Warp scheduler                                 |
|  Total: 128 SMs x 128 cores = 16,384 CUDA cores    |
|  All cores execute in lockstep (SIMD):             |
|  One instruction broadcast to 32 cores (1 warp)    |</p>
</section>
<section id="why-gpus-excel-at-compute-intensive-tasks">
<h1>Why GPUs Excel at Compute-Intensive Tasks<a class="headerlink" href="#why-gpus-excel-at-compute-intensive-tasks" title="Link to this heading">ïƒ</a></h1>
<section id="massive-parallelism">
<h2>1. Massive Parallelism<a class="headerlink" href="#massive-parallelism" title="Link to this heading">ïƒ</a></h2>
<p><strong>Problem</strong>: Add 1 million numbers</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>CPU approach (4 cores):
Split into 4 chunks of 250,000 each
Each core processes 250,000 additions sequentially</p>
<p>Core 1: [sum 250,000 numbers]
Core 2: [sum 250,000 numbers]  } Parallel
Core 3: [sum 250,000 numbers]
Core 4: [sum 250,000 numbers]</p>
<p>Time: 250,000 additions / core_speed</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">..</span> <span class="n">code</span><span class="o">-</span><span class="n">block</span><span class="p">::</span> <span class="n">python</span>
</pre></div>
</div>
<p>GPU approach (16,384 cores):
Split into 16,384 chunks of ~61 each
Each core processes ~61 additions</p>
<p>Cores 1-16,384: [sum ~61 numbers each]  } All parallel</p>
<p>Time: 61 additions / core_speed</p>
<p>Speedup: 250,000 / 61 ~ 4,000x faster!</p>
</section>
<section id="perfect-for-data-parallel-problems">
<h2>2. Perfect for Data-Parallel Problems<a class="headerlink" href="#perfect-for-data-parallel-problems" title="Link to this heading">ïƒ</a></h2>
<p><strong>Data-parallel</strong>: Same operation on different data elements</p>
<p>Example: Image processing - apply filter to each pixel</p>
<p>Image: 1920x1080 = 2,073,600 pixels
Operation: Apply blur filter to each pixel</p>
<p>CPU (4 cores):
Time: 518,400 pixels per core</p>
<p>GPU (8,192 cores):
Time: ~253 pixels per core</p>
<p>Speedup: 518,400 / 253 ~ 2,048x faster!</p>
</section>
<section id="high-memory-bandwidth">
<h2>3. High Memory Bandwidth<a class="headerlink" href="#high-memory-bandwidth" title="Link to this heading">ïƒ</a></h2>
<p>CPU Memory Bandwidth:</p>
<p>GPU Memory Bandwidth:</p>
<p>Ratio: GPU has 10-20x more memory bandwidth!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Why</span> <span class="n">it</span> <span class="n">matters</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<p>Compute-intensive task often needs:
1. Read data from memory
2. Perform computation
3. Write result to memory</p>
<p>With 16,384 cores all reading/writing:</p>
</section>
</section>
<section id="what-gpus-are-good-at">
<h1>What GPUs Are Good At<a class="headerlink" href="#what-gpus-are-good-at" title="Link to this heading">ïƒ</a></h1>
<p><strong>Perfect for GPUs</strong> (embarrassingly parallel):</p>
<ol class="arabic">
<li><p><strong>Matrix Operations</strong>
.. code-block:: python</p>
<p># Matrix multiplication: C = A x B
# Each element of C can be computed independently</p>
<p>C[i,j] = sum(A[i,k] * B[k,j] for k in range(n))</p>
<p># For 1000x1000 matrices:
# 1,000,000 independent calculations
# Perfect for 16,384 GPU cores!</p>
</li>
<li><p><strong>Image/Video Processing</strong>
.. code-block:: python</p>
<p># Apply operation to each pixel
for each pixel in image:</p>
<blockquote>
<div><p>output[pixel] = transform(input[pixel])</p>
</div></blockquote>
<p># 4K video frame: 8,294,400 pixels
# All can be processed in parallel on GPU</p>
</li>
<li><p><strong>Deep Learning</strong>
.. code-block:: python</p>
<p># Neural network: mostly matrix multiplications
layer_output = activation(weights &#64; inputs + bias)</p>
<p># Millions of multiply-add operations
# All can run in parallel</p>
</li>
<li><p><strong>Scientific Simulations</strong>
.. code-block:: python</p>
<p># N-body simulation: calculate force on each particle
for particle_i in particles:</p>
<blockquote>
<div><dl class="simple">
<dt>force = sum(calculate_force(particle_i, particle_j)</dt><dd><p>for particle_j in other_particles)</p>
</dd>
</dl>
</div></blockquote>
<p># Each particleâ€™s force is independent
# GPU can calculate all simultaneously</p>
</li>
</ol>
<p><strong>Poor for GPUs</strong> (lots of branching/dependencies):</p>
<ol class="arabic">
<li><p><strong>Complex Control Flow</strong>
.. code-block:: python</p>
<p># Lots of if/else statements
def complex_logic(data):</p>
<blockquote>
<div><dl class="simple">
<dt>if data.type == â€˜Aâ€™:</dt><dd><dl class="simple">
<dt>if data.value &gt; threshold_1:</dt><dd><p>return process_A1(data)</p>
</dd>
<dt>else:</dt><dd><p>return process_A2(data)</p>
</dd>
</dl>
</dd>
<dt>elif data.type == â€˜Bâ€™:</dt><dd><p># â€¦ more branching</p>
</dd>
</dl>
</div></blockquote>
<p># Different threads take different paths
# GPU cores must wait for slowest path (divergence)</p>
</li>
<li><p><strong>Recursive Algorithms</strong>
.. code-block:: python</p>
<dl>
<dt>def quicksort(arr):</dt><dd><dl class="simple">
<dt>if len(arr) &lt;= 1:</dt><dd><p>return arr</p>
</dd>
</dl>
<p>pivot = arr[0]
left = quicksort([x for x in arr[1:] if x &lt; pivot])
right = quicksort([x for x in arr[1:] if x &gt;= pivot])
return left + [pivot] + right</p>
</dd>
</dl>
<p># Highly sequential, data-dependent
# Cannot parallelize effectively on GPU</p>
</li>
<li><p><strong>Database Queries with Joins</strong>
.. code-block:: sql</p>
<p>â€“ Complex joins with unpredictable data access patterns
SELECT * FROM table1
JOIN table2 ON table1.id = table2.foreign_id
WHERE complex_condition(table1.data)</p>
<p>â€“ Irregular memory access
â€“ Branch-heavy logic
â€“ CPU better suited</p>
</li>
</ol>
<p>â€”</p>
<p>CPU vs GPU Architecture</p>
</section>
<section id="silicon-real-estate-comparison">
<h1>Silicon Real Estate Comparison<a class="headerlink" href="#silicon-real-estate-comparison" title="Link to this heading">ïƒ</a></h1>
<p>CPU Die Layout (approximate):
|  ############ Control Logic (40%)           |
|  Fetch, decode, branch predict, OoO, etc.  |
|  ######## Cache (30%)                       |
|  L1, L2, L3 caches                          |
|  ###### Compute Units (20%)                 |
|  ALUs, FPUs, actual computation             |
|  ## Other (10%)                             |
|  Memory controller, I/O, etc.               |</p>
<p>GPU Die Layout (approximate):
|  #####################################      |
|  #####################################      |
|  #####################################      |
|  #####################################      |
|  #####################################      |
|  Compute Units (80-85%)                     |
|  Thousands of simple ALUs                   |
|  #####################################      |
|  #####################################      |
|  #####################################      |
|  #####################################      |
|  ## Cache (5-10%)                           |
|  ## Control (5%)                            |
|  # Other (5%)                               |</p>
</section>
<section id="performance-comparison">
<h1>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading">ïƒ</a></h1>
<p><strong>Example: Matrix Multiplication (2048x2048)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import numpy as np
import time</p>
<p>Generate random matrices
A = np.random.rand(2048, 2048)
B = np.random.rand(2048, 2048)</p>
<p>CPU (NumPy with optimized BLAS)
start = time.time()
C_cpu = np.dot(A, B)
cpu_time = time.time() - start
print(fâ€CPU time: {cpu_time:.3f}sâ€)</p>
<p>GPU (using CuPy - CUDA arrays)
import cupy as cp
A_gpu = cp.array(A)
B_gpu = cp.array(B)</p>
<p>start = time.time()
C_gpu = cp.dot(A_gpu, B_gpu)
cp.cuda.Stream.null.synchronize()  # Wait for GPU
gpu_time = time.time() - start
print(fâ€GPU time: {gpu_time:.3f}sâ€)</p>
<p>print(fâ€Speedup: {cpu_time / gpu_time:.1f}xâ€)</p>
<p>Typical results:
CPU time: 0.850s (Intel i9, 8 cores)
GPU time: 0.012s (NVIDIA RTX 4090)
Speedup: 70.8x</p>
</section>
<section id="detailed-benchmark-results">
<h1>Detailed Benchmark Results<a class="headerlink" href="#detailed-benchmark-results" title="Link to this heading">ïƒ</a></h1>
<div class="line-block">
<div class="line">Task | CPU (8-core i9) | GPU (RTX 4090) | Speedup |</div>
<div class="line">Matrix Multiply (2048^2) | 850 ms | 12 ms | 70x |</div>
<div class="line">Image Convolution (4K) | 1200 ms | 8 ms | 150x |</div>
<div class="line">FFT (16M points) | 2500 ms | 15 ms | 166x |</div>
<div class="line">Neural Net Forward Pass | 5000 ms | 25 ms | 200x |</div>
<div class="line">Ray Tracing (1080p frame) | 45000 ms | 16 ms | 2800x |</div>
</div>
<p>â€”</p>
<p>When to Use What</p>
</section>
<section id="decision-matrix">
<h1>Decision Matrix<a class="headerlink" href="#decision-matrix" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">::</span>
</pre></div>
</div>
<blockquote>
<div><div class="line-block">
<div class="line">DECISION TREE                           |</div>
</div>
</div></blockquote>
<dl class="simple">
<dt>Is the problem parallelizable?</dt><dd><p>Reason: Massive parallelism advantage</p>
</dd>
</dl>
</section>
<section id="practical-guidelines">
<h1>Practical Guidelines<a class="headerlink" href="#practical-guidelines" title="Link to this heading">ïƒ</a></h1>
<section id="use-cpu-when">
<h2>Use CPU When:<a class="headerlink" href="#use-cpu-when" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Complex</span> <span class="n">branching</span> <span class="n">logic</span>
</pre></div>
</div>
<blockquote>
<div><p>if/else trees, switch statements, dynamic dispatch</p>
</div></blockquote>
<dl class="simple">
<dt>[[OK]] Irregular memory access patterns</dt><dd><p>Hash tables, tree structures, linked lists</p>
</dd>
<dt>[[OK]] Small datasets (&lt; 10K elements)</dt><dd><p>GPU transfer overhead &gt; computation time</p>
</dd>
<dt>[[OK]] Frequent host-device communication</dt><dd><p>Need to move data between CPU/GPU often</p>
</dd>
<dt>[[OK]] Sequential dependencies</dt><dd><p>Each step depends on previous result</p>
</dd>
<dt>[[OK]] Debugging and development</dt><dd><p>CPU tools more mature, easier to debug</p>
</dd>
</dl>
</section>
<section id="use-gpu-when">
<h2>Use GPU When:<a class="headerlink" href="#use-gpu-when" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Massive</span> <span class="n">data</span> <span class="n">parallelism</span>
</pre></div>
</div>
<blockquote>
<div><p>Same operation on millions of data points</p>
</div></blockquote>
<dl class="simple">
<dt>[[OK]] Matrix operations</dt><dd><p>Linear algebra, transformations</p>
</dd>
<dt>[[OK]] Regular memory access patterns</dt><dd><p>Arrays, grids, uniform data structures</p>
</dd>
<dt>[[OK]] Minimal branching</dt><dd><p>Straight-line code, vectorizable operations</p>
</dd>
<dt>[[OK]] Large datasets (&gt; 100K elements)</dt><dd><p>Enough work to saturate GPU cores</p>
</dd>
<dt>[[OK]] Can keep data on GPU</dt><dd><p>Minimize CPU&lt;-&gt;GPU transfers</p>
</dd>
</dl>
</section>
<section id="use-hyperthreading-when">
<h2>Use Hyperthreading When:<a class="headerlink" href="#use-hyperthreading-when" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Workload</span> <span class="n">has</span> <span class="n">varied</span> <span class="n">resource</span> <span class="n">usage</span>
</pre></div>
</div>
<blockquote>
<div><p>Some threads use ALU, others use FPU</p>
</div></blockquote>
<dl class="simple">
<dt>[[OK]] Latency hiding</dt><dd><p>Memory-bound code with cache misses</p>
</dd>
<dt>[[OK]] Server workloads</dt><dd><p>Many small tasks with idle time</p>
</dd>
<dt>[[FAIL]] DONâ€™T use for CPU-intensive Python</dt><dd><p>GIL + context switching = slower</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">---</span>
</pre></div>
</div>
<p>Practical Examples</p>
</section>
</section>
<section id="example-1-image-processing">
<h1>Example 1: Image Processing<a class="headerlink" href="#example-1-image-processing" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Task: Apply Gaussian blur to 100 images (4K resolution)</p>
<p>Approach 1: Sequential (CPU, 1 core)
import cv2
for img_path in image_paths:</p>
<blockquote>
<div><p>img = cv2.imread(img_path)
blurred = cv2.GaussianBlur(img, (15, 15), 0)
cv2.imwrite(output_path, blurred)</p>
</div></blockquote>
<p>Time: ~300 seconds</p>
<p>Approach 2: CPU Multiprocessing (4 physical cores)
from concurrent.futures import ProcessPoolExecutor</p>
<dl class="simple">
<dt>def process_image(img_path):</dt><dd><p>img = cv2.imread(img_path)
blurred = cv2.GaussianBlur(img, (15, 15), 0)
cv2.imwrite(output_path, blurred)</p>
</dd>
<dt>with ProcessPoolExecutor(max_workers=4) as executor:</dt><dd><p>executor.map(process_image, image_paths)</p>
</dd>
</dl>
<p>Time: ~80 seconds (3.75x speedup)</p>
<p>Approach 3: GPU (CUDA)
import cupy as cp
import cupyx.scipy.ndimage as ndimage</p>
<dl class="simple">
<dt>def process_image*gpu(img_path):</dt><dd><p>img = cv2.imread(img_path)
img_gpu = cp.array(img)
blurred_gpu = ndimage.gaussian_filter(img_gpu, sigma=3)
blurred = cp.asnumpy(blurred_gpu)
cv2.imwrite(output_path, blurred)</p>
</dd>
<dt>for img_path in image_paths:</dt><dd><p>process_image*gpu(img_path)</p>
</dd>
</dl>
<p>Time: ~4 seconds (75x speedup!)</p>
</section>
<section id="example-2-monte-carlo-simulation">
<h1>Example 2: Monte Carlo Simulation<a class="headerlink" href="#example-2-monte-carlo-simulation" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Task: Estimate pi using Monte Carlo (1 billion points)</p>
<p>import numpy as np
import time</p>
<p>Sequential CPU
def estimate_pi*cpu(n):</p>
<blockquote>
<div><p>inside = 0
for * in range(n):</p>
<blockquote>
<div><p>x, y = np.random.random(), np.random.random()
if x_x + y_y &lt;= 1:</p>
<blockquote>
<div><p>inside += 1</p>
</div></blockquote>
</div></blockquote>
<p>return 4 * inside / n</p>
</div></blockquote>
<p>start = time.time()
pi_estimate = estimate_pi*cpu(1*000*000*000)
print(fâ€CPU time: {time.time() - start:.2f}sâ€)
Output: CPU time: 180.00s</p>
<p>CPU Multiprocessing (4 cores)
from concurrent.futures import ProcessPoolExecutor</p>
<dl>
<dt>def estimate_pi*parallel(n, num_workers=4):</dt><dd><p>chunk_size = n // num_workers
with ProcessPoolExecutor(max_workers=num_workers) as executor:</p>
<blockquote>
<div><p>results = executor.map(estimate_pi*cpu, [chunk_size] * num_workers)</p>
</div></blockquote>
<p>return sum(results) / num_workers</p>
</dd>
</dl>
<p>start = time.time()
pi_estimate = estimate_pi*parallel(1*000*000*000, 4)
print(fâ€CPU parallel time: {time.time() - start:.2f}sâ€)
Output: CPU parallel time: 47.00s (3.8x speedup)</p>
<p>import cupy as cp</p>
<dl class="simple">
<dt>def estimate_pi*gpu(n):</dt><dd><p># Generate all random numbers at once on GPU
points = cp.random.random((n, 2))
# Vectorized distance calculation
inside = cp.sum(cp.sum(points**2, axis=1) &lt;= 1)
return 4 * float(inside) / n</p>
</dd>
</dl>
<p>start = time.time()
pi_estimate = estimate_pi*gpu(1*000*000*000)
cp.cuda.Stream.null.synchronize()
print(fâ€GPU time: {time.time() - start:.2f}sâ€)
Output: GPU time: 1.50s (120x speedup!)</p>
</section>
<section id="example-3-neural-network-training">
<h1>Example 3: Neural Network Training<a class="headerlink" href="#example-3-neural-network-training" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Task: Train a neural network on MNIST dataset</p>
<p>import torch
import torch.nn as nn
import time</p>
<p>Define simple network
model = nn.Sequential(</p>
<blockquote>
<div><p>nn.Flatten(),
nn.Linear(784, 128),
nn.ReLU(),
nn.Linear(128, 10)</p>
</div></blockquote>
<p>)</p>
<p>CPU Training
model_cpu = model.to(â€˜cpuâ€™)
optimizer = torch.optim.Adam(model_cpu.parameters())</p>
<p>start = time.time()
for epoch in range(10):</p>
<blockquote>
<div><dl>
<dt>for batch in train_loader:</dt><dd><p>data, target = batch
data, target = data.to(â€˜cpuâ€™), target.to(â€˜cpuâ€™)</p>
<p>optimizer.zero_grad()
output = model_cpu(data)
loss = nn.functional.cross_entropy(output, target)
loss.backward()
optimizer.step()</p>
</dd>
</dl>
</div></blockquote>
<p>cpu_time = time.time() - start
print(fâ€CPU training time: {cpu_time:.2f}sâ€)
Output: CPU training time: 450.00s</p>
<p>GPU Training
model_gpu = model.to(â€˜cudaâ€™)
optimizer = torch.optim.Adam(model_gpu.parameters())</p>
<p>start = time.time()
for epoch in range(10):</p>
<blockquote>
<div><dl>
<dt>for batch in train_loader:</dt><dd><p>data, target = batch
data, target = data.to(â€˜cudaâ€™), target.to(â€˜cudaâ€™)</p>
<p>optimizer.zero_grad()
output = model_gpu(data)
loss = nn.functional.cross_entropy(output, target)
loss.backward()
optimizer.step()</p>
</dd>
</dl>
</div></blockquote>
<p>torch.cuda.synchronize()
gpu_time = time.time() - start
print(fâ€GPU training time: {gpu_time:.2f}sâ€)
print(fâ€Speedup: {cpu_time / gpu_time:.1f}xâ€)
Output: GPU training time: 15.00s</p>
<blockquote>
<div><p>Speedup: 30.0x</p>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">---</span>
</pre></div>
</div>
</section>
<section id="core-principles">
<h1>Core Principles<a class="headerlink" href="#core-principles" title="Link to this heading">ïƒ</a></h1>
<ol class="arabic">
<li><p><strong>Physical Cores = True Parallelism</strong>
- Only physical cores provide true simultaneous execution
- Thread count beyond physical cores = diminishing returns
- For CPU-bound: optimal workers = physical cores</p></li>
<li><p><strong>Hyperthreading = Resource Sharing</strong>
- 2 threads share 1 physical coreâ€™s execution units
- ~20-40% improvement in best case (not 2x)
- Can hurt performance for CPU-intensive tasks
- Best for varied workloads with different resource needs</p></li>
<li><p><strong>GPUs = Massive Parallelism</strong>
- 1000s of simple cores vs few complex cores
- Optimized for throughput, not latency
- Perfect for data-parallel workloads
- Trade-off: Simple operations, massive scale</p></li>
<li><p><strong>Architecture Determines Use Case</strong></p>
<p>CPU: Complex tasks, few parallel streams
GPU: Simple tasks, massive parallel streams
HT:  Resource sharing within a core</p>
</li>
</ol>
</section>
<section id="quick-reference">
<h1>Quick Reference<a class="headerlink" href="#quick-reference" title="Link to this heading">ïƒ</a></h1>
<div class="line-block">
<div class="line">Workload | Best Solution | Why |</div>
<div class="line">Web server (1000s connections) | CPU + Asyncio | I/O-bound, need async |</div>
<div class="line">Video encoding (1 file) | CPU + multiprocessing | CPU-bound, need all cores |</div>
<div class="line">Deep learning training | GPU | Data-parallel, matrix ops |</div>
<div class="line">Database query | CPU | Complex branching, irregular access |</div>
<div class="line">Image batch processing | GPU | Same operation, many images |</div>
<div class="line">Code compilation | CPU + multiprocessing | Complex, parallelizable |</div>
<div class="line">Scientific simulation | GPU | Numerical computation, data-parallel |</div>
<div class="line">Game physics | GPU | Many objects, same equations |</div>
</div>
</section>
<section id="key-takeaways">
<h1>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading">ïƒ</a></h1>
<ul class="simple">
<li><p><strong>Know your hardware</strong>: Physical cores determine max CPU parallelism</p></li>
<li><p><strong>Match tool to task</strong>: CPU for complex, GPU for simple-but-massive</p></li>
<li><p><strong>Hyperthreading helps</strong> when threads use different resources</p></li>
<li><p><strong>Profile first</strong>: Measure before choosing architecture</p></li>
<li><p><strong>Consider transfer costs</strong>: CPU&lt;-&gt;GPU data movement is expensive</p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="key_concepts.html" class="btn btn-neutral float-left" title="Concurrency" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="threading_basics.html" class="btn btn-neutral float-right" title="Key Points About start()" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Concurrent Programs.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>