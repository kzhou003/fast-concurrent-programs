

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>What is the GIL? &mdash; Triton GPU Programming Guide 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Key Differences: CPU vs GPU" href="../gpu-concepts/gpu-fundamentals.html" />
    <link rel="prev" title="Key Concept" href="semaphore_explained.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Triton GPU Programming Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">CPU Concurrency</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#parallelism">Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#visual-comparison">Visual Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#threading-concurrent-futures-threadpoolexecutor">Threading (concurrent.futures.ThreadPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#multiprocessing-concurrent-futures-processpoolexecutor">Multiprocessing (concurrent.futures.ProcessPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#when-to-use-what">When to Use What</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#what-is-the-gil">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#key-points">Key Points:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#impact-on-performance">Impact on Performance:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#observing-the-gil-from-script-06">Observing the GIL (from script 06):</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#event-loop">Event Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#how-it-works-from-script-07">How It Works (from script 07):</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#event-loop-lifecycle">Event Loop Lifecycle:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#modern-vs-old-patterns">Modern vs Old Patterns:</a><ul>
<li class="toctree-l2"><a class="reference internal" href="key_concepts.html#id1">Coroutines</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#what-are-coroutines">What are Coroutines?</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#defining-coroutines">Defining Coroutines:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#key-features">Key Features:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#example-from-script-08">Example from Script 08:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#execution-flow">Execution Flow:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#important-rules">Important Rules:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#tasks">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#task-characteristics">Task Characteristics:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#example-from-script-09">Example from Script 09:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#futures">Futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#waiting-for-multiple-tasks">Waiting for Multiple Tasks:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#comparison-table">Comparison Table:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#hybrid-workloads">Hybrid Workloads:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#time-measurement-time-clock-time-perf-counter">1. Time Measurement (<code class="docutils literal notranslate"><span class="pre">time.clock()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#coroutine-syntax-asyncio-coroutine-async-def">2. Coroutine Syntax (<code class="docutils literal notranslate"><span class="pre">&#64;asyncio.coroutine</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#task-creation-asyncio-task-asyncio-create-task">3. Task Creation (<code class="docutils literal notranslate"><span class="pre">asyncio.Task()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">asyncio.create_task()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#event-loop-management">4. Event Loop Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#future-callbacks-callbacks-await">5. Future Callbacks (Callbacks -&gt; <code class="docutils literal notranslate"><span class="pre">await</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#blocking-calls-in-async-code">6. Blocking Calls in Async Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#string-formatting-f-strings">7. String Formatting (<code class="docutils literal notranslate"><span class="pre">%</span></code> -&gt; f-strings)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#migration-checklist">Migration Checklist</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#compatibility">Compatibility</a><ul>
<li class="toctree-l2"><a class="reference internal" href="key_concepts.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#quick-reference-guide">Quick Reference Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#further-reading">Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html">Physical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#logical-cores">Logical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#the-concept">The Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#how-it-works">How It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#technical-implementation">Technical Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#hyperthreading-limitations">Hyperthreading Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#checking-hyperthreading-status">Checking Hyperthreading Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#the-fundamental-constraint">The Fundamental Constraint</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#why-more-threads-more-speed">Why More Threads != More Speed</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#optimal-worker-count">Optimal Worker Count</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#real-world-example">Real-World Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#cpu-vs-gpu-different-design-philosophies">CPU vs GPU: Different Design Philosophies</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#key-differences">Key Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#simd-and-gpu-architecture">SIMD and GPU Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#why-gpus-excel-at-compute-intensive-tasks">Why GPUs Excel at Compute-Intensive Tasks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hardware_parallelism.html#massive-parallelism">1. Massive Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="hardware_parallelism.html#perfect-for-data-parallel-problems">2. Perfect for Data-Parallel Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="hardware_parallelism.html#high-memory-bandwidth">3. High Memory Bandwidth</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#what-gpus-are-good-at">What GPUs Are Good At</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#silicon-real-estate-comparison">Silicon Real Estate Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#detailed-benchmark-results">Detailed Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#decision-matrix">Decision Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#practical-guidelines">Practical Guidelines</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hardware_parallelism.html#use-cpu-when">Use CPU When:</a></li>
<li class="toctree-l2"><a class="reference internal" href="hardware_parallelism.html#use-gpu-when">Use GPU When:</a></li>
<li class="toctree-l2"><a class="reference internal" href="hardware_parallelism.html#use-hyperthreading-when">Use Hyperthreading When:</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#example-1-image-processing">Example 1: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#example-2-monte-carlo-simulation">Example 2: Monte Carlo Simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#example-3-neural-network-training">Example 3: Neural Network Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#core-principles">Core Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html">Key Points About start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#example">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#key-points-about-join">Key Points About join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#id1">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#without-join-danger">WITHOUT join() - DANGER!</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#with-join-correct">WITH join() - CORRECT!</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#mistake-1-calling-the-function-directly-instead-of-start">Mistake 1: Calling the function directly instead of start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#mistake-2-forgetting-join">Mistake 2: Forgetting join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#mistake-3-thinking-threads-share-data-automatically">Mistake 3: Thinking threads share data automatically</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_event_loop.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="asyncio_event_loop.html#asyncio-event-loop">Asyncio Event Loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_event_loop.html#async-await-pattern">Async/Await Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_event_loop.html#task-functions">Task Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_event_loop.html#main-function">Main Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_event_loop.html#entry-point">Entry Point</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_event_loop.html#changes-made">Changes Made</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_event_loop.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_coroutine.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="asyncio_coroutine.html#finite-state-machine-fsm">Finite State Machine (FSM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_coroutine.html#coroutines">Coroutines</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_coroutine.html#state-transitions">State Transitions</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_coroutine.html#state-machine-structure">State Machine Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_coroutine.html#start-state">Start State</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_coroutine.html#intermediate-states-state-1-2-3">Intermediate States (State 1, 2, 3)</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_coroutine.html#end-state">End State</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_coroutine.html#changes-made">Changes Made</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_coroutine.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_coroutine.html#use-cases">Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_and_futures.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#command-line-arguments">Command-Line Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#async-task-results">Async Task Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#concurrent-computation">Concurrent Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#first-coroutine-sum-of-n-integers">First Coroutine - Sum of N Integers</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#second-coroutine-factorial">Second Coroutine - Factorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#main-function">Main Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#changes-made">Changes Made</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#old-pattern-deprecated">Old Pattern (Deprecated)</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#new-pattern-modern">New Pattern (Modern)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_and_futures.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_and_futures.html#examples">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#example-1">Example 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_and_futures.html#example-2">Example 2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_task_manipulation.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="asyncio_task_manipulation.html#asyncio-tasks">Asyncio Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_task_manipulation.html#concurrent-execution">Concurrent Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_task_manipulation.html#asyncio-gather">asyncio.gather()</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_task_manipulation.html#factorial-computation">Factorial Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_task_manipulation.html#fibonacci-computation">Fibonacci Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_task_manipulation.html#binomial-coefficient-computation">Binomial Coefficient Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_task_manipulation.html#main-function">Main Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="asyncio_task_manipulation.html#changes-made">Changes Made</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_task_manipulation.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrent_futures_pooling.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="concurrent_futures_pooling.html#executor-pattern">Executor Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrent_futures_pooling.html#execution-models">Execution Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrent_futures_pooling.html#cpu-intensive-task">CPU-Intensive Task</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrent_futures_pooling.html#evaluation-function">Evaluation Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrent_futures_pooling.html#sequential-execution">Sequential Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrent_futures_pooling.html#thread-pool-execution">Thread Pool Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrent_futures_pooling.html#process-pool-execution">Process Pool Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrent_futures_pooling.html#changes-made">Changes Made</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="concurrent_futures_pooling.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#internal-structure">Internal Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#automatic-locking">Automatic Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#put-item"><code class="docutils literal notranslate"><span class="pre">put(item)</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#problem-with-manual-locks">Problem with Manual Locks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="queue_explained.html#problems">Problems:</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_explained.html#benefits">Benefits:</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#how-queue-does-locking">How Queue Does Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#locking-benefits">Locking Benefits</a><ul>
<li class="toctree-l2"><a class="reference internal" href="queue_explained.html#start-all">Start all</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#execution-flow">Execution Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#thread-safe-data-structure">1. <strong>Thread-Safe Data Structure</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#blocks-correctly">2. <strong>Blocks Correctly</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#no-busy-waiting">3. <strong>No Busy-Waiting</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#task-tracking">4. <strong>Task Tracking</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#safe-for-multiple-producers-consumers">5. <strong>Safe for Multiple Producers/Consumers</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#step-by-step-what-happens-in-put">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">put()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#step-by-step-what-happens-in-get">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">get()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#mistake-1-forgetting-lock">Mistake 1: Forgetting Lock</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#mistake-2-busy-waiting">Mistake 2: Busy-Waiting</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#mistake-3-race-condition">Mistake 3: Race Condition</a><ul>
<li class="toctree-l2"><a class="reference internal" href="queue_explained.html#add-tasks">Add tasks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="queue_internal_mechanics.html">The Answer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#the-items-don-t-have-conditions">The Items Don’t Have Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#condition-variables-explained">Condition Variables Explained</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#step-by-step-execution">Step-by-Step Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#timeline-put-with-condition-variable">Timeline: put() with Condition Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#id1">Step-by-Step Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#timeline-get-with-condition-variable">Timeline: get() with Condition Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#scenario-queue-with-maxsize-2-multiple-producers-consumers">Scenario: Queue with maxsize=2, multiple producers/consumers</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#what-people-might-think-wrong">What People Might Think (WRONG):</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#the-actual-truth">The Actual Truth:</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#instead-of-per-item-events">Instead of Per-Item Events</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#not-empty-notify-what-it-does">not_empty.notify() - What It Does</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#diagram-condition-variable-notification">Diagram: Condition Variable Notification</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#does-each-item-get-a-condition">“Does each item get a condition?”</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#the-flow">The Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="queue_internal_mechanics.html#what-queue-put-actually-does">What Queue.put() Actually Does</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="queue_internal_mechanics.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html">Without task_done() - Can’t Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#with-task-done-can-track-completion">With task_done() - Can Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#internal-counter-system">Internal Counter System</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#visual-timeline">Visual Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#code-simplified">Code (Simplified)</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#two-operations">Two Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#timeline-all-three-conditions">Timeline: All Three Conditions</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#put">put()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#get">get()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#task-done">task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#join">join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#scenario-1-producer-1-consumer">Scenario: 1 Producer, 1 Consumer</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-1-put-increment-counter">Step 1: put() - Increment Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-2-get-item-removed-counter-unchanged">Step 2: get() - Item Removed, Counter Unchanged</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-3-task-done-decrement-counter">Step 3: task_done() - Decrement Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-4-join-wait-then-return">Step 4: join() - Wait, Then Return</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#diagram-tracking-one-task">Diagram: Tracking One Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#diagram-multiple-tasks">Diagram: Multiple Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#counter">Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#condition-variable">Condition Variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#together">Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#without-all-tasks-done-condition">Without all_tasks*done Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#queue-join-without-task-done">Queue.join() Without task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#why-it-blocks-forever">Why It Blocks Forever</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#queue-join-with-task-done">Queue.join() With task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#why-it-works">Why It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-1-put-item">Step 1: Put Item</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-2-get-and-process">Step 2: Get and Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-3-mark-done">Step 3: Mark Done</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#flow-diagram">Flow Diagram</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#without-task-done-problematic">Without task_done() - PROBLEMATIC</a><ul>
<li class="toctree-l2"><a class="reference internal" href="task_done_queue_explained.html#add-tasks">Add tasks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#with-task-done-correct">With task_done() - CORRECT</a><ul>
<li class="toctree-l2"><a class="reference internal" href="task_done_queue_explained.html#id17">Add tasks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#use-case-1-verify-all-work-complete">Use Case 1: Verify All Work Complete</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#use-case-2-track-progress">Use Case 2: Track Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#use-case-3-batch-processing">Use Case 3: Batch Processing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="task_done_queue_explained.html#main">Main</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#scenario-main-thread-needs-to-know-when-workers-finish">Scenario: Main thread needs to know when workers finish</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#timeline">Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#better-code-pattern">Better Code Pattern</a><ul>
<li class="toctree-l2"><a class="reference internal" href="task_done_queue_explained.html#in-main">In main</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#why-we-need-task-done">Why We Need task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#the-pattern">The Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html">Regular Lock vs RLock</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#why-rlock-is-needed-here">Why RLock is Needed Here</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#use-regular-lock-when">Use Regular Lock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#use-rlock-when">Use RLock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#regular-lock-would-deadlock">Regular Lock - Would Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#rlock-no-deadlock">RLock - No Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#counting-semaphore-counter-1">1. Counting Semaphore (Counter &gt; 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#binary-semaphore-counter-0-or-1">2. Binary Semaphore (Counter = 0 or 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#execution-timeline">Execution Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#acquire"><code class="docutils literal notranslate"><span class="pre">acquire()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#counting-semaphore-3-spots-available">Counting Semaphore (3 spots available)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#binary-semaphore-producer-consumer">Binary Semaphore (Producer-Consumer)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#example-1-swimming-pool-with-limited-capacity">Example 1: Swimming Pool with Limited Capacity</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#example-2-producer-consumer-like-the-code">Example 2: Producer-Consumer (Like the Code)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#limiting-concurrent-access">1. <strong>Limiting Concurrent Access</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#producer-consumer-communication">2. <strong>Producer-Consumer Communication</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#synchronizing-multiple-threads">3. <strong>Synchronizing Multiple Threads</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#lock-threading-lock">Lock (<code class="docutils literal notranslate"><span class="pre">threading.Lock</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#semaphore-counting">Semaphore (Counting)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#semaphore-binary-used-as-signal">Semaphore (Binary - Used as Signal)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-does-python-have-a-gil">Why Does Python Have a GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-the-gil-works">How the GIL Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="#gil-behavior-with-different-operations">GIL Behavior with Different Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-critical-difference">The Critical Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="#real-world-analogy">Real-World Analogy</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-problem-with-threading-for-cpu-bound">The Problem with Threading for CPU-bound</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#sequential">Sequential</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#the-solution-multiprocessing">The Solution: Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-multiprocessing-bypasses-the-gil">How Multiprocessing Bypasses the GIL</a></li>
<li class="toctree-l1"><a class="reference internal" href="#trade-offs-of-multiprocessing">Trade-offs of Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="#when-the-trade-off-is-worth-it">When the Trade-off is Worth It</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-problem-wasted-time">The Problem: Wasted Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-solution-threading">The Solution: Threading</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-threading-works-for-i-o">Why Threading Works for I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-the-os-helps">How the OS Helps</a></li>
<li class="toctree-l1"><a class="reference internal" href="#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-not-multiprocessing-for-i-o">Why Not Multiprocessing for I/O?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#threading-trade-offs">Threading Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-problem-with-threading-overhead">The Problem with Threading: Overhead</a></li>
<li class="toctree-l1"><a class="reference internal" href="#asyncio-cooperative-multitasking">Asyncio: Cooperative Multitasking</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-asyncio-works">How Asyncio Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="#event-loop-visualization">Event Loop Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="#asyncio-vs-threading-detailed-comparison">Asyncio vs Threading: Detailed Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="#when-asyncio-shines">When Asyncio Shines</a></li>
<li class="toctree-l1"><a class="reference internal" href="#asyncio-trade-offs">Asyncio Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="#cpu-bound-with-threading-the-gil-dance">CPU-bound with Threading: The GIL Dance</a></li>
<li class="toctree-l1"><a class="reference internal" href="#cpu-bound-with-multiprocessing-true-parallel">CPU-bound with Multiprocessing: True Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="#i-o-bound-with-threading-gil-released">I/O-bound with Threading: GIL Released</a></li>
<li class="toctree-l1"><a class="reference internal" href="#i-o-bound-with-asyncio-event-loop-magic">I/O-bound with Asyncio: Event Loop Magic</a></li>
<li class="toctree-l1"><a class="reference internal" href="#benchmark-cpu-bound-task-computing-pi">Benchmark: CPU-bound Task (Computing pi)</a></li>
<li class="toctree-l1"><a class="reference internal" href="#benchmark-i-o-bound-task-web-requests">Benchmark: I/O-bound Task (Web Requests)</a></li>
<li class="toctree-l1"><a class="reference internal" href="#benchmark-mixed-workload">Benchmark: Mixed Workload</a></li>
<li class="toctree-l1"><a class="reference internal" href="#quick-reference-table">Quick Reference Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="#code-templates">Code Templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-gil-controls-everything">1. The GIL Controls Everything</a></li>
<li class="toctree-l1"><a class="reference internal" href="#resource-usage-matters">2. Resource Usage Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="#trade-offs-are-real">3. Trade-offs are Real</a></li>
<li class="toctree-l1"><a class="reference internal" href="#know-your-workload">4. Know Your Workload</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html">Key Differences: CPU vs GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#streaming-multiprocessors-sms">Streaming Multiprocessors (SMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#thread-organization">Thread Organization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#example-visualization">Example Visualization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#the-performance-pyramid">The Performance Pyramid</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#memory-coalescing">Memory Coalescing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#l2-cache">L2 Cache</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#example-matrix-multiplication">Example: Matrix Multiplication</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#registers">Registers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#pattern-1-streaming-bandwidth-bound">Pattern 1: Streaming (Bandwidth-Bound)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#pattern-2-staged-computation-compute-bound">Pattern 2: Staged Computation (Compute-Bound)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#pattern-3-reduction-mixed">Pattern 3: Reduction (Mixed)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#the-golden-rules">The Golden Rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#example-naive-vs-optimized-softmax">Example: Naive vs Optimized Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#key-metrics">Key Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#tools-for-profiling">Tools for Profiling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html">Warps and SIMD Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#thread-divergence">Thread Divergence</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/execution-model.html#occupancy">Occupancy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#what-is-occupancy">What is Occupancy?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#factors-limiting-occupancy">Factors Limiting Occupancy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#example-calculation">Example Calculation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#why-occupancy-matters">Why Occupancy Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#the-occupancy-sweet-spot">The Occupancy Sweet Spot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#grid-and-block-dimensions">Grid and Block Dimensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#choosing-block-size">Choosing Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#within-a-block">Within a Block</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#between-blocks">Between Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-level-reductions">Warp-Level Reductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#overlap-compute-and-memory">Overlap Compute and Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#traditional-approach">Traditional Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#persistent-approach">Persistent Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#key-factors">Key Factors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#profiling-tools">Profiling Tools</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/execution-model.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html">Step 1: Profile First</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#step-2-identify-bottleneck">Step 2: Identify Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-kernel-fusion">Strategy 1: Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tiling">Strategy 2: Tiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-vectorized-loads">Strategy 3: Vectorized Loads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-memory-coalescing">Strategy 4: Memory Coalescing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-use-tensor-cores">Strategy 1: Use Tensor Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-increase-arithmetic-intensity">Strategy 2: Increase Arithmetic Intensity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-minimize-thread-divergence">Strategy 3: Minimize Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-optimize-loop-structure">Strategy 4: Optimize Loop Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-reduce-register-usage">Strategy 1: Reduce Register Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tune-shared-memory">Strategy 2: Tune Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-adjust-block-size">Strategy 3: Adjust Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#why-auto-tune">Why Auto-Tune?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#triton-auto-tuning">Triton Auto-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#persistent-kernels">Persistent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#recomputation">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-reduction">Pattern: Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-element-wise">Pattern: Element-wise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-matrix-multiply">Pattern: Matrix Multiply</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-bandwidth">Issue: Low Bandwidth</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-compute-utilization">Issue: Low Compute Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-lower-than-pytorch">Issue: Lower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#before-you-optimize">Before You Optimize</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#memory-optimizations">Memory Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#compute-optimizations">Compute Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#occupancy-optimization">Occupancy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#advanced">Advanced</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#summary">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/triton-concepts.html">Triton Concepts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#introduction-to-triton">Introduction to Triton</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#vector-addition-example">Vector Addition Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#the-kernel-launch-mechanism">The Kernel Launch Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#the-grid-syntax-sugar-python-s-getitem">The <cite>[grid]</cite> Syntax Sugar: Python’s <cite>__getitem__</cite></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#how-it-works">How It Works</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#implementation-details">Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#the-complete-flow">The Complete Flow</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#grid-calculation-triton-cdiv">Grid Calculation: <cite>triton.cdiv()</cite></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#purpose">Purpose</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#example-with-vector-addition">Example with Vector Addition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#why-ceiling-division">Why Ceiling Division?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#program-indexing-and-data-partitioning">Program Indexing and Data Partitioning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#the-spmd-model">The SPMD Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#program-id-assignment">Program ID Assignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#index-calculation">Index Calculation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#data-access-pattern">Data Access Pattern</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#memory-load-and-store">Memory Load and Store</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#how-tl-program-id-works">How <cite>tl.program_id()</cite> Works</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#implementation-stack">Implementation Stack</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#flow-diagram">Flow Diagram</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#kernel-execution-and-asynchronous-behavior">Kernel Execution and Asynchronous Behavior</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#is-kernel-launch-synchronous-or-asynchronous">Is Kernel Launch Synchronous or Asynchronous?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#manual-synchronization">Manual Synchronization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#implicit-synchronization">Implicit Synchronization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#why-asynchronous-by-default">Why Asynchronous by Default?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#advanced-topics">Advanced Topics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#multi-dimensional-grids">Multi-Dimensional Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#constexpr-parameters">Constexpr Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#kernel-caching">Kernel Caching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#performance-considerations">Performance Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#grid-size-selection">Grid Size Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#masking-overhead">Masking Overhead</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#memory-coalescing">Memory Coalescing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#kernel-compilation-warmup-and-gpu-initialization">Kernel Compilation, Warmup, and GPU Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#is-hip-detect-amd-gpu-backend">is_hip() - Detect AMD GPU Backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#is-cdna-detect-amd-cdna-architecture">is_cdna() - Detect AMD CDNA Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#warmup-pre-compile-kernel-without-execution">warmup() - Pre-compile Kernel Without Execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#init-handles-initialize-gpu-binary-handles">_init_handles() - Initialize GPU Binary Handles</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#complete-workflow-example-fused-softmax">Complete Workflow Example: Fused Softmax</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#data-flow-diagram">Data Flow Diagram</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#summary-table">Summary Table</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#compiler-optimization-hints-tl-assume">Compiler Optimization Hints: <cite>tl.assume()</cite></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#what-is-tl-assume">What is tl.assume()?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#why-assumptions-help-optimization">Why Assumptions Help Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#matrix-multiplication-example">Matrix Multiplication Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#when-to-use-tl-assume">When to Use <cite>tl.assume()</cite></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#dangerous-example-don-t-do-this">Dangerous Example: DON’T DO THIS</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#related-functions-tl-static-assert">Related Functions: <cite>tl.static_assert()</cite></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#performance-impact-in-matrix-multiplication">Performance Impact in Matrix Multiplication</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#how-compiler-uses-assumptions">How Compiler Uses Assumptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#summary-tl-assume-guidelines">Summary: <cite>tl.assume()</cite> Guidelines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#comparison-with-other-optimization-techniques">Comparison with Other Optimization Techniques</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#summary">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-concepts/triton-concepts.html#references">References</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#spmd-single-program-multiple-data">SPMD (Single Program, Multiple Data)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#program-id-and-block-processing">Program ID and Block Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#grid-size">Grid Size</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#memory-hierarchy">Memory Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#memory-coalescing">Memory Coalescing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#masking-for-boundary-conditions">Masking for Boundary Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#step-by-step-execution">Step-by-Step Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#bandwidth-bound-operation">Bandwidth-Bound Operation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#arithmetic-intensity">Arithmetic Intensity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#theoretical-performance">Theoretical Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#triton-jit-decorator"><code class="docutils literal notranslate"><span class="pre">&#64;triton.jit</span></code> Decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#constexpr-for-compile-time-constants"><code class="docutils literal notranslate"><span class="pre">constexpr</span></code> for Compile-Time Constants</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#launch-grid-syntax">Launch Grid Syntax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#expected-results">Expected Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#pointer-arithmetic">1. Pointer Arithmetic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#vectorized-operations">2. Vectorized Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#masked-memory-operations">3. Masked Memory Operations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#next-steps">Next Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#standard-softmax-formula">Standard Softmax Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#naive-pytorch-implementation">Naive PyTorch Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#dram-global-memory">DRAM (Global Memory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#sram-shared-memory-l1-cache">SRAM (Shared Memory / L1 Cache)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#the-key-insight">The Key Insight</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#block-level-processing">Block-Level Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#step-1-load-row-into-sram">Step 1: Load Row Into SRAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#step-2-compute-max-reduction">Step 2: Compute Max (Reduction)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#step-3-exponentiation">Step 3: Exponentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#step-4-compute-sum-another-reduction">Step 4: Compute Sum (Another Reduction)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#step-5-normalize-and-write-back">Step 5: Normalize and Write Back</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#why-must-block-size-be-power-of-2">Why Must BLOCK_SIZE Be Power of 2?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#number-of-warps">Number of Warps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#number-of-pipeline-stages">Number of Pipeline Stages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#computing-occupancy">Computing Occupancy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#the-pattern">The Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#why-subtract-max">Why Subtract Max?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#padding-with-inf">Padding with -inf</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#theoretical-speedup">Theoretical Speedup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#actual-performance">Actual Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#reduction-operations">Reduction Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#block-vs-thread">Block vs Thread</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#row-too-large-for-sram">1. Row Too Large for SRAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#numerical-precision">2. Numerical Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#masking-errors">3. Masking Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#jit-fusion-torch-jit-script">JIT Fusion (torch.jit.script)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#manual-cuda">Manual CUDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#cudnn-cublas">CuDNN/CuBLAS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#extensions">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html">Matrix Multiplication</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#basic-algorithm">Basic Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#why-it-s-hard-to-optimize">Why It’s Hard to Optimize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#blocked-algorithm">Blocked Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#why-tiling-works">Why Tiling Works</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#understanding-strides">Understanding Strides</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#block-pointers">Block Pointers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#advancing-pointers">Advancing Pointers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#the-problem-with-row-major-ordering">The Problem with Row-Major Ordering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#grouped-ordering-swizzling">Grouped Ordering (Swizzling)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#the-configuration-space">The Configuration Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#triton-s-auto-tuner">Triton’s Auto-Tuner</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#id3">)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#good-configurations">Good Configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#step-1-compute-program-ids">Step 1: Compute Program IDs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#step-2-initialize-pointers">Step 2: Initialize Pointers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#step-3-accumulation-loop">Step 3: Accumulation Loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#step-4-apply-activation-optional">Step 4: Apply Activation (Optional)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#step-5-store-result">Step 5: Store Result</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#what-are-tensor-cores">What Are Tensor Cores?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#how-tensor-cores-work">How Tensor Cores Work</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#triton-and-tensor-cores">Triton and Tensor Cores</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#arithmetic-intensity">Arithmetic Intensity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#roofline-model">Roofline Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#expected-performance">Expected Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#software-pipelining">Software Pipelining</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#loop-unrolling">Loop Unrolling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#register-pressure">Register Pressure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#non-contiguous-tensors">1. Non-Contiguous Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#wrong-stride-calculation">2. Wrong Stride Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#boundary-conditions">3. Boundary Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#numerical-precision">4. Numerical Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#warm-up">1. Warm-up</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#measure-tflops">2. Measure TFLOPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#compare-against-cublas-rocblas">3. Compare Against cuBLAS/rocBLAS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html#test-different-sizes">4. Test Different Sizes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html">Low Memory Dropout</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#purpose">Purpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#mathematical-definition">Mathematical Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#id1">}</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#scaling-factor">Scaling Factor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#standard-pytorch-dropout">Standard PyTorch Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#the-backward-pass-problem">The Backward Pass Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#additional-complexity">Additional Complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#key-insight">Key Insight</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#the-triton-implementation">The Triton Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#the-challenge">The Challenge</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#the-solution-counter-based-prng">The Solution: Counter-Based PRNG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#the-philox-algorithm">The Philox Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#using-philox-in-triton">Using Philox in Triton</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#memory-comparison">Memory Comparison</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#computational-cost">Computational Cost</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#when-to-use-seeded-dropout">When to Use Seeded Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#ensuring-same-random-numbers">Ensuring Same Random Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#seed-management">Seed Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#testing-reproducibility">Testing Reproducibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#the-tl-where-function">The <code class="docutils literal notranslate"><span class="pre">tl.where</span></code> Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#masking-for-boundary-conditions">Masking for Boundary Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#random-number-distribution">Random Number Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#different-random-distributions">Different Random Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#quality-of-randomness">Quality of Randomness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#thread-safety-and-race-conditions">Thread Safety and Race Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#expected-performance">Expected Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#bottleneck-analysis">Bottleneck Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#integration-with-pytorch">Integration with PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html#seed-generation-strategies">Seed Generation Strategies</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html">Layer Norm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#the-formula">The Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#step-by-step-math">Step-by-Step Math</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#why-layer-normalization">Why Layer Normalization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#batch-norm-vs-layer-norm">Batch Norm vs Layer Norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#kernel-structure">Kernel Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#computing-the-mean">Computing the Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#computing-the-variance">Computing the Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#normalization-and-transformation">Normalization and Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#gradient-mathematics">Gradient Mathematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#gradient-for-biases">Gradient for Biases</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#gradient-for-weights">Gradient for Weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#gradient-for-input-complex">Gradient for Input (Complex!)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#the-challenge">The Challenge</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#two-stage-reduction">Two-Stage Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#group-assignment">Group Assignment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#using-locks">Using Locks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#in-kernel">In kernel:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#stage-2-final-reduction">Stage 2: Final Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#memory-layout">Memory Layout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#stride-usage">Stride Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#why-use-float32-for-accumulation">Why Use float32 for Accumulation?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#computational-complexity">Computational Complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#memory-bandwidth">Memory Bandwidth</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#optimization-opportunities">Optimization Opportunities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#numerical-stability">1. Numerical Stability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#dimension-confusion">2. Dimension Confusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#gradient-accumulation-race-conditions">3. Gradient Accumulation Race Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#forgetting-to-sum-weight-gradients">4. Forgetting to Sum Weight Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#rmsnorm-simpler-variant">RMSNorm (Simpler Variant)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#groupnorm">GroupNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#fp8-and-mixed-precision">FP8 and Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#pytorch-implementation">PyTorch Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#triton-advantages">Triton Advantages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html">Fused Attention</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#standard-attention-formula">Standard Attention Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#step-by-step-computation">Step-by-Step Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#the-memory-problem">The Memory Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#insight-1-we-don-t-need-to-store-s-and-p">Insight 1: We Don’t Need to Store S and P</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#insight-2-online-softmax">Insight 2: Online Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#insight-3-tiling">Insight 3: Tiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#standard-softmax">Standard Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#block-wise-computation">Block-wise Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#updating-the-output">Updating the Output</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#the-inner-loop">The Inner Loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#causal-masking">Causal Masking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#stages-for-causal-attention">Stages for Causal Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#tensor-descriptors">Tensor Descriptors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#reduced-shared-memory">Reduced Shared Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#fp8-support">FP8 Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#preprocess-step">Preprocess Step</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#computing-dk-and-dv">Computing dK and dV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#computing-dq">Computing dQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#why-recomputation">Why Recomputation?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#memory-complexity">Memory Complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#compute-complexity">Compute Complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#arithmetic-intensity">Arithmetic Intensity</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#id9">]</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#sram-overflow">1. SRAM Overflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#numerical-instability">2. Numerical Instability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#causal-mask-errors">3. Causal Mask Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#forgetting-to-update-statistics">4. Forgetting to Update Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#multi-query-attention-mqa">Multi-Query Attention (MQA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#grouped-query-attention-gqa">Grouped Query Attention (GQA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#sliding-window-attention">Sliding Window Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html#flash-attention-3">Flash Attention 3</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html">Extern Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#nvidia-s-libdevice">NVIDIA’s libdevice</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#amd-s-device-libraries">AMD’s Device Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#what-functions-are-available">What Functions Are Available?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#triton-built-in-math">Triton Built-in Math</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#when-you-need-more">When You Need More</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#the-simple-way-default-path">The Simple Way (Default Path)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#the-custom-path-way">The Custom Path Way</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#compilation-process">Compilation Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#type-dispatch">Type Dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#calling-convention">Calling Convention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#triton-s-libdevice-wrapper">Triton’s libdevice Wrapper</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#hyperbolic">Hyperbolic</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#the-math">The Math</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#why-use-libdevice">Why Use libdevice?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#full-example">Full Example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#test">Test</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#libdevice-performance">Libdevice Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#when-to-use-vs-triton-intrinsics">When to Use vs Triton Intrinsics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#common-errors">Common Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#verifying-libraries">Verifying Libraries</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#nvidia">NVIDIA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#amd">AMD</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#calling-custom-external-functions">Calling Custom External Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#mixing-multiple-external-libraries">Mixing Multiple External Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#nvidia-vs-amd">NVIDIA vs AMD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#function-name-differences">Function Name Differences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#in-cuda">In CUDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html#in-triton">In Triton</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#static-on-device-scheduling">Static On-Device Scheduling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#group-problem-representation">Group Problem Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#grouped-gemm-kernel-basic-version">1. Grouped GEMM Kernel (Basic Version)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#tma-tensor-memory-accelerator-version">2. TMA (Tensor Memory Accelerator) Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#host-function-setup">3. Host Function Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#id1">)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#pointer-indirection">Pointer Indirection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#leading-dimension-stride">Leading Dimension (Stride)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#when-grouped-gemm-wins">When Grouped GEMM Wins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#when-to-use-separate-kernels">When to Use Separate Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#benchmarking-results">Benchmarking Results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#id2">)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#why-fixed-number-of-ctas">Why Fixed Number of CTAs?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#work-distribution">Work Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#forgetting-contiguity">1. Forgetting Contiguity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#incorrect-leading-dimensions">2. Incorrect Leading Dimensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#mixed-precision-issues">3. Mixed Precision Issues</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#persistent-kernel-pattern">Persistent Kernel Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#naive-matmul-baseline">1. Naive Matmul (Baseline)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#id1">)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#persistent-matmul">2. Persistent Matmul</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#id4">)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#tma-matmul">3. TMA Matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#tma-persistent-with-epilogue-subtiling">4. TMA Persistent with Epilogue Subtiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#viewing-profile-data">Viewing Profile Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#host-side-tensordescriptor">Host-side (TensorDescriptor)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#device-side-tl-make-tensor-descriptor">Device-side (tl.make_tensor*descriptor)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#how-it-works">How It Works</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#expected-speedups-relative-to-naive">Expected speedups (relative to naive):</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#when-each-variant-wins">When Each Variant Wins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#fp16-float16">FP16 (Float16)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#fp8-float8">FP8 (Float8)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#wrong-b-matrix-layout-for-tma">1. Wrong B Matrix Layout for TMA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#mixing-host-and-device-descriptors">2. Mixing Host and Device Descriptors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#forgetting-fp8-support-check">3. Forgetting FP8 Support Check</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#block-scaling-fundamentals">Block Scaling Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#quantization-formats">Quantization Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#memory-savings">Memory Savings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#why-preshuffling">Why Preshuffling?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#d-preshuffled-layout">5D Preshuffled Layout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#reshaping-and-transposing">Reshaping and Transposing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#mfma-scale-organization">MFMA Scale Organization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#thread-level-access-pattern">Thread-level Access Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#nvidia-kernel-tma-based">1. NVIDIA Kernel (TMA-based)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#amd-cdna4-kernel">2. AMD CDNA4 Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#nvidia-version">NVIDIA Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#amd-version">AMD Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#theoretical-speedup">Theoretical Speedup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#real-world-performance">Real-world Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#quantization-error">Quantization Error</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#e8m0-scale-format-amd">E8M0 Scale Format (AMD)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#example">Example:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#command-line-benchmarking">Command-line Benchmarking</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#nvidia-fp4">NVIDIA FP4</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#nvidia-fp8">NVIDIA FP8</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#validation">Validation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#nvidia">NVIDIA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#unsupported-hardware">1. Unsupported Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#format-mismatch">2. Format Mismatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#wrong-scale-shape">3. Wrong Scale Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#missing-tma-allocator">4. Missing TMA Allocator</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#summary">Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton Compiler</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-1-python-ast-parsing">Stage 1: Python AST Parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-2-code-generation-ttir">Stage 2: Code Generation (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-3-triton-gpu-ir-ttgir">Stage 3: Triton GPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-4-llvm-ir">Stage 4: LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-5-ptx-amdgcn">Stage 5: PTX / AMDGCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-6-binary-cubin-hsaco">Stage 6: Binary (CUBIN / HSACO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#block-based-programming-model">Block-based Programming Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#jit-compilation">JIT Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#mlir-infrastructure">MLIR Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#python-components">Python Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#c-components">C++ Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#backend-components">Backend Components</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/01-overview.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#implementation">Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#source-code-extraction">Source Code Extraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#ast-parsing">AST Parsing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#cache-key-generation">Cache Key Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#tracking-global-variables">Tracking Global Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#handling-nested-function-calls">Handling Nested Function Calls</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#initialization">Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#kernel-call-handling">Kernel Call Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#argument-specialization">Argument Specialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#launch-metadata">Launch Metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#attributes-and-hints">Attributes and Hints</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html">CodeGenerator Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ast-visitor-pattern">AST Visitor Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#triton-language-primitives">Triton Language Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#type-inference">Type Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#example-ttir-output">Example TTIR Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#compilation-orchestration">Compilation Orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ttir-ttgir-transformation">TTIR -&gt; TTGIR Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ttgir-llvm-ir">TTGIR -&gt; LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#llvm-ir-ptx">LLVM IR -&gt; PTX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ptx-cubin">PTX -&gt; CUBIN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-key-components">Cache Key Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-directory-structure">Cache Directory Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-lookup">Cache Lookup</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html">The NVCC Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#nvcc-compilation-stages">NVCC Compilation Stages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-assembly-output">PTX Assembly Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#the-llvm-path">The LLVM Path</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#llvm-ir-stage">LLVM IR Stage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-generated-by-triton">PTX Generated by Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#same-tools-same-artifacts">Same Tools, Same Artifacts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#source-language">Source Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compiler-stack">Compiler Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-time">Compilation Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#optimization-levels">Optimization Levels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#can-triton-and-cuda-c-work-together">Can Triton and CUDA C++ Work Together?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#advantages-of-llvm-backend">Advantages of LLVM Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#why-not-use-nvcc">Why Not Use NVCC?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#trade-offs">Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#file-types">File Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#example-directory-structures">Example Directory Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-inspection">PTX Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#cubin-inspection">CUBIN Inspection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-paths-compared">Compilation Paths Compared</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html">Traditional Compiler Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#example-matrix-multiplication">Example: Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#mlir-philosophy">MLIR Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#dialects">1. Dialects</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#common-dialects">Common Dialects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-s-custom-dialects">Triton’s Custom Dialects</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#operations">2. Operations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#operation-anatomy">Operation Anatomy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#ssa-form-static-single-assignment">SSA Form (Static Single Assignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#operation-examples">Operation Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#regions-and-blocks">3. Regions and Blocks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#regions">Regions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#blocks">Blocks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#example-with-regions-and-blocks">Example with Regions and Blocks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#types">4. Types</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#built-in-types">Built-in Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-custom-types">Triton Custom Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#type-conversion">Type Conversion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#attributes">5. Attributes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#common-attribute-types">Common Attribute Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-layout-attributes">Triton Layout Attributes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#passes">6. Passes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#pass-types">Pass Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#example-pass-pipeline">Example Pass Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-specific-passes">Triton-Specific Passes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-triton-uses-mlir">Why Triton Uses MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-s-mlir-dialects">Triton’s MLIR Dialects</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-dialect-tt">Triton Dialect (tt)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tritongpu-dialect-ttg">TritonGPU Dialect (ttg)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tritonnvidiagpu-dialect-ttng">TritonNvidiaGPU Dialect (ttng)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#python-source">Python Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-1-triton-ir-ttir">Stage 1: Triton IR (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-2-tritongpu-ir-ttgir">Stage 2: TritonGPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-3-llvm-dialect">Stage 3: LLVM Dialect</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-4-llvm-ir-actual">Stage 4: LLVM IR (Actual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#command-line-tools">Command-Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tablegen-for-defining-operations">TableGen for Defining Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#debugging-mlir">Debugging MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#official-documentation">Official Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tutorials">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-specific">Triton-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#key-concepts-recap">Key Concepts Recap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-mlir-matters-for-triton">Why MLIR Matters for Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#the-big-picture">The Big Picture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../learning-paths.html">Learning Paths</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#memory-optimization">Memory Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#compute-optimization">Compute Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#backward-pass-training">Backward Pass / Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#advanced-techniques">Advanced Techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#after-path-1-fast-track">After Path 1 (Fast Track)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#after-path-2-comprehensive">After Path 2 (Comprehensive)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-paths.html#after-path-3-transformer">After Path 3 (Transformer)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html">CUDA Out of Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#out-of-shared-memory">Out of Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-results">Wrong Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nan-or-inf-values">NaN or Inf Values</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slower-than-pytorch">Slower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#low-gpu-utilization">Low GPU Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#compilation-errors">Compilation Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slow-compilation">Slow Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nvidia-specific">NVIDIA-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#amd-specific">AMD-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-gpu-selected">Wrong GPU Selected</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#print-debugging">Print Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#profiling">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#assertions">Assertions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#unit-testing">Unit Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#when-stuck">When Stuck</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#cuda">CUDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#rocm-amd">ROCm (AMD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#flash-attention">Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#normalization">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#optimization-techniques">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#id1">Triton</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../references.html#books">Books</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#gpu-programming">GPU Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#deep-learning">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-tools">NVIDIA Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-tools">AMD Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#tutorials-and-courses">Tutorials and Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#community">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#blogs-and-articles">Blogs and Articles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#triton-examples">Triton Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#production-usage">Production Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-gpus">NVIDIA GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-gpus">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#subscribe-to">Subscribe To</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#conferences">Conferences</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../references.html#citation">Citation</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Triton GPU Programming Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">What is the GIL?</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/cpu-concurrency/patterns_problems_mapping.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Why Different Approaches for CPU-bound vs I/O-bound Problems?</p>
<p>A deep dive into the technical reasons behind choosing multiprocessing for CPU-bound tasks and threading/asyncio for I/O-bound tasks in Python.</p>
<p>Table of Contents
1. <a class="reference external" href="#the-fundamental-problem-the-gil">The Fundamental Problem: The GIL</a>
2. <a class="reference external" href="#how-cpu-and-io-operations-differ">How CPU and I/O Operations Differ</a>
3. <a class="reference external" href="#why-multiprocessing-for-cpu-bound">Why Multiprocessing for CPU-bound</a>
4. <a class="reference external" href="#why-threading-for-io-bound">Why Threading for I/O-bound</a>
5. <a class="reference external" href="#why-asyncio-for-io-bound">Why Asyncio for I/O-bound</a>
6. <a class="reference external" href="#deep-dive-what-happens-under-the-hood">Deep Dive: What Happens Under the Hood</a>
7. <a class="reference external" href="#performance-analysis">Performance Analysis</a>
8. <a class="reference external" href="#decision-tree">Decision Tree</a></p>
<p>—</p>
<p>The Fundamental Problem: The GIL</p>
<section id="what-is-the-gil">
<h1>What is the GIL?<a class="headerlink" href="#what-is-the-gil" title="Link to this heading"></a></h1>
<p>The <strong>Global Interpreter Lock (GIL)</strong> is a mutex (mutual exclusion lock) in CPython that protects access to Python objects, preventing multiple threads from executing Python bytecode simultaneously.</p>
</section>
<section id="why-does-python-have-a-gil">
<h1>Why Does Python Have a GIL?<a class="headerlink" href="#why-does-python-have-a-gil" title="Link to this heading"></a></h1>
<p>Historical Context:
| Python was designed in the late 1980s       |
| when single-core CPUs were the norm        |
| Design Decision:                            |
| * Simple memory management (ref counting)   |
| * Easy C extension integration              |
| * Thread-safe by default                    |
| * Trade-off: One GIL = Simple design       |</p>
</section>
<section id="how-the-gil-works">
<h1>How the GIL Works<a class="headerlink" href="#how-the-gil-works" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Conceptual representation of GIL behavior</p>
<dl class="simple">
<dt>Thread 1: [Acquire GIL] -&gt; Execute Python Code -&gt; [Release GIL]</dt><dd><p>down</p>
</dd>
</dl>
<p>Thread 2:                    [Waiting…]  -&gt; [Acquire GIL] -&gt; Execute
.. code-block:: text</p>
<p><strong>Key Point</strong>: Only ONE thread can execute Python bytecode at a time, even on a multi-core CPU.</p>
</section>
<section id="gil-behavior-with-different-operations">
<h1>GIL Behavior with Different Operations<a class="headerlink" href="#gil-behavior-with-different-operations" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>CPU-bound operation
def cpu_intensive():</p>
<blockquote>
<div><p>total = 0
for i in range(10*000*000):</p>
<blockquote>
<div><p>total += i</p>
</div></blockquote>
<p>return total</p>
</div></blockquote>
<p>Thread 1 acquires GIL -&gt; executes -&gt; releases after ~100 bytecodes -&gt; repeat
Thread 2 waits -&gt; acquires GIL -&gt; executes -&gt; releases -&gt; repeat
Result: Threads take TURNS, no parallel execution
.. code-block:: text</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>I/O-bound operation
def io_intensive():</p>
<blockquote>
<div><p>response = requests.get(’<a class="reference external" href="https://api.example.com/data">https://api.example.com/data</a>’)
return response.json()</p>
</div></blockquote>
<p>Thread 1 acquires GIL -&gt; starts I/O -&gt; RELEASES GIL during I/O wait
Thread 2 can now acquire GIL and execute while Thread 1 waits
Result: Threads can work while others are waiting for I/O</p>
</section>
<section id="the-critical-difference">
<h1>The Critical Difference<a class="headerlink" href="#the-critical-difference" title="Link to this heading"></a></h1>
<div class="line-block">
<div class="line">Operation Type | GIL Released During Operation? | Result |</div>
<div class="line">CPU-bound (pure Python) | [[FAIL]] No | Threads execute sequentially |</div>
<div class="line">I/O-bound (network, disk) | [[OK]] Yes | Threads can work concurrently |</div>
<div class="line">C extensions (NumPy, etc.) | [[OK]] Often yes | Can achieve parallelism |</div>
</div>
<p>—</p>
<p>How CPU and I/O Operations Differ</p>
</section>
<section id="cpu-bound-operations">
<h1>CPU-bound Operations<a class="headerlink" href="#cpu-bound-operations" title="Link to this heading"></a></h1>
<p><strong>Definition</strong>: Operations where execution time is determined by CPU processing speed.</p>
<p><strong>Characteristics</strong>:</p>
<p>CPU Usage:  ######################## (100%)
I/O Wait:   (minimal or none)
Bottleneck: CPU cycles</p>
<p>Example Timeline:
0ms   –[&gt;] Processing –[&gt;] Processing –[&gt;] Processing –[&gt;] Done</p>
<blockquote>
<div><p>(CPU busy)      (CPU busy)      (CPU busy)</p>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>CPU fetches instructions from memory</p></li>
<li><p>CPU executes mathematical/logical operations</p></li>
<li><p>CPU writes results to memory/registers</p></li>
<li><p>Repeat continuously</p></li>
</ol>
<p><strong>No waiting</strong> - CPU is constantly working.</p>
</section>
<section id="i-o-bound-operations">
<h1>I/O-bound Operations<a class="headerlink" href="#i-o-bound-operations" title="Link to this heading"></a></h1>
<p><strong>Definition</strong>: Operations where execution time is determined by waiting for input/output.</p>
<p><strong>Characteristics</strong>:</p>
<p>CPU Usage:  #_______#_______# (sporadic, mostly idle)
I/O Wait:   _########_######## (most of the time)
Bottleneck: Waiting for external resources</p>
<p>Example Timeline:
0ms   –[&gt;] Request –[&gt;] Waiting… –[&gt;] Waiting… –[&gt;] Response –[&gt;] Process</p>
<blockquote>
<div><dl class="simple">
<dt>(CPU)        (I/O device)   (I/O device)   (network)   (CPU)</dt><dd><p>100mus           50ms           50ms         100ms      1ms</p>
</dd>
</dl>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>CPU initiates I/O request (network/disk)</p></li>
<li><p><strong>CPU sits idle waiting</strong> for response</p></li>
<li><p>I/O device/network does the work</p></li>
<li><p>Response arrives</p></li>
<li><p>CPU processes the response (brief)</p></li>
</ol>
<p><strong>Lots of waiting</strong> - CPU is idle most of the time.</p>
</section>
<section id="real-world-analogy">
<h1>Real-World Analogy<a class="headerlink" href="#real-world-analogy" title="Link to this heading"></a></h1>
<p><strong>CPU-bound</strong> (Computing factorial):</p>
<dl class="simple">
<dt>You: Calculate 1000! in your head</dt><dd><p>+-[&gt;] You must think continuously
+-[&gt;] Cannot do anything else while thinking
+-[&gt;] Limited by your brain’s processing speed</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p><strong>I/O-bound</strong> (Ordering pizza):
.. code-block:: text</p>
<dl class="simple">
<dt>You: Call pizza place -&gt; Wait 30 min -&gt; Receive pizza</dt><dd><p>+-[&gt;] Phone call: 1 minute (active)
+-[&gt;] Waiting: 29 minutes (idle - can do other things!)
+-[&gt;] Receive: 1 minute (active)
+-[&gt;] Limited by pizza shop’s speed, not yours</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>—</p>
<p>Why Multiprocessing for CPU-bound</p>
</section>
<section id="the-problem-with-threading-for-cpu-bound">
<h1>The Problem with Threading for CPU-bound<a class="headerlink" href="#the-problem-with-threading-for-cpu-bound" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>CPU-intensive task with threading
import threading
import time</p>
<dl class="simple">
<dt>def cpu_work():</dt><dd><p>total = sum(i_i for i in range(10*000*000))
return total</p>
</dd>
</dl>
<section id="sequential">
<h2>Sequential<a class="headerlink" href="#sequential" title="Link to this heading"></a></h2>
<p>start = time.perf_counter()
cpu_work()
cpu_work()
print(f”Sequential: {time.perf_counter() - start:.2f}s”)
Output: Sequential: 2.50s</p>
<p>Threading (SAME TIME OR WORSE!)
start = time.perf_counter()
t1 = threading.Thread(target=cpu_work)
t2 = threading.Thread(target=cpu_work)
t1.start(); t2.start()
t1.join(); t2.join()
print(f”Threading: {time.perf_counter() - start:.2f}s”)
Output: Threading: 2.55s (no improvement!)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>**Why no speedup?**
</pre></div>
</div>
<p>With GIL (Threading):
Core 1: [Thread 1][Thread 2][Thread 1][Thread 2][Thread 1][Thread 2]
Core 2: [idle…………………………………………….]
Core 3: [idle…………………………………………….]
Core 4: [idle…………………………………………….]
Time:   ==============================================================================================================[&gt;]</p>
<p>Result: Only using 1 core, taking turns due to GIL</p>
</section>
</section>
<section id="the-solution-multiprocessing">
<h1>The Solution: Multiprocessing<a class="headerlink" href="#the-solution-multiprocessing" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>CPU-intensive task with multiprocessing
from concurrent.futures import ProcessPoolExecutor
import time</p>
<dl class="simple">
<dt>def cpu_work():</dt><dd><p>return sum(i_i for i in range(10*000*000))</p>
</dd>
</dl>
<p>Multiprocessing
start = time.perf_counter()
with ProcessPoolExecutor(max_workers=2) as executor:</p>
<blockquote>
<div><p>futures = [executor.submit(cpu_work) for * in range(2)]
results = [f.result() for f in futures]</p>
</div></blockquote>
<p>print(f”Multiprocessing: {time.perf_counter() - start:.2f}s”)
Output: Multiprocessing: 1.30s (nearly 2x speedup!)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Why</span> <span class="n">it</span> <span class="n">works</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<p>Without GIL (Multiprocessing):
Process 1 on Core 1: [########################] Complete
Process 2 on Core 2: [########################] Complete
Process 3 on Core 3: [idle]
Process 4 on Core 4: [idle]
Time:                ==================================================[&gt;]</p>
<p>Result: Using 2 cores in TRUE parallel execution</p>
</section>
<section id="how-multiprocessing-bypasses-the-gil">
<h1>How Multiprocessing Bypasses the GIL<a class="headerlink" href="#how-multiprocessing-bypasses-the-gil" title="Link to this heading"></a></h1>
<p>Each process has:
- <strong>Its own Python interpreter</strong>
- <strong>Its own GIL</strong> (doesn’t interfere with other processes)
- <strong>Its own memory space</strong>
- <strong>Its own process ID</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>|   Process 1      |  |   Process 2      |  |   Process 3      |
|  | GIL #1     |  |  |  | GIL #2     |  |  |  | GIL #3     |  |
|  | Interpreter|  |  |  | Interpreter|  |  |  | Interpreter|  |
|  |   Memory   |  |  |  |   Memory   |  |  |  |   Memory   |  |
CPU Core 1           CPU Core 2           CPU Core 3
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</section>
<section id="trade-offs-of-multiprocessing">
<h1>Trade-offs of Multiprocessing<a class="headerlink" href="#trade-offs-of-multiprocessing" title="Link to this heading"></a></h1>
<p><strong>Advantages</strong>:
- [[OK]] True parallelism - uses multiple CPU cores
- [[OK]] No GIL interference between processes
- [[OK]] Process isolation (crash in one doesn’t affect others)
- [[OK]] Can achieve near-linear speedup for CPU-bound tasks</p>
<p><strong>Disadvantages</strong>:
- [[FAIL]] Higher memory usage (each process has full Python interpreter)
- [[FAIL]] Slower startup time (creating processes is expensive)
- [[FAIL]] Inter-process communication is complex and slow
- [[FAIL]] Cannot share memory directly (must pickle/unpickle data)</p>
</section>
<section id="when-the-trade-off-is-worth-it">
<h1>When the Trade-off is Worth It<a class="headerlink" href="#when-the-trade-off-is-worth-it" title="Link to this heading"></a></h1>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Memory cost per process: ~10-50 MB
Speedup for CPU-bound tasks: 2-8x (depending on cores)</p>
<p>Example:
Task: Process 1000 images (CPU-intensive)
.. code-block:: text</p>
<p>—</p>
<p>Why Threading for I/O-bound</p>
</section>
<section id="the-problem-wasted-time">
<h1>The Problem: Wasted Time<a class="headerlink" href="#the-problem-wasted-time" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Sequential I/O operations
import requests
import time</p>
<p>urls = [’<a class="reference external" href="https://api1.com">https://api1.com</a>’, ‘<a class="reference external" href="https://api2.com">https://api2.com</a>’, ‘<a class="reference external" href="https://api3.com">https://api3.com</a>’]</p>
<p>start = time.perf_counter()
for url in urls:</p>
<blockquote>
<div><p>response = requests.get(url)  # Takes 2 seconds each
process(response)</p>
</div></blockquote>
<p>print(f”Sequential: {time.perf_counter() - start:.2f}s”)
Output: Sequential: 6.00s (2s + 2s + 2s)</p>
<p>Timeline:
0s    2s    4s    6s
<a href="#id11"><span class="problematic" id="id12">|Wait1|Wait2|Wait3|</span></a>  &lt;- CPU is IDLE during all this time!</p>
</section>
<section id="the-solution-threading">
<h1>The Solution: Threading<a class="headerlink" href="#the-solution-threading" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Threaded I/O operations
import threading
import requests
import time</p>
<dl class="simple">
<dt>def fetch(url):</dt><dd><p>response = requests.get(url)
process(response)</p>
</dd>
</dl>
<p>urls = [’<a class="reference external" href="https://api1.com">https://api1.com</a>’, ‘<a class="reference external" href="https://api2.com">https://api2.com</a>’, ‘<a class="reference external" href="https://api3.com">https://api3.com</a>’]</p>
<p>start = time.perf_counter()
threads = [threading.Thread(target=fetch, args=(url,)) for url in urls]
for t in threads: t.start()
for t in threads: t.join()
print(f”Threading: {time.perf_counter() - start:.2f}s”)
Output: Threading: 2.05s (all wait in parallel!)</p>
<p>Timeline:
0s    2s</p>
</section>
<section id="why-threading-works-for-i-o">
<h1>Why Threading Works for I/O<a class="headerlink" href="#why-threading-works-for-i-o" title="Link to this heading"></a></h1>
<p><strong>The GIL is Released During I/O Operations!</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>What happens under the hood:</p>
<dl class="simple">
<dt>Thread 1: [Acquire GIL] -&gt; Start network request -&gt; [Release GIL] -&gt; Wait…</dt><dd><p>down</p>
</dd>
<dt>Thread 2:                    [Acquire GIL] -&gt; Start disk read -&gt; [Release GIL]</dt><dd><p>down</p>
</dd>
</dl>
<p>Thread 3:                                      [Acquire GIL] -&gt; Start DB query</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>**Key Insight**: While Thread 1 waits for network I/O, Threads 2 and 3 can start their I/O operations. All three are waiting simultaneously!
</pre></div>
</div>
</section>
<section id="how-the-os-helps">
<h1>How the OS Helps<a class="headerlink" href="#how-the-os-helps" title="Link to this heading"></a></h1>
<p>When Python releases the GIL during I/O:</p>
<p>Python Thread          Operating System           I/O Device</p>
<p>The OS handles I/O asynchronously while the thread waits, allowing other threads to work.</p>
</section>
<section id="performance-comparison">
<h1>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Real example: Downloading 10 web pages</p>
<p>Sequential (1 thread):
Total: 5.0 seconds</p>
<p>Threading (10 threads):</p>
<p>Speedup: 10x! (near-perfect for I/O-bound)</p>
</section>
<section id="why-not-multiprocessing-for-i-o">
<h1>Why Not Multiprocessing for I/O?<a class="headerlink" href="#why-not-multiprocessing-for-i-o" title="Link to this heading"></a></h1>
<p>Threading for I/O:</p>
<p>Multiprocessing for I/O:</p>
<p>Verdict: Threading is MORE EFFICIENT for I/O
.. code-block:: text</p>
</section>
<section id="threading-trade-offs">
<h1>Threading Trade-offs<a class="headerlink" href="#threading-trade-offs" title="Link to this heading"></a></h1>
<p><strong>Advantages</strong>:
- [[OK]] Lightweight (minimal memory overhead)
- [[OK]] Fast to create/destroy
- [[OK]] Easy data sharing (shared memory)
- [[OK]] Perfect for I/O-bound tasks</p>
<p><strong>Disadvantages</strong>:
- [[FAIL]] No speedup for CPU-bound tasks (GIL)
- [[FAIL]] Race conditions possible with shared state
- [[FAIL]] More complex debugging
- [[FAIL]] Limited by GIL for Python code execution</p>
<p>—</p>
<p>Why Asyncio for I/O-bound</p>
</section>
<section id="the-problem-with-threading-overhead">
<h1>The Problem with Threading: Overhead<a class="headerlink" href="#the-problem-with-threading-overhead" title="Link to this heading"></a></h1>
<p>Creating 10,000 threads:</p>
<p>Creating 10,000 asyncio tasks:</p>
</section>
<section id="asyncio-cooperative-multitasking">
<h1>Asyncio: Cooperative Multitasking<a class="headerlink" href="#asyncio-cooperative-multitasking" title="Link to this heading"></a></h1>
<p><strong>Threading</strong> (Preemptive - OS decides when to switch):</p>
<p>OS: “Thread 1, you’ve used enough CPU, I’m switching to Thread 2”
Thread 1: “But I’m not done!”
OS: “Too bad, Thread 2’s turn now”
.. code-block:: text</p>
<p><strong>Asyncio</strong> (Cooperative - code decides when to yield):</p>
<p>Task 1: “I’m about to wait for network, let me yield control”
Event Loop: “Thanks! I’ll run Task 2 now”
Task 2: “I’m about to wait for disk, let me yield”
Event Loop: “Got it! I’ll check if Task 1’s network response arrived”</p>
</section>
<section id="how-asyncio-works">
<h1>How Asyncio Works<a class="headerlink" href="#how-asyncio-works" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Asyncio example: 10,000 concurrent requests
import asyncio
import aiohttp</p>
<dl>
<dt>async def fetch(session, url):</dt><dd><dl class="simple">
<dt>async with session.get(url) as response:</dt><dd><p>return await response.text()</p>
</dd>
</dl>
</dd>
<dt>async def main():</dt><dd><p>urls = [f’<a class="reference external" href="https://api.example.com/item">https://api.example.com/item</a>/{i}’ for i in range(10000)]
async with aiohttp.ClientSession() as session:</p>
<blockquote>
<div><p>tasks = [fetch(session, url) for url in urls]
results = await asyncio.gather(<a href="#id1"><span class="problematic" id="id2">*</span></a>tasks)</p>
</div></blockquote>
</dd>
</dl>
<p>asyncio.run(main())
Can handle 10,000 requests efficiently!</p>
</section>
<section id="event-loop-visualization">
<h1>Event Loop Visualization<a class="headerlink" href="#event-loop-visualization" title="Link to this heading"></a></h1>
<p>Event Loop (Single Thread):
|  Ready Queue: [Task 1, Task 5, Task 12, …]          |
|  Waiting for I/O: {Task 2: socket 1,                   |
|                    Task 3: socket 2,                   |
|                    Task 4: socket 3, …}              |
|  Flow:                                                  |
|  1. Get next ready task                                |
|  2. Run until it awaits something                      |
|  3. Check which I/O operations completed               |
|  4. Move completed tasks to ready queue                |
|  5. Repeat                                             |</p>
</section>
<section id="asyncio-vs-threading-detailed-comparison">
<h1>Asyncio vs Threading: Detailed Comparison<a class="headerlink" href="#asyncio-vs-threading-detailed-comparison" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Threading approach
import threading
import requests</p>
<dl class="simple">
<dt>def fetch_url(url):</dt><dd><p>response = requests.get(url)
return response.text</p>
</dd>
</dl>
<p>urls = [f’<a class="reference external" href="https://api.example.com">https://api.example.com</a>/{i}’ for i in range(1000)]
threads = [threading.Thread(target=fetch_url, args=(url,)) for url in urls]</p>
<p>Problem: Creating 1000 threads!
Memory: ~8 GB (1000 x 8 MB stack per thread)
OS overhead: Managing 1000 threads
.. code-block:: text</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Asyncio approach
import asyncio
import aiohttp</p>
<dl>
<dt>async def fetch_url(session, url):</dt><dd><dl class="simple">
<dt>async with session.get(url) as response:</dt><dd><p>return await response.text()</p>
</dd>
</dl>
</dd>
<dt>async def main():</dt><dd><p>urls = [f’<a class="reference external" href="https://api.example.com">https://api.example.com</a>/{i}’ for i in range(1000)]
async with aiohttp.ClientSession() as session:</p>
<blockquote>
<div><p>tasks = [fetch_url(session, url) for url in urls]
results = await asyncio.gather(<a href="#id3"><span class="problematic" id="id4">*</span></a>tasks)</p>
</div></blockquote>
</dd>
</dl>
<p>asyncio.run(main())</p>
<p>Solution: Single thread, 1000 lightweight tasks
Memory: ~50 MB total
OS overhead: None (all managed by Python)</p>
</section>
<section id="performance-characteristics">
<h1>Performance Characteristics<a class="headerlink" href="#performance-characteristics" title="Link to this heading"></a></h1>
<div class="line-block">
<div class="line">Metric | Threading (1000 ops) | Asyncio (1000 ops) |</div>
<div class="line">Memory Usage | ~8 GB | ~50 MB |</div>
<div class="line">Context Switch | OS-level (slow) | Python-level (fast) |</div>
<div class="line">Scalability | ~1000s | ~100,000s |</div>
<div class="line">Startup Time | Slow (create threads) | Fast (create tasks) |</div>
<div class="line">CPU Overhead | High (OS scheduling) | Low (event loop) |</div>
</div>
</section>
<section id="when-asyncio-shines">
<h1>When Asyncio Shines<a class="headerlink" href="#when-asyncio-shines" title="Link to this heading"></a></h1>
<p><strong>Perfect for</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Web</span> <span class="n">servers</span> <span class="p">(</span><span class="n">handle</span> <span class="n">many</span> <span class="n">simultaneous</span> <span class="n">connections</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Web</span> <span class="n">scraping</span> <span class="p">(</span><span class="n">thousands</span> <span class="n">of</span> <span class="n">HTTP</span> <span class="n">requests</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Database</span> <span class="n">queries</span> <span class="p">(</span><span class="n">many</span> <span class="n">concurrent</span> <span class="n">queries</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Microservices</span> <span class="p">(</span><span class="n">coordinating</span> <span class="n">many</span> <span class="n">API</span> <span class="n">calls</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Chat</span> <span class="n">applications</span> <span class="p">(</span><span class="n">many</span> <span class="n">idle</span> <span class="n">connections</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">IoT</span> <span class="n">systems</span> <span class="p">(</span><span class="n">many</span> <span class="n">devices</span> <span class="n">sending</span> <span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Not</span> <span class="n">ideal</span> <span class="k">for</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>[[FAIL]] CPU-intensive tasks (use multiprocessing)
[[FAIL]] Blocking libraries (must use async-compatible libraries)
[[FAIL]] Simple scripts with few I/O operations (threading is simpler)
.. code-block:: text</p>
</section>
<section id="asyncio-trade-offs">
<h1>Asyncio Trade-offs<a class="headerlink" href="#asyncio-trade-offs" title="Link to this heading"></a></h1>
<p><strong>Advantages</strong>:
- [[OK]] Extremely lightweight (handle 100,000+ concurrent operations)
- [[OK]] Low memory overhead
- [[OK]] Fast context switching (Python-level)
- [[OK]] Single-threaded (no race conditions)
- [[OK]] Explicit concurrency (clear control flow with <code class="docutils literal notranslate"><span class="pre">await</span></code>)</p>
<p><strong>Disadvantages</strong>:
- [[FAIL]] Requires async-compatible libraries (can’t use standard <code class="docutils literal notranslate"><span class="pre">requests</span></code>, etc.)
- [[FAIL]] Learning curve (async/await paradigm)
- [[FAIL]] Viral nature (once you go async, everything must be async)
- [[FAIL]] No speedup for CPU-bound tasks
- [[FAIL]] One blocking operation blocks everything</p>
<p>—</p>
<p>Deep Dive: What Happens Under the Hood</p>
</section>
<section id="cpu-bound-with-threading-the-gil-dance">
<h1>CPU-bound with Threading: The GIL Dance<a class="headerlink" href="#cpu-bound-with-threading-the-gil-dance" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Example: Two threads computing sum
import threading</p>
<dl>
<dt>def compute_sum(n):</dt><dd><p>total = 0
for i in range(n):</p>
<blockquote>
<div><p>total += i</p>
</div></blockquote>
<p>return total</p>
</dd>
</dl>
<p>t1 = threading.Thread(target=compute_sum, args=(10*000*000,))
t2 = threading.Thread(target=compute_sum, args=(10*000*000,))</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">actually</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Time -&gt;
0ms    Thread 1: [Acquire GIL]
1ms              : Execute: total = 0
2ms              : Execute: total += 1
3ms              : Execute: total += 2
…
100ms            : [Release GIL] (every ~100 bytecodes or 5ms)
100ms  Thread 2:                 [Acquire GIL]
101ms            :                 Execute: total = 0
102ms            :                 Execute: total += 1
…
200ms            :                 [Release GIL]
200ms  Thread 1: [Acquire GIL]
…
(continues alternating)</p>
<p>Result: Threads take turns executing Python bytecode
No parallelism for CPU work!
.. code-block:: text</p>
</section>
<section id="cpu-bound-with-multiprocessing-true-parallel">
<h1>CPU-bound with Multiprocessing: True Parallel<a class="headerlink" href="#cpu-bound-with-multiprocessing-true-parallel" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>from concurrent.futures import ProcessPoolExecutor</p>
<dl>
<dt>def compute_sum(n):</dt><dd><p>total = 0
for i in range(n):</p>
<blockquote>
<div><p>total += i</p>
</div></blockquote>
<p>return total</p>
</dd>
<dt>with ProcessPoolExecutor(max_workers=2) as executor:</dt><dd><p>future1 = executor.submit(compute_sum, 10*000*000)
future2 = executor.submit(compute_sum, 10*000*000)</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">actually</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Time -&gt;
0ms    Process 1 (Core 1): [Start] Create interpreter, load code
10ms                       : Execute: total = 0
11ms                       : Execute: total += 1
12ms                       : Execute: total += 2
…
1000ms                     : [Done] Return result via pipe</p>
<p>0ms    Process 2 (Core 2): [Start] Create interpreter, load code
10ms                       : Execute: total = 0
11ms                       : Execute: total += 1
12ms                       : Execute: total += 2
…
1000ms                     : [Done] Return result via pipe</p>
<p>Result: Both processes execute SIMULTANEOUSLY on different cores
True parallelism!
.. code-block:: text</p>
</section>
<section id="i-o-bound-with-threading-gil-released">
<h1>I/O-bound with Threading: GIL Released<a class="headerlink" href="#i-o-bound-with-threading-gil-released" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import threading
import requests</p>
<dl class="simple">
<dt>def fetch_url(url):</dt><dd><p>response = requests.get(url)  # I/O operation
return response.text</p>
</dd>
</dl>
<p>t1 = threading.Thread(target=fetch_url, args=(’<a class="reference external" href="https://api1.com">https://api1.com</a>’,))
t2 = threading.Thread(target=fetch_url, args=(’<a class="reference external" href="https://api2.com">https://api2.com</a>’,))</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">actually</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Time -&gt;
0ms    Thread 1: [Acquire GIL]
1ms             : Prepare HTTP request
2ms             : [Release GIL] &lt;- Call to C library (requests)
2ms             : [OS: Send network packet]
3ms             : [OS: Waiting for response…]</p>
<p>2ms    Thread 2:                [Acquire GIL] &lt;- Can run while T1 waits!
3ms             :                Prepare HTTP request
4ms             :                [Release GIL] &lt;- Call to C library
4ms             :                [OS: Send network packet]
5ms             :                [OS: Waiting for response…]</p>
<p>200ms           : [OS: T1’s response arrives]
200ms  Thread 1: [Acquire GIL]
201ms           : Process response
202ms           : [Done]</p>
<p>210ms           : [OS: T2’s response arrives]
210ms  Thread 2: [Acquire GIL]
211ms           : Process response
212ms           : [Done]</p>
<p>Result: Both threads waited concurrently (overlapped I/O)
Total time: ~210ms instead of 400ms sequential
.. code-block:: text</p>
</section>
<section id="i-o-bound-with-asyncio-event-loop-magic">
<h1>I/O-bound with Asyncio: Event Loop Magic<a class="headerlink" href="#i-o-bound-with-asyncio-event-loop-magic" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import asyncio
import aiohttp</p>
<dl class="simple">
<dt>async def fetch_url(session, url):</dt><dd><dl class="simple">
<dt>async with session.get(url) as response:</dt><dd><p>return await response.text()</p>
</dd>
</dl>
</dd>
<dt>async def main():</dt><dd><dl class="simple">
<dt>async with aiohttp.ClientSession() as session:</dt><dd><p>task1 = asyncio.create_task(fetch_url(session, ‘<a class="reference external" href="https://api1.com">https://api1.com</a>’))
task2 = asyncio.create_task(fetch_url(session, ‘<a class="reference external" href="https://api2.com">https://api2.com</a>’))
results = await asyncio.gather(task1, task2)</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">actually</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Time -&gt; (Single Thread)
0ms    Event Loop: Create task1
1ms              : Create task2
2ms              : Run task1 until await
3ms    Task 1   : Send HTTP request
4ms              : await response.get() &lt;- Yields control
4ms    Event Loop: task1 waiting for I/O, switch to task2
5ms    Task 2   : Send HTTP request
6ms              : await response.get() &lt;- Yields control
6ms    Event Loop: Both tasks waiting, check I/O status
…
200ms  Event Loop: task1’s I/O completed
200ms  Task 1   : Process response
201ms            : Return result
201ms  Event Loop: task1 done, check task2
210ms  Event Loop: task2’s I/O completed
210ms  Task 2   : Process response
211ms            : Return result
212ms  Event Loop: Both tasks done, gather returns</p>
<p>Result: Single thread efficiently managing multiple I/O operations
No thread overhead, same concurrency benefit
.. code-block:: text</p>
<p>—</p>
<p>Performance Analysis</p>
</section>
<section id="benchmark-cpu-bound-task-computing-pi">
<h1>Benchmark: CPU-bound Task (Computing pi)<a class="headerlink" href="#benchmark-cpu-bound-task-computing-pi" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>def compute_pi(iterations):</dt><dd><p>“””CPU-intensive: Monte Carlo pi approximation”””
inside = 0
for * in range(iterations):</p>
<blockquote>
<div><p>x, y = random.random(), random.random()
if x_x + y_y &lt;= 1:</p>
<blockquote>
<div><p>inside += 1</p>
</div></blockquote>
</div></blockquote>
<p>return 4 * inside / iterations</p>
</dd>
</dl>
<p>ITERATIONS = 10*000*000
TASKS = 4
.. code-block:: text</p>
<p><strong>Results on 4-core CPU</strong>:</p>
<div class="line-block">
<div class="line">Approach | Time | Speedup | Memory |</div>
<div class="line">Sequential | 10.0s | 1.0x | 50 MB |</div>
<div class="line">Threading (4 threads) | 10.2s | 0.98x [[FAIL]] | 55 MB |</div>
<div class="line">Asyncio (4 tasks) | 10.1s | 0.99x [[FAIL]] | 52 MB |</div>
<div class="line">Multiprocessing (4 proc) | 2.7s | 3.7x [[OK]] | 200 MB |</div>
</div>
<p><strong>Analysis</strong>:
- Threading/Asyncio: No improvement (GIL limitation)
- Multiprocessing: Near-linear speedup (3.7x on 4 cores)
- Memory trade-off is worth it for 3.7x speedup</p>
</section>
<section id="benchmark-i-o-bound-task-web-requests">
<h1>Benchmark: I/O-bound Task (Web Requests)<a class="headerlink" href="#benchmark-i-o-bound-task-web-requests" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>async def fetch_page(session, url):</dt><dd><p>“””I/O-intensive: Download web page”””
async with session.get(url) as response:</p>
<blockquote>
<div><p>return await response.text()</p>
</div></blockquote>
</dd>
</dl>
<p>URLS = 100 (each takes ~0.5s to fetch)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Results</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="line-block">
<div class="line">Approach | Time | Speedup | Memory | Max Concurrent |</div>
<div class="line">Sequential | 50.0s | 1.0x | 50 MB | 1 |</div>
<div class="line">Threading (10 threads) | 5.2s | 9.6x [[OK]] | 130 MB | 10 |</div>
<div class="line">Threading (100 threads) | 1.8s | 27.8x [[OK]] | 850 MB | 100 |</div>
<div class="line">Asyncio (100 tasks) | 1.5s | 33.3x [[OK]] | 65 MB | 100 |</div>
<div class="line">Multiprocessing (4 proc) | 13.0s | 3.8x [[FAIL]] | 200 MB | 4 |</div>
</div>
<p><strong>Analysis</strong>:
- Threading: Good speedup, but memory grows with threads
- Asyncio: Best speedup with lowest memory
- Multiprocessing: Poor choice (high overhead, limited concurrency)</p>
</section>
<section id="benchmark-mixed-workload">
<h1>Benchmark: Mixed Workload<a class="headerlink" href="#benchmark-mixed-workload" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>async def process_data(session, url):</dt><dd><p>“””Fetch data (I/O) then process (CPU)”””
# I/O: Fetch data (2 seconds)
async with session.get(url) as response:</p>
<blockquote>
<div><p>data = await response.json()</p>
</div></blockquote>
<p># CPU: Heavy processing (1 second)
result = complex_computation(data)
return result</p>
</dd>
</dl>
<p>TASKS = 10</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Results</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="line-block">
<div class="line">Approach | Time | Analysis |</div>
<div class="line">Sequential | 30.0s | (2s I/O + 1s CPU) x 10 |</div>
<div class="line">Threading | 15.5s | I/O concurrent, CPU sequential |</div>
<div class="line">Asyncio | 15.2s | I/O concurrent, CPU sequential |</div>
<div class="line">Asyncio + ProcessPool | 5.8s | I/O concurrent, CPU parallel [[OK]] |</div>
</div>
<p><strong>Best approach for mixed workload</strong>:
.. code-block:: python</p>
<p>import asyncio
from concurrent.futures import ProcessPoolExecutor</p>
<dl>
<dt>async def process_data(session, url, executor):</dt><dd><p># I/O: Use asyncio
async with session.get(url) as response:</p>
<blockquote>
<div><p>data = await response.json()</p>
</div></blockquote>
<p># CPU: Offload to process pool
loop = asyncio.get_event*loop()
result = await loop.run_in*executor(executor, complex_computation, data)
return result</p>
</dd>
<dt>async def main():</dt><dd><dl class="simple">
<dt>with ProcessPoolExecutor(max_workers=4) as executor:</dt><dd><dl class="simple">
<dt>async with aiohttp.ClientSession() as session:</dt><dd><p>tasks = [process_data(session, url, executor) for url in urls]
results = await asyncio.gather(<a href="#id5"><span class="problematic" id="id6">*</span></a>tasks)</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>—</p>
<p>Decision Tree</p>
<p>Use this decision tree to choose the right approach:</p>
<p>Start: What type of operation?</p>
</section>
<section id="quick-reference-table">
<h1>Quick Reference Table<a class="headerlink" href="#quick-reference-table" title="Link to this heading"></a></h1>
<div class="line-block">
<div class="line">Scenario | Solution | Reason |</div>
<div class="line">Image processing (1000 images) | Multiprocessing | CPU-bound, benefits from parallel cores |</div>
<div class="line">Web scraping (100 pages) | Asyncio | I/O-bound, many concurrent connections |</div>
<div class="line">REST API server | Asyncio | I/O-bound, handle many simultaneous requests |</div>
<div class="line">Video encoding | Multiprocessing | CPU-intensive, utilize all cores |</div>
<div class="line">Database queries (10 concurrent) | Threading | I/O-bound, simple implementation |</div>
<div class="line">Database queries (1000 concurrent) | Asyncio | I/O-bound, need high concurrency |</div>
<div class="line">File downloads (5 files) | Threading | I/O-bound, blocking library OK |</div>
<div class="line">WebSocket server (10000 clients) | Asyncio | I/O-bound, need extreme scalability |</div>
<div class="line">Scientific computation | Multiprocessing | CPU-intensive calculations |</div>
<div class="line">Real-time chat (1000 users) | Asyncio | I/O-bound, many idle connections |</div>
</div>
</section>
<section id="code-templates">
<h1>Code Templates<a class="headerlink" href="#code-templates" title="Link to this heading"></a></h1>
<p><strong>CPU-bound Template</strong>:
.. code-block:: python</p>
<p>from concurrent.futures import ProcessPoolExecutor
import os</p>
<dl>
<dt>def cpu_intensive*task(data):</dt><dd><p># Your CPU-heavy computation here
result = complex_calculation(data)
return result</p>
</dd>
<dt>def main():</dt><dd><p>data_items = […]  # Your data</p>
<p># Use number of CPU cores
max_workers = os.cpu_count()</p>
<dl class="simple">
<dt>with ProcessPoolExecutor(max_workers=max_workers) as executor:</dt><dd><p>results = executor.map(cpu_intensive*task, data_items)</p>
</dd>
</dl>
<p>return list(results)</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p><strong>I/O-bound Template (Few operations, blocking library)</strong>:
.. code-block:: python</p>
<p>from concurrent.futures import ThreadPoolExecutor
import requests</p>
<dl>
<dt>def io_intensive*task(url):</dt><dd><p>response = requests.get(url)
return process(response)</p>
</dd>
<dt>def main():</dt><dd><p>urls = […]  # Your URLs</p>
<dl class="simple">
<dt>with ThreadPoolExecutor(max_workers=10) as executor:</dt><dd><p>results = executor.map(io_intensive*task, urls)</p>
</dd>
</dl>
<p>return list(results)</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p><strong>I/O-bound Template (Many operations, async library)</strong>:
.. code-block:: python</p>
<p>import asyncio
import aiohttp</p>
<dl>
<dt>async def io_intensive*task(session, url):</dt><dd><dl class="simple">
<dt>async with session.get(url) as response:</dt><dd><p>data = await response.text()
return process(data)</p>
</dd>
</dl>
</dd>
<dt>async def main():</dt><dd><p>urls = […]  # Your URLs</p>
<dl class="simple">
<dt>async with aiohttp.ClientSession() as session:</dt><dd><p>tasks = [io_intensive*task(session, url) for url in urls]
results = await asyncio.gather(<a href="#id7"><span class="problematic" id="id8">*</span></a>tasks)</p>
</dd>
</dl>
<p>return results</p>
</dd>
<dt>if __name** == ‘<strong>main</strong>’:</dt><dd><p>asyncio.run(main())</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Mixed</span> <span class="n">Workload</span> <span class="n">Template</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import asyncio
import aiohttp
from concurrent.futures import ProcessPoolExecutor</p>
<dl>
<dt>def cpu_intensive(data):</dt><dd><p># CPU-heavy work here
return complex_calculation(data)</p>
</dd>
<dt>async def mixed_task(session, url, executor):</dt><dd><p># I/O part (async)
async with session.get(url) as response:</p>
<blockquote>
<div><p>data = await response.json()</p>
</div></blockquote>
<p># CPU part (process pool)
loop = asyncio.get_event*loop()
result = await loop.run_in*executor(executor, cpu_intensive, data)</p>
<p>return result</p>
</dd>
<dt>async def main():</dt><dd><p>urls = […]</p>
<dl class="simple">
<dt>with ProcessPoolExecutor(max_workers=4) as executor:</dt><dd><dl class="simple">
<dt>async with aiohttp.ClientSession() as session:</dt><dd><p>tasks = [mixed_task(session, url, executor) for url in urls]
results = await asyncio.gather(<a href="#id9"><span class="problematic" id="id10">*</span></a>tasks)</p>
</dd>
</dl>
</dd>
</dl>
<p>return results</p>
</dd>
<dt>if <strong>name</strong> == ‘<strong>main</strong>’:</dt><dd><p>asyncio.run(main())</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>—</p>
<p>Summary: The Core Principles</p>
</section>
<section id="the-gil-controls-everything">
<h1>1. The GIL Controls Everything<a class="headerlink" href="#the-gil-controls-everything" title="Link to this heading"></a></h1>
<p>Python’s GIL:</p>
<p>Therefore:</p>
</section>
<section id="resource-usage-matters">
<h1>2. Resource Usage Matters<a class="headerlink" href="#resource-usage-matters" title="Link to this heading"></a></h1>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Operation Type  |  Resource Bottleneck  |  Solution  |  Why?
CPU-bound       |  CPU cycles          |  Multi-    |  Bypass GIL,</p>
<blockquote>
<div><div class="line-block">
<div class="line">(computation)       |  processing|  use all cores</div>
</div>
</div></blockquote>
<p>I/O-bound       |  Waiting for I/O     |  Asyncio/  |  GIL released,
(few ops)       |  (network, disk)     |  Threading |  work during wait
I/O-bound       |  Waiting for I/O     |  Asyncio   |  Lightweight,
(many ops)      |  + scalability       |            |  handles 1000s
.. code-block:: text</p>
</section>
<section id="trade-offs-are-real">
<h1>3. Trade-offs are Real<a class="headerlink" href="#trade-offs-are-real" title="Link to this heading"></a></h1>
<p><strong>Multiprocessing</strong>:
- Pros: True parallelism, bypasses GIL
- Cons: Memory overhead, slow startup, complex IPC
- <strong>Use when</strong>: CPU-bound work benefits &gt; memory cost</p>
<p><strong>Threading</strong>:
- Pros: Lightweight, easy data sharing, fast startup
- Cons: No speedup for CPU work, race conditions possible
- <strong>Use when</strong>: I/O-bound with moderate concurrency</p>
<p><strong>Asyncio</strong>:
- Pros: Extremely lightweight, handles 100,000+ operations
- Cons: Requires async libraries, learning curve
- <strong>Use when</strong>: I/O-bound with high concurrency</p>
</section>
<section id="know-your-workload">
<h1>4. Know Your Workload<a class="headerlink" href="#know-your-workload" title="Link to this heading"></a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Profile your code first!
import time</p>
<dl>
<dt>def profile_task(task_func):</dt><dd><p># CPU time (actual processing)
cpu_start = time.process_time()
# Wall time (including waiting)
wall_start = time.perf_counter()</p>
<p>result = task_func()</p>
<p>cpu_time = time.process_time() - cpu_start
wall_time = time.perf_counter() - wall_start</p>
<dl class="simple">
<dt>if cpu_time / wall_time &gt; 0.8:</dt><dd><p>print(“CPU-bound -&gt; Use multiprocessing”)</p>
</dd>
<dt>else:</dt><dd><p>print(“I/O-bound -&gt; Use asyncio/threading”)</p>
</dd>
</dl>
<p>return result</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>—</p>
<p>Final Thoughts</p>
<p>The choice between multiprocessing, threading, and asyncio isn’t about which is “better” - it’s about matching the tool to the task:</p>
<ul class="simple">
<li><p><strong>Multiprocessing</strong>: Powerful but heavy. Use when you need true parallel CPU computation.</p></li>
<li><p><strong>Threading</strong>: Simple and effective for I/O. Use when blocking libraries are needed.</p></li>
<li><p><strong>Asyncio</strong>: Lightweight and scalable for I/O. Use when you need to handle many concurrent operations.</p></li>
</ul>
<p>Understanding the GIL and how Python interacts with the OS is key to making the right choice. Always profile your code, measure the results, and choose based on your specific requirements.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="semaphore_explained.html" class="btn btn-neutral float-left" title="Key Concept" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../gpu-concepts/gpu-fundamentals.html" class="btn btn-neutral float-right" title="Key Differences: CPU vs GPU" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Concurrent Programs.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>