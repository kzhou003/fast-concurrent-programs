

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>What is the GIL? &mdash; Fast Concurrent Programming Guide 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1aac1d93" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/sidebar-fix.js?v=6c2f6f50"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Key Differences: CPU vs GPU" href="../gpu-concepts/gpu-fundamentals.html" />
    <link rel="prev" title="Key Concept" href="semaphore_explained.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Fast Concurrent Programming Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">CPU Concurrency</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#parallelism">Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#visual-comparison">Visual Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#threading-concurrent-futures-threadpoolexecutor">Threading (concurrent.futures.ThreadPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#multiprocessing-concurrent-futures-processpoolexecutor">Multiprocessing (concurrent.futures.ProcessPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#when-to-use-what">When to Use What</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#what-is-the-gil">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#key-points">Key Points:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#impact-on-performance">Impact on Performance:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#observing-the-gil-from-script-06">Observing the GIL (from script 06):</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#event-loop">Event Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#how-it-works-from-script-07">How It Works (from script 07):</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#event-loop-lifecycle">Event Loop Lifecycle:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#modern-vs-old-patterns">Modern vs Old Patterns:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#what-are-coroutines">What are Coroutines?</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#defining-coroutines">Defining Coroutines:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#key-features">Key Features:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#example-from-script-08">Example from Script 08:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#execution-flow">Execution Flow:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#important-rules">Important Rules:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#tasks">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#task-characteristics">Task Characteristics:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#example-from-script-09">Example from Script 09:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#futures">Futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#waiting-for-multiple-tasks">Waiting for Multiple Tasks:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#comparison-table">Comparison Table:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#hybrid-workloads">Hybrid Workloads:</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#time-measurement-time-clock-time-perf-counter">1. Time Measurement (<code class="docutils literal notranslate"><span class="pre">time.clock()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#coroutine-syntax-asyncio-coroutine-async-def">2. Coroutine Syntax (<code class="docutils literal notranslate"><span class="pre">&#64;asyncio.coroutine</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#task-creation-asyncio-task-asyncio-create-task">3. Task Creation (<code class="docutils literal notranslate"><span class="pre">asyncio.Task()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">asyncio.create_task()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#event-loop-management">4. Event Loop Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#future-callbacks-callbacks-await">5. Future Callbacks (Callbacks -&gt; <code class="docutils literal notranslate"><span class="pre">await</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#blocking-calls-in-async-code">6. Blocking Calls in Async Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#string-formatting-f-strings">7. String Formatting (<code class="docutils literal notranslate"><span class="pre">%</span></code> -&gt; f-strings)</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#migration-checklist">Migration Checklist</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#compatibility">Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#quick-reference-guide">Quick Reference Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_concepts.html#further-reading">Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html">Physical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#logical-cores">Logical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#the-concept">The Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#how-it-works">How It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#technical-implementation">Technical Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#hyperthreading-limitations">Hyperthreading Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#checking-hyperthreading-status">Checking Hyperthreading Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#the-fundamental-constraint">The Fundamental Constraint</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#why-more-threads-more-speed">Why More Threads != More Speed</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#optimal-worker-count">Optimal Worker Count</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#real-world-example">Real-World Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#cpu-vs-gpu-different-design-philosophies">CPU vs GPU: Different Design Philosophies</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#key-differences">Key Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#simd-and-gpu-architecture">SIMD and GPU Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#why-gpus-excel-at-compute-intensive-tasks">Why GPUs Excel at Compute-Intensive Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#what-gpus-are-good-at">What GPUs Are Good At</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#silicon-real-estate-comparison">Silicon Real Estate Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#detailed-benchmark-results">Detailed Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#decision-matrix">Decision Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#practical-guidelines">Practical Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#example-1-image-processing">Example 1: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#example-2-monte-carlo-simulation">Example 2: Monte Carlo Simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#example-3-neural-network-training">Example 3: Neural Network Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#core-principles">Core Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware_parallelism.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html">Key Points About start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#example">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#key-points-about-join">Key Points About join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#id1">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#without-join-danger">WITHOUT join() - DANGER!</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#with-join-correct">WITH join() - CORRECT!</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#mistake-1-calling-the-function-directly-instead-of-start">Mistake 1: Calling the function directly instead of start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#mistake-2-forgetting-join">Mistake 2: Forgetting join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="threading_basics.html#mistake-3-thinking-threads-share-data-automatically">Mistake 3: Thinking threads share data automatically</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_event_loop.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_event_loop.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_coroutine.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_coroutine.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_coroutine.html#use-cases">Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_and_futures.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_and_futures.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_and_futures.html#examples">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_task_manipulation.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio_task_manipulation.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrent_futures_pooling.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrent_futures_pooling.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#internal-structure">Internal Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#automatic-locking">Automatic Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#put-item"><code class="docutils literal notranslate"><span class="pre">put(item)</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#problem-with-manual-locks">Problem with Manual Locks</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#how-queue-does-locking">How Queue Does Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#locking-benefits">Locking Benefits</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#execution-flow">Execution Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#thread-safe-data-structure">1. <strong>Thread-Safe Data Structure</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#blocks-correctly">2. <strong>Blocks Correctly</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#no-busy-waiting">3. <strong>No Busy-Waiting</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#task-tracking">4. <strong>Task Tracking</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#safe-for-multiple-producers-consumers">5. <strong>Safe for Multiple Producers/Consumers</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#step-by-step-what-happens-in-put">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">put()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#step-by-step-what-happens-in-get">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">get()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#mistake-1-forgetting-lock">Mistake 1: Forgetting Lock</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#mistake-2-busy-waiting">Mistake 2: Busy-Waiting</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_explained.html#mistake-3-race-condition">Mistake 3: Race Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_internal_mechanics.html">The Answer</a></li>
<li class="toctree-l1"><a class="reference internal" href="queue_internal_mechanics.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html">Without task_done() - Canâ€™t Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#with-task-done-can-track-completion">With task_done() - Can Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#internal-counter-system">Internal Counter System</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#visual-timeline">Visual Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#code-simplified">Code (Simplified)</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#two-operations">Two Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#timeline-all-three-conditions">Timeline: All Three Conditions</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#put">put()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#get">get()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#task-done">task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#join">join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#scenario-1-producer-1-consumer">Scenario: 1 Producer, 1 Consumer</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-1-put-increment-counter">Step 1: put() - Increment Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-2-get-item-removed-counter-unchanged">Step 2: get() - Item Removed, Counter Unchanged</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-3-task-done-decrement-counter">Step 3: task_done() - Decrement Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-4-join-wait-then-return">Step 4: join() - Wait, Then Return</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#diagram-tracking-one-task">Diagram: Tracking One Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#diagram-multiple-tasks">Diagram: Multiple Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#counter">Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#condition-variable">Condition Variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#together">Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#without-all-tasks-done-condition">Without all_tasks*done Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#queue-join-without-task-done">Queue.join() Without task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#why-it-blocks-forever">Why It Blocks Forever</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#queue-join-with-task-done">Queue.join() With task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#why-it-works">Why It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-1-put-item">Step 1: Put Item</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-2-get-and-process">Step 2: Get and Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#step-3-mark-done">Step 3: Mark Done</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#flow-diagram">Flow Diagram</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#without-task-done-problematic">Without task_done() - PROBLEMATIC</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#with-task-done-correct">With task_done() - CORRECT</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#use-case-1-verify-all-work-complete">Use Case 1: Verify All Work Complete</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#use-case-2-track-progress">Use Case 2: Track Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#use-case-3-batch-processing">Use Case 3: Batch Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#scenario-main-thread-needs-to-know-when-workers-finish">Scenario: Main thread needs to know when workers finish</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#timeline">Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#better-code-pattern">Better Code Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#why-we-need-task-done">Why We Need task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_done_queue_explained.html#the-pattern">The Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html">Regular Lock vs RLock</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#why-rlock-is-needed-here">Why RLock is Needed Here</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#use-regular-lock-when">Use Regular Lock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#use-rlock-when">Use RLock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#regular-lock-would-deadlock">Regular Lock - Would Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlock_explained.html#rlock-no-deadlock">RLock - No Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#counting-semaphore-counter-1">1. Counting Semaphore (Counter &gt; 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#binary-semaphore-counter-0-or-1">2. Binary Semaphore (Counter = 0 or 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#execution-timeline">Execution Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#acquire"><code class="docutils literal notranslate"><span class="pre">acquire()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#counting-semaphore-3-spots-available">Counting Semaphore (3 spots available)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#binary-semaphore-producer-consumer">Binary Semaphore (Producer-Consumer)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#example-1-swimming-pool-with-limited-capacity">Example 1: Swimming Pool with Limited Capacity</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#example-2-producer-consumer-like-the-code">Example 2: Producer-Consumer (Like the Code)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#limiting-concurrent-access">1. <strong>Limiting Concurrent Access</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#producer-consumer-communication">2. <strong>Producer-Consumer Communication</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#synchronizing-multiple-threads">3. <strong>Synchronizing Multiple Threads</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#lock-threading-lock">Lock (<code class="docutils literal notranslate"><span class="pre">threading.Lock</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#semaphore-counting">Semaphore (Counting)</a></li>
<li class="toctree-l1"><a class="reference internal" href="semaphore_explained.html#semaphore-binary-used-as-signal">Semaphore (Binary - Used as Signal)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-does-python-have-a-gil">Why Does Python Have a GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-the-gil-works">How the GIL Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="#gil-behavior-with-different-operations">GIL Behavior with Different Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-critical-difference">The Critical Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="#real-world-analogy">Real-World Analogy</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-problem-with-threading-for-cpu-bound">The Problem with Threading for CPU-bound</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-solution-multiprocessing">The Solution: Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-multiprocessing-bypasses-the-gil">How Multiprocessing Bypasses the GIL</a></li>
<li class="toctree-l1"><a class="reference internal" href="#trade-offs-of-multiprocessing">Trade-offs of Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="#when-the-trade-off-is-worth-it">When the Trade-off is Worth It</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-problem-wasted-time">The Problem: Wasted Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-solution-threading">The Solution: Threading</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-threading-works-for-i-o">Why Threading Works for I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-the-os-helps">How the OS Helps</a></li>
<li class="toctree-l1"><a class="reference internal" href="#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-not-multiprocessing-for-i-o">Why Not Multiprocessing for I/O?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#threading-trade-offs">Threading Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-problem-with-threading-overhead">The Problem with Threading: Overhead</a></li>
<li class="toctree-l1"><a class="reference internal" href="#asyncio-cooperative-multitasking">Asyncio: Cooperative Multitasking</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-asyncio-works">How Asyncio Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="#event-loop-visualization">Event Loop Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="#asyncio-vs-threading-detailed-comparison">Asyncio vs Threading: Detailed Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="#when-asyncio-shines">When Asyncio Shines</a></li>
<li class="toctree-l1"><a class="reference internal" href="#asyncio-trade-offs">Asyncio Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="#cpu-bound-with-threading-the-gil-dance">CPU-bound with Threading: The GIL Dance</a></li>
<li class="toctree-l1"><a class="reference internal" href="#cpu-bound-with-multiprocessing-true-parallel">CPU-bound with Multiprocessing: True Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="#i-o-bound-with-threading-gil-released">I/O-bound with Threading: GIL Released</a></li>
<li class="toctree-l1"><a class="reference internal" href="#i-o-bound-with-asyncio-event-loop-magic">I/O-bound with Asyncio: Event Loop Magic</a></li>
<li class="toctree-l1"><a class="reference internal" href="#benchmark-cpu-bound-task-computing-pi">Benchmark: CPU-bound Task (Computing pi)</a></li>
<li class="toctree-l1"><a class="reference internal" href="#benchmark-i-o-bound-task-web-requests">Benchmark: I/O-bound Task (Web Requests)</a></li>
<li class="toctree-l1"><a class="reference internal" href="#benchmark-mixed-workload">Benchmark: Mixed Workload</a></li>
<li class="toctree-l1"><a class="reference internal" href="#quick-reference-table">Quick Reference Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="#code-templates">Code Templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="#the-gil-controls-everything">1. The GIL Controls Everything</a></li>
<li class="toctree-l1"><a class="reference internal" href="#resource-usage-matters">2. Resource Usage Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="#trade-offs-are-real">3. Trade-offs are Real</a></li>
<li class="toctree-l1"><a class="reference internal" href="#know-your-workload">4. Know Your Workload</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html">Key Differences: CPU vs GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#streaming-multiprocessors-sms">Streaming Multiprocessors (SMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#thread-organization">Thread Organization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#example-visualization">Example Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#l2-cache">L2 Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#registers">Registers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html">Warps and SIMD Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#thread-divergence">Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#what-is-occupancy">What is Occupancy?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#factors-limiting-occupancy">Factors Limiting Occupancy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#example-calculation">Example Calculation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#why-occupancy-matters">Why Occupancy Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#the-occupancy-sweet-spot">The Occupancy Sweet Spot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#grid-and-block-dimensions">Grid and Block Dimensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#choosing-block-size">Choosing Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#within-a-block">Within a Block</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#between-blocks">Between Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-level-reductions">Warp-Level Reductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#overlap-compute-and-memory">Overlap Compute and Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#traditional-approach">Traditional Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#persistent-approach">Persistent Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#key-factors">Key Factors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#profiling-tools">Profiling Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html">Step 1: Profile First</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#step-2-identify-bottleneck">Step 2: Identify Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-kernel-fusion">Strategy 1: Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tiling">Strategy 2: Tiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-vectorized-loads">Strategy 3: Vectorized Loads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-memory-coalescing">Strategy 4: Memory Coalescing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-use-tensor-cores">Strategy 1: Use Tensor Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-increase-arithmetic-intensity">Strategy 2: Increase Arithmetic Intensity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-minimize-thread-divergence">Strategy 3: Minimize Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-optimize-loop-structure">Strategy 4: Optimize Loop Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-reduce-register-usage">Strategy 1: Reduce Register Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tune-shared-memory">Strategy 2: Tune Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-adjust-block-size">Strategy 3: Adjust Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#why-auto-tune">Why Auto-Tune?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#triton-auto-tuning">Triton Auto-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#persistent-kernels">Persistent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#recomputation">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-reduction">Pattern: Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-element-wise">Pattern: Element-wise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-matrix-multiply">Pattern: Matrix Multiply</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-bandwidth">Issue: Low Bandwidth</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-compute-utilization">Issue: Low Compute Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-lower-than-pytorch">Issue: Lower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#before-you-optimize">Before You Optimize</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#memory-optimizations">Memory Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#compute-optimizations">Compute Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#occupancy-optimization">Occupancy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#advanced">Advanced</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/triton-concepts.html">Triton Concepts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/01-vector-add.html#next-steps">Next Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/02-fused-softmax.html#extensions">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/03-matrix-multiplication.html">Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/04-low-memory-dropout.html">Low Memory Dropout</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/05-layer-norm.html">Layer Norm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/06-fused-attention.html">Fused Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/07-extern-functions.html">Extern Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/08-grouped-gemm.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/09-persistent-matmul.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-tutorials/10-block-scaled-matmul.html#summary">Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton Compiler</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-1-python-ast-parsing">Stage 1: Python AST Parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-2-code-generation-ttir">Stage 2: Code Generation (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-3-triton-gpu-ir-ttgir">Stage 3: Triton GPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-4-llvm-ir">Stage 4: LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-5-ptx-amdgcn">Stage 5: PTX / AMDGCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-6-binary-cubin-hsaco">Stage 6: Binary (CUBIN / HSACO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#block-based-programming-model">Block-based Programming Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#jit-compilation">JIT Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#mlir-infrastructure">MLIR Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#python-components">Python Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#c-components">C++ Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#backend-components">Backend Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html">CodeGenerator Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ast-visitor-pattern">AST Visitor Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#triton-language-primitives">Triton Language Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#type-inference">Type Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#example-ttir-output">Example TTIR Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#compilation-orchestration">Compilation Orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ttir-ttgir-transformation">TTIR -&gt; TTGIR Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ttgir-llvm-ir">TTGIR -&gt; LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#llvm-ir-ptx">LLVM IR -&gt; PTX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ptx-cubin">PTX -&gt; CUBIN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-key-components">Cache Key Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-directory-structure">Cache Directory Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-lookup">Cache Lookup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html">The NVCC Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#nvcc-compilation-stages">NVCC Compilation Stages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-assembly-output">PTX Assembly Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#the-llvm-path">The LLVM Path</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#llvm-ir-stage">LLVM IR Stage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-generated-by-triton">PTX Generated by Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#same-tools-same-artifacts">Same Tools, Same Artifacts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#source-language">Source Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compiler-stack">Compiler Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-time">Compilation Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#optimization-levels">Optimization Levels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#can-triton-and-cuda-c-work-together">Can Triton and CUDA C++ Work Together?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#advantages-of-llvm-backend">Advantages of LLVM Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#why-not-use-nvcc">Why Not Use NVCC?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#trade-offs">Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#file-types">File Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#example-directory-structures">Example Directory Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-inspection">PTX Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#cubin-inspection">CUBIN Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-paths-compared">Compilation Paths Compared</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html">Traditional Compiler Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#example-matrix-multiplication">Example: Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#mlir-philosophy">MLIR Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#dialects">1. Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#operations">2. Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#regions-and-blocks">3. Regions and Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#types">4. Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#attributes">5. Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#passes">6. Passes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-triton-uses-mlir">Why Triton Uses MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-s-mlir-dialects">Tritonâ€™s MLIR Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#python-source">Python Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-1-triton-ir-ttir">Stage 1: Triton IR (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-2-tritongpu-ir-ttgir">Stage 2: TritonGPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-3-llvm-dialect">Stage 3: LLVM Dialect</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-4-llvm-ir-actual">Stage 4: LLVM IR (Actual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#command-line-tools">Command-Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tablegen-for-defining-operations">TableGen for Defining Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#debugging-mlir">Debugging MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#official-documentation">Official Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tutorials">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-specific">Triton-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#key-concepts-recap">Key Concepts Recap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-mlir-matters-for-triton">Why MLIR Matters for Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#the-big-picture">The Big Picture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../learning-paths.html">Learning Paths</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html">CUDA Out of Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#out-of-shared-memory">Out of Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-results">Wrong Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nan-or-inf-values">NaN or Inf Values</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slower-than-pytorch">Slower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#low-gpu-utilization">Low GPU Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#compilation-errors">Compilation Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slow-compilation">Slow Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nvidia-specific">NVIDIA-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#amd-specific">AMD-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-gpu-selected">Wrong GPU Selected</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#print-debugging">Print Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#profiling">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#assertions">Assertions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#unit-testing">Unit Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#when-stuck">When Stuck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#cuda">CUDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#rocm-amd">ROCm (AMD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#flash-attention">Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#normalization">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#optimization-techniques">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#id1">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#gpu-programming">GPU Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#deep-learning">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-tools">NVIDIA Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-tools">AMD Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#tutorials-and-courses">Tutorials and Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#community">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#blogs-and-articles">Blogs and Articles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#triton-examples">Triton Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#production-usage">Production Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-gpus">NVIDIA GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-gpus">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#subscribe-to">Subscribe To</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#conferences">Conferences</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Fast Concurrent Programming Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">What is the GIL?</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/cpu-concurrency/patterns_problems_mapping.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Why Different Approaches for CPU-bound vs I/O-bound Problems?</p>
<p>A deep dive into the technical reasons behind choosing multiprocessing for CPU-bound tasks and threading/asyncio for I/O-bound tasks in Python.</p>
<p>Table of Contents
1. <a class="reference external" href="#the-fundamental-problem-the-gil">The Fundamental Problem: The GIL</a>
2. <a class="reference external" href="#how-cpu-and-io-operations-differ">How CPU and I/O Operations Differ</a>
3. <a class="reference external" href="#why-multiprocessing-for-cpu-bound">Why Multiprocessing for CPU-bound</a>
4. <a class="reference external" href="#why-threading-for-io-bound">Why Threading for I/O-bound</a>
5. <a class="reference external" href="#why-asyncio-for-io-bound">Why Asyncio for I/O-bound</a>
6. <a class="reference external" href="#deep-dive-what-happens-under-the-hood">Deep Dive: What Happens Under the Hood</a>
7. <a class="reference external" href="#performance-analysis">Performance Analysis</a>
8. <a class="reference external" href="#decision-tree">Decision Tree</a></p>
<p>â€”</p>
<p>The Fundamental Problem: The GIL</p>
<section id="what-is-the-gil">
<h1>What is the GIL?<a class="headerlink" href="#what-is-the-gil" title="Link to this heading">ïƒ</a></h1>
<p>The <strong>Global Interpreter Lock (GIL)</strong> is a mutex (mutual exclusion lock) in CPython that protects access to Python objects, preventing multiple threads from executing Python bytecode simultaneously.</p>
</section>
<section id="why-does-python-have-a-gil">
<h1>Why Does Python Have a GIL?<a class="headerlink" href="#why-does-python-have-a-gil" title="Link to this heading">ïƒ</a></h1>
<p>Historical Context:
| Python was designed in the late 1980s       |
| when single-core CPUs were the norm        |
| Design Decision:                            |
| * Simple memory management (ref counting)   |
| * Easy C extension integration              |
| * Thread-safe by default                    |
| * Trade-off: One GIL = Simple design       |</p>
</section>
<section id="how-the-gil-works">
<h1>How the GIL Works<a class="headerlink" href="#how-the-gil-works" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Conceptual representation of GIL behavior</p>
<dl class="simple">
<dt>Thread 1: [Acquire GIL] -&gt; Execute Python Code -&gt; [Release GIL]</dt><dd><p>down</p>
</dd>
</dl>
<p>Thread 2:                    [Waitingâ€¦]  -&gt; [Acquire GIL] -&gt; Execute
.. code-block:: text</p>
<p><strong>Key Point</strong>: Only ONE thread can execute Python bytecode at a time, even on a multi-core CPU.</p>
</section>
<section id="gil-behavior-with-different-operations">
<h1>GIL Behavior with Different Operations<a class="headerlink" href="#gil-behavior-with-different-operations" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>CPU-bound operation
def cpu_intensive():</p>
<blockquote>
<div><p>total = 0
for i in range(10*000*000):</p>
<blockquote>
<div><p>total += i</p>
</div></blockquote>
<p>return total</p>
</div></blockquote>
<p>Thread 1 acquires GIL -&gt; executes -&gt; releases after ~100 bytecodes -&gt; repeat
Thread 2 waits -&gt; acquires GIL -&gt; executes -&gt; releases -&gt; repeat
Result: Threads take TURNS, no parallel execution
.. code-block:: text</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>I/O-bound operation
def io_intensive():</p>
<blockquote>
<div><p>response = requests.get(â€™<a class="reference external" href="https://api.example.com/data">https://api.example.com/data</a>â€™)
return response.json()</p>
</div></blockquote>
<p>Thread 1 acquires GIL -&gt; starts I/O -&gt; RELEASES GIL during I/O wait
Thread 2 can now acquire GIL and execute while Thread 1 waits
Result: Threads can work while others are waiting for I/O</p>
</section>
<section id="the-critical-difference">
<h1>The Critical Difference<a class="headerlink" href="#the-critical-difference" title="Link to this heading">ïƒ</a></h1>
<div class="line-block">
<div class="line">Operation Type | GIL Released During Operation? | Result |</div>
<div class="line">CPU-bound (pure Python) | [[FAIL]] No | Threads execute sequentially |</div>
<div class="line">I/O-bound (network, disk) | [[OK]] Yes | Threads can work concurrently |</div>
<div class="line">C extensions (NumPy, etc.) | [[OK]] Often yes | Can achieve parallelism |</div>
</div>
<p>â€”</p>
<p>How CPU and I/O Operations Differ</p>
</section>
<section id="cpu-bound-operations">
<h1>CPU-bound Operations<a class="headerlink" href="#cpu-bound-operations" title="Link to this heading">ïƒ</a></h1>
<p><strong>Definition</strong>: Operations where execution time is determined by CPU processing speed.</p>
<p><strong>Characteristics</strong>:</p>
<p>CPU Usage:  ######################## (100%)
I/O Wait:   (minimal or none)
Bottleneck: CPU cycles</p>
<p>Example Timeline:
0ms   â€“[&gt;] Processing â€“[&gt;] Processing â€“[&gt;] Processing â€“[&gt;] Done</p>
<blockquote>
<div><p>(CPU busy)      (CPU busy)      (CPU busy)</p>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>CPU fetches instructions from memory</p></li>
<li><p>CPU executes mathematical/logical operations</p></li>
<li><p>CPU writes results to memory/registers</p></li>
<li><p>Repeat continuously</p></li>
</ol>
<p><strong>No waiting</strong> - CPU is constantly working.</p>
</section>
<section id="i-o-bound-operations">
<h1>I/O-bound Operations<a class="headerlink" href="#i-o-bound-operations" title="Link to this heading">ïƒ</a></h1>
<p><strong>Definition</strong>: Operations where execution time is determined by waiting for input/output.</p>
<p><strong>Characteristics</strong>:</p>
<p>CPU Usage:  #_______#_______# (sporadic, mostly idle)
I/O Wait:   _########_######## (most of the time)
Bottleneck: Waiting for external resources</p>
<p>Example Timeline:
0ms   â€“[&gt;] Request â€“[&gt;] Waitingâ€¦ â€“[&gt;] Waitingâ€¦ â€“[&gt;] Response â€“[&gt;] Process</p>
<blockquote>
<div><dl class="simple">
<dt>(CPU)        (I/O device)   (I/O device)   (network)   (CPU)</dt><dd><p>100mus           50ms           50ms         100ms      1ms</p>
</dd>
</dl>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>CPU initiates I/O request (network/disk)</p></li>
<li><p><strong>CPU sits idle waiting</strong> for response</p></li>
<li><p>I/O device/network does the work</p></li>
<li><p>Response arrives</p></li>
<li><p>CPU processes the response (brief)</p></li>
</ol>
<p><strong>Lots of waiting</strong> - CPU is idle most of the time.</p>
</section>
<section id="real-world-analogy">
<h1>Real-World Analogy<a class="headerlink" href="#real-world-analogy" title="Link to this heading">ïƒ</a></h1>
<p><strong>CPU-bound</strong> (Computing factorial):</p>
<dl class="simple">
<dt>You: Calculate 1000! in your head</dt><dd><p>+-[&gt;] You must think continuously
+-[&gt;] Cannot do anything else while thinking
+-[&gt;] Limited by your brainâ€™s processing speed</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p><strong>I/O-bound</strong> (Ordering pizza):
.. code-block:: text</p>
<dl class="simple">
<dt>You: Call pizza place -&gt; Wait 30 min -&gt; Receive pizza</dt><dd><p>+-[&gt;] Phone call: 1 minute (active)
+-[&gt;] Waiting: 29 minutes (idle - can do other things!)
+-[&gt;] Receive: 1 minute (active)
+-[&gt;] Limited by pizza shopâ€™s speed, not yours</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>â€”</p>
<p>Why Multiprocessing for CPU-bound</p>
</section>
<section id="the-problem-with-threading-for-cpu-bound">
<h1>The Problem with Threading for CPU-bound<a class="headerlink" href="#the-problem-with-threading-for-cpu-bound" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>CPU-intensive task with threading
import threading
import time</p>
<dl class="simple">
<dt>def cpu_work():</dt><dd><p>total = sum(i_i for i in range(10*000*000))
return total</p>
</dd>
</dl>
<section id="sequential">
<h2>Sequential<a class="headerlink" href="#sequential" title="Link to this heading">ïƒ</a></h2>
<p>start = time.perf_counter()
cpu_work()
cpu_work()
print(fâ€Sequential: {time.perf_counter() - start:.2f}sâ€)
Output: Sequential: 2.50s</p>
<p>Threading (SAME TIME OR WORSE!)
start = time.perf_counter()
t1 = threading.Thread(target=cpu_work)
t2 = threading.Thread(target=cpu_work)
t1.start(); t2.start()
t1.join(); t2.join()
print(fâ€Threading: {time.perf_counter() - start:.2f}sâ€)
Output: Threading: 2.55s (no improvement!)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>**Why no speedup?**
</pre></div>
</div>
<p>With GIL (Threading):
Core 1: [Thread 1][Thread 2][Thread 1][Thread 2][Thread 1][Thread 2]
Core 2: [idleâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦.]
Core 3: [idleâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦.]
Core 4: [idleâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦.]
Time:   ==============================================================================================================[&gt;]</p>
<p>Result: Only using 1 core, taking turns due to GIL</p>
</section>
</section>
<section id="the-solution-multiprocessing">
<h1>The Solution: Multiprocessing<a class="headerlink" href="#the-solution-multiprocessing" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>CPU-intensive task with multiprocessing
from concurrent.futures import ProcessPoolExecutor
import time</p>
<dl class="simple">
<dt>def cpu_work():</dt><dd><p>return sum(i_i for i in range(10*000*000))</p>
</dd>
</dl>
<p>Multiprocessing
start = time.perf_counter()
with ProcessPoolExecutor(max_workers=2) as executor:</p>
<blockquote>
<div><p>futures = [executor.submit(cpu_work) for * in range(2)]
results = [f.result() for f in futures]</p>
</div></blockquote>
<p>print(fâ€Multiprocessing: {time.perf_counter() - start:.2f}sâ€)
Output: Multiprocessing: 1.30s (nearly 2x speedup!)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Why</span> <span class="n">it</span> <span class="n">works</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<p>Without GIL (Multiprocessing):
Process 1 on Core 1: [########################] Complete
Process 2 on Core 2: [########################] Complete
Process 3 on Core 3: [idle]
Process 4 on Core 4: [idle]
Time:                ==================================================[&gt;]</p>
<p>Result: Using 2 cores in TRUE parallel execution</p>
</section>
<section id="how-multiprocessing-bypasses-the-gil">
<h1>How Multiprocessing Bypasses the GIL<a class="headerlink" href="#how-multiprocessing-bypasses-the-gil" title="Link to this heading">ïƒ</a></h1>
<p>Each process has:
- <strong>Its own Python interpreter</strong>
- <strong>Its own GIL</strong> (doesnâ€™t interfere with other processes)
- <strong>Its own memory space</strong>
- <strong>Its own process ID</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>|   Process 1      |  |   Process 2      |  |   Process 3      |
|  | GIL #1     |  |  |  | GIL #2     |  |  |  | GIL #3     |  |
|  | Interpreter|  |  |  | Interpreter|  |  |  | Interpreter|  |
|  |   Memory   |  |  |  |   Memory   |  |  |  |   Memory   |  |
CPU Core 1           CPU Core 2           CPU Core 3
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</section>
<section id="trade-offs-of-multiprocessing">
<h1>Trade-offs of Multiprocessing<a class="headerlink" href="#trade-offs-of-multiprocessing" title="Link to this heading">ïƒ</a></h1>
<p><strong>Advantages</strong>:
- [[OK]] True parallelism - uses multiple CPU cores
- [[OK]] No GIL interference between processes
- [[OK]] Process isolation (crash in one doesnâ€™t affect others)
- [[OK]] Can achieve near-linear speedup for CPU-bound tasks</p>
<p><strong>Disadvantages</strong>:
- [[FAIL]] Higher memory usage (each process has full Python interpreter)
- [[FAIL]] Slower startup time (creating processes is expensive)
- [[FAIL]] Inter-process communication is complex and slow
- [[FAIL]] Cannot share memory directly (must pickle/unpickle data)</p>
</section>
<section id="when-the-trade-off-is-worth-it">
<h1>When the Trade-off is Worth It<a class="headerlink" href="#when-the-trade-off-is-worth-it" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Memory cost per process: ~10-50 MB
Speedup for CPU-bound tasks: 2-8x (depending on cores)</p>
<p>Example:
Task: Process 1000 images (CPU-intensive)
.. code-block:: text</p>
<p>â€”</p>
<p>Why Threading for I/O-bound</p>
</section>
<section id="the-problem-wasted-time">
<h1>The Problem: Wasted Time<a class="headerlink" href="#the-problem-wasted-time" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Sequential I/O operations
import requests
import time</p>
<p>urls = [â€™<a class="reference external" href="https://api1.com">https://api1.com</a>â€™, â€˜<a class="reference external" href="https://api2.com">https://api2.com</a>â€™, â€˜<a class="reference external" href="https://api3.com">https://api3.com</a>â€™]</p>
<p>start = time.perf_counter()
for url in urls:</p>
<blockquote>
<div><p>response = requests.get(url)  # Takes 2 seconds each
process(response)</p>
</div></blockquote>
<p>print(fâ€Sequential: {time.perf_counter() - start:.2f}sâ€)
Output: Sequential: 6.00s (2s + 2s + 2s)</p>
<p>Timeline:
0s    2s    4s    6s
<a href="#id11"><span class="problematic" id="id12">|Wait1|Wait2|Wait3|</span></a>  &lt;- CPU is IDLE during all this time!</p>
</section>
<section id="the-solution-threading">
<h1>The Solution: Threading<a class="headerlink" href="#the-solution-threading" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Threaded I/O operations
import threading
import requests
import time</p>
<dl class="simple">
<dt>def fetch(url):</dt><dd><p>response = requests.get(url)
process(response)</p>
</dd>
</dl>
<p>urls = [â€™<a class="reference external" href="https://api1.com">https://api1.com</a>â€™, â€˜<a class="reference external" href="https://api2.com">https://api2.com</a>â€™, â€˜<a class="reference external" href="https://api3.com">https://api3.com</a>â€™]</p>
<p>start = time.perf_counter()
threads = [threading.Thread(target=fetch, args=(url,)) for url in urls]
for t in threads: t.start()
for t in threads: t.join()
print(fâ€Threading: {time.perf_counter() - start:.2f}sâ€)
Output: Threading: 2.05s (all wait in parallel!)</p>
<p>Timeline:
0s    2s</p>
</section>
<section id="why-threading-works-for-i-o">
<h1>Why Threading Works for I/O<a class="headerlink" href="#why-threading-works-for-i-o" title="Link to this heading">ïƒ</a></h1>
<p><strong>The GIL is Released During I/O Operations!</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>What happens under the hood:</p>
<dl class="simple">
<dt>Thread 1: [Acquire GIL] -&gt; Start network request -&gt; [Release GIL] -&gt; Waitâ€¦</dt><dd><p>down</p>
</dd>
<dt>Thread 2:                    [Acquire GIL] -&gt; Start disk read -&gt; [Release GIL]</dt><dd><p>down</p>
</dd>
</dl>
<p>Thread 3:                                      [Acquire GIL] -&gt; Start DB query</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>**Key Insight**: While Thread 1 waits for network I/O, Threads 2 and 3 can start their I/O operations. All three are waiting simultaneously!
</pre></div>
</div>
</section>
<section id="how-the-os-helps">
<h1>How the OS Helps<a class="headerlink" href="#how-the-os-helps" title="Link to this heading">ïƒ</a></h1>
<p>When Python releases the GIL during I/O:</p>
<p>Python Thread          Operating System           I/O Device</p>
<p>The OS handles I/O asynchronously while the thread waits, allowing other threads to work.</p>
</section>
<section id="performance-comparison">
<h1>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Real example: Downloading 10 web pages</p>
<p>Sequential (1 thread):
Total: 5.0 seconds</p>
<p>Threading (10 threads):</p>
<p>Speedup: 10x! (near-perfect for I/O-bound)</p>
</section>
<section id="why-not-multiprocessing-for-i-o">
<h1>Why Not Multiprocessing for I/O?<a class="headerlink" href="#why-not-multiprocessing-for-i-o" title="Link to this heading">ïƒ</a></h1>
<p>Threading for I/O:</p>
<p>Multiprocessing for I/O:</p>
<p>Verdict: Threading is MORE EFFICIENT for I/O
.. code-block:: text</p>
</section>
<section id="threading-trade-offs">
<h1>Threading Trade-offs<a class="headerlink" href="#threading-trade-offs" title="Link to this heading">ïƒ</a></h1>
<p><strong>Advantages</strong>:
- [[OK]] Lightweight (minimal memory overhead)
- [[OK]] Fast to create/destroy
- [[OK]] Easy data sharing (shared memory)
- [[OK]] Perfect for I/O-bound tasks</p>
<p><strong>Disadvantages</strong>:
- [[FAIL]] No speedup for CPU-bound tasks (GIL)
- [[FAIL]] Race conditions possible with shared state
- [[FAIL]] More complex debugging
- [[FAIL]] Limited by GIL for Python code execution</p>
<p>â€”</p>
<p>Why Asyncio for I/O-bound</p>
</section>
<section id="the-problem-with-threading-overhead">
<h1>The Problem with Threading: Overhead<a class="headerlink" href="#the-problem-with-threading-overhead" title="Link to this heading">ïƒ</a></h1>
<p>Creating 10,000 threads:</p>
<p>Creating 10,000 asyncio tasks:</p>
</section>
<section id="asyncio-cooperative-multitasking">
<h1>Asyncio: Cooperative Multitasking<a class="headerlink" href="#asyncio-cooperative-multitasking" title="Link to this heading">ïƒ</a></h1>
<p><strong>Threading</strong> (Preemptive - OS decides when to switch):</p>
<p>OS: â€œThread 1, youâ€™ve used enough CPU, Iâ€™m switching to Thread 2â€
Thread 1: â€œBut Iâ€™m not done!â€
OS: â€œToo bad, Thread 2â€™s turn nowâ€
.. code-block:: text</p>
<p><strong>Asyncio</strong> (Cooperative - code decides when to yield):</p>
<p>Task 1: â€œIâ€™m about to wait for network, let me yield controlâ€
Event Loop: â€œThanks! Iâ€™ll run Task 2 nowâ€
Task 2: â€œIâ€™m about to wait for disk, let me yieldâ€
Event Loop: â€œGot it! Iâ€™ll check if Task 1â€™s network response arrivedâ€</p>
</section>
<section id="how-asyncio-works">
<h1>How Asyncio Works<a class="headerlink" href="#how-asyncio-works" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Asyncio example: 10,000 concurrent requests
import asyncio
import aiohttp</p>
<dl>
<dt>async def fetch(session, url):</dt><dd><dl class="simple">
<dt>async with session.get(url) as response:</dt><dd><p>return await response.text()</p>
</dd>
</dl>
</dd>
<dt>async def main():</dt><dd><p>urls = [fâ€™<a class="reference external" href="https://api.example.com/item">https://api.example.com/item</a>/{i}â€™ for i in range(10000)]
async with aiohttp.ClientSession() as session:</p>
<blockquote>
<div><p>tasks = [fetch(session, url) for url in urls]
results = await asyncio.gather(<a href="#id1"><span class="problematic" id="id2">*</span></a>tasks)</p>
</div></blockquote>
</dd>
</dl>
<p>asyncio.run(main())
Can handle 10,000 requests efficiently!</p>
</section>
<section id="event-loop-visualization">
<h1>Event Loop Visualization<a class="headerlink" href="#event-loop-visualization" title="Link to this heading">ïƒ</a></h1>
<p>Event Loop (Single Thread):
|  Ready Queue: [Task 1, Task 5, Task 12, â€¦]          |
|  Waiting for I/O: {Task 2: socket 1,                   |
|                    Task 3: socket 2,                   |
|                    Task 4: socket 3, â€¦}              |
|  Flow:                                                  |
|  1. Get next ready task                                |
|  2. Run until it awaits something                      |
|  3. Check which I/O operations completed               |
|  4. Move completed tasks to ready queue                |
|  5. Repeat                                             |</p>
</section>
<section id="asyncio-vs-threading-detailed-comparison">
<h1>Asyncio vs Threading: Detailed Comparison<a class="headerlink" href="#asyncio-vs-threading-detailed-comparison" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Threading approach
import threading
import requests</p>
<dl class="simple">
<dt>def fetch_url(url):</dt><dd><p>response = requests.get(url)
return response.text</p>
</dd>
</dl>
<p>urls = [fâ€™<a class="reference external" href="https://api.example.com">https://api.example.com</a>/{i}â€™ for i in range(1000)]
threads = [threading.Thread(target=fetch_url, args=(url,)) for url in urls]</p>
<p>Problem: Creating 1000 threads!
Memory: ~8 GB (1000 x 8 MB stack per thread)
OS overhead: Managing 1000 threads
.. code-block:: text</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Asyncio approach
import asyncio
import aiohttp</p>
<dl>
<dt>async def fetch_url(session, url):</dt><dd><dl class="simple">
<dt>async with session.get(url) as response:</dt><dd><p>return await response.text()</p>
</dd>
</dl>
</dd>
<dt>async def main():</dt><dd><p>urls = [fâ€™<a class="reference external" href="https://api.example.com">https://api.example.com</a>/{i}â€™ for i in range(1000)]
async with aiohttp.ClientSession() as session:</p>
<blockquote>
<div><p>tasks = [fetch_url(session, url) for url in urls]
results = await asyncio.gather(<a href="#id3"><span class="problematic" id="id4">*</span></a>tasks)</p>
</div></blockquote>
</dd>
</dl>
<p>asyncio.run(main())</p>
<p>Solution: Single thread, 1000 lightweight tasks
Memory: ~50 MB total
OS overhead: None (all managed by Python)</p>
</section>
<section id="performance-characteristics">
<h1>Performance Characteristics<a class="headerlink" href="#performance-characteristics" title="Link to this heading">ïƒ</a></h1>
<div class="line-block">
<div class="line">Metric | Threading (1000 ops) | Asyncio (1000 ops) |</div>
<div class="line">Memory Usage | ~8 GB | ~50 MB |</div>
<div class="line">Context Switch | OS-level (slow) | Python-level (fast) |</div>
<div class="line">Scalability | ~1000s | ~100,000s |</div>
<div class="line">Startup Time | Slow (create threads) | Fast (create tasks) |</div>
<div class="line">CPU Overhead | High (OS scheduling) | Low (event loop) |</div>
</div>
</section>
<section id="when-asyncio-shines">
<h1>When Asyncio Shines<a class="headerlink" href="#when-asyncio-shines" title="Link to this heading">ïƒ</a></h1>
<p><strong>Perfect for</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Web</span> <span class="n">servers</span> <span class="p">(</span><span class="n">handle</span> <span class="n">many</span> <span class="n">simultaneous</span> <span class="n">connections</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Web</span> <span class="n">scraping</span> <span class="p">(</span><span class="n">thousands</span> <span class="n">of</span> <span class="n">HTTP</span> <span class="n">requests</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Database</span> <span class="n">queries</span> <span class="p">(</span><span class="n">many</span> <span class="n">concurrent</span> <span class="n">queries</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Microservices</span> <span class="p">(</span><span class="n">coordinating</span> <span class="n">many</span> <span class="n">API</span> <span class="n">calls</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">Chat</span> <span class="n">applications</span> <span class="p">(</span><span class="n">many</span> <span class="n">idle</span> <span class="n">connections</span><span class="p">)</span>
<span class="p">[[</span><span class="n">OK</span><span class="p">]]</span> <span class="n">IoT</span> <span class="n">systems</span> <span class="p">(</span><span class="n">many</span> <span class="n">devices</span> <span class="n">sending</span> <span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Not</span> <span class="n">ideal</span> <span class="k">for</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>[[FAIL]] CPU-intensive tasks (use multiprocessing)
[[FAIL]] Blocking libraries (must use async-compatible libraries)
[[FAIL]] Simple scripts with few I/O operations (threading is simpler)
.. code-block:: text</p>
</section>
<section id="asyncio-trade-offs">
<h1>Asyncio Trade-offs<a class="headerlink" href="#asyncio-trade-offs" title="Link to this heading">ïƒ</a></h1>
<p><strong>Advantages</strong>:
- [[OK]] Extremely lightweight (handle 100,000+ concurrent operations)
- [[OK]] Low memory overhead
- [[OK]] Fast context switching (Python-level)
- [[OK]] Single-threaded (no race conditions)
- [[OK]] Explicit concurrency (clear control flow with <code class="docutils literal notranslate"><span class="pre">await</span></code>)</p>
<p><strong>Disadvantages</strong>:
- [[FAIL]] Requires async-compatible libraries (canâ€™t use standard <code class="docutils literal notranslate"><span class="pre">requests</span></code>, etc.)
- [[FAIL]] Learning curve (async/await paradigm)
- [[FAIL]] Viral nature (once you go async, everything must be async)
- [[FAIL]] No speedup for CPU-bound tasks
- [[FAIL]] One blocking operation blocks everything</p>
<p>â€”</p>
<p>Deep Dive: What Happens Under the Hood</p>
</section>
<section id="cpu-bound-with-threading-the-gil-dance">
<h1>CPU-bound with Threading: The GIL Dance<a class="headerlink" href="#cpu-bound-with-threading-the-gil-dance" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Example: Two threads computing sum
import threading</p>
<dl>
<dt>def compute_sum(n):</dt><dd><p>total = 0
for i in range(n):</p>
<blockquote>
<div><p>total += i</p>
</div></blockquote>
<p>return total</p>
</dd>
</dl>
<p>t1 = threading.Thread(target=compute_sum, args=(10*000*000,))
t2 = threading.Thread(target=compute_sum, args=(10*000*000,))</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">actually</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Time -&gt;
0ms    Thread 1: [Acquire GIL]
1ms              : Execute: total = 0
2ms              : Execute: total += 1
3ms              : Execute: total += 2
â€¦
100ms            : [Release GIL] (every ~100 bytecodes or 5ms)
100ms  Thread 2:                 [Acquire GIL]
101ms            :                 Execute: total = 0
102ms            :                 Execute: total += 1
â€¦
200ms            :                 [Release GIL]
200ms  Thread 1: [Acquire GIL]
â€¦
(continues alternating)</p>
<p>Result: Threads take turns executing Python bytecode
No parallelism for CPU work!
.. code-block:: text</p>
</section>
<section id="cpu-bound-with-multiprocessing-true-parallel">
<h1>CPU-bound with Multiprocessing: True Parallel<a class="headerlink" href="#cpu-bound-with-multiprocessing-true-parallel" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>from concurrent.futures import ProcessPoolExecutor</p>
<dl>
<dt>def compute_sum(n):</dt><dd><p>total = 0
for i in range(n):</p>
<blockquote>
<div><p>total += i</p>
</div></blockquote>
<p>return total</p>
</dd>
<dt>with ProcessPoolExecutor(max_workers=2) as executor:</dt><dd><p>future1 = executor.submit(compute_sum, 10*000*000)
future2 = executor.submit(compute_sum, 10*000*000)</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">actually</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Time -&gt;
0ms    Process 1 (Core 1): [Start] Create interpreter, load code
10ms                       : Execute: total = 0
11ms                       : Execute: total += 1
12ms                       : Execute: total += 2
â€¦
1000ms                     : [Done] Return result via pipe</p>
<p>0ms    Process 2 (Core 2): [Start] Create interpreter, load code
10ms                       : Execute: total = 0
11ms                       : Execute: total += 1
12ms                       : Execute: total += 2
â€¦
1000ms                     : [Done] Return result via pipe</p>
<p>Result: Both processes execute SIMULTANEOUSLY on different cores
True parallelism!
.. code-block:: text</p>
</section>
<section id="i-o-bound-with-threading-gil-released">
<h1>I/O-bound with Threading: GIL Released<a class="headerlink" href="#i-o-bound-with-threading-gil-released" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import threading
import requests</p>
<dl class="simple">
<dt>def fetch_url(url):</dt><dd><p>response = requests.get(url)  # I/O operation
return response.text</p>
</dd>
</dl>
<p>t1 = threading.Thread(target=fetch_url, args=(â€™<a class="reference external" href="https://api1.com">https://api1.com</a>â€™,))
t2 = threading.Thread(target=fetch_url, args=(â€™<a class="reference external" href="https://api2.com">https://api2.com</a>â€™,))</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">actually</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Time -&gt;
0ms    Thread 1: [Acquire GIL]
1ms             : Prepare HTTP request
2ms             : [Release GIL] &lt;- Call to C library (requests)
2ms             : [OS: Send network packet]
3ms             : [OS: Waiting for responseâ€¦]</p>
<p>2ms    Thread 2:                [Acquire GIL] &lt;- Can run while T1 waits!
3ms             :                Prepare HTTP request
4ms             :                [Release GIL] &lt;- Call to C library
4ms             :                [OS: Send network packet]
5ms             :                [OS: Waiting for responseâ€¦]</p>
<p>200ms           : [OS: T1â€™s response arrives]
200ms  Thread 1: [Acquire GIL]
201ms           : Process response
202ms           : [Done]</p>
<p>210ms           : [OS: T2â€™s response arrives]
210ms  Thread 2: [Acquire GIL]
211ms           : Process response
212ms           : [Done]</p>
<p>Result: Both threads waited concurrently (overlapped I/O)
Total time: ~210ms instead of 400ms sequential
.. code-block:: text</p>
</section>
<section id="i-o-bound-with-asyncio-event-loop-magic">
<h1>I/O-bound with Asyncio: Event Loop Magic<a class="headerlink" href="#i-o-bound-with-asyncio-event-loop-magic" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import asyncio
import aiohttp</p>
<dl class="simple">
<dt>async def fetch_url(session, url):</dt><dd><dl class="simple">
<dt>async with session.get(url) as response:</dt><dd><p>return await response.text()</p>
</dd>
</dl>
</dd>
<dt>async def main():</dt><dd><dl class="simple">
<dt>async with aiohttp.ClientSession() as session:</dt><dd><p>task1 = asyncio.create_task(fetch_url(session, â€˜<a class="reference external" href="https://api1.com">https://api1.com</a>â€™))
task2 = asyncio.create_task(fetch_url(session, â€˜<a class="reference external" href="https://api2.com">https://api2.com</a>â€™))
results = await asyncio.gather(task1, task2)</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">What</span> <span class="n">actually</span> <span class="n">happens</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Time -&gt; (Single Thread)
0ms    Event Loop: Create task1
1ms              : Create task2
2ms              : Run task1 until await
3ms    Task 1   : Send HTTP request
4ms              : await response.get() &lt;- Yields control
4ms    Event Loop: task1 waiting for I/O, switch to task2
5ms    Task 2   : Send HTTP request
6ms              : await response.get() &lt;- Yields control
6ms    Event Loop: Both tasks waiting, check I/O status
â€¦
200ms  Event Loop: task1â€™s I/O completed
200ms  Task 1   : Process response
201ms            : Return result
201ms  Event Loop: task1 done, check task2
210ms  Event Loop: task2â€™s I/O completed
210ms  Task 2   : Process response
211ms            : Return result
212ms  Event Loop: Both tasks done, gather returns</p>
<p>Result: Single thread efficiently managing multiple I/O operations
No thread overhead, same concurrency benefit
.. code-block:: text</p>
<p>â€”</p>
<p>Performance Analysis</p>
</section>
<section id="benchmark-cpu-bound-task-computing-pi">
<h1>Benchmark: CPU-bound Task (Computing pi)<a class="headerlink" href="#benchmark-cpu-bound-task-computing-pi" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>def compute_pi(iterations):</dt><dd><p>â€œâ€â€CPU-intensive: Monte Carlo pi approximationâ€â€â€
inside = 0
for * in range(iterations):</p>
<blockquote>
<div><p>x, y = random.random(), random.random()
if x_x + y_y &lt;= 1:</p>
<blockquote>
<div><p>inside += 1</p>
</div></blockquote>
</div></blockquote>
<p>return 4 * inside / iterations</p>
</dd>
</dl>
<p>ITERATIONS = 10*000*000
TASKS = 4
.. code-block:: text</p>
<p><strong>Results on 4-core CPU</strong>:</p>
<div class="line-block">
<div class="line">Approach | Time | Speedup | Memory |</div>
<div class="line">Sequential | 10.0s | 1.0x | 50 MB |</div>
<div class="line">Threading (4 threads) | 10.2s | 0.98x [[FAIL]] | 55 MB |</div>
<div class="line">Asyncio (4 tasks) | 10.1s | 0.99x [[FAIL]] | 52 MB |</div>
<div class="line">Multiprocessing (4 proc) | 2.7s | 3.7x [[OK]] | 200 MB |</div>
</div>
<p><strong>Analysis</strong>:
- Threading/Asyncio: No improvement (GIL limitation)
- Multiprocessing: Near-linear speedup (3.7x on 4 cores)
- Memory trade-off is worth it for 3.7x speedup</p>
</section>
<section id="benchmark-i-o-bound-task-web-requests">
<h1>Benchmark: I/O-bound Task (Web Requests)<a class="headerlink" href="#benchmark-i-o-bound-task-web-requests" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>async def fetch_page(session, url):</dt><dd><p>â€œâ€â€I/O-intensive: Download web pageâ€â€â€
async with session.get(url) as response:</p>
<blockquote>
<div><p>return await response.text()</p>
</div></blockquote>
</dd>
</dl>
<p>URLS = 100 (each takes ~0.5s to fetch)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Results</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="line-block">
<div class="line">Approach | Time | Speedup | Memory | Max Concurrent |</div>
<div class="line">Sequential | 50.0s | 1.0x | 50 MB | 1 |</div>
<div class="line">Threading (10 threads) | 5.2s | 9.6x [[OK]] | 130 MB | 10 |</div>
<div class="line">Threading (100 threads) | 1.8s | 27.8x [[OK]] | 850 MB | 100 |</div>
<div class="line">Asyncio (100 tasks) | 1.5s | 33.3x [[OK]] | 65 MB | 100 |</div>
<div class="line">Multiprocessing (4 proc) | 13.0s | 3.8x [[FAIL]] | 200 MB | 4 |</div>
</div>
<p><strong>Analysis</strong>:
- Threading: Good speedup, but memory grows with threads
- Asyncio: Best speedup with lowest memory
- Multiprocessing: Poor choice (high overhead, limited concurrency)</p>
</section>
<section id="benchmark-mixed-workload">
<h1>Benchmark: Mixed Workload<a class="headerlink" href="#benchmark-mixed-workload" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>async def process_data(session, url):</dt><dd><p>â€œâ€â€Fetch data (I/O) then process (CPU)â€â€â€
# I/O: Fetch data (2 seconds)
async with session.get(url) as response:</p>
<blockquote>
<div><p>data = await response.json()</p>
</div></blockquote>
<p># CPU: Heavy processing (1 second)
result = complex_computation(data)
return result</p>
</dd>
</dl>
<p>TASKS = 10</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Results</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="line-block">
<div class="line">Approach | Time | Analysis |</div>
<div class="line">Sequential | 30.0s | (2s I/O + 1s CPU) x 10 |</div>
<div class="line">Threading | 15.5s | I/O concurrent, CPU sequential |</div>
<div class="line">Asyncio | 15.2s | I/O concurrent, CPU sequential |</div>
<div class="line">Asyncio + ProcessPool | 5.8s | I/O concurrent, CPU parallel [[OK]] |</div>
</div>
<p><strong>Best approach for mixed workload</strong>:
.. code-block:: python</p>
<p>import asyncio
from concurrent.futures import ProcessPoolExecutor</p>
<dl>
<dt>async def process_data(session, url, executor):</dt><dd><p># I/O: Use asyncio
async with session.get(url) as response:</p>
<blockquote>
<div><p>data = await response.json()</p>
</div></blockquote>
<p># CPU: Offload to process pool
loop = asyncio.get_event*loop()
result = await loop.run_in*executor(executor, complex_computation, data)
return result</p>
</dd>
<dt>async def main():</dt><dd><dl class="simple">
<dt>with ProcessPoolExecutor(max_workers=4) as executor:</dt><dd><dl class="simple">
<dt>async with aiohttp.ClientSession() as session:</dt><dd><p>tasks = [process_data(session, url, executor) for url in urls]
results = await asyncio.gather(<a href="#id5"><span class="problematic" id="id6">*</span></a>tasks)</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>â€”</p>
<p>Decision Tree</p>
<p>Use this decision tree to choose the right approach:</p>
<p>Start: What type of operation?</p>
</section>
<section id="quick-reference-table">
<h1>Quick Reference Table<a class="headerlink" href="#quick-reference-table" title="Link to this heading">ïƒ</a></h1>
<div class="line-block">
<div class="line">Scenario | Solution | Reason |</div>
<div class="line">Image processing (1000 images) | Multiprocessing | CPU-bound, benefits from parallel cores |</div>
<div class="line">Web scraping (100 pages) | Asyncio | I/O-bound, many concurrent connections |</div>
<div class="line">REST API server | Asyncio | I/O-bound, handle many simultaneous requests |</div>
<div class="line">Video encoding | Multiprocessing | CPU-intensive, utilize all cores |</div>
<div class="line">Database queries (10 concurrent) | Threading | I/O-bound, simple implementation |</div>
<div class="line">Database queries (1000 concurrent) | Asyncio | I/O-bound, need high concurrency |</div>
<div class="line">File downloads (5 files) | Threading | I/O-bound, blocking library OK |</div>
<div class="line">WebSocket server (10000 clients) | Asyncio | I/O-bound, need extreme scalability |</div>
<div class="line">Scientific computation | Multiprocessing | CPU-intensive calculations |</div>
<div class="line">Real-time chat (1000 users) | Asyncio | I/O-bound, many idle connections |</div>
</div>
</section>
<section id="code-templates">
<h1>Code Templates<a class="headerlink" href="#code-templates" title="Link to this heading">ïƒ</a></h1>
<p><strong>CPU-bound Template</strong>:
.. code-block:: python</p>
<p>from concurrent.futures import ProcessPoolExecutor
import os</p>
<dl>
<dt>def cpu_intensive*task(data):</dt><dd><p># Your CPU-heavy computation here
result = complex_calculation(data)
return result</p>
</dd>
<dt>def main():</dt><dd><p>data_items = [â€¦]  # Your data</p>
<p># Use number of CPU cores
max_workers = os.cpu_count()</p>
<dl class="simple">
<dt>with ProcessPoolExecutor(max_workers=max_workers) as executor:</dt><dd><p>results = executor.map(cpu_intensive*task, data_items)</p>
</dd>
</dl>
<p>return list(results)</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p><strong>I/O-bound Template (Few operations, blocking library)</strong>:
.. code-block:: python</p>
<p>from concurrent.futures import ThreadPoolExecutor
import requests</p>
<dl>
<dt>def io_intensive*task(url):</dt><dd><p>response = requests.get(url)
return process(response)</p>
</dd>
<dt>def main():</dt><dd><p>urls = [â€¦]  # Your URLs</p>
<dl class="simple">
<dt>with ThreadPoolExecutor(max_workers=10) as executor:</dt><dd><p>results = executor.map(io_intensive*task, urls)</p>
</dd>
</dl>
<p>return list(results)</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p><strong>I/O-bound Template (Many operations, async library)</strong>:
.. code-block:: python</p>
<p>import asyncio
import aiohttp</p>
<dl>
<dt>async def io_intensive*task(session, url):</dt><dd><dl class="simple">
<dt>async with session.get(url) as response:</dt><dd><p>data = await response.text()
return process(data)</p>
</dd>
</dl>
</dd>
<dt>async def main():</dt><dd><p>urls = [â€¦]  # Your URLs</p>
<dl class="simple">
<dt>async with aiohttp.ClientSession() as session:</dt><dd><p>tasks = [io_intensive*task(session, url) for url in urls]
results = await asyncio.gather(<a href="#id7"><span class="problematic" id="id8">*</span></a>tasks)</p>
</dd>
</dl>
<p>return results</p>
</dd>
<dt>if __name** == â€˜<strong>main</strong>â€™:</dt><dd><p>asyncio.run(main())</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Mixed</span> <span class="n">Workload</span> <span class="n">Template</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import asyncio
import aiohttp
from concurrent.futures import ProcessPoolExecutor</p>
<dl>
<dt>def cpu_intensive(data):</dt><dd><p># CPU-heavy work here
return complex_calculation(data)</p>
</dd>
<dt>async def mixed_task(session, url, executor):</dt><dd><p># I/O part (async)
async with session.get(url) as response:</p>
<blockquote>
<div><p>data = await response.json()</p>
</div></blockquote>
<p># CPU part (process pool)
loop = asyncio.get_event*loop()
result = await loop.run_in*executor(executor, cpu_intensive, data)</p>
<p>return result</p>
</dd>
<dt>async def main():</dt><dd><p>urls = [â€¦]</p>
<dl class="simple">
<dt>with ProcessPoolExecutor(max_workers=4) as executor:</dt><dd><dl class="simple">
<dt>async with aiohttp.ClientSession() as session:</dt><dd><p>tasks = [mixed_task(session, url, executor) for url in urls]
results = await asyncio.gather(<a href="#id9"><span class="problematic" id="id10">*</span></a>tasks)</p>
</dd>
</dl>
</dd>
</dl>
<p>return results</p>
</dd>
<dt>if <strong>name</strong> == â€˜<strong>main</strong>â€™:</dt><dd><p>asyncio.run(main())</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>â€”</p>
<p>Summary: The Core Principles</p>
</section>
<section id="the-gil-controls-everything">
<h1>1. The GIL Controls Everything<a class="headerlink" href="#the-gil-controls-everything" title="Link to this heading">ïƒ</a></h1>
<p>Pythonâ€™s GIL:</p>
<p>Therefore:</p>
</section>
<section id="resource-usage-matters">
<h1>2. Resource Usage Matters<a class="headerlink" href="#resource-usage-matters" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Operation Type  |  Resource Bottleneck  |  Solution  |  Why?
CPU-bound       |  CPU cycles          |  Multi-    |  Bypass GIL,</p>
<blockquote>
<div><div class="line-block">
<div class="line">(computation)       |  processing|  use all cores</div>
</div>
</div></blockquote>
<p>I/O-bound       |  Waiting for I/O     |  Asyncio/  |  GIL released,
(few ops)       |  (network, disk)     |  Threading |  work during wait
I/O-bound       |  Waiting for I/O     |  Asyncio   |  Lightweight,
(many ops)      |  + scalability       |            |  handles 1000s
.. code-block:: text</p>
</section>
<section id="trade-offs-are-real">
<h1>3. Trade-offs are Real<a class="headerlink" href="#trade-offs-are-real" title="Link to this heading">ïƒ</a></h1>
<p><strong>Multiprocessing</strong>:
- Pros: True parallelism, bypasses GIL
- Cons: Memory overhead, slow startup, complex IPC
- <strong>Use when</strong>: CPU-bound work benefits &gt; memory cost</p>
<p><strong>Threading</strong>:
- Pros: Lightweight, easy data sharing, fast startup
- Cons: No speedup for CPU work, race conditions possible
- <strong>Use when</strong>: I/O-bound with moderate concurrency</p>
<p><strong>Asyncio</strong>:
- Pros: Extremely lightweight, handles 100,000+ operations
- Cons: Requires async libraries, learning curve
- <strong>Use when</strong>: I/O-bound with high concurrency</p>
</section>
<section id="know-your-workload">
<h1>4. Know Your Workload<a class="headerlink" href="#know-your-workload" title="Link to this heading">ïƒ</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Profile your code first!
import time</p>
<dl>
<dt>def profile_task(task_func):</dt><dd><p># CPU time (actual processing)
cpu_start = time.process_time()
# Wall time (including waiting)
wall_start = time.perf_counter()</p>
<p>result = task_func()</p>
<p>cpu_time = time.process_time() - cpu_start
wall_time = time.perf_counter() - wall_start</p>
<dl class="simple">
<dt>if cpu_time / wall_time &gt; 0.8:</dt><dd><p>print(â€œCPU-bound -&gt; Use multiprocessingâ€)</p>
</dd>
<dt>else:</dt><dd><p>print(â€œI/O-bound -&gt; Use asyncio/threadingâ€)</p>
</dd>
</dl>
<p>return result</p>
</dd>
</dl>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>â€”</p>
<p>Final Thoughts</p>
<p>The choice between multiprocessing, threading, and asyncio isnâ€™t about which is â€œbetterâ€ - itâ€™s about matching the tool to the task:</p>
<ul class="simple">
<li><p><strong>Multiprocessing</strong>: Powerful but heavy. Use when you need true parallel CPU computation.</p></li>
<li><p><strong>Threading</strong>: Simple and effective for I/O. Use when blocking libraries are needed.</p></li>
<li><p><strong>Asyncio</strong>: Lightweight and scalable for I/O. Use when you need to handle many concurrent operations.</p></li>
</ul>
<p>Understanding the GIL and how Python interacts with the OS is key to making the right choice. Always profile your code, measure the results, and choose based on your specific requirements.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="semaphore_explained.html" class="btn btn-neutral float-left" title="Key Concept" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../gpu-concepts/gpu-fundamentals.html" class="btn btn-neutral float-right" title="Key Differences: CPU vs GPU" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Concurrent Programs.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>