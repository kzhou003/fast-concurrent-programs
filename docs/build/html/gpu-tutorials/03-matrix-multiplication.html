

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Overview &mdash; Fast Concurrent Programming Guide 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1aac1d93" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/sidebar-fix.js?v=6c2f6f50"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Overview" href="04-low-memory-dropout.html" />
    <link rel="prev" title="Overview" href="02-fused-softmax.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Fast Concurrent Programming Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">CPU Concurrency</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#parallelism">Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#visual-comparison">Visual Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#threading-concurrent-futures-threadpoolexecutor">Threading (concurrent.futures.ThreadPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#multiprocessing-concurrent-futures-processpoolexecutor">Multiprocessing (concurrent.futures.ProcessPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#when-to-use-what">When to Use What</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#what-is-the-gil">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-points">Key Points:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#impact-on-performance">Impact on Performance:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#observing-the-gil-from-script-06">Observing the GIL (from script 06):</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop">Event Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#how-it-works-from-script-07">How It Works (from script 07):</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop-lifecycle">Event Loop Lifecycle:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#modern-vs-old-patterns">Modern vs Old Patterns:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#what-are-coroutines">What are Coroutines?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#defining-coroutines">Defining Coroutines:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-features">Key Features:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#example-from-script-08">Example from Script 08:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#execution-flow">Execution Flow:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#important-rules">Important Rules:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#tasks">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#task-characteristics">Task Characteristics:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#example-from-script-09">Example from Script 09:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#futures">Futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#waiting-for-multiple-tasks">Waiting for Multiple Tasks:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#comparison-table">Comparison Table:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#hybrid-workloads">Hybrid Workloads:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#time-measurement-time-clock-time-perf-counter">1. Time Measurement (<code class="docutils literal notranslate"><span class="pre">time.clock()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#coroutine-syntax-asyncio-coroutine-async-def">2. Coroutine Syntax (<code class="docutils literal notranslate"><span class="pre">&#64;asyncio.coroutine</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#task-creation-asyncio-task-asyncio-create-task">3. Task Creation (<code class="docutils literal notranslate"><span class="pre">asyncio.Task()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">asyncio.create_task()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop-management">4. Event Loop Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#future-callbacks-callbacks-await">5. Future Callbacks (Callbacks -&gt; <code class="docutils literal notranslate"><span class="pre">await</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#blocking-calls-in-async-code">6. Blocking Calls in Async Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#string-formatting-f-strings">7. String Formatting (<code class="docutils literal notranslate"><span class="pre">%</span></code> -&gt; f-strings)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#migration-checklist">Migration Checklist</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#compatibility">Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#quick-reference-guide">Quick Reference Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#further-reading">Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html">Physical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#logical-cores">Logical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#the-concept">The Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#how-it-works">How It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#technical-implementation">Technical Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#hyperthreading-limitations">Hyperthreading Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#checking-hyperthreading-status">Checking Hyperthreading Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#the-fundamental-constraint">The Fundamental Constraint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#optimal-worker-count">Optimal Worker Count</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#real-world-example">Real-World Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#cpu-vs-gpu-different-design-philosophies">CPU vs GPU: Different Design Philosophies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#key-differences">Key Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#simd-and-gpu-architecture">SIMD and GPU Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#why-gpus-excel-at-compute-intensive-tasks">Why GPUs Excel at Compute-Intensive Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#what-gpus-are-good-at">What GPUs Are Good At</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#silicon-real-estate-comparison">Silicon Real Estate Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#detailed-benchmark-results">Detailed Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#decision-matrix">Decision Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#practical-guidelines">Practical Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-1-image-processing">Example 1: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-2-monte-carlo-simulation">Example 2: Monte Carlo Simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-3-neural-network-training">Example 3: Neural Network Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#core-principles">Core Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html">Key Points About start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#example">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#key-points-about-join">Key Points About join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#id1">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#without-join-danger">WITHOUT join() - DANGER!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#with-join-correct">WITH join() - CORRECT!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-1-calling-the-function-directly-instead-of-start">Mistake 1: Calling the function directly instead of start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-2-forgetting-join">Mistake 2: Forgetting join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-3-thinking-threads-share-data-automatically">Mistake 3: Thinking threads share data automatically</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_event_loop.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_event_loop.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html#use-cases">Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_and_futures.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_and_futures.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_task_manipulation.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_task_manipulation.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/concurrent_futures_pooling.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/concurrent_futures_pooling.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#internal-structure">Internal Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#automatic-locking">Automatic Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#put-item"><code class="docutils literal notranslate"><span class="pre">put(item)</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#problem-with-manual-locks">Problem with Manual Locks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#how-queue-does-locking">How Queue Does Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#locking-benefits">Locking Benefits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#execution-flow">Execution Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#thread-safe-data-structure">1. <strong>Thread-Safe Data Structure</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#blocks-correctly">2. <strong>Blocks Correctly</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#no-busy-waiting">3. <strong>No Busy-Waiting</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#task-tracking">4. <strong>Task Tracking</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#safe-for-multiple-producers-consumers">5. <strong>Safe for Multiple Producers/Consumers</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#step-by-step-what-happens-in-put">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">put()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#step-by-step-what-happens-in-get">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">get()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-1-forgetting-lock">Mistake 1: Forgetting Lock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-2-busy-waiting">Mistake 2: Busy-Waiting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-3-race-condition">Mistake 3: Race Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_internal_mechanics.html">The Answer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_internal_mechanics.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html">Without task_done() - Canâ€™t Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#internal-counter-system">Internal Counter System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#visual-timeline">Visual Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#code-simplified">Code (Simplified)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#two-operations">Two Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#timeline-all-three-conditions">Timeline: All Three Conditions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#put">put()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#get">get()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#task-done">task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#join">join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#scenario-1-producer-1-consumer">Scenario: 1 Producer, 1 Consumer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-1-put-increment-counter">Step 1: put() - Increment Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#diagram-tracking-one-task">Diagram: Tracking One Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#diagram-multiple-tasks">Diagram: Multiple Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#counter">Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#condition-variable">Condition Variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#together">Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#without-all-tasks-done-condition">Without all_tasks*done Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#queue-join-without-task-done">Queue.join() Without task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-it-blocks-forever">Why It Blocks Forever</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#queue-join-with-task-done">Queue.join() With task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-it-works">Why It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-1-put-item">Step 1: Put Item</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-2-get-and-process">Step 2: Get and Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-3-mark-done">Step 3: Mark Done</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#flow-diagram">Flow Diagram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#without-task-done-problematic">Without task_done() - PROBLEMATIC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-1-verify-all-work-complete">Use Case 1: Verify All Work Complete</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-2-track-progress">Use Case 2: Track Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-3-batch-processing">Use Case 3: Batch Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#scenario-main-thread-needs-to-know-when-workers-finish">Scenario: Main thread needs to know when workers finish</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#timeline">Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#better-code-pattern">Better Code Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-we-need-task-done">Why We Need task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#the-pattern">The Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html">Regular Lock vs RLock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#why-rlock-is-needed-here">Why RLock is Needed Here</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#use-regular-lock-when">Use Regular Lock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#use-rlock-when">Use RLock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#regular-lock-would-deadlock">Regular Lock - Would Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#counting-semaphore-counter-1">1. Counting Semaphore (Counter &gt; 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#binary-semaphore-counter-0-or-1">2. Binary Semaphore (Counter = 0 or 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#execution-timeline">Execution Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#acquire"><code class="docutils literal notranslate"><span class="pre">acquire()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#counting-semaphore-3-spots-available">Counting Semaphore (3 spots available)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#example-1-swimming-pool-with-limited-capacity">Example 1: Swimming Pool with Limited Capacity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#limiting-concurrent-access">1. <strong>Limiting Concurrent Access</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#synchronizing-multiple-threads">3. <strong>Synchronizing Multiple Threads</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#lock-threading-lock">Lock (<code class="docutils literal notranslate"><span class="pre">threading.Lock</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#semaphore-counting">Semaphore (Counting)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-does-python-have-a-gil">Why Does Python Have a GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-the-gil-works">How the GIL Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#gil-behavior-with-different-operations">GIL Behavior with Different Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-critical-difference">The Critical Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#real-world-analogy">Real-World Analogy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-with-threading-for-cpu-bound">The Problem with Threading for CPU-bound</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-solution-multiprocessing">The Solution: Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-multiprocessing-bypasses-the-gil">How Multiprocessing Bypasses the GIL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#trade-offs-of-multiprocessing">Trade-offs of Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#when-the-trade-off-is-worth-it">When the Trade-off is Worth It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-wasted-time">The Problem: Wasted Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-solution-threading">The Solution: Threading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-threading-works-for-i-o">Why Threading Works for I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-the-os-helps">How the OS Helps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-not-multiprocessing-for-i-o">Why Not Multiprocessing for I/O?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-with-threading-overhead">The Problem with Threading: Overhead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#asyncio-cooperative-multitasking">Asyncio: Cooperative Multitasking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-asyncio-works">How Asyncio Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#event-loop-visualization">Event Loop Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#asyncio-vs-threading-detailed-comparison">Asyncio vs Threading: Detailed Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#when-asyncio-shines">When Asyncio Shines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#cpu-bound-with-threading-the-gil-dance">CPU-bound with Threading: The GIL Dance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-cpu-bound-task-computing-pi">Benchmark: CPU-bound Task (Computing pi)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-i-o-bound-task-web-requests">Benchmark: I/O-bound Task (Web Requests)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-mixed-workload">Benchmark: Mixed Workload</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#quick-reference-table">Quick Reference Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#code-templates">Code Templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-gil-controls-everything">1. The GIL Controls Everything</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#resource-usage-matters">2. Resource Usage Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#know-your-workload">4. Know Your Workload</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html">Key Differences: CPU vs GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#streaming-multiprocessors-sms">Streaming Multiprocessors (SMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#thread-organization">Thread Organization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#example-visualization">Example Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#l2-cache">L2 Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#registers">Registers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html">Warps and SIMD Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#thread-divergence">Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#what-is-occupancy">What is Occupancy?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#factors-limiting-occupancy">Factors Limiting Occupancy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#example-calculation">Example Calculation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#why-occupancy-matters">Why Occupancy Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#the-occupancy-sweet-spot">The Occupancy Sweet Spot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#grid-and-block-dimensions">Grid and Block Dimensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#choosing-block-size">Choosing Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#within-a-block">Within a Block</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#between-blocks">Between Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-level-reductions">Warp-Level Reductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#overlap-compute-and-memory">Overlap Compute and Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#traditional-approach">Traditional Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#persistent-approach">Persistent Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#key-factors">Key Factors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#profiling-tools">Profiling Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html">Step 1: Profile First</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#step-2-identify-bottleneck">Step 2: Identify Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-kernel-fusion">Strategy 1: Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tiling">Strategy 2: Tiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-vectorized-loads">Strategy 3: Vectorized Loads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-memory-coalescing">Strategy 4: Memory Coalescing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-use-tensor-cores">Strategy 1: Use Tensor Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-increase-arithmetic-intensity">Strategy 2: Increase Arithmetic Intensity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-minimize-thread-divergence">Strategy 3: Minimize Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-optimize-loop-structure">Strategy 4: Optimize Loop Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-reduce-register-usage">Strategy 1: Reduce Register Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tune-shared-memory">Strategy 2: Tune Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-adjust-block-size">Strategy 3: Adjust Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#why-auto-tune">Why Auto-Tune?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#triton-auto-tuning">Triton Auto-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#persistent-kernels">Persistent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#recomputation">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-reduction">Pattern: Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-element-wise">Pattern: Element-wise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-matrix-multiply">Pattern: Matrix Multiply</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-bandwidth">Issue: Low Bandwidth</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-compute-utilization">Issue: Low Compute Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-lower-than-pytorch">Issue: Lower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#before-you-optimize">Before You Optimize</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#memory-optimizations">Memory Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#compute-optimizations">Compute Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#occupancy-optimization">Occupancy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#advanced">Advanced</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/triton-concepts.html">Triton Concepts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="01-vector-add.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-vector-add.html#next-steps">Next Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-fused-softmax.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-fused-softmax.html#extensions">Extensions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-low-memory-dropout.html">Low Memory Dropout</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-layer-norm.html">Layer Norm</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-fused-attention.html">Fused Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-extern-functions.html">Extern Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-grouped-gemm.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-grouped-gemm.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-persistent-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-persistent-matmul.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-block-scaled-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-block-scaled-matmul.html#summary">Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton Compiler</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-1-python-ast-parsing">Stage 1: Python AST Parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-2-code-generation-ttir">Stage 2: Code Generation (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-3-triton-gpu-ir-ttgir">Stage 3: Triton GPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-4-llvm-ir">Stage 4: LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#block-based-programming-model">Block-based Programming Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#jit-compilation">JIT Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#mlir-infrastructure">MLIR Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#python-components">Python Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#c-components">C++ Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#backend-components">Backend Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html">CodeGenerator Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ast-visitor-pattern">AST Visitor Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#type-inference">Type Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#compilation-orchestration">Compilation Orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ttgir-llvm-ir">TTGIR -&gt; LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#llvm-ir-ptx">LLVM IR -&gt; PTX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-key-components">Cache Key Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-directory-structure">Cache Directory Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html">The NVCC Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#the-llvm-path">The LLVM Path</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#llvm-ir-stage">LLVM IR Stage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-generated-by-triton">PTX Generated by Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#same-tools-same-artifacts">Same Tools, Same Artifacts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#source-language">Source Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compiler-stack">Compiler Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-time">Compilation Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#can-triton-and-cuda-c-work-together">Can Triton and CUDA C++ Work Together?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#advantages-of-llvm-backend">Advantages of LLVM Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#why-not-use-nvcc">Why Not Use NVCC?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#trade-offs">Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#file-types">File Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#example-directory-structures">Example Directory Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-inspection">PTX Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-paths-compared">Compilation Paths Compared</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html">Traditional Compiler Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#example-matrix-multiplication">Example: Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#mlir-philosophy">MLIR Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#dialects">1. Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-triton-uses-mlir">Why Triton Uses MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-s-mlir-dialects">Tritonâ€™s MLIR Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#python-source">Python Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-2-tritongpu-ir-ttgir">Stage 2: TritonGPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-3-llvm-dialect">Stage 3: LLVM Dialect</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-4-llvm-ir-actual">Stage 4: LLVM IR (Actual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#command-line-tools">Command-Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#debugging-mlir">Debugging MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#official-documentation">Official Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tutorials">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-specific">Triton-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#key-concepts-recap">Key Concepts Recap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-mlir-matters-for-triton">Why MLIR Matters for Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#the-big-picture">The Big Picture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../learning-paths.html">Learning Paths</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html">CUDA Out of Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#out-of-shared-memory">Out of Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-results">Wrong Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nan-or-inf-values">NaN or Inf Values</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slower-than-pytorch">Slower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#low-gpu-utilization">Low GPU Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#compilation-errors">Compilation Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slow-compilation">Slow Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nvidia-specific">NVIDIA-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#amd-specific">AMD-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-gpu-selected">Wrong GPU Selected</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#print-debugging">Print Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#profiling">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#assertions">Assertions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#unit-testing">Unit Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#when-stuck">When Stuck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#cuda">CUDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#rocm-amd">ROCm (AMD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#flash-attention">Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#normalization">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#optimization-techniques">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#id1">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#gpu-programming">GPU Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#deep-learning">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-tools">NVIDIA Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-tools">AMD Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#tutorials-and-courses">Tutorials and Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#community">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#blogs-and-articles">Blogs and Articles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#triton-examples">Triton Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#production-usage">Production Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-gpus">NVIDIA GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-gpus">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#subscribe-to">Subscribe To</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#conferences">Conferences</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Fast Concurrent Programming Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/gpu-tutorials/03-matrix-multiplication.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Matrix Multiplication in Triton</p>
<section id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Link to this heading">ïƒ</a></h1>
<p>Matrix multiplication (GEMM: General Matrix Multiply) is the cornerstone of deep learning. This tutorial shows how to write a high-performance matmul kernel that rivals cuBLAS/rocBLAS. Youâ€™ll learn advanced GPU optimization techniques including <strong>tiling</strong>, <strong>cache optimization</strong>, and <strong>auto-tuning</strong>.</p>
<p>What Youâ€™ll Learn
- <strong>Block-level tiling</strong> for matrix multiplication
- <strong>Multi-dimensional pointer arithmetic</strong>
- <strong>L2 cache optimization</strong> through program reordering
- <strong>Auto-tuning</strong> for performance optimization
- Why matmul is <strong>compute-bound</strong> (not memory-bound)
- <strong>Tensor Cores</strong> and specialized hardware</p>
<p>The Matrix Multiplication Problem</p>
<section id="basic-algorithm">
<h2>Basic Algorithm<a class="headerlink" href="#basic-algorithm" title="Link to this heading">ïƒ</a></h2>
<p>Compute C = A x B where:
- A has shape (M, K)
- B has shape (K, N)
- C has shape (M, N)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>
</pre></div>
</div>
<p>Time complexity: <strong>O(M x N x K)</strong>
For 1024x1024 matrices: ~1 billion operations!</p>
</section>
<section id="why-it-s-hard-to-optimize">
<h2>Why Itâ€™s Hard to Optimize<a class="headerlink" href="#why-it-s-hard-to-optimize" title="Link to this heading">ïƒ</a></h2>
<p><strong>Naive implementation problems</strong>:
1. Poor memory locality
2. Inefficient use of cache
3. Not utilizing GPU parallelism
4. Missing out on specialized hardware (Tensor Cores)</p>
<p><strong>Our goal</strong>: Achieve 10+ TFLOPS (Tera-FLOPs, trillions of operations per second)!</p>
<p>GPU Matrix Multiplication Strategy</p>
</section>
<section id="blocked-algorithm">
<h2>Blocked Algorithm<a class="headerlink" href="#blocked-algorithm" title="Link to this heading">ïƒ</a></h2>
<p>Instead of computing one element at a time, we compute <strong>blocks</strong> of C:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>for m in range(0, M, BLOCK_M):           # Parallel on GPU</dt><dd><dl>
<dt>for n in range(0, N, BLOCK_N):       # Parallel on GPU</dt><dd><p>accumulator = zeros(BLOCK_M, BLOCK_N)
for k in range(0, K, BLOCK_K):   # Sequential within each block</p>
<blockquote>
<div><p>A_block = load A[m:m+BLOCK_M, k:k+BLOCK_K]
B_block = load B[k:k+BLOCK_K, n:n+BLOCK_N]
accumulator += dot(A_block, B_block)</p>
</div></blockquote>
<p>store accumulator to C[m:m+BLOCK_M, n:n+BLOCK_N]</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Key</span> <span class="n">insight</span><span class="o">**</span><span class="p">:</span> <span class="n">The</span> <span class="n">outer</span> <span class="n">two</span> <span class="n">loops</span> <span class="n">are</span> <span class="n">parallelized</span> <span class="n">across</span> <span class="n">GPU</span> <span class="n">programs</span><span class="p">,</span> <span class="k">while</span> <span class="n">the</span> <span class="n">K</span> <span class="n">loop</span> <span class="ow">is</span> <span class="n">sequential</span> <span class="n">within</span> <span class="n">each</span> <span class="n">program</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="why-tiling-works">
<h2>Why Tiling Works<a class="headerlink" href="#why-tiling-works" title="Link to this heading">ïƒ</a></h2>
<p><strong>Memory hierarchy</strong>:</p>
<dl class="simple">
<dt>Registers (fastest, ~1 cycle)</dt><dd><p>down</p>
</dd>
<dt>L1/Shared Memory (~10 cycles, 192 KB)</dt><dd><p>down</p>
</dd>
<dt>L2 Cache (~100 cycles, 40 MB)</dt><dd><p>down</p>
</dd>
</dl>
<p>HBM/Global Memory (slowest, ~400 cycles, 40-80 GB)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">With</span> <span class="n">tiling</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Load A_block into shared memory (reused BLOCK_N times)</p></li>
<li><p>Load B_block into shared memory (reused BLOCK_M times)</p></li>
<li><p>Accumulate into registers (reused K/BLOCK_K times)</p></li>
<li><p>Write result once to global memory</p></li>
</ol>
<p><strong>Data reuse</strong> = fewer slow memory accesses!</p>
<p>Pointer Arithmetic in 2D</p>
</section>
<section id="understanding-strides">
<h2>Understanding Strides<a class="headerlink" href="#understanding-strides" title="Link to this heading">ïƒ</a></h2>
<p>For a row-major matrix A[M, K]:
.. code-block:: python</p>
<p>Where:
- <code class="docutils literal notranslate"><span class="pre">stride_M</span></code> = number of elements to next row = K
- <code class="docutils literal notranslate"><span class="pre">stride_K</span></code> = number of elements to next column = 1</p>
<p>Example for A[4, 3]:</p>
<p>Elements in memory: [a00, a01, a02, a10, a11, a12, a20, a21, a22, a30, a31, a32]
A[2, 1] = A_ptr + 2*3 + 1*1 = A_ptr[7] = a21 [OK]</p>
</section>
<section id="block-pointers">
<h2>Block Pointers<a class="headerlink" href="#block-pointers" title="Link to this heading">ïƒ</a></h2>
<p>To get pointers to a block A[m:m+BLOCK_M, k:k+BLOCK_K]:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>offs_k = tl.arange(0, BLOCK_K)
a_ptrs = a_ptr + offs_am[:, None]*stride_am + offs_k[None, :]*stride_ak</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Broadcasting</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">offs_am[:,</span> <span class="pre">None]</span></code> has shape (BLOCK_M, 1)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">offs_k[None,</span> <span class="pre">:]</span></code> has shape (1, BLOCK_K)</p></li>
<li><p>Result <code class="docutils literal notranslate"><span class="pre">a_ptrs</span></code> has shape (BLOCK_M, BLOCK_K)</p></li>
</ul>
<p><strong>Example</strong> (BLOCK_M=2, BLOCK_K=2, pid_m=0):
.. code-block:: python</p>
<p>offs_k = [0, 1]          # shape (2,)</p>
<dl class="simple">
<dt>offs_am[:, None] = [[0],   # shape (2, 1)</dt><dd><p>[1]]</p>
</dd>
</dl>
<p>offs_k[None, :] = [[0, 1]]  # shape (1, 2)</p>
<p>Broadcasting multiplication:
offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak
= [[0*stride_am + 0*stride_ak, 0*stride_am + 1*stride_ak],</p>
<blockquote>
<div><p>[1*stride_am + 0*stride_ak, 1*stride_am + 1*stride_ak]]</p>
</div></blockquote>
<p>This gives pointers to a 2x2 block!</p>
</section>
<section id="advancing-pointers">
<h2>Advancing Pointers<a class="headerlink" href="#advancing-pointers" title="Link to this heading">ïƒ</a></h2>
<p>To move to next K block:
.. code-block:: python</p>
<p>b_ptrs += BLOCK_K * stride_bk</p>
<p>This shifts all pointers in the block by BLOCK_K positions in the K dimension.</p>
<p>L2 Cache Optimization</p>
</section>
<section id="the-problem-with-row-major-ordering">
<h2>The Problem with Row-Major Ordering<a class="headerlink" href="#the-problem-with-row-major-ordering" title="Link to this heading">ïƒ</a></h2>
<p>If we process blocks in simple row-major order:</p>
<p>Program 0 -&gt; Block C[0, 0]  (needs A[0, :] and B[:, 0])
Program 1 -&gt; Block C[0, 1]  (needs A[0, :] and B[:, 1])
â€¦
Program 9 -&gt; Block C[1, 0]  (needs A[1, :] and B[:, 0])</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>**Issue**: By the time we compute C[1, 0], B[:, 0] might be evicted from L2 cache!
</pre></div>
</div>
</section>
<section id="grouped-ordering-swizzling">
<h2>Grouped Ordering (Swizzling)<a class="headerlink" href="#grouped-ordering-swizzling" title="Link to this heading">ïƒ</a></h2>
<p>Instead, process blocks in groups:
.. code-block:: python</p>
<p>group_id = pid // num_pid*in_group
first_pid*m = group_id * GROUP_SIZE*M
group_size*m = min(num_pid*m - first_pid*m, GROUP_SIZE*M)
pid_m = first_pid*m + ((pid % num_pid*in_group) % group_size*m)
pid_n = (pid % num_pid*in_group) // group_size*m</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Visual</span> <span class="n">Example</span><span class="o">**</span> <span class="p">(</span><span class="n">GROUP_SIZE</span><span class="o">*</span><span class="n">M</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="n">x9</span> <span class="n">blocks</span><span class="p">):</span>
</pre></div>
</div>
<p>Row-major order:</p>
<p>0-&gt; 1-&gt; 2-&gt; 3-&gt; 4-&gt; 5-&gt; 6-&gt; 7-&gt; 8-&gt;
9-&gt; 10-&gt; â€¦</p>
<p>Needs to load: 90 unique blocks into SRAM</p>
<p>Grouped order:</p>
<p>0down 3down 6down 1down 4down 7down 2down 5down 8down
9down 12down 15down 10down 13down 16down 11down 14down 17down
â€¦</p>
<p>Needs to load: 54 unique blocks into SRAM</p>
<p><strong>Savings</strong>: 40% fewer loads from HBM! This can improve performance by 10-20%.</p>
<p>Auto-Tuning</p>
</section>
<section id="the-configuration-space">
<h2>The Configuration Space<a class="headerlink" href="#the-configuration-space" title="Link to this heading">ïƒ</a></h2>
<p>Many parameters affect performance:
- <code class="docutils literal notranslate"><span class="pre">BLOCK_SIZE*M</span></code>: 32, 64, 128, 256
- <code class="docutils literal notranslate"><span class="pre">BLOCK_SIZE*N</span></code>: 32, 64, 128, 256
- <code class="docutils literal notranslate"><span class="pre">BLOCK_SIZE*K</span></code>: 16, 32, 64, 128
- <code class="docutils literal notranslate"><span class="pre">GROUP_SIZE*M</span></code>: 4, 8, 16
- <code class="docutils literal notranslate"><span class="pre">num_warps</span></code>: 2, 4, 8, 16
- <code class="docutils literal notranslate"><span class="pre">num_stages</span></code>: 2, 3, 4, 5</p>
<p>Total combinations: ~1000+</p>
<p><strong>Problem</strong>: Optimal configuration depends on:
- Matrix sizes (M, N, K)
- GPU architecture (compute capability, SRAM size, etc.)
- Data types (fp16, fp32, int8, etc.)</p>
</section>
<section id="triton-s-auto-tuner">
<h2>Tritonâ€™s Auto-Tuner<a class="headerlink" href="#triton-s-auto-tuner" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">configs</span><span class="o">=</span><span class="n">get_autotune</span><span class="o">*</span><span class="n">config</span><span class="p">(),</span>
<span class="n">key</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;K&#39;</span><span class="p">],</span>
</pre></div>
</div>
<p>)
&#64;triton.jit
def matmul_kernel(â€¦):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">How</span> <span class="n">it</span> <span class="n">works</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Define list of candidate configurations</p></li>
<li><p>Specify key parameters (M, N, K)</p></li>
<li><p>At runtime, Triton:
- Tries each configuration
- Measures performance
- Caches best config for this (M, N, K)
- Uses cached result for future calls</p></li>
</ol>
<p><strong>Example config</strong>:
.. code-block:: python</p>
<blockquote>
<div><p>{â€˜BLOCK_SIZE*Mâ€™: 128, â€˜BLOCK_SIZE*Nâ€™: 256, â€˜BLOCK_SIZE*Kâ€™: 64, â€˜GROUP_SIZE*Mâ€™: 8},
num_stages=3,
num_warps=8</p>
</div></blockquote>
<section id="id1">
<h3>)<a class="headerlink" href="#id1" title="Link to this heading">ïƒ</a></h3>
</section>
</section>
<section id="good-configurations">
<h2>Good Configurations<a class="headerlink" href="#good-configurations" title="Link to this heading">ïƒ</a></h2>
<p><strong>For NVIDIA (CUDA)</strong>:
- Large blocks (128x256) for large matrices
- More stages (3-4) to hide latency
- 8 warps for good occupancy</p>
<p><strong>For AMD (HIP)</strong>:
- Medium blocks (64x64, 128x128)
- Fewer stages (2) due to different architecture
- Different thread group sizes</p>
<p><strong>FP8 (8-bit floating point)</strong>:
- Even larger blocks (256x256)
- Can fit more data in SRAM
- Tensor Cores process 4x more data per cycle</p>
<p>The Kernel Implementation</p>
</section>
<section id="step-1-compute-program-ids">
<h2>Step 1: Compute Program IDs<a class="headerlink" href="#step-1-compute-program-ids" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>num_pid*m = tl.cdiv(M, BLOCK_SIZE*M)
num_pid*n = tl.cdiv(N, BLOCK_SIZE*N)</p>
<p>Map linear program ID to 2D (pid_m, pid_n) using grouped ordering.</p>
</section>
<section id="step-2-initialize-pointers">
<h2>Step 2: Initialize Pointers<a class="headerlink" href="#step-2-initialize-pointers" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>offs_bn = (pid_n * BLOCK_SIZE*N + tl.arange(0, BLOCK_SIZE*N)) % N
offs_k = tl.arange(0, BLOCK_SIZE*K)</p>
<p>a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k[None, :]*stride_ak)
b_ptrs = b_ptr + (offs_k[:, None]*stride_bk + offs_bn[None, :]*stride_bn)</p>
<p>Create pointers for first blocks of A and B.</p>
<p><strong>Note the modulo</strong>:
- <code class="docutils literal notranslate"><span class="pre">offs_am</span> <span class="pre">%</span> <span class="pre">M</span></code> handles M not being multiple of BLOCK_SIZE*M
- Wraps around, so we load valid (though repeated) data
- Doesnâ€™t matter because we mask the output store</p>
</section>
<section id="step-3-accumulation-loop">
<h2>Step 3: Accumulation Loop<a class="headerlink" href="#step-3-accumulation-loop" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>for k in range(0, tl.cdiv(K, BLOCK_SIZE*K)):</dt><dd><p>a = tl.load(a_ptrs, mask=offs_k[None, :] &lt; K - k_BLOCK*SIZE_K, other=0.0)
b = tl.load(b_ptrs, mask=offs_k[:, None] &lt; K - k_BLOCK*SIZE_K, other=0.0)</p>
<p>accumulator = tl.dot(a, b, accumulator)</p>
<p>a_ptrs += BLOCK_SIZE*K * stride_ak
b_ptrs += BLOCK_SIZE*K * stride_bk</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Key</span> <span class="n">points</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Accumulate in <code class="docutils literal notranslate"><span class="pre">float32</span></code> for numerical accuracy</p></li>
<li><p>Mask loads when K not multiple of BLOCK_SIZE*K</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tl.dot()</span></code> uses hardware accelerators (Tensor Cores on NVIDIA)</p></li>
</ul>
</section>
<section id="step-4-apply-activation-optional">
<h2>Step 4: Apply Activation (Optional)<a class="headerlink" href="#step-4-apply-activation-optional" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">accumulator</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">accumulator</span><span class="p">)</span>
</pre></div>
</div>
<p>c = accumulator.to(tl.float16)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>**Kernel fusion**: Apply activation while data is in registers (fast)!
</pre></div>
</div>
</section>
<section id="step-5-store-result">
<h2>Step 5: Store Result<a class="headerlink" href="#step-5-store-result" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>offs_cn = pid_n * BLOCK_SIZE*N + tl.arange(0, BLOCK_SIZE*N)
c_ptrs = c_ptr + stride_cm*offs_cm[:, None] + stride_cn*offs_cn[None, :]
c_mask = (offs_cm[:, None] &lt; M) &amp; (offs_cn[None, :] &lt; N)
tl.store(c_ptrs, c, mask=c_mask)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Masking</span><span class="o">**</span> <span class="n">ensures</span> <span class="n">we</span> <span class="n">don</span><span class="s1">&#39;t write out of bounds.</span>
</pre></div>
</div>
<p>Tensor Cores</p>
</section>
<section id="what-are-tensor-cores">
<h2>What Are Tensor Cores?<a class="headerlink" href="#what-are-tensor-cores" title="Link to this heading">ïƒ</a></h2>
<p>Special hardware units on modern GPUs for matrix multiplication:
- <strong>NVIDIA</strong>: Tensor Cores (Volta, Turing, Ampere, Hopper)
- <strong>AMD</strong>: Matrix Cores (CDNA2, CDNA3)</p>
<p><strong>Performance</strong>:
- Regular CUDA cores: 1 FP16 multiply-add per cycle per core
- Tensor Cores: 64-256 FP16 multiply-adds per cycle per core
- <strong>Speedup</strong>: 10-100x for matmul!</p>
</section>
<section id="how-tensor-cores-work">
<h2>How Tensor Cores Work<a class="headerlink" href="#how-tensor-cores-work" title="Link to this heading">ïƒ</a></h2>
<p>Operate on small matrices (e.g., 16x16):</p>
<p>D = A x B + C</p>
<p>Where A, B, C, D are 16x16 matrices.</p>
<p><strong>Single instruction</strong>, massive computation!</p>
</section>
<section id="triton-and-tensor-cores">
<h2>Triton and Tensor Cores<a class="headerlink" href="#triton-and-tensor-cores" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>When you use <code class="docutils literal notranslate"><span class="pre">tl.dot()</span></code> with appropriate types and sizes, Triton automatically:
1. Detects Tensor Core availability
2. Arranges data in Tensor Core format
3. Emits Tensor Core instructions (e.g., <code class="docutils literal notranslate"><span class="pre">mma.sync</span></code>)</p>
<p><strong>Requirements for Tensor Cores</strong>:
- FP16, BF16, TF32, FP8, or INT8 inputs
- Block sizes that are multiples of Tensor Core dimensions (usually 16)</p>
<p>Performance Analysis</p>
</section>
<section id="arithmetic-intensity">
<h2>Arithmetic Intensity<a class="headerlink" href="#arithmetic-intensity" title="Link to this heading">ïƒ</a></h2>
<p>For matmul C = A x B:
- <strong>FLOPs</strong>: 2MNK (each output element: K multiplies + K adds)
- <strong>Memory</strong>: 2(MK + KN + MN) bytes (read A, B, write C)</p>
<p>Arithmetic Intensity = 2MNK / (2(MK + KN + MN))</p>
<p>For square matrices (M=N=K):</p>
<p>AI = 2N^3 / (4N^2) = N/2</p>
<p>For N=1024: AI = 512 FLOPs/byte</p>
<p><strong>This is very high!</strong> Matmul is <strong>compute-bound</strong>, not memory-bound.</p>
</section>
<section id="roofline-model">
<h2>Roofline Model<a class="headerlink" href="#roofline-model" title="Link to this heading">ïƒ</a></h2>
<p>Performance is limited by:</p>
<p>min(Peak_Compute, Peak_Memory*BW * AI)</p>
<p>For A100:
- Peak FP16 Tensor Core: 312 TFLOPS
- Peak Memory BW: 2 TB/s
- For N=1024: 2 TB/s * 512 = 1024 TFLOPS</p>
<p><strong>Bottleneck</strong>: Compute (312 TFLOPS), not memory!</p>
<p>For small matrices (N=64): AI = 32
- Memory bound: 2 TB/s * 32 = 64 TFLOPS
- Can achieve much less than peak compute</p>
</section>
<section id="expected-performance">
<h2>Expected Performance<a class="headerlink" href="#expected-performance" title="Link to this heading">ïƒ</a></h2>
<p><strong>Theoretical Peak</strong> (A100, FP16):
- 312 TFLOPS with Tensor Cores</p>
<p><strong>Achievable</strong>:
- cuBLAS: ~280-300 TFLOPS (90-95% peak)
- Triton (optimized): ~250-280 TFLOPS (80-90% peak)
- Naive implementation: ~10-50 TFLOPS (&lt;20% peak)</p>
<p><strong>Why not 100%?</strong>
- Launch overhead
- Imperfect tiling (boundaries)
- L2 cache misses
- Pipeline stalls</p>
<p>Advanced Optimizations</p>
</section>
<section id="software-pipelining">
<h2>Software Pipelining<a class="headerlink" href="#software-pipelining" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Idea</span><span class="o">**</span><span class="p">:</span> <span class="n">Overlap</span> <span class="n">memory</span> <span class="n">loads</span> <span class="k">with</span> <span class="n">computation</span>
</pre></div>
</div>
<p>Without pipelining:</p>
<p>Load A, B -&gt; Wait -&gt; Compute -&gt; Load A, B -&gt; Wait -&gt; Compute</p>
<p>With 3-stage pipelining:</p>
<p>Stage 0: Load A_0, B_0
Stage 1: Load A_1, B_1 | Compute with A_0, B_0
Stage 2: Load A_2, B_2 | Compute with A_1, B_1
Stage 3: Load A_3, B_3 | Compute with A_2, B_2
â€¦</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>**Benefit**: Computation and memory loads happen simultaneously!
</pre></div>
</div>
<p><strong>Cost</strong>: Need more registers and SRAM to hold multiple stages.</p>
</section>
<section id="loop-unrolling">
<h2>Loop Unrolling<a class="headerlink" href="#loop-unrolling" title="Link to this heading">ïƒ</a></h2>
<p>Triton automatically unrolls the K loop when possible:
.. code-block:: python</p>
<blockquote>
<div><p>accumulator = tl.dot(a, b, accumulator)</p>
</div></blockquote>
<p>Becomes (if K/BLOCK_K = 4):
.. code-block:: python</p>
<p>accumulator = tl.dot(a1, b1, accumulator)
accumulator = tl.dot(a2, b2, accumulator)
accumulator = tl.dot(a3, b3, accumulator)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Benefits</span><span class="o">**</span><span class="p">:</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Eliminates loop overhead</p></li>
<li><p>Better instruction-level parallelism</p></li>
<li><p>Easier for compiler to optimize</p></li>
</ul>
</section>
<section id="register-pressure">
<h2>Register Pressure<a class="headerlink" href="#register-pressure" title="Link to this heading">ïƒ</a></h2>
<p>Each thread needs registers for:
- A block elements
- B block elements
- Accumulator elements
- Temporary variables</p>
<p><strong>Example</strong> (BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, 8 warps):
- Threads per block: 8 * 32 = 256
- Elements per thread: (128*128) / 256 = 64 for accumulator
- Registers needed: ~100-150 per thread</p>
<p><strong>Limited supply</strong>: 65536 registers per SM on A100</p>
<p>If too many registers -&gt; fewer blocks per SM -&gt; lower occupancy -&gt; lower performance.</p>
<p><strong>Balance</strong>: Larger blocks = more reuse but more register pressure.</p>
<p>Common Pitfalls</p>
</section>
<section id="non-contiguous-tensors">
<h2>1. Non-Contiguous Tensors<a class="headerlink" href="#non-contiguous-tensors" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Non-contiguous tensors have unexpected strides -&gt; wrong pointer arithmetic -&gt; incorrect results.</p>
<p><strong>Solution</strong>: Call <code class="docutils literal notranslate"><span class="pre">.contiguous()</span></code> or handle arbitrary strides.</p>
</section>
<section id="wrong-stride-calculation">
<h2>2. Wrong Stride Calculation<a class="headerlink" href="#wrong-stride-calculation" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Donâ€™t hardcode strides! Transposed matrices have different strides.</p>
</section>
<section id="boundary-conditions">
<h2>3. Boundary Conditions<a class="headerlink" href="#boundary-conditions" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>c_mask = (offs_cm[:, None] &lt; M) &amp; (offs_cn[None, :] &lt; N)  # Mask stores</p>
<p>Forgetting these -&gt; out-of-bounds accesses -&gt; crashes or wrong results.</p>
</section>
<section id="numerical-precision">
<h2>4. Numerical Precision<a class="headerlink" href="#numerical-precision" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Using FP16 for accumulation -&gt; loss of precision -&gt; degraded accuracy.</p>
<p><strong>Best practice</strong>: Accumulate in FP32, cast to FP16 for storage.</p>
<p>Benchmarking Tips</p>
</section>
<section id="warm-up">
<h2>1. Warm-up<a class="headerlink" href="#warm-up" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>First few kernel launches are slow (compilation, cache loading).</p>
</section>
<section id="measure-tflops">
<h2>2. Measure TFLOPS<a class="headerlink" href="#measure-tflops" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>**Why 2MNK?**
</pre></div>
</div>
<ul class="simple">
<li><p>Each of MN output elements: K multiply-adds</p></li>
<li><p>Multiply-add = 2 FLOPs (1 multiply + 1 add)</p></li>
<li><p>Total: 2MNK FLOPs</p></li>
</ul>
</section>
<section id="compare-against-cublas-rocblas">
<h2>3. Compare Against cuBLAS/rocBLAS<a class="headerlink" href="#compare-against-cublas-rocblas" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>These are <strong>highly optimized</strong> by vendors. Matching them is a huge achievement!</p>
</section>
<section id="test-different-sizes">
<h2>4. Test Different Sizes<a class="headerlink" href="#test-different-sizes" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Performance varies with size:
- Small: Memory-bound, lower TFLOPS
- Medium: Transition zone
- Large: Compute-bound, approaching peak</p>
<p>Key Takeaways</p>
<ol class="arabic simple">
<li><p><strong>Tiling/Blocking is essential</strong>: Reuse data in fast SRAM</p></li>
<li><p><strong>Pointer arithmetic</strong>: Understanding strides is crucial for multi-dimensional arrays</p></li>
<li><p><strong>L2 cache matters</strong>: Grouped ordering can give 10-20% speedup</p></li>
<li><p><strong>Auto-tuning is powerful</strong>: Optimal configs vary with size and hardware</p></li>
<li><p><strong>Tensor Cores are game-changers</strong>: 10-100x speedup for matmul</p></li>
<li><p><strong>Matmul is compute-bound</strong>: Unlike vector add (memory-bound)</p></li>
<li><p><strong>Accumulate in FP32</strong>: Maintain numerical accuracy</p></li>
<li><p><strong>Triton simplifies complex optimizations</strong>: Achieves near-cuBLAS performance with readable code</p></li>
</ol>
<p>Matrix multiplication is the foundation of deep learning. Mastering these concepts will help you understand and optimize transformers, CNNs, and other neural architectures!</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="02-fused-softmax.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="04-low-memory-dropout.html" class="btn btn-neutral float-right" title="Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Concurrent Programs.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>