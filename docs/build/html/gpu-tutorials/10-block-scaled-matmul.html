

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Overview &mdash; Fast Concurrent Programming Guide 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1aac1d93" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/sidebar-fix.js?v=6c2f6f50"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Key Features" href="../triton-compiler/01-overview.html" />
    <link rel="prev" title="Overview" href="09-persistent-matmul.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Fast Concurrent Programming Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">CPU Concurrency</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#parallelism">Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#visual-comparison">Visual Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#threading-concurrent-futures-threadpoolexecutor">Threading (concurrent.futures.ThreadPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#multiprocessing-concurrent-futures-processpoolexecutor">Multiprocessing (concurrent.futures.ProcessPoolExecutor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#when-to-use-what">When to Use What</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#what-is-the-gil">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-points">Key Points:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#impact-on-performance">Impact on Performance:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#observing-the-gil-from-script-06">Observing the GIL (from script 06):</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop">Event Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#how-it-works-from-script-07">How It Works (from script 07):</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop-lifecycle">Event Loop Lifecycle:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#modern-vs-old-patterns">Modern vs Old Patterns:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#what-are-coroutines">What are Coroutines?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#defining-coroutines">Defining Coroutines:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-features">Key Features:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#example-from-script-08">Example from Script 08:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#execution-flow">Execution Flow:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#important-rules">Important Rules:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#tasks">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#task-characteristics">Task Characteristics:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#example-from-script-09">Example from Script 09:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#futures">Futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#waiting-for-multiple-tasks">Waiting for Multiple Tasks:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#comparison-table">Comparison Table:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#hybrid-workloads">Hybrid Workloads:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#time-measurement-time-clock-time-perf-counter">1. Time Measurement (<code class="docutils literal notranslate"><span class="pre">time.clock()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#coroutine-syntax-asyncio-coroutine-async-def">2. Coroutine Syntax (<code class="docutils literal notranslate"><span class="pre">&#64;asyncio.coroutine</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#task-creation-asyncio-task-asyncio-create-task">3. Task Creation (<code class="docutils literal notranslate"><span class="pre">asyncio.Task()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">asyncio.create_task()</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#event-loop-management">4. Event Loop Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#future-callbacks-callbacks-await">5. Future Callbacks (Callbacks -&gt; <code class="docutils literal notranslate"><span class="pre">await</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#blocking-calls-in-async-code">6. Blocking Calls in Async Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#string-formatting-f-strings">7. String Formatting (<code class="docutils literal notranslate"><span class="pre">%</span></code> -&gt; f-strings)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#migration-checklist">Migration Checklist</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#compatibility">Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#quick-reference-guide">Quick Reference Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/key_concepts.html#further-reading">Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html">Physical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#logical-cores">Logical Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#the-concept">The Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#how-it-works">How It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#technical-implementation">Technical Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#hyperthreading-limitations">Hyperthreading Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#checking-hyperthreading-status">Checking Hyperthreading Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#the-fundamental-constraint">The Fundamental Constraint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#optimal-worker-count">Optimal Worker Count</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#real-world-example">Real-World Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#cpu-vs-gpu-different-design-philosophies">CPU vs GPU: Different Design Philosophies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#key-differences">Key Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#simd-and-gpu-architecture">SIMD and GPU Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#why-gpus-excel-at-compute-intensive-tasks">Why GPUs Excel at Compute-Intensive Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#what-gpus-are-good-at">What GPUs Are Good At</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#silicon-real-estate-comparison">Silicon Real Estate Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#detailed-benchmark-results">Detailed Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#decision-matrix">Decision Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#practical-guidelines">Practical Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-1-image-processing">Example 1: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-2-monte-carlo-simulation">Example 2: Monte Carlo Simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#example-3-neural-network-training">Example 3: Neural Network Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#core-principles">Core Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/hardware_parallelism.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html">Key Points About start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#example">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#key-points-about-join">Key Points About join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#id1">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#without-join-danger">WITHOUT join() - DANGER!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#with-join-correct">WITH join() - CORRECT!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-1-calling-the-function-directly-instead-of-start">Mistake 1: Calling the function directly instead of start()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-2-forgetting-join">Mistake 2: Forgetting join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/threading_basics.html#mistake-3-thinking-threads-share-data-automatically">Mistake 3: Thinking threads share data automatically</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_event_loop.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_event_loop.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_coroutine.html#use-cases">Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_and_futures.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_and_futures.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_task_manipulation.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/asyncio_task_manipulation.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/concurrent_futures_pooling.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/concurrent_futures_pooling.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#internal-structure">Internal Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#automatic-locking">Automatic Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#put-item"><code class="docutils literal notranslate"><span class="pre">put(item)</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#problem-with-manual-locks">Problem with Manual Locks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#how-queue-does-locking">How Queue Does Locking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#locking-benefits">Locking Benefits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#execution-flow">Execution Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#thread-safe-data-structure">1. <strong>Thread-Safe Data Structure</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#blocks-correctly">2. <strong>Blocks Correctly</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#no-busy-waiting">3. <strong>No Busy-Waiting</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#task-tracking">4. <strong>Task Tracking</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#safe-for-multiple-producers-consumers">5. <strong>Safe for Multiple Producers/Consumers</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#step-by-step-what-happens-in-put">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">put()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#step-by-step-what-happens-in-get">Step-by-Step: What Happens in <code class="docutils literal notranslate"><span class="pre">get()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-1-forgetting-lock">Mistake 1: Forgetting Lock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-2-busy-waiting">Mistake 2: Busy-Waiting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_explained.html#mistake-3-race-condition">Mistake 3: Race Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_internal_mechanics.html">The Answer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/queue_internal_mechanics.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html">Without task_done() - Canâ€™t Track Completion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#internal-counter-system">Internal Counter System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#visual-timeline">Visual Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#code-simplified">Code (Simplified)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#two-operations">Two Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#timeline-all-three-conditions">Timeline: All Three Conditions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#put">put()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#get">get()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#task-done">task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#join">join()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#scenario-1-producer-1-consumer">Scenario: 1 Producer, 1 Consumer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-1-put-increment-counter">Step 1: put() - Increment Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#diagram-tracking-one-task">Diagram: Tracking One Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#diagram-multiple-tasks">Diagram: Multiple Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#counter">Counter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#condition-variable">Condition Variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#together">Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#without-all-tasks-done-condition">Without all_tasks*done Condition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#queue-join-without-task-done">Queue.join() Without task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-it-blocks-forever">Why It Blocks Forever</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#queue-join-with-task-done">Queue.join() With task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-it-works">Why It Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-1-put-item">Step 1: Put Item</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-2-get-and-process">Step 2: Get and Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#step-3-mark-done">Step 3: Mark Done</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#flow-diagram">Flow Diagram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#without-task-done-problematic">Without task_done() - PROBLEMATIC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-1-verify-all-work-complete">Use Case 1: Verify All Work Complete</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-2-track-progress">Use Case 2: Track Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#use-case-3-batch-processing">Use Case 3: Batch Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#scenario-main-thread-needs-to-know-when-workers-finish">Scenario: Main thread needs to know when workers finish</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#timeline">Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#better-code-pattern">Better Code Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#why-we-need-task-done">Why We Need task_done()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/task_done_queue_explained.html#the-pattern">The Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html">Regular Lock vs RLock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#why-rlock-is-needed-here">Why RLock is Needed Here</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#use-regular-lock-when">Use Regular Lock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#use-rlock-when">Use RLock When:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/rlock_explained.html#regular-lock-would-deadlock">Regular Lock - Would Deadlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#counting-semaphore-counter-1">1. Counting Semaphore (Counter &gt; 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#binary-semaphore-counter-0-or-1">2. Binary Semaphore (Counter = 0 or 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#execution-timeline">Execution Timeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#acquire"><code class="docutils literal notranslate"><span class="pre">acquire()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#counting-semaphore-3-spots-available">Counting Semaphore (3 spots available)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#example-1-swimming-pool-with-limited-capacity">Example 1: Swimming Pool with Limited Capacity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#limiting-concurrent-access">1. <strong>Limiting Concurrent Access</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#synchronizing-multiple-threads">3. <strong>Synchronizing Multiple Threads</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#lock-threading-lock">Lock (<code class="docutils literal notranslate"><span class="pre">threading.Lock</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/semaphore_explained.html#semaphore-counting">Semaphore (Counting)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html">What is the GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-does-python-have-a-gil">Why Does Python Have a GIL?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-the-gil-works">How the GIL Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#gil-behavior-with-different-operations">GIL Behavior with Different Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-critical-difference">The Critical Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#cpu-bound-operations">CPU-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#i-o-bound-operations">I/O-bound Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#real-world-analogy">Real-World Analogy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-with-threading-for-cpu-bound">The Problem with Threading for CPU-bound</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-solution-multiprocessing">The Solution: Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-multiprocessing-bypasses-the-gil">How Multiprocessing Bypasses the GIL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#trade-offs-of-multiprocessing">Trade-offs of Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#when-the-trade-off-is-worth-it">When the Trade-off is Worth It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-wasted-time">The Problem: Wasted Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-solution-threading">The Solution: Threading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-threading-works-for-i-o">Why Threading Works for I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-the-os-helps">How the OS Helps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#why-not-multiprocessing-for-i-o">Why Not Multiprocessing for I/O?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-problem-with-threading-overhead">The Problem with Threading: Overhead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#asyncio-cooperative-multitasking">Asyncio: Cooperative Multitasking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#how-asyncio-works">How Asyncio Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#event-loop-visualization">Event Loop Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#asyncio-vs-threading-detailed-comparison">Asyncio vs Threading: Detailed Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#performance-characteristics">Performance Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#when-asyncio-shines">When Asyncio Shines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#cpu-bound-with-threading-the-gil-dance">CPU-bound with Threading: The GIL Dance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-cpu-bound-task-computing-pi">Benchmark: CPU-bound Task (Computing pi)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-i-o-bound-task-web-requests">Benchmark: I/O-bound Task (Web Requests)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#benchmark-mixed-workload">Benchmark: Mixed Workload</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#quick-reference-table">Quick Reference Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#code-templates">Code Templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#the-gil-controls-everything">1. The GIL Controls Everything</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#resource-usage-matters">2. Resource Usage Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu-concurrency/patterns_problems_mapping.html#know-your-workload">4. Know Your Workload</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html">Key Differences: CPU vs GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#streaming-multiprocessors-sms">Streaming Multiprocessors (SMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#thread-organization">Thread Organization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/gpu-fundamentals.html#example-visualization">Example Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#l2-cache">L2 Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#registers">Registers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/memory-hierarchy.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html">Warps and SIMD Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#thread-divergence">Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#what-is-occupancy">What is Occupancy?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#factors-limiting-occupancy">Factors Limiting Occupancy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#example-calculation">Example Calculation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#why-occupancy-matters">Why Occupancy Matters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#the-occupancy-sweet-spot">The Occupancy Sweet Spot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#grid-and-block-dimensions">Grid and Block Dimensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#choosing-block-size">Choosing Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#within-a-block">Within a Block</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#between-blocks">Between Blocks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-shuffles">Warp Shuffles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#warp-level-reductions">Warp-Level Reductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#overlap-compute-and-memory">Overlap Compute and Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#traditional-approach">Traditional Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#persistent-approach">Persistent Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#key-factors">Key Factors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/execution-model.html#profiling-tools">Profiling Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html">Step 1: Profile First</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#step-2-identify-bottleneck">Step 2: Identify Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-kernel-fusion">Strategy 1: Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tiling">Strategy 2: Tiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-vectorized-loads">Strategy 3: Vectorized Loads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-memory-coalescing">Strategy 4: Memory Coalescing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-use-tensor-cores">Strategy 1: Use Tensor Cores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-increase-arithmetic-intensity">Strategy 2: Increase Arithmetic Intensity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-minimize-thread-divergence">Strategy 3: Minimize Thread Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-4-optimize-loop-structure">Strategy 4: Optimize Loop Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-1-reduce-register-usage">Strategy 1: Reduce Register Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-2-tune-shared-memory">Strategy 2: Tune Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#strategy-3-adjust-block-size">Strategy 3: Adjust Block Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#why-auto-tune">Why Auto-Tune?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#triton-auto-tuning">Triton Auto-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#warp-specialization">Warp Specialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#persistent-kernels">Persistent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#recomputation">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-reduction">Pattern: Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-element-wise">Pattern: Element-wise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#pattern-matrix-multiply">Pattern: Matrix Multiply</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-bandwidth">Issue: Low Bandwidth</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-low-compute-utilization">Issue: Low Compute Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#issue-lower-than-pytorch">Issue: Lower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#before-you-optimize">Before You Optimize</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#memory-optimizations">Memory Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#compute-optimizations">Compute Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#occupancy-optimization">Occupancy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/performance-optimization.html#advanced">Advanced</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-concepts/triton-concepts.html">Triton Concepts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="01-vector-add.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-vector-add.html#next-steps">Next Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-fused-softmax.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-fused-softmax.html#extensions">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-matrix-multiplication.html">Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-low-memory-dropout.html">Low Memory Dropout</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-layer-norm.html">Layer Norm</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-fused-attention.html">Fused Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-extern-functions.html">Extern Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-grouped-gemm.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-grouped-gemm.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-persistent-matmul.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-persistent-matmul.html#summary">Summary</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton Compiler</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-1-python-ast-parsing">Stage 1: Python AST Parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-2-code-generation-ttir">Stage 2: Code Generation (TTIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-3-triton-gpu-ir-ttgir">Stage 3: Triton GPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#stage-4-llvm-ir">Stage 4: LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#block-based-programming-model">Block-based Programming Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#jit-compilation">JIT Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#mlir-infrastructure">MLIR Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#python-components">Python Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#c-components">C++ Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/01-overview.html#backend-components">Backend Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/02-jit-decorator.html#summary">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html">CodeGenerator Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ast-visitor-pattern">AST Visitor Pattern</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#type-inference">Type Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#compilation-orchestration">Compilation Orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#ttgir-llvm-ir">TTGIR -&gt; LLVM IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#llvm-ir-ptx">LLVM IR -&gt; PTX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-key-components">Cache Key Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/03-compilation-pipeline.html#cache-directory-structure">Cache Directory Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html">The NVCC Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#the-llvm-path">The LLVM Path</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#llvm-ir-stage">LLVM IR Stage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-generated-by-triton">PTX Generated by Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#same-tools-same-artifacts">Same Tools, Same Artifacts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#source-language">Source Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compiler-stack">Compiler Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-time">Compilation Time</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#can-triton-and-cuda-c-work-together">Can Triton and CUDA C++ Work Together?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#advantages-of-llvm-backend">Advantages of LLVM Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#why-not-use-nvcc">Why Not Use NVCC?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#trade-offs">Trade-offs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#file-types">File Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#example-directory-structures">Example Directory Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#ptx-inspection">PTX Inspection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#compilation-paths-compared">Compilation Paths Compared</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/04-cuda-comparison.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html">Traditional Compiler Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#example-matrix-multiplication">Example: Matrix Multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#mlir-philosophy">MLIR Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#dialects">1. Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-triton-uses-mlir">Why Triton Uses MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-s-mlir-dialects">Tritonâ€™s MLIR Dialects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#python-source">Python Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-2-tritongpu-ir-ttgir">Stage 2: TritonGPU IR (TTGIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-3-llvm-dialect">Stage 3: LLVM Dialect</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#stage-4-llvm-ir-actual">Stage 4: LLVM IR (Actual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#command-line-tools">Command-Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#debugging-mlir">Debugging MLIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#official-documentation">Official Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#tutorials">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#triton-specific">Triton-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#key-concepts-recap">Key Concepts Recap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#why-mlir-matters-for-triton">Why MLIR Matters for Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../triton-compiler/05-mlir-concepts.html#the-big-picture">The Big Picture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../learning-paths.html">Learning Paths</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html">CUDA Out of Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#out-of-shared-memory">Out of Shared Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-results">Wrong Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nan-or-inf-values">NaN or Inf Values</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slower-than-pytorch">Slower Than PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#low-gpu-utilization">Low GPU Utilization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#compilation-errors">Compilation Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#slow-compilation">Slow Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#nvidia-specific">NVIDIA-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#amd-specific">AMD-Specific</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#wrong-gpu-selected">Wrong GPU Selected</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#print-debugging">Print Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#profiling">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#assertions">Assertions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#unit-testing">Unit Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting.html#when-stuck">When Stuck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#cuda">CUDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#rocm-amd">ROCm (AMD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#flash-attention">Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#normalization">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#optimization-techniques">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#id1">Triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#gpu-programming">GPU Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#deep-learning">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-tools">NVIDIA Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-tools">AMD Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#tutorials-and-courses">Tutorials and Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#community">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#blogs-and-articles">Blogs and Articles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#triton-examples">Triton Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#production-usage">Production Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#nvidia-gpus">NVIDIA GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#amd-gpus">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#subscribe-to">Subscribe To</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html#conferences">Conferences</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Fast Concurrent Programming Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/gpu-tutorials/10-block-scaled-matmul.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Tutorial 10: Block Scaled Matrix Multiplication</p>
<section id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Link to this heading">ïƒ</a></h1>
<p><strong>Block scaled matmul</strong> enables low-precision matrix multiplication (FP4/FP8) with per-block scaling factors. This technique is crucial for:</p>
<ul class="simple">
<li><p><strong>Quantized inference</strong> - Running large language models with reduced memory</p></li>
<li><p><strong>Mixed precision training</strong> - Different precision for different operations</p></li>
<li><p><strong>Memory bandwidth optimization</strong> - Fewer bits transferred = faster compute</p></li>
<li><p><strong>Hardware acceleration</strong> - Specialized tensor cores for low-precision ops</p></li>
</ul>
<p>This tutorial supports <strong>four quantization formats</strong>:
1. <strong>nvfp4</strong> - NVIDIAâ€™s FP4 format (16 elements per scale, NVIDIA-only)
2. <strong>mxfp4</strong> - Microscaling FP4 (32 elements per scale, OCP standard)
3. <strong>mxfp8</strong> - Microscaling FP8 (32 elements per scale)
4. <strong>mixed</strong> - FP8 x FP4 mixed precision</p>
<p><strong>Hardware requirements:</strong>
- <strong>NVIDIA</strong>: Blackwell (compute capability 10.0+) with 5th-gen Tensor Cores
- <strong>AMD</strong>: CDNA4 architecture (MI300 series) with scaled MFMA instructions</p>
<p>Key Concepts</p>
<section id="block-scaling-fundamentals">
<h2>Block Scaling Fundamentals<a class="headerlink" href="#block-scaling-fundamentals" title="Link to this heading">ïƒ</a></h2>
<p>Instead of storing full-precision values, we store:</p>
<p>Low-precision value + Scale factor</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Standard</span> <span class="n">matmul</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<p>C = A &#64; B</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Block</span> <span class="n">scaled</span> <span class="n">matmul</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<p>C = (A * scale_a) &#64; (B * scale_b)</p>
<dl class="simple">
<dt>where:</dt><dd><p>A, B are low-precision (fp4/fp8)
scale_a, scale_b are per-block scale factors
C is full precision (fp16/fp32)</p>
</dd>
</dl>
</section>
<section id="quantization-formats">
<h2>Quantization Formats<a class="headerlink" href="#quantization-formats" title="Link to this heading">ïƒ</a></h2>
<div class="line-block">
<div class="line">Format | Bits/elem | Vec Size | Hardware | Notes |</div>
<div class="line">nvfp4 | 4 | 16 | NVIDIA Blackwell | Proprietary, optimized for NVIDIA |</div>
<div class="line">mxfp4 | 4 | 32 | NVIDIA/AMD | OCP standard, better portability |</div>
<div class="line">mxfp8 | 8 | 32 | NVIDIA/AMD | Higher precision, still efficient |</div>
<div class="line">mixed | 4x8 | varies | NVIDIA Blackwell | A in fp8, B in fp4 |</div>
</div>
<p><strong>Vec Size</strong> = number of elements sharing one scale factor</p>
</section>
<section id="memory-savings">
<h2>Memory Savings<a class="headerlink" href="#memory-savings" title="Link to this heading">ïƒ</a></h2>
<p>FP16: 2 bytes per element
FP8:  1 byte per element  -&gt; 2x memory reduction
FP4:  0.5 bytes per element -&gt; 4x memory reduction</p>
<dl class="simple">
<dt>Plus scale factors:</dt><dd><ul class="simple">
<li><p>1 scale per 16-32 elements</p></li>
<li><p>Scales typically stored as fp8 or e8m0 (exponent only)</p></li>
<li><p>Overhead: ~3-6% of original size</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Example</span> <span class="k">for</span> <span class="mi">8192</span><span class="n">x8192</span> <span class="n">matrix</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<ul class="simple">
<li><p>FP16: 128 MB</p></li>
<li><p>FP8 + scales: 64 MB + 2 MB = 66 MB (48% reduction)</p></li>
<li><p>FP4 + scales: 32 MB + 2 MB = 34 MB (73% reduction)</p></li>
</ul>
<p>Scale Preshuffling (NVIDIA)</p>
</section>
<section id="why-preshuffling">
<h2>Why Preshuffling?<a class="headerlink" href="#why-preshuffling" title="Link to this heading">ïƒ</a></h2>
<p>Tensor cores load scales in specific patterns. To avoid non-contiguous access, scales must be reorganized:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Each row has K // VEC_SIZE scales</p>
<p>Preshuffled layout: [M // 128, K // VEC_SIZE // 4, 32, 4, 4]
Organized for 128-element blocks along M</p>
</section>
<section id="d-preshuffled-layout">
<h2>5D Preshuffled Layout<a class="headerlink" href="#d-preshuffled-layout" title="Link to this heading">ïƒ</a></h2>
<dl class="simple">
<dt>Dimension breakdown:</dt><dd><p>[M // 128]         - Number of 128-row blocks
[K // VEC_SIZE // 4] - Number of K scale blocks
[32]               - 32 rows per sub-block
[4]                - 4 scale groups
[4]                - 4 scales per group</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Memory</span> <span class="n">access</span> <span class="n">pattern</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Load</span> <span class="mi">128</span> <span class="n">rows</span> <span class="n">x</span> <span class="p">(</span><span class="n">BLOCK_K</span> <span class="o">//</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="n">scales</span> <span class="n">contiguously</span>
<span class="n">No</span> <span class="n">strided</span> <span class="n">access</span> <span class="o">-&gt;</span> <span class="n">better</span> <span class="n">memory</span> <span class="n">bandwidth</span>
</pre></div>
</div>
</section>
<section id="reshaping-and-transposing">
<h2>Reshaping and Transposing<a class="headerlink" href="#reshaping-and-transposing" title="Link to this heading">ïƒ</a></h2>
<p>Inside the kernel:
.. code-block:: python</p>
<p>scale_a = a_scale*desc.load([0, offs_scale*m, offs_scale*k, 0, 0])</p>
<p>Reshape to 5D
scale_a = scale_a.reshape(rep_m, rep_k, 32, 4, 4)</p>
<p>Transpose to logical 2D layout
scale_a = scale_a.trans(0, 3, 2, 1, 4).reshape(BLOCK_M, BLOCK_K // VEC_SIZE)</p>
<p>Now ready for tl.dot_scaled</p>
<p>Scale Preshuffling (AMD CDNA4)</p>
</section>
<section id="mfma-scale-organization">
<h2>MFMA Scale Organization<a class="headerlink" href="#mfma-scale-organization" title="Link to this heading">ïƒ</a></h2>
<p>AMDâ€™s MFMA (Matrix Fused Multiply-Add) instructions require different shuffling:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Reorganize scales for MFMA instructions.</span>

<span class="sd">mfma_nonkdim: 16 or 32</span>
<span class="sd">  - 16: mfma_scaled*16x16x128</span>
<span class="sd">  - 32: mfma_scaled*32x32x64</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">sm</span><span class="p">,</span> <span class="n">sn</span> <span class="o">=</span> <span class="n">scales</span><span class="o">.</span><span class="n">shape</span>

<span class="k">if</span> <span class="n">mfma_nonkdim</span> <span class="o">==</span> <span class="mi">32</span><span class="p">:</span>
    <span class="c1"># For 32x32 MFMA: pack 4 ops in order 0,1,2,3</span>
    <span class="n">scales_shuffled</span> <span class="o">=</span> <span class="n">scales</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sm</span> <span class="o">//</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">sn</span> <span class="o">//</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">scales_shuffled</span> <span class="o">=</span> <span class="n">scales_shuffled</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

<span class="k">elif</span> <span class="n">mfma_nonkdim</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
    <span class="c1"># For 16x16 MFMA: pack 4 ops in order 0,2,1,3</span>
    <span class="n">scales_shuffled</span> <span class="o">=</span> <span class="n">scales</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sm</span> <span class="o">//</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">sn</span> <span class="o">//</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">scales_shuffled</span> <span class="o">=</span> <span class="n">scales_shuffled</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

<span class="k">return</span> <span class="n">scales_shuffled</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sm</span> <span class="o">//</span> <span class="mi">32</span><span class="p">,</span> <span class="n">sn</span> <span class="o">*</span> <span class="mi">32</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Key</span> <span class="n">insight</span><span class="p">:</span><span class="o">**</span> <span class="n">Each</span> <span class="n">thread</span> <span class="n">needs</span> <span class="mi">4</span> <span class="n">scale</span> <span class="n">values</span> <span class="k">for</span> <span class="mi">4</span> <span class="n">MFMA</span> <span class="n">operations</span><span class="p">,</span> <span class="n">packed</span> <span class="n">contiguously</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="thread-level-access-pattern">
<h2>Thread-level Access Pattern<a class="headerlink" href="#thread-level-access-pattern" title="Link to this heading">ïƒ</a></h2>
<dl class="simple">
<dt>Without shuffling:</dt><dd><p>Thread 0 needs scales at: [0, 32, 64, 96] - strided access</p>
</dd>
<dt>With shuffling:</dt><dd><p>Thread 0 needs scales at: [0, 1, 2, 3]    - contiguous access
Thread 1 needs scales at: [4, 5, 6, 7]
â€¦</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Benefits</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Vectorized memory loads (4 bytes at once)</p></li>
<li><p>Better global memory coalescing</p></li>
<li><p>Lower LDS (shared memory) pressure</p></li>
</ul>
<p>Code Walkthrough</p>
</section>
<section id="nvidia-kernel-tma-based">
<h2>1. NVIDIA Kernel (TMA-based)<a class="headerlink" href="#nvidia-kernel-tma-based" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>def block_scaled*matmul_kernel(</dt><dd><p>a_desc, b_desc, c_desc,
a_scale*desc, b_scale*desc,
M, N, K,
output_type: tl.constexpr,
ELEM_PER*BYTE_A: tl.constexpr,  # 1 for fp8, 2 for fp4
ELEM_PER*BYTE_B: tl.constexpr,
VEC_SIZE: tl.constexpr,          # 16 for nvfp4, 32 for mxfp4/mxfp8
BLOCK_M: tl.constexpr,
BLOCK_N: tl.constexpr,
BLOCK_K: tl.constexpr,
rep_m: tl.constexpr,             # BLOCK_M // 128
rep_n: tl.constexpr,             # BLOCK_N // 128
rep_k: tl.constexpr,             # BLOCK_K // VEC_SIZE // 4
NUM_STAGES: tl.constexpr,</p>
</dd>
<dt>):</dt><dd><p>pid = tl.program_id(axis=0)
num_pid*m = tl.cdiv(M, BLOCK_M)
pid_m = pid % num_pid*m
pid_n = pid // num_pid*m</p>
<p>offs_am = pid_m * BLOCK_M
offs_bn = pid_n * BLOCK_N
offs_k*a = 0
offs_k*b = 0
offs_scale*m = pid_m * rep_m
offs_scale*n = pid_n * rep_n
offs_scale*k = 0</p>
<p>accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)</p>
<p># Pipelined loop over K
for k in tl.range(0, tl.cdiv(K, BLOCK_K), num_stages=NUM_STAGES):</p>
<blockquote>
<div><p># Load data blocks
a = a_desc.load([offs_am, offs_k*a])
b = b_desc.load([offs_bn, offs_k*b])</p>
<p># Load and reshape scales
scale_a = a_scale*desc.load([0, offs_scale*m, offs_scale*k, 0, 0])
scale_b = b_scale*desc.load([0, offs_scale*n, offs_scale*k, 0, 0])</p>
<p># Reshape from 5D to 2D
scale_a = scale_a.reshape(rep_m, rep_k, 32, 4, 4) </p>
<blockquote>
<div><p>.trans(0, 3, 2, 1, 4) .reshape(BLOCK_M, BLOCK_K // VEC_SIZE)</p>
</div></blockquote>
<dl class="simple">
<dt>scale_b = scale_b.reshape(rep_n, rep_k, 32, 4, 4) </dt><dd><p>.trans(0, 3, 2, 1, 4) .reshape(BLOCK_N, BLOCK_K // VEC_SIZE)</p>
</dd>
</dl>
<p># Perform scaled dot product
if ELEM_PER*BYTE_A == 1 and ELEM_PER*BYTE_B == 2:</p>
<blockquote>
<div><p># Mixed precision: A is fp8, B is fp4
accumulator = tl.dot_scaled(</p>
<blockquote>
<div><p>a, scale_a, â€œe4m3â€,
b.T, scale_b, â€œe2m1â€,
accumulator</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<dl>
<dt>elif ELEM_PER*BYTE_A == 2 and ELEM_PER*BYTE_B == 2:</dt><dd><p># Both fp4
accumulator = tl.dot_scaled(</p>
<blockquote>
<div><p>a, scale_a, â€œe2m1â€,
b.T, scale_b, â€œe2m1â€,
accumulator</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>else:</dt><dd><p># Both fp8
accumulator = tl.dot_scaled(</p>
<blockquote>
<div><p>a, scale_a, â€œe4m3â€,
b.T, scale_b, â€œe4m3â€,
accumulator</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p># Advance pointers
offs_k*a += BLOCK_K // ELEM_PER*BYTE_A
offs_k*b += BLOCK_K // ELEM_PER*BYTE_B
offs_scale*k += rep_k</p>
</div></blockquote>
<p># Store result
c_desc.store([offs_am, offs_bn], accumulator.to(output_dtype))</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Key</span> <span class="n">operations</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tl.dot_scaled()</span></code> - Tritonâ€™s scaled matmul intrinsic</p></li>
<li><p>Format strings: <code class="docutils literal notranslate"><span class="pre">&quot;e4m3&quot;</span></code> (fp8), <code class="docutils literal notranslate"><span class="pre">&quot;e2m1&quot;</span></code> (fp4)</p></li>
<li><p>Automatic broadcast of scales across VEC_SIZE elements</p></li>
</ul>
</section>
<section id="amd-cdna4-kernel">
<h2>2. AMD CDNA4 Kernel<a class="headerlink" href="#amd-cdna4-kernel" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>def block_scaled*matmul_kernel*cdna4(</dt><dd><p>a_ptr, b_ptr, c_ptr,
a_scales*ptr, b_scales*ptr,
M, N, K,
stride_am, stride_ak, stride_bk, stride_bn,
stride_cm, stride_cn,
stride_asm, stride_ask, stride_bsn, stride_bsk,
BLOCK_M: tl.constexpr,
BLOCK_N: tl.constexpr,
BLOCK_K: tl.constexpr,
mfma_nonkdim: tl.constexpr,  # 16 or 32</p>
</dd>
<dt>):</dt><dd><p>pid = tl.program_id(axis=0)
num_pid*n = tl.cdiv(N, BLOCK_N)
pid_m = pid // num_pid*n
pid_n = pid % num_pid*n</p>
<p># Packed data: 2 fp4 elements per byte
offs_k = tl.arange(0, BLOCK_K // 2)
offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M
offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N</p>
<p>a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak
b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn</p>
<p># Scale pointers (32 elements per scale)
SCALE_GROUP*SIZE = 32
offs_ks = tl.arange(0, BLOCK_K // SCALE_GROUP*SIZE * 32)</p>
<p>offs_asm = (pid_m * (BLOCK_M // 32) + tl.arange(0, BLOCK_M // 32)) % M
a_scale*ptrs = a_scales*ptr + offs_asm[:, None] * stride_asm + </p>
<blockquote>
<div><p>offs_ks[None, :] * stride_ask</p>
</div></blockquote>
<p>offs_asn = (pid_n * (BLOCK_N // 32) + tl.arange(0, BLOCK_N // 32)) % N
b_scale*ptrs = b_scales*ptr + offs_asn[:, None] * stride_bsn + </p>
<blockquote>
<div><p>offs_ks[None, :] * stride_bsk</p>
</div></blockquote>
<p>accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)</p>
<p>num_k*iter = tl.cdiv(K, BLOCK_K // 2)
for k in range(num_k*iter):</p>
<blockquote>
<div><p># Load and unshuffle scales
if mfma_nonkdim == 32:</p>
<blockquote>
<div><dl class="simple">
<dt>a_scales = tl.load(a_scale*ptrs) </dt><dd><p>.reshape(BLOCK_M // 32, BLOCK_K // 32 // 8, 2, 32, 4, 1) .permute(0, 3, 1, 4, 2, 5) .reshape(BLOCK_M, BLOCK_K // 32)</p>
</dd>
<dt>b_scales = tl.load(b_scale*ptrs) </dt><dd><p>.reshape(BLOCK_N // 32, BLOCK_K // 32 // 8, 2, 32, 4, 1) .permute(0, 3, 1, 4, 2, 5) .reshape(BLOCK_N, BLOCK_K // 32)</p>
</dd>
</dl>
</div></blockquote>
<dl>
<dt>elif mfma_nonkdim == 16:</dt><dd><dl class="simple">
<dt>a_scales = tl.load(a_scale*ptrs) </dt><dd><p>.reshape(BLOCK_M // 32, BLOCK_K // 32 // 8, 4, 16, 2, 2, 1) .permute(0, 5, 3, 1, 4, 2, 6) .reshape(BLOCK_M, BLOCK_K // 32)</p>
</dd>
</dl>
<p># Similar for b_scales</p>
</dd>
</dl>
<p># Load packed data
a = tl.load(a_ptrs)
b = tl.load(b_ptrs)</p>
<p># Scaled matmul
accumulator += tl.dot_scaled(a, a_scales, â€œe2m1â€,</p>
<blockquote>
<div><p>b, b_scales, â€œe2m1â€)</p>
</div></blockquote>
<p># Advance pointers
a_ptrs += (BLOCK_K // 2) * stride_ak
b_ptrs += (BLOCK_K // 2) * stride_bk
a_scale*ptrs += BLOCK_K * stride_ask
b_scale*ptrs += BLOCK_K * stride_bsk</p>
</div></blockquote>
<p>c = accumulator.to(c_ptr.type.element_ty)
# Store with write-through cache hint
tl.store(c_ptrs, c, mask=c_mask, cache_modifier=â€.wtâ€)</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">AMD</span><span class="o">-</span><span class="n">specific</span> <span class="n">features</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Explicit unshuffling of scales in kernel</p></li>
<li><p>Support for two MFMA shapes (16x16, 32x32)</p></li>
<li><p>Write-through cache modifier for better performance</p></li>
<li><p>E8M0 scale format (exponent-only, 8 bits)</p></li>
</ul>
<p>Initialization and Setup</p>
</section>
<section id="nvidia-version">
<h2>NVIDIA Version<a class="headerlink" href="#nvidia-version" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configuration based on format</span>
<span class="n">BLOCK_M</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">BLOCK_N</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">BLOCK_K</span> <span class="o">=</span> <span class="mi">256</span> <span class="k">if</span> <span class="s2">&quot;fp4&quot;</span> <span class="ow">in</span> <span class="n">block_scale</span><span class="o">*</span><span class="nb">type</span> <span class="k">else</span> <span class="mi">128</span>
<span class="n">VEC_SIZE</span> <span class="o">=</span> <span class="mi">16</span> <span class="k">if</span> <span class="n">block_scale</span><span class="o">*</span><span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;nvfp4&quot;</span> <span class="k">else</span> <span class="mi">32</span>
<span class="n">ELEM_PER</span><span class="o">*</span><span class="n">BYTE_A</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="s2">&quot;fp4&quot;</span> <span class="ow">in</span> <span class="n">block_scale</span><span class="o">*</span><span class="nb">type</span> <span class="k">else</span> <span class="mi">1</span>
<span class="n">ELEM_PER</span><span class="o">*</span><span class="n">BYTE_B</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">block_scale</span><span class="o">*</span><span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;mxfp8&quot;</span> <span class="k">else</span> <span class="mi">2</span>

<span class="c1"># Generate random data using mxfp helper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">triton.tools.mxfp</span><span class="w"> </span><span class="kn">import</span> <span class="n">MXFP4Tensor</span>

<span class="n">a_ref</span> <span class="o">=</span> <span class="n">MXFP4Tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
<span class="n">b_ref</span> <span class="o">=</span> <span class="n">MXFP4Tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>  <span class="c1"># Transposed</span>

<span class="c1"># Pack for fp4 (2 elements per byte)</span>
<span class="k">if</span> <span class="s2">&quot;fp4&quot;</span> <span class="ow">in</span> <span class="n">block_scale</span><span class="o">*</span><span class="nb">type</span> <span class="ow">and</span> <span class="n">block_scale</span><span class="o">*</span><span class="nb">type</span> <span class="o">!=</span> <span class="s2">&quot;mxfp8&quot;</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a_ref</span><span class="o">.</span><span class="n">to_packed</span><span class="o">*</span><span class="n">tensor</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a_ref</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">)</span>

<span class="c1"># Create TMA descriptors</span>
<span class="n">a_desc</span> <span class="o">=</span> <span class="n">TensorDescriptor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_K</span> <span class="o">//</span> <span class="n">ELEM_PER</span><span class="o">*</span><span class="n">BYTE_A</span><span class="p">])</span>
<span class="n">b_desc</span> <span class="o">=</span> <span class="n">TensorDescriptor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">[</span><span class="n">BLOCK_N</span><span class="p">,</span> <span class="n">BLOCK_K</span> <span class="o">//</span> <span class="n">ELEM_PER</span><span class="o">*</span><span class="n">BYTE_B</span><span class="p">])</span>

<span class="c1"># Generate scales in 5D preshuffled format</span>
<span class="n">a_scale</span><span class="o">*</span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">M</span> <span class="o">//</span> <span class="mi">128</span><span class="p">,</span> <span class="n">K</span> <span class="o">//</span> <span class="n">VEC_SIZE</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">]</span>
<span class="n">b_scale</span><span class="o">*</span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">N</span> <span class="o">//</span> <span class="mi">128</span><span class="p">,</span> <span class="n">K</span> <span class="o">//</span> <span class="n">VEC_SIZE</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">]</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span>
<span class="n">a_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">a_scale</span><span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span>
<span class="n">b_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">b_scale</span><span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span>

<span class="c1"># Reshape to 5D TMA format</span>
<span class="n">a_scale</span> <span class="o">=</span> <span class="n">a_scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a_scale</span><span class="o">*</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a_scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">b_scale</span> <span class="o">=</span> <span class="n">b_scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">b_scale</span><span class="o">*</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">b_scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>

<span class="n">a_scale</span><span class="o">*</span><span class="n">desc</span> <span class="o">=</span> <span class="n">TensorDescriptor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">a_scale</span><span class="p">,</span> <span class="n">block_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">rep_m</span><span class="p">,</span> <span class="n">rep_k</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">256</span><span class="p">])</span>
<span class="n">b_scale</span><span class="o">*</span><span class="n">desc</span> <span class="o">=</span> <span class="n">TensorDescriptor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">b_scale</span><span class="p">,</span> <span class="n">block_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">rep_n</span><span class="p">,</span> <span class="n">rep_k</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">256</span><span class="p">])</span>

<span class="k">return</span> <span class="n">a_desc</span><span class="p">,</span> <span class="n">a_scale</span><span class="o">*</span><span class="n">desc</span><span class="p">,</span> <span class="n">b_desc</span><span class="p">,</span> <span class="n">b_scale</span><span class="o">*</span><span class="n">desc</span><span class="p">,</span> <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="amd-version">
<h2>AMD Version<a class="headerlink" href="#amd-version" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BLOCK_M</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">BLOCK_N</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">BLOCK_K</span> <span class="o">=</span> <span class="mi">256</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">MXFP4Tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">MXFP4Tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>

<span class="c1"># E8M0 scales (exponent only, 8 bits)</span>
<span class="n">x_scales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">124</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span> <span class="o">//</span> <span class="mi">32</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">w_scales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">124</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span> <span class="o">//</span> <span class="mi">32</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="n">x_scales</span> <span class="o">=</span> <span class="n">x_scales</span><span class="o">.</span><span class="n">T</span>
<span class="n">w_scales</span> <span class="o">=</span> <span class="n">w_scales</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># Preshuffle for MFMA access pattern</span>
<span class="n">x_scales</span><span class="o">*</span><span class="n">shuffled</span> <span class="o">=</span> <span class="n">shuffle_scales</span><span class="o">*</span><span class="n">cdna4</span><span class="p">(</span><span class="n">x_scales</span><span class="p">,</span> <span class="n">mfma_nonkdim</span><span class="p">)</span>
<span class="n">w_scales</span><span class="o">*</span><span class="n">shuffled</span> <span class="o">=</span> <span class="n">shuffle_scales</span><span class="o">*</span><span class="n">cdna4</span><span class="p">(</span><span class="n">w_scales</span><span class="p">,</span> <span class="n">mfma_nonkdim</span><span class="p">)</span>

<span class="c1"># Pack 2 fp4 elements per byte</span>
<span class="n">x_packed</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to_packed</span><span class="o">*</span><span class="n">tensor</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">w_packed</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">to_packed</span><span class="o">*</span><span class="n">tensor</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">return</span> <span class="n">x_packed</span><span class="p">,</span> <span class="n">w_packed</span><span class="p">,</span> <span class="n">x_scales</span><span class="o">*</span><span class="n">shuffled</span><span class="p">,</span> <span class="n">w_scales</span><span class="o">*</span><span class="n">shuffled</span><span class="p">,</span> <span class="o">...</span>
</pre></div>
</div>
<p>Performance Characteristics</p>
</section>
<section id="theoretical-speedup">
<h2>Theoretical Speedup<a class="headerlink" href="#theoretical-speedup" title="Link to this heading">ïƒ</a></h2>
<p>Compute-bound workload:</p>
<p>FP16 throughput: 312 TFLOPS (H100)
FP8 throughput:  989 TFLOPS (H100) -&gt; 3.17x faster
FP4 throughput:  1978 TFLOPS (theoretical) -&gt; 6.3x faster</p>
<p>Memory bandwidth reduction:</p>
<p>FP16: 2 bytes/elem
FP8:  1 byte/elem + scales -&gt; ~45% savings
FP4:  0.5 bytes/elem + scales -&gt; ~70% savings</p>
</section>
<section id="real-world-performance">
<h2>Real-world Performance<a class="headerlink" href="#real-world-performance" title="Link to this heading">ïƒ</a></h2>
<p>From benchmarking on H100:
- <strong>mxfp8</strong>: 1.8-2.2x speedup over FP16
- <strong>mxfp4</strong>: 2.5-3.5x speedup over FP16
- <strong>nvfp4</strong>: 3.0-4.0x speedup over FP16 (NVIDIA-specific optimizations)
- <strong>mixed (fp8xfp4)</strong>: 2.2-3.0x speedup</p>
<p><strong>Factors affecting performance:</strong>
- Matrix size (larger = better amortization)
- Scale overhead (smaller VEC_SIZE = more overhead)
- Memory vs compute bound (FP4 helps more when memory-bound)</p>
<p>Numerical Considerations</p>
</section>
<section id="quantization-error">
<h2>Quantization Error<a class="headerlink" href="#quantization-error" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>FP8 (E4M3) range: +/-448, ~2 decimal digits
FP4 (E2M1) range: +/-6, ~1 decimal digit</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">**</span><span class="n">Per</span><span class="o">-</span><span class="n">block</span> <span class="n">scaling</span> <span class="n">helps</span><span class="p">:</span><span class="o">**</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>fp4_val = quantize_fp16*to_fp4(1234.5)  # Overflow!</p>
<p>With scaling:
scale = 1234.5 / 6.0  # ~205
fp4_val = quantize_fp16*to_fp4(1234.5 / scale)  # ~ 6
reconstructed = fp4_val * scale  # ~ 1234.5</p>
</section>
<section id="e8m0-scale-format-amd">
<h2>E8M0 Scale Format (AMD)<a class="headerlink" href="#e8m0-scale-format-amd" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Convert 8-bit exponent-only to float32.&quot;&quot;&quot;</span>
<span class="c1"># No mantissa, only exponent</span>
<span class="c1"># Value = 2^(exponent - 127)</span>
<span class="k">return</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mi">127</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
<section id="example">
<h3>Example:<a class="headerlink" href="#example" title="Link to this heading">ïƒ</a></h3>
<p>x = 135 -&gt; 2^(135-127) = 2^8 = 256
x = 127 -&gt; 2^0 = 1
x = 119 -&gt; 2^(-8) = 0.00390625</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>**Why exponent-only?**
</pre></div>
</div>
<ul class="simple">
<li><p>Scales are typically powers of 2</p></li>
<li><p>8 bits gives wide dynamic range</p></li>
<li><p>Simpler hardware implementation</p></li>
<li><p>Exact representation for power-of-2 scales</p></li>
</ul>
<p>Usage Examples</p>
</section>
</section>
<section id="command-line-benchmarking">
<h2>Command-line Benchmarking<a class="headerlink" href="#command-line-benchmarking" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<section id="nvidia-fp8">
<h3>NVIDIA FP8<a class="headerlink" href="#nvidia-fp8" title="Link to this heading">ïƒ</a></h3>
<p>python 10-block-scaled-matmul.py â€“format mxfp8 â€“K 4096 â€“bench</p>
<p>AMD MXFP4 (automatic detection)
python 10-block-scaled-matmul.py â€“format mxfp4 â€“bench</p>
<p>Mixed precision
python 10-block-scaled-matmul.py â€“format mixed â€“K_range 2048 8192 â€“K_step 2048</p>
</section>
</section>
<section id="validation">
<h2>Validation<a class="headerlink" href="#validation" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>AMD with both MFMA shapes
validate_block*scaled_amd(8192, 8192, 8192, block_scale*type=â€mxfp4â€, mfma_nonkdim=16)
[[OK]] (pass mxfp4, mfma_nonk*dim 16)</p>
<p>validate_block*scaled_amd(8192, 8192, 8192, block_scale*type=â€mxfp4â€, mfma_nonkdim=32)
[[OK]] (pass mxfp4, mfma_nonk*dim 32)</p>
<p>Common Pitfalls</p>
</section>
<section id="unsupported-hardware">
<h2>1. Unsupported Hardware<a class="headerlink" href="#unsupported-hardware" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[blocked] This example requires GPU support for block scaled matmul&quot;</span><span class="p">)</span>
<span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>def supports_block*scaling():</dt><dd><dl class="simple">
<dt>return (is_cuda() and torch.cuda.get_device*capability()[0] == 10) or </dt><dd><p>is_hip*cdna4()</p>
</dd>
</dl>
</dd>
</dl>
</section>
<section id="format-mismatch">
<h2>2. Format Mismatch<a class="headerlink" href="#format-mismatch" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl class="simple">
<dt>if is_hip*cdna4():</dt><dd><p>assert args.format == â€œmxfp4â€, â€œAMD only supports mxfp4â€</p>
</dd>
</dl>
</section>
<section id="wrong-scale-shape">
<h2>3. Wrong Scale Shape<a class="headerlink" href="#wrong-scale-shape" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>assert a_scale.shape == [M // 128, K // VEC_SIZE // 4, 32, 16]</p>
<p>After packing to 5D
assert a_scale.shape == [1, M // 128, K // VEC_SIZE // 4, 2, 256]</p>
</section>
<section id="missing-tma-allocator">
<h2>4. Missing TMA Allocator<a class="headerlink" href="#missing-tma-allocator" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl class="simple">
<dt>def alloc_fn(size: int, alignment: int, stream: Optional[int]):</dt><dd><p>return torch.empty(size, device=â€cudaâ€, dtype=torch.int8)</p>
</dd>
</dl>
<p>triton.set_allocator(alloc_fn)</p>
</section>
</section>
<section id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Link to this heading">ïƒ</a></h1>
<p><strong>Block scaled matmul</strong> enables efficient low-precision matrix multiplication:</p>
<ul class="simple">
<li><p><strong>4-8x memory reduction</strong> compared to FP16</p></li>
<li><p><strong>2-4x compute speedup</strong> on specialized hardware</p></li>
<li><p><strong>Per-block scaling</strong> maintains numerical accuracy</p></li>
<li><p><strong>Hardware acceleration</strong> via 5th-gen Tensor Cores (NVIDIA) and CDNA4 (AMD)</p></li>
</ul>
<p><strong>Supported formats:</strong>
- <code class="docutils literal notranslate"><span class="pre">nvfp4</span></code>: NVIDIA-optimized FP4 (16 elem/scale)
- <code class="docutils literal notranslate"><span class="pre">mxfp4</span></code>: OCP standard FP4 (32 elem/scale)
- <code class="docutils literal notranslate"><span class="pre">mxfp8</span></code>: OCP standard FP8 (32 elem/scale)
- <code class="docutils literal notranslate"><span class="pre">mixed</span></code>: FP8xFP4 mixed precision</p>
<p><strong>Key techniques:</strong>
- <strong>Scale preshuffling</strong> for contiguous memory access
- <strong>TMA descriptors</strong> for hardware-accelerated loads
- <strong>tl.dot_scaled</strong> intrinsic for scaled operations
- <strong>5D tensor layouts</strong> optimized for tensor cores</p>
<p><strong>When to use:</strong>
- Large language model inference
- Memory-constrained workloads
- Inference on edge devices
- Training with mixed precision</p>
<p><strong>Requirements:</strong>
- NVIDIA Blackwell (CC 10.0+) or AMD CDNA4
- Triton with mxfp support
- Careful attention to scale layout and format</p>
<p>This is the cutting edge of GPU matrix multiplication, enabling the next generation of efficient AI!</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="09-persistent-matmul.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../triton-compiler/01-overview.html" class="btn btn-neutral float-right" title="Key Features" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Concurrent Programs.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>