%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{CPU Concurrency}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}


        \usepackage[utf8]{inputenc}
        \usepackage[T1]{fontenc}
        \usepackage{textcomp}
        \usepackage{times}
        \raggedbottom
    

\title{Fast Concurrent Programming Guide}
\date{Nov 28, 2025}
\release{1.0}
\author{Fast Concurrent Programs}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxAtStartPar
A comprehensive guide to concurrent and parallel programming, covering both CPU\sphinxhyphen{}based concurrency
(threading, asyncio, multiprocessing) and GPU\sphinxhyphen{}based parallelism (Triton, CUDA).

\sphinxAtStartPar
Welcome! This documentation provides in\sphinxhyphen{}depth tutorials and explanations of concurrent programming
techniques for modern Python applications, from multi\sphinxhyphen{}threaded CPU code to massively parallel GPU
kernels.


\chapter{What is Concurrent Programming?}
\label{\detokenize{index:what-is-concurrent-programming}}
\sphinxAtStartPar
Concurrent programming allows multiple tasks to make progress simultaneously, improving performance
and responsiveness. This guide covers two main approaches:

\sphinxAtStartPar
\sphinxstylestrong{CPU Concurrency} (Threads, Async, Processes)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Threading for I/O\sphinxhyphen{}bound tasks

\item {} 
\sphinxAtStartPar
Asyncio for async I/O operations

\item {} 
\sphinxAtStartPar
Multiprocessing for CPU\sphinxhyphen{}bound parallel tasks

\item {} 
\sphinxAtStartPar
Synchronization primitives (locks, semaphores, queues)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{GPU Parallelism} (Triton, CUDA)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Massively parallel computation on GPUs

\item {} 
\sphinxAtStartPar
Custom kernels for deep learning

\item {} 
\sphinxAtStartPar
High\sphinxhyphen{}performance numerical computing

\item {} 
\sphinxAtStartPar
Triton language for accessible GPU programming

\end{itemize}


\chapter{Who This Guide Is For}
\label{\detokenize{index:who-this-guide-is-for}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Python Developers}: Learn concurrent programming patterns

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ML Engineers}: Optimize neural network operations on GPUs

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Performance Engineers}: Achieve maximum utilization

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Students}: Learn both CPU and GPU parallel programming

\end{itemize}


\chapter{Prerequisites}
\label{\detokenize{index:prerequisites}}
\sphinxAtStartPar
\sphinxstylestrong{For CPU Concurrency:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Python programming experience

\item {} 
\sphinxAtStartPar
Basic understanding of threads and processes

\item {} 
\sphinxAtStartPar
Familiarity with Python standard library

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{For GPU Programming:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Python and PyTorch knowledge

\item {} 
\sphinxAtStartPar
Basic linear algebra

\item {} 
\sphinxAtStartPar
Access to NVIDIA or AMD GPU (for GPU sections)

\end{itemize}


\chapter{Getting Started}
\label{\detokenize{index:getting-started}}
\sphinxAtStartPar
\sphinxstylestrong{New to concurrent programming?} Start with {\hyperref[\detokenize{cpu-concurrency/key_concepts::doc}]{\sphinxcrossref{\DUrole{doc}{Key Concepts in Concurrent Programming}}}}.

\sphinxAtStartPar
\sphinxstylestrong{Want to learn GPU programming?} Begin with {\hyperref[\detokenize{gpu-concepts/gpu-fundamentals::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Fundamentals}}}}.

\sphinxAtStartPar
\sphinxstylestrong{Experienced with one area?} Jump directly to the section you need.


\chapter{Documentation Structure}
\label{\detokenize{index:documentation-structure}}
\sphinxstepscope


\section{Key Concepts in Concurrent Programming}
\label{\detokenize{cpu-concurrency/key_concepts:key-concepts-in-concurrent-programming}}\label{\detokenize{cpu-concurrency/key_concepts::doc}}
\sphinxAtStartPar
This document explains the fundamental concepts used across the concurrent programming examples (scripts 06\sphinxhyphen{}10).


\subsection{Table of Contents}
\label{\detokenize{cpu-concurrency/key_concepts:table-of-contents}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/key_concepts:concurrency-vs-parallelism}]{\sphinxsamedocref{Concurrency vs Parallelism}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/key_concepts:threading-vs-multiprocessing}]{\sphinxsamedocref{Threading vs Multiprocessing}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/key_concepts:the-global-interpreter-lock-gil}]{\sphinxsamedocref{The Global Interpreter Lock (GIL)}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/key_concepts:asyncio-and-event-loops}]{\sphinxsamedocref{Asyncio and Event Loops}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/key_concepts:coroutines}]{\sphinxsamedocref{Coroutines}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/key_concepts:tasks-and-futures}]{\sphinxsamedocref{Tasks and Futures}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/key_concepts:cpu-bound-vs-io-bound-operations}]{\sphinxsamedocref{CPU\sphinxhyphen{}bound vs I/O\sphinxhyphen{}bound Operations}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/key_concepts:python-312-migration-changes}]{\sphinxsamedocref{Python 3.12 Migration Changes}}}

\end{enumerate}

\sphinxAtStartPar
—


\subsection{Concurrency vs Parallelism}
\label{\detokenize{cpu-concurrency/key_concepts:id1}}

\subsubsection{Concurrency}
\label{\detokenize{cpu-concurrency/key_concepts:concurrency}}
\sphinxAtStartPar
\sphinxstylestrong{Definition}: Multiple tasks making progress by sharing time on the same resource.

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}:
\sphinxhyphen{} Tasks interleave execution
\sphinxhyphen{} Single CPU core can run concurrent tasks
\sphinxhyphen{} One task pauses, another runs
\sphinxhyphen{} Like juggling \sphinxhyphen{} one ball in hand at a time, but all balls are in play

\sphinxAtStartPar
\sphinxstylestrong{Python Examples}:
\sphinxhyphen{} Threading (limited by GIL)
\sphinxhyphen{} Asyncio (scripts 07\sphinxhyphen{}10)

\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} I/O\sphinxhyphen{}bound operations
\sphinxhyphen{} Network requests
\sphinxhyphen{} File operations
\sphinxhyphen{} User interfaces


\subsubsection{Parallelism}
\label{\detokenize{cpu-concurrency/key_concepts:parallelism}}
\sphinxAtStartPar
\sphinxstylestrong{Definition}: Multiple tasks executing simultaneously on different resources.

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}:
\sphinxhyphen{} True simultaneous execution
\sphinxhyphen{} Requires multiple CPU cores
\sphinxhyphen{} Each task runs on its own core
\sphinxhyphen{} Like having multiple jugglers

\sphinxAtStartPar
\sphinxstylestrong{Python Examples}:
\sphinxhyphen{} Multiprocessing (script 06)
\sphinxhyphen{} ProcessPoolExecutor

\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} CPU\sphinxhyphen{}intensive computations
\sphinxhyphen{} Data processing
\sphinxhyphen{} Scientific computing
\sphinxhyphen{} Video encoding


\subsubsection{Visual Comparison}
\label{\detokenize{cpu-concurrency/key_concepts:visual-comparison}}
\sphinxAtStartPar
Concurrency (Threading/Asyncio):
Core 1: {[}Task A{]}{[}Task B{]}{[}Task A{]}{[}Task C{]}{[}Task B{]}{[}Task A{]}
Time:   ————————————————\sphinxhyphen{}\textgreater{}

\sphinxAtStartPar
Parallelism (Multiprocessing):
Core 1: {[}Task A——————————{]}
Core 2: {[}Task B——————————{]}
Core 3: {[}Task C——————————{]}
Time:   ————————————————\sphinxhyphen{}\textgreater{}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Threading vs Multiprocessing}
\label{\detokenize{cpu-concurrency/key_concepts:id2}}

\subsubsection{Threading (concurrent.futures.ThreadPoolExecutor)}
\label{\detokenize{cpu-concurrency/key_concepts:threading-concurrent-futures-threadpoolexecutor}}
\sphinxAtStartPar
\sphinxstylestrong{How it Works}:
\sphinxhyphen{} Multiple threads in a single process
\sphinxhyphen{} Share the same memory space
\sphinxhyphen{} Limited by GIL in CPython

\sphinxAtStartPar
\sphinxstylestrong{Advantages}:
\sphinxhyphen{} Low memory overhead
\sphinxhyphen{} Fast context switching
\sphinxhyphen{} Easy data sharing between threads
\sphinxhyphen{} Good for I/O\sphinxhyphen{}bound tasks

\sphinxAtStartPar
\sphinxstylestrong{Disadvantages}:
\sphinxhyphen{} Cannot achieve true parallelism for CPU\sphinxhyphen{}bound tasks (GIL limitation)
\sphinxhyphen{} Race conditions possible with shared memory
\sphinxhyphen{} Limited by single CPU core for CPU\sphinxhyphen{}intensive work

\sphinxAtStartPar
\sphinxstylestrong{Example} (from script 06):
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{with concurrent.futures.ThreadPoolExecutor(max\_workers=5) as executor:}\begin{description}
\sphinxlineitem{for item in number\_list:}
\sphinxAtStartPar
executor.submit(evaluate, item)

\end{description}

\end{description}


\subsubsection{Multiprocessing (concurrent.futures.ProcessPoolExecutor)}
\label{\detokenize{cpu-concurrency/key_concepts:multiprocessing-concurrent-futures-processpoolexecutor}}
\sphinxAtStartPar
\sphinxstylestrong{How it Works}:
\sphinxhyphen{} Separate Python processes
\sphinxhyphen{} Each has its own memory space
\sphinxhyphen{} Each has its own Python interpreter
\sphinxhyphen{} Bypasses the GIL

\sphinxAtStartPar
\sphinxstylestrong{Advantages}:
\sphinxhyphen{} True parallelism for CPU\sphinxhyphen{}bound tasks
\sphinxhyphen{} No GIL limitations
\sphinxhyphen{} Can fully utilize multiple CPU cores
\sphinxhyphen{} Process isolation (crash in one doesn’t affect others)

\sphinxAtStartPar
\sphinxstylestrong{Disadvantages}:
\sphinxhyphen{} Higher memory overhead
\sphinxhyphen{} Slower process creation
\sphinxhyphen{} Inter\sphinxhyphen{}process communication overhead
\sphinxhyphen{} More complex data sharing

\sphinxAtStartPar
\sphinxstylestrong{Example} (from script 06):
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{with concurrent.futures.ProcessPoolExecutor(max\_workers=5) as executor:}\begin{description}
\sphinxlineitem{for item in number\_list:}
\sphinxAtStartPar
executor.submit(evaluate, item)

\end{description}

\end{description}


\subsubsection{When to Use What}
\label{\detokenize{cpu-concurrency/key_concepts:when-to-use-what}}
\begin{DUlineblock}{0em}
\item[] Scenario | Use Threading | Use Multiprocessing |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}————\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| I/O\sphinxhyphen{}bound tasks (network, files) | {[}{[}OK{]}{]} Yes | {[}{[}FAIL{]}{]} Overkill |
| CPU\sphinxhyphen{}bound tasks | {[}{[}FAIL{]}{]} No (GIL) | {[}{[}OK{]}{]} Yes |
| Need shared memory | {[}{[}OK{]}{]} Yes | {[}{[}FAIL{]}{]} Complex |
| Need true parallelism | {[}{[}FAIL{]}{]} No | {[}{[}OK{]}{]} Yes |
| Low memory available | {[}{[}OK{]}{]} Yes | {[}{[}FAIL{]}{]} No |

\sphinxAtStartPar
—


\subsection{The Global Interpreter Lock (GIL)}
\label{\detokenize{cpu-concurrency/key_concepts:id3}}

\subsubsection{What is the GIL?}
\label{\detokenize{cpu-concurrency/key_concepts:what-is-the-gil}}
\sphinxAtStartPar
The GIL is a mutex that protects access to Python objects, preventing multiple native threads from executing Python bytecode simultaneously.


\subsubsection{Key Points:}
\label{\detokenize{cpu-concurrency/key_concepts:key-points}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Only one thread can execute Python bytecode at a time

\item {} 
\sphinxAtStartPar
Protects Python’s memory management (reference counting)

\item {} 
\sphinxAtStartPar
Makes CPython memory\sphinxhyphen{}safe but limits parallelism

\item {} 
\sphinxAtStartPar
Released during I/O operations

\item {} 
\sphinxAtStartPar
Not present in Jython, IronPython, or PyPy (with STM)

\end{enumerate}


\subsubsection{Impact on Performance:}
\label{\detokenize{cpu-concurrency/key_concepts:impact-on-performance}}
\sphinxAtStartPar
\sphinxstylestrong{CPU\sphinxhyphen{}bound tasks with threading}:
.. code\sphinxhyphen{}block:: python


\section{With GIL, threads don’t help CPU\sphinxhyphen{}bound tasks:}
\label{\detokenize{cpu-concurrency/key_concepts:with-gil-threads-don-t-help-cpu-bound-tasks}}
\sphinxAtStartPar
Sequential:        \#\#\#\#\#\#\#\#\#\#\#\# (10 seconds)
Threading (4):     \#\#\#\#\#\#\#\#\#\#\#\# (10 seconds) \textless{}\sphinxhyphen{} No improvement!
Multiprocessing:   \#\#\# (2.5 seconds) \textless{}\sphinxhyphen{} True speedup!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{I}\PYG{o}{/}\PYG{n}{O}\PYG{o}{\PYGZhy{}}\PYG{n}{bound} \PYG{n}{tasks} \PYG{k}{with} \PYG{n}{threading}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{GIL is released during I/O, so threading helps:}
\label{\detokenize{cpu-concurrency/key_concepts:gil-is-released-during-i-o-so-threading-helps}}
\sphinxAtStartPar
Sequential:        \#\#\#\#\#\#\#\#\#\#\#\# (10 seconds)
Threading (4):     \#\#\# (2.5 seconds) \textless{}\sphinxhyphen{} Good improvement!
Asyncio:           \#\# (2 seconds) \textless{}\sphinxhyphen{} Even better!

\sphinxAtStartPar
Sequential Execution:     3.47 seconds
Thread Pool Execution:    3.31 seconds  (minimal improvement)
Process Pool Execution:   1.23 seconds  (3x speedup!)

\sphinxAtStartPar
The thread pool shows minimal improvement because the CPU\sphinxhyphen{}bound task is limited by the GIL.

\sphinxAtStartPar
—


\subsection{Asyncio and Event Loops}
\label{\detokenize{cpu-concurrency/key_concepts:id4}}

\subsubsection{Event Loop}
\label{\detokenize{cpu-concurrency/key_concepts:event-loop}}
\sphinxAtStartPar
The event loop is the core of asyncio. It:
\sphinxhyphen{} Manages and schedules coroutines
\sphinxhyphen{} Handles I/O operations
\sphinxhyphen{} Coordinates concurrent tasks
\sphinxhyphen{} Runs in a single thread

\sphinxAtStartPar
\sphinxstylestrong{Conceptual Model}:
\begin{description}
\sphinxlineitem{Event Loop:}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{*{1}{\X{1}{1}}}
\sphinxtoprule
\sphinxtableatstartofbodyhook\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Check for ready tasks

\item {} 
\sphinxAtStartPar
Execute task until await

\item {} 
\sphinxAtStartPar
Switch to next ready task

\item {} 
\sphinxAtStartPar
Handle I/O operations

\item {} 
\sphinxAtStartPar
Repeat

\end{enumerate}
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\end{description}


\subsubsection{How It Works (from script 07):}
\label{\detokenize{cpu-concurrency/key_concepts:how-it-works-from-script-07}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def main():}
\sphinxAtStartPar
loop = asyncio.get\_event*loop()
end\_loop = loop.time() + 60
await task\_A(end\_loop)

\sphinxlineitem{if \sphinxstylestrong{name} == ‘\sphinxstylestrong{main}’:}
\sphinxAtStartPar
asyncio.run(main())  \# Creates and runs event loop

\end{description}


\subsubsection{Event Loop Lifecycle:}
\label{\detokenize{cpu-concurrency/key_concepts:event-loop-lifecycle}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{asyncio.run()}} creates a new event loop

\item {} 
\sphinxAtStartPar
Schedules the main coroutine

\item {} 
\sphinxAtStartPar
Runs the loop until main completes

\item {} 
\sphinxAtStartPar
Closes and cleans up the loop

\end{enumerate}


\subsubsection{Modern vs Old Patterns:}
\label{\detokenize{cpu-concurrency/key_concepts:modern-vs-old-patterns}}
\sphinxAtStartPar
\sphinxstylestrong{Old (Deprecated)}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
loop = asyncio.get\_event*loop()
loop.run\_until*complete(coro())
loop.close()

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Modern} \PYG{p}{(}\PYG{n}{Python} \PYG{l+m+mf}{3.7}\PYG{o}{+}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
asyncio.run(coro())  \# Handles everything automatically

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Coroutines}
\label{\detokenize{cpu-concurrency/key_concepts:id5}}

\subsubsection{What are Coroutines?}
\label{\detokenize{cpu-concurrency/key_concepts:what-are-coroutines}}
\sphinxAtStartPar
Coroutines are functions that can pause and resume execution, yielding control back to the event loop.


\subsubsection{Defining Coroutines:}
\label{\detokenize{cpu-concurrency/key_concepts:defining-coroutines}}
\sphinxAtStartPar
\sphinxstylestrong{Old Syntax (Deprecated)}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
@asyncio.coroutine
def my\_coroutine():
\begin{quote}

\sphinxAtStartPar
result = yield from other\_coroutine()
return result
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Modern} \PYG{n}{Syntax} \PYG{p}{(}\PYG{n}{Python} \PYG{l+m+mf}{3.5}\PYG{o}{+}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def my\_coroutine():}
\sphinxAtStartPar
result = await other\_coroutine()
return result

\end{description}


\subsubsection{Key Features:}
\label{\detokenize{cpu-concurrency/key_concepts:key-features}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Non\sphinxhyphen{}blocking}: Can pause execution without blocking the thread

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Cooperative}: Explicitly yield control with \sphinxcode{\sphinxupquote{await}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chainable}: Can call other coroutines

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Return Values}: Can return results like regular functions

\end{enumerate}


\subsubsection{Example from Script 08:}
\label{\detokenize{cpu-concurrency/key_concepts:example-from-script-08}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def state1(transition\_value):}
\sphinxAtStartPar
output\_value = f’State 1 with transition value = \{transition\_value\}n’
await asyncio.sleep(1)  \# Yields control
\begin{description}
\sphinxlineitem{if input\_value == 0:}
\sphinxAtStartPar
result = await state3(input\_value)  \# Calls another coroutine

\end{description}

\sphinxAtStartPar
return output\_value + f’State 1 calling \{result\}’  \# Returns value

\end{description}


\subsubsection{Execution Flow:}
\label{\detokenize{cpu-concurrency/key_concepts:execution-flow}}\begin{description}
\sphinxlineitem{state1() starts}
\sphinxAtStartPar
down

\sphinxlineitem{Executes synchronous code}
\sphinxAtStartPar
down

\sphinxlineitem{await asyncio.sleep(1)  \textless{}\sphinxhyphen{} Yields control to event loop}
\sphinxAtStartPar
down                       \textless{}\sphinxhyphen{} Other tasks can run here

\sphinxlineitem{Resumes after sleep}
\sphinxAtStartPar
down

\sphinxlineitem{await state3()          \textless{}\sphinxhyphen{} Calls another coroutine}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
Returns result


\subsubsection{Important Rules:}
\label{\detokenize{cpu-concurrency/key_concepts:important-rules}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Must use \textasciigrave{}\textasciigrave{}await\textasciigrave{}\textasciigrave{}} inside async functions (can’t use \sphinxcode{\sphinxupquote{yield from}})

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Can only \textasciigrave{}\textasciigrave{}await\textasciigrave{}\textasciigrave{}} awaitable objects (coroutines, tasks, futures)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Must be called} with \sphinxcode{\sphinxupquote{await}} or scheduled as a task

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use \textasciigrave{}\textasciigrave{}asyncio.sleep()\textasciigrave{}\textasciigrave{}} never \sphinxcode{\sphinxupquote{time.sleep()}} in async code

\end{enumerate}

\sphinxAtStartPar
—


\subsection{Tasks and Futures}
\label{\detokenize{cpu-concurrency/key_concepts:id6}}

\subsubsection{Tasks}
\label{\detokenize{cpu-concurrency/key_concepts:tasks}}
\sphinxAtStartPar
Tasks are wrappers around coroutines that schedule them for execution.

\sphinxAtStartPar
\sphinxstylestrong{Creating Tasks}:

\sphinxAtStartPar
\sphinxstylestrong{Old (Deprecated)}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
task = asyncio.Task(my\_coroutine())

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Modern}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
task = asyncio.create\_task(my\_coroutine())


\subsubsection{Task Characteristics:}
\label{\detokenize{cpu-concurrency/key_concepts:task-characteristics}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Automatically scheduled when created

\item {} 
\sphinxAtStartPar
Run concurrently with other tasks

\item {} 
\sphinxAtStartPar
Can be cancelled

\item {} 
\sphinxAtStartPar
Can be awaited for results

\end{itemize}


\subsubsection{Example from Script 09:}
\label{\detokenize{cpu-concurrency/key_concepts:example-from-script-09}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def main():}\begin{description}
\sphinxlineitem{tasks = {[}}
\sphinxAtStartPar
asyncio.create\_task(factorial(10)),
asyncio.create\_task(fibonacci(10)),
asyncio.create\_task(binomial\_coefficient(20, 10))

\end{description}

\sphinxAtStartPar
{]}
await asyncio.gather({\color{red}\bfseries{}*}tasks)  \# Wait for all tasks

\end{description}


\subsubsection{Futures}
\label{\detokenize{cpu-concurrency/key_concepts:futures}}
\sphinxAtStartPar
Futures represent the eventual result of an asynchronous operation.

\sphinxAtStartPar
\sphinxstylestrong{Modern Approach} (from script 10):
\sphinxhyphen{} Rarely needed in application code
\sphinxhyphen{} Tasks return values directly
\sphinxhyphen{} Use \sphinxcode{\sphinxupquote{await}} instead of callbacks

\sphinxAtStartPar
\sphinxstylestrong{Old Pattern (Deprecated)}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
future = asyncio.Future()
future.add\_done*callback(callback\_function)
future.set\_result(value)
result = future.result()

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Modern} \PYG{n}{Pattern}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
result = await my\_coroutine()  \# Direct return value


\subsubsection{Waiting for Multiple Tasks:}
\label{\detokenize{cpu-concurrency/key_concepts:waiting-for-multiple-tasks}}
\sphinxAtStartPar
\sphinxstylestrong{asyncio.gather()} (Recommended):
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
results = await asyncio.gather(task1, task2, task3)
Returns results in order
========================
Raises exception if any task fails
==================================

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{asyncio}\PYG{o}{.}\PYG{n}{wait}\PYG{p}{(}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*} \PYG{p}{(}\PYG{n}{Old} \PYG{n}{pattern}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
done, pending = await asyncio.wait({[}task1, task2, task3{]})
Returns sets of done and pending tasks
======================================
More complex to use
===================

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{CPU\sphinxhyphen{}bound vs I/O\sphinxhyphen{}bound Operations}
\label{\detokenize{cpu-concurrency/key_concepts:id9}}

\subsubsection{CPU\sphinxhyphen{}bound Operations}
\label{\detokenize{cpu-concurrency/key_concepts:cpu-bound-operations}}
\sphinxAtStartPar
Operations limited by CPU processing speed.

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}:
\sphinxhyphen{} Spend most time computing
\sphinxhyphen{} Use CPU intensively
\sphinxhyphen{} Little to no waiting for external resources
\sphinxhyphen{} Performance scales with CPU power

\sphinxAtStartPar
\sphinxstylestrong{Examples}:
\sphinxhyphen{} Mathematical calculations (factorial, fibonacci)
\sphinxhyphen{} Data processing and transformations
\sphinxhyphen{} Compression/decompression
\sphinxhyphen{} Cryptography
\sphinxhyphen{} Image/video processing

\sphinxAtStartPar
\sphinxstylestrong{Best Solution}:
\sphinxhyphen{} \sphinxstylestrong{Multiprocessing} (ProcessPoolExecutor)
\sphinxhyphen{} Utilizes multiple CPU cores
\sphinxhyphen{} Bypasses GIL limitations

\sphinxAtStartPar
\sphinxstylestrong{Example} (from script 06):
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{def count(number):}\begin{description}
\sphinxlineitem{for i in range(0, 10000000):  \# CPU\sphinxhyphen{}intensive loop}
\sphinxAtStartPar
i += 1

\end{description}

\sphinxAtStartPar
return i * number

\end{description}


\section{Use ProcessPoolExecutor for CPU\sphinxhyphen{}bound tasks}
\label{\detokenize{cpu-concurrency/key_concepts:use-processpoolexecutor-for-cpu-bound-tasks}}\begin{description}
\sphinxlineitem{with concurrent.futures.ProcessPoolExecutor(max\_workers=5) as executor:}\begin{description}
\sphinxlineitem{for item in number\_list:}
\sphinxAtStartPar
executor.submit(evaluate, item)

\end{description}

\end{description}

\sphinxAtStartPar
Operations limited by input/output speed (waiting for data).

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}:
\sphinxhyphen{} Spend most time waiting
\sphinxhyphen{} CPU is mostly idle
\sphinxhyphen{} Depend on external resources
\sphinxhyphen{} Performance limited by I/O speed

\sphinxAtStartPar
\sphinxstylestrong{Examples}:
\sphinxhyphen{} Network requests (HTTP, API calls)
\sphinxhyphen{} Database queries
\sphinxhyphen{} File read/write operations
\sphinxhyphen{} User input
\sphinxhyphen{} Sleep/delays

\sphinxAtStartPar
\sphinxstylestrong{Best Solutions}:
1. \sphinxstylestrong{Asyncio} (best for many I/O operations)
2. \sphinxstylestrong{Threading} (good for blocking I/O)

\sphinxAtStartPar
\sphinxstylestrong{Example} (from scripts 07\sphinxhyphen{}10):
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{async def fetch\_data():}
\sphinxAtStartPar
await asyncio.sleep(2)  \# Simulates I/O wait
\# During this wait, other tasks can run
return data

\end{description}

\begin{DUlineblock}{0em}
\item[] Aspect | CPU\sphinxhyphen{}bound | I/O\sphinxhyphen{}bound |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| \sphinxstylestrong{Bottleneck} | CPU processing | Waiting for I/O |
| \sphinxstylestrong{CPU Usage} | High (100\%) | Low (often idle) |
| \sphinxstylestrong{Best Solution} | Multiprocessing | Asyncio / Threading |
| \sphinxstylestrong{Scales With} | More CPU cores | Concurrent operations |
| \sphinxstylestrong{GIL Impact} | High (blocks parallelism) | Low (released during I/O) |
| \sphinxstylestrong{Example} | Video encoding | API requests |

\sphinxAtStartPar
Some applications have both:
.. code\sphinxhyphen{}block:: python


\section{Combine approaches:}
\label{\detokenize{cpu-concurrency/key_concepts:combine-approaches}}\begin{description}
\sphinxlineitem{async def process\_data():}
\sphinxAtStartPar
\# I/O: Fetch data asynchronously
data = await fetch\_from*api()

\sphinxAtStartPar
\# CPU: Process in separate process
with ProcessPoolExecutor() as executor:
\begin{quote}

\sphinxAtStartPar
loop = asyncio.get\_event*loop()
result = await loop.run\_in*executor(executor, cpu\_intensive*func, data)
\end{quote}

\sphinxAtStartPar
return result

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Python 3.12 Migration Changes}
\label{\detokenize{cpu-concurrency/key_concepts:id10}}
\sphinxAtStartPar
This section summarizes all deprecated features replaced during migration.


\subsubsection{1. Time Measurement (\sphinxstyleliteralintitle{\sphinxupquote{time.clock()}} \sphinxhyphen{}\textgreater{} \sphinxstyleliteralintitle{\sphinxupquote{time.perf\_counter()}})}
\label{\detokenize{cpu-concurrency/key_concepts:time-measurement-time-clock-time-perf-counter}}
\sphinxAtStartPar
\sphinxstylestrong{Issue}: \sphinxcode{\sphinxupquote{time.clock()}} was deprecated in Python 3.3 and removed in Python 3.8.

\sphinxAtStartPar
\sphinxstylestrong{Before}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
start = time.clock()
… work …
============
elapsed = time.clock() \sphinxhyphen{} start

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{After}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
start = time.perf\_counter()
… work …
============
elapsed = time.perf\_counter() \sphinxhyphen{} start

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why**: ``perf\PYGZus{}counter()`` provides:
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Higher resolution timing

\item {} 
\sphinxAtStartPar
System\sphinxhyphen{}wide consistency

\item {} 
\sphinxAtStartPar
Better precision for benchmarking

\end{itemize}


\subsubsection{2. Coroutine Syntax (\sphinxstyleliteralintitle{\sphinxupquote{@asyncio.coroutine}} \sphinxhyphen{}\textgreater{} \sphinxstyleliteralintitle{\sphinxupquote{async def}})}
\label{\detokenize{cpu-concurrency/key_concepts:coroutine-syntax-asyncio-coroutine-async-def}}
\sphinxAtStartPar
\sphinxstylestrong{Issue}: \sphinxcode{\sphinxupquote{@asyncio.coroutine}} decorator deprecated in Python 3.8.

\sphinxAtStartPar
\sphinxstylestrong{Before}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
@asyncio.coroutine
def my\_coroutine():
\begin{quote}

\sphinxAtStartPar
result = yield from other\_coroutine()
return result
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{After}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def my\_coroutine():}
\sphinxAtStartPar
result = await other\_coroutine()
return result

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why**: ``async/await`` is:
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
More explicit and readable

\item {} 
\sphinxAtStartPar
Native language syntax

\item {} 
\sphinxAtStartPar
Better error messages

\item {} 
\sphinxAtStartPar
Type checker friendly

\end{itemize}


\subsubsection{3. Task Creation (\sphinxstyleliteralintitle{\sphinxupquote{asyncio.Task()}} \sphinxhyphen{}\textgreater{} \sphinxstyleliteralintitle{\sphinxupquote{asyncio.create\_task()}})}
\label{\detokenize{cpu-concurrency/key_concepts:task-creation-asyncio-task-asyncio-create-task}}
\sphinxAtStartPar
\sphinxstylestrong{Issue}: Direct \sphinxcode{\sphinxupquote{asyncio.Task()}} constructor deprecated.

\sphinxAtStartPar
\sphinxstylestrong{Before}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
task = asyncio.Task(my\_coroutine())

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{After}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
task = asyncio.create\_task(my\_coroutine())

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why**: ``create\PYGZus{}task()`` is:
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
The official API

\item {} 
\sphinxAtStartPar
Handles edge cases better

\item {} 
\sphinxAtStartPar
Works with custom event loops

\item {} 
\sphinxAtStartPar
More future\sphinxhyphen{}proof

\end{itemize}


\subsubsection{4. Event Loop Management}
\label{\detokenize{cpu-concurrency/key_concepts:event-loop-management}}
\sphinxAtStartPar
\sphinxstylestrong{Issue}: Manual loop management is verbose and error\sphinxhyphen{}prone.

\sphinxAtStartPar
\sphinxstylestrong{Before}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
loop = asyncio.get\_event*loop()
loop.run\_until*complete(my\_coroutine())
loop.close()

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{After}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
asyncio.run(my\_coroutine())

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why**: ``asyncio.run()``:
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Handles loop creation and cleanup

\item {} 
\sphinxAtStartPar
Properly closes the loop

\item {} 
\sphinxAtStartPar
Cleaner and safer

\item {} 
\sphinxAtStartPar
Recommended since Python 3.7

\end{itemize}


\subsubsection{5. Future Callbacks (Callbacks \sphinxhyphen{}\textgreater{} \sphinxstyleliteralintitle{\sphinxupquote{await}})}
\label{\detokenize{cpu-concurrency/key_concepts:future-callbacks-callbacks-await}}
\sphinxAtStartPar
\sphinxstylestrong{Issue}: Callback\sphinxhyphen{}based code is harder to read and maintain.

\sphinxAtStartPar
\sphinxstylestrong{Before}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
future = asyncio.Future()
\begin{description}
\sphinxlineitem{def callback(future):}
\sphinxAtStartPar
print(future.result())

\end{description}

\sphinxAtStartPar
future.add\_done*callback(callback)
await some\_coroutine(future)
future.set\_result(value)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{After}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
result = await some\_coroutine()
print(result)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Why}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Direct} \PYG{k}{await}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
More readable (linear flow)

\item {} 
\sphinxAtStartPar
Better error handling

\item {} 
\sphinxAtStartPar
No callback hell

\item {} 
\sphinxAtStartPar
Exception propagation works naturally

\end{itemize}


\subsubsection{6. Blocking Calls in Async Code}
\label{\detokenize{cpu-concurrency/key_concepts:blocking-calls-in-async-code}}
\sphinxAtStartPar
\sphinxstylestrong{Issue}: Blocking calls freeze the entire event loop.

\sphinxAtStartPar
\sphinxstylestrong{Before}:
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{async def bad\_function():}
\sphinxAtStartPar
time.sleep(5)  \# {[}{[}FAIL{]}{]} Blocks entire event loop!

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{After}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def good\_function():}
\sphinxAtStartPar
await asyncio.sleep(5)  \# {[}{[}OK{]}{]} Yields control

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why**: ``asyncio.sleep()``:
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Doesn’t block the event loop

\item {} 
\sphinxAtStartPar
Allows other tasks to run

\item {} 
\sphinxAtStartPar
Proper async behavior

\end{itemize}


\subsubsection{7. String Formatting (\sphinxstyleliteralintitle{\sphinxupquote{\%}} \sphinxhyphen{}\textgreater{} f\sphinxhyphen{}strings)}
\label{\detokenize{cpu-concurrency/key_concepts:string-formatting-f-strings}}
\sphinxAtStartPar
\sphinxstylestrong{Issue}: Old\sphinxhyphen{}style formatting is less readable.

\sphinxAtStartPar
\sphinxstylestrong{Before}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
print(‘Value: \%s’ \% value)
print(‘X: \%s, Y: \%s’ \% (x, y))

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{After}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
print(f’Value: \{value\}’)
print(f’X: \{x\}, Y: \{y\}’)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Why}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{f}\PYG{o}{\PYGZhy{}}\PYG{n}{strings} \PYG{n}{are}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
More readable

\item {} 
\sphinxAtStartPar
Faster (evaluated at runtime)

\item {} 
\sphinxAtStartPar
Support expressions: \sphinxcode{\sphinxupquote{f\textquotesingle{}\{x + y\}\textquotesingle{}}}

\item {} 
\sphinxAtStartPar
Standard since Python 3.6

\end{itemize}


\subsubsection{Migration Checklist}
\label{\detokenize{cpu-concurrency/key_concepts:migration-checklist}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{[}x{]} Replace \sphinxcode{\sphinxupquote{time.clock()}} with \sphinxcode{\sphinxupquote{time.perf\_counter()}}

\item {} 
\sphinxAtStartPar
{[}x{]} Replace \sphinxcode{\sphinxupquote{@asyncio.coroutine}} with \sphinxcode{\sphinxupquote{async def}}

\item {} 
\sphinxAtStartPar
{[}x{]} Replace \sphinxcode{\sphinxupquote{yield from}} with \sphinxcode{\sphinxupquote{await}}

\item {} 
\sphinxAtStartPar
{[}x{]} Replace \sphinxcode{\sphinxupquote{asyncio.Task()}} with \sphinxcode{\sphinxupquote{asyncio.create\_task()}}

\item {} 
\sphinxAtStartPar
{[}x{]} Replace manual loop management with \sphinxcode{\sphinxupquote{asyncio.run()}}

\item {} 
\sphinxAtStartPar
{[}x{]} Replace Future callbacks with direct \sphinxcode{\sphinxupquote{await}}

\item {} 
\sphinxAtStartPar
{[}x{]} Replace \sphinxcode{\sphinxupquote{time.sleep()}} with \sphinxcode{\sphinxupquote{asyncio.sleep()}} in async code

\item {} 
\sphinxAtStartPar
{[}x{]} Replace \sphinxcode{\sphinxupquote{\%}} formatting with f\sphinxhyphen{}strings

\item {} 
\sphinxAtStartPar
{[}x{]} Test all scripts for compatibility

\end{itemize}


\subsubsection{Compatibility}
\label{\detokenize{cpu-concurrency/key_concepts:compatibility}}
\sphinxAtStartPar
All migrated scripts are compatible with:
\sphinxhyphen{} {[}{[}OK{]}{]} Python 3.12 (tested)
\sphinxhyphen{} {[}{[}OK{]}{]} Python 3.11
\sphinxhyphen{} {[}{[}OK{]}{]} Python 3.10
\sphinxhyphen{} {[}{[}OK{]}{]} Python 3.9
\sphinxhyphen{} {[}{[}OK{]}{]} Python 3.8
\sphinxhyphen{} {[}{[}OK{]}{]} Python 3.7 (minimum for \sphinxcode{\sphinxupquote{asyncio.run()}})

\sphinxAtStartPar
—


\subsection{Summary}
\label{\detokenize{cpu-concurrency/key_concepts:summary}}

\subsubsection{Quick Reference Guide}
\label{\detokenize{cpu-concurrency/key_concepts:quick-reference-guide}}
\begin{DUlineblock}{0em}
\item[] Task Type | Best Solution | Example Script |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}————\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| CPU\sphinxhyphen{}intensive | ProcessPoolExecutor | Script 06 |
| I/O\sphinxhyphen{}intensive | Asyncio | Scripts 07\sphinxhyphen{}10 |
| Mixed workload | Asyncio + ProcessPool | \sphinxhyphen{} |
| Simple concurrency | ThreadPoolExecutor | \sphinxhyphen{} |
| Event scheduling | Asyncio event loop | Script 07 |
| State machines | Asyncio coroutines | Script 08 |
| Parallel tasks | asyncio.gather() | Script 09 |
| With CLI args | asyncio.run() | Script 10 |


\subsubsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/key_concepts:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Understand your workload}: CPU\sphinxhyphen{}bound vs I/O\sphinxhyphen{}bound determines the solution

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Respect the GIL}: Use multiprocessing for CPU\sphinxhyphen{}bound tasks

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use modern syntax}: async/await is clearer than callbacks

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Never block the loop}: Use asyncio.sleep(), not time.sleep()

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Leverage asyncio.run()}: Simplest way to run async code

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Create tasks properly}: Use asyncio.create\_task()

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Gather results}: Use asyncio.gather() for multiple tasks

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Profile first}: Measure before optimizing

\end{enumerate}


\subsubsection{Further Reading}
\label{\detokenize{cpu-concurrency/key_concepts:further-reading}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.python.org/3/library/asyncio.html}{Python asyncio documentation}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.python.org/3/library/concurrent.futures.html}{concurrent.futures documentation}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.python.org/3/glossary.html\#term-global-interpreter-lock}{Understanding the GIL}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.python.org/dev/peps/pep-0492/}{PEP 492 \sphinxhyphen{} Coroutines with async/await}

\end{itemize}

\sphinxstepscope


\section{Hardware Parallelism: Cores, Hyperthreading, and GPUs}
\label{\detokenize{cpu-concurrency/hardware_parallelism:hardware-parallelism-cores-hyperthreading-and-gpus}}\label{\detokenize{cpu-concurrency/hardware_parallelism::doc}}
\sphinxAtStartPar
A comprehensive guide to understanding physical cores, hyperthreading, and why GPUs excel at compute\sphinxhyphen{}intensive parallel tasks.


\subsection{Table of Contents}
\label{\detokenize{cpu-concurrency/hardware_parallelism:table-of-contents}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/hardware_parallelism:physical-cores-vs-logical-cores}]{\sphinxsamedocref{Physical Cores vs Logical Cores}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/hardware_parallelism:what-is-hyperthreading}]{\sphinxsamedocref{What is Hyperthreading?}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/hardware_parallelism:how-threading-is-bounded-by-physical-cores}]{\sphinxsamedocref{How Threading is Bounded by Physical Cores}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/hardware_parallelism:why-gpus-excel-at-parallel-computing}]{\sphinxsamedocref{Why GPUs Excel at Parallel Computing}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/hardware_parallelism:cpu-vs-gpu-architecture}]{\sphinxsamedocref{CPU vs GPU Architecture}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/hardware_parallelism:when-to-use-what}]{\sphinxsamedocref{When to Use What}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/hardware_parallelism:practical-examples}]{\sphinxsamedocref{Practical Examples}}}

\end{enumerate}

\sphinxAtStartPar
—


\subsection{Physical Cores vs Logical Cores}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id1}}

\subsubsection{Physical Cores}
\label{\detokenize{cpu-concurrency/hardware_parallelism:physical-cores}}
\sphinxAtStartPar
\sphinxstylestrong{Definition}: An actual, independent processing unit on the CPU chip with its own:
\sphinxhyphen{} Arithmetic Logic Unit (ALU)
\sphinxhyphen{} Floating Point Unit (FPU)
\sphinxhyphen{} L1 and L2 cache
\sphinxhyphen{} Execution units

\sphinxAtStartPar
Physical CPU Chip:
+—————————————————\textendash{}+
|                                                     |
|  +———\sphinxhyphen{}+  +———\sphinxhyphen{}+  +———\sphinxhyphen{}+         |
|  |  Core 0  |  |  Core 1  |  |  Core 2  |   …   |
|  |          |  |          |  |          |         |
|  | +——+ |  | +——+ |  | +——+ |         |
|  | | ALU  | |  | | ALU  | |  | | ALU  | |         |
|  | {\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|} |  | {\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|} |  | {\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|} |         |
|  | | FPU  | |  | | FPU  | |  | | FPU  | |         |
|  | {\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|} |  | {\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|} |  | {\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|} |         |
|  | | L1/L2| |  | | L1/L2| |  | | L1/L2| |         |
|  | +——+ |  | +——+ |  | +——+ |         |
|  +———\sphinxhyphen{}+  +———\sphinxhyphen{}+  +———\sphinxhyphen{}+         |
|                                                     |
|              Shared L3 Cache                        |
+—————————————————\textendash{}+

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}:
\sphinxhyphen{} {[}{[}OK{]}{]} True parallel execution
\sphinxhyphen{} {[}{[}OK{]}{]} Independent computation streams
\sphinxhyphen{} {[}{[}OK{]}{]} Each can execute different instructions simultaneously
\sphinxhyphen{} {[}{[}OK{]}{]} Maximum performance for CPU\sphinxhyphen{}bound tasks


\subsubsection{Logical Cores}
\label{\detokenize{cpu-concurrency/hardware_parallelism:logical-cores}}
\sphinxAtStartPar
\sphinxstylestrong{Definition}: Virtual cores created by technologies like Intel’s Hyperthreading or AMD’s Simultaneous Multithreading (SMT).

\sphinxAtStartPar
\sphinxstylestrong{Example}:
\sphinxhyphen{} CPU: Intel Core i7 with 4 physical cores
\sphinxhyphen{} With Hyperthreading: Shows as 8 logical cores
\sphinxhyphen{} Ratio: 2 logical cores per 1 physical core

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import os


\section{Check on your system}
\label{\detokenize{cpu-concurrency/hardware_parallelism:check-on-your-system}}
\sphinxAtStartPar
logical\_cores = os.cpu\_count()  \# e.g., 8
Physical cores require platform\sphinxhyphen{}specific code:
==============================================
On Linux: check /proc/cpuinfo
=============================
On macOS: sysctl hw.physicalcpu
===============================
Typically: physical\_cores = logical\_cores / 2 (if HT enabled)
=============================================================

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{What is Hyperthreading?}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id2}}

\subsubsection{The Concept}
\label{\detokenize{cpu-concurrency/hardware_parallelism:the-concept}}
\sphinxAtStartPar
\sphinxstylestrong{Hyperthreading (HT)} allows a single physical core to execute two instruction streams (threads) simultaneously by sharing the core’s execution resources.


\subsubsection{How It Works}
\label{\detokenize{cpu-concurrency/hardware_parallelism:how-it-works}}
\sphinxAtStartPar
A CPU core has multiple execution units but they’re not always all in use:

\sphinxAtStartPar
Without Hyperthreading (one thread per core):
Time \sphinxhyphen{}\textgreater{}
Core execution units: {[}ALU{]}{[}FPU{]}{[}Load{]}{[}Store{]}{[}Branch{]}
\begin{quote}

\sphinxAtStartPar
down    down     down     down      down
\end{quote}
\begin{description}
\sphinxlineitem{Thread A:             {[}{[}\#{]}{]}  {[} {]}   {[}{[}\#{]}{]}   {[} {]}    {[}{[}\#{]}{]}   \textless{}\sphinxhyphen{} Only 60\% utilized}\begin{quote}

\sphinxAtStartPar
up         up            up
\end{quote}

\sphinxAtStartPar
Used units  (unused)   Used units

\end{description}

\sphinxAtStartPar
Wasted capacity: 40\%

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{:}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
With Hyperthreading (two threads per core):
Time \sphinxhyphen{}\textgreater{}
Core execution units: {[}ALU{]}{[}FPU{]}{[}Load{]}{[}Store{]}{[}Branch{]}
\begin{quote}

\sphinxAtStartPar
down    down     down     down      down
\end{quote}

\sphinxAtStartPar
Thread A:             {[}{[}\#{]}{]}  {[} {]}   {[}{[}\#{]}{]}   {[} {]}    {[}{[}\#{]}{]}
Thread B:             {[} {]}  {[}{[}\#{]}{]}   {[} {]}   {[}{[}\#{]}{]}    {[} {]}   \textless{}\sphinxhyphen{} Fills the gaps!
\begin{quote}

\sphinxAtStartPar
up    up     up     up      up
\end{quote}

\sphinxAtStartPar
Combined utilization: {[}{[}\#{]}{]}  {[}{[}\#{]}{]}   {[}{[}\#{]}{]}   {[}{[}\#{]}{]}    {[}{[}\#{]}{]}   \textless{}\sphinxhyphen{} \textasciitilde{}85\% utilized

\sphinxAtStartPar
Better resource usage!


\subsubsection{Technical Implementation}
\label{\detokenize{cpu-concurrency/hardware_parallelism:technical-implementation}}
\sphinxAtStartPar
Each physical core with HT has:

\sphinxAtStartPar
Physical Core with Hyperthreading:
+—————————————\sphinxhyphen{}+
|  Duplicated (per thread):              |
|  +———\sphinxhyphen{}+      +———\sphinxhyphen{}+        |
|  | Thread 1 |      | Thread 2 |        |
|  |          |      |          |        |
|  | * PC     |      | * PC     |        |  PC = Program Counter
|  | * Regs   |      | * Regs   |        |  Regs = Registers
|  | * State  |      | * State  |        |
|  +———\sphinxhyphen{}+      +———\sphinxhyphen{}+        |
|                                        |
|  Shared (between both threads):        |
|  +———————————\sphinxhyphen{}+  |
|  | * ALU (Arithmetic Logic Unit)    |  |
|  | * FPU (Floating Point Unit)      |  |
|  | * L1/L2 Cache                    |  |
|  | * Execution Units                |  |
|  | * Load/Store Units               |  |
|  +———————————\sphinxhyphen{}+  |
+—————————————\sphinxhyphen{}+

\sphinxAtStartPar
\sphinxstylestrong{Key Insight}: Two threads share the same execution hardware but have separate architectural state (registers, program counter).


\subsubsection{Performance Characteristics}
\label{\detokenize{cpu-concurrency/hardware_parallelism:performance-characteristics}}
\sphinxAtStartPar
\sphinxstylestrong{Best Case} (threads use different execution units):

\sphinxAtStartPar
Thread A: Integer operations (uses ALU)
Thread B: Floating\sphinxhyphen{}point operations (uses FPU)
Result: \textasciitilde{}70\sphinxhyphen{}80\% better performance than single thread

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Worst} \PYG{n}{Case}\PYG{o}{*}\PYG{o}{*} \PYG{p}{(}\PYG{n}{threads} \PYG{n}{compete} \PYG{k}{for} \PYG{n}{same} \PYG{n}{resources}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
Thread A: Integer operations (needs ALU)
Thread B: Integer operations (also needs ALU)
Result: \textasciitilde{}10\sphinxhyphen{}20\% better performance (mostly from hiding latency)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Reality}\PYG{o}{*}\PYG{o}{*} \PYG{p}{(}\PYG{n}{typical} \PYG{n}{workloads}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
Hyperthreading improvement: 20\sphinxhyphen{}40\% on average
Still much less than true dual\sphinxhyphen{}core: 100\% improvement


\subsubsection{Hyperthreading Limitations}
\label{\detokenize{cpu-concurrency/hardware_parallelism:hyperthreading-limitations}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Not True Parallelism}

\sphinxAtStartPar
1 physical core + HT = 1.3x performance (not 2x)
2 physical cores = 2x performance

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Shared Resources Create Contention}

\sphinxAtStartPar
Both threads need cache \sphinxhyphen{}\textgreater{} cache thrashing
Both threads need FPU \sphinxhyphen{}\textgreater{} one waits
Both threads need memory \sphinxhyphen{}\textgreater{} bandwidth split

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Can Hurt Performance in Some Cases}
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
\# CPU\sphinxhyphen{}intensive Python code with GIL
\# 4 physical cores, 8 logical cores

\sphinxAtStartPar
\# Using 4 workers (physical cores): 3.8x speedup {[}{[}OK{]}{]}
\# Using 8 workers (logical cores): 3.2x speedup {[}{[}FAIL{]}{]} (worse!)

\sphinxAtStartPar
\# Why? OS scheduling overhead + resource contention

\end{enumerate}


\subsubsection{Checking Hyperthreading Status}
\label{\detokenize{cpu-concurrency/hardware_parallelism:checking-hyperthreading-status}}
\sphinxAtStartPar
\sphinxstylestrong{Linux}:
.. code\sphinxhyphen{}block:: bash


\section{Check if HT is enabled}
\label{\detokenize{cpu-concurrency/hardware_parallelism:check-if-ht-is-enabled}}
\sphinxAtStartPar
lscpu | grep “Thread(s) per core”
Output: Thread(s) per core: 2  \textless{}\sphinxhyphen{} HT enabled
===========================================
Output: Thread(s) per core: 1  \textless{}\sphinxhyphen{} HT disabled
============================================


\section{Or check CPU info}
\label{\detokenize{cpu-concurrency/hardware_parallelism:or-check-cpu-info}}
\sphinxAtStartPar
grep \sphinxhyphen{}E “siblings|cpu cores” /proc/cpuinfo | head \sphinxhyphen{}2
siblings = logical cores per physical CPU
=========================================
cpu cores = physical cores per physical CPU
===========================================

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{macOS}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Logical cores}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id3}}
\sphinxAtStartPar
sysctl hw.logicalcpu
Output: hw.logicalcpu: 8
========================


\section{Physical cores}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id4}}
\sphinxAtStartPar
sysctl hw.physicalcpu
Output: hw.physicalcpu: 4
=========================


\section{If logical \textgreater{} physical, HT is enabled}
\label{\detokenize{cpu-concurrency/hardware_parallelism:if-logical-physical-ht-is-enabled}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Python}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import os
import subprocess

\sphinxAtStartPar
logical\_cores = os.cpu\_count()


\section{Platform\sphinxhyphen{}specific physical core detection}
\label{\detokenize{cpu-concurrency/hardware_parallelism:platform-specific-physical-core-detection}}
\sphinxAtStartPar
import platform
if platform.system() == ‘Darwin’:  \# macOS
\begin{quote}
\begin{description}
\sphinxlineitem{result = subprocess.run({[}‘sysctl’, ‘\sphinxhyphen{}n’, ‘hw.physicalcpu’{]},}
\sphinxAtStartPar
capture\_output=True, text=True)

\end{description}

\sphinxAtStartPar
physical\_cores = int(result.stdout.strip())
\end{quote}
\begin{description}
\sphinxlineitem{elif platform.system() == ‘Linux’:}
\sphinxAtStartPar
\# Count unique physical IDs
with open(‘/proc/cpuinfo’) as f:
\begin{quote}
\begin{description}
\sphinxlineitem{physical\_cores = len(set(}
\sphinxAtStartPar
line.split(‘:’){[}1{]}.strip()
for line in f
if line.startswith(‘physical id’)

\end{description}

\sphinxAtStartPar
))
\end{quote}

\sphinxlineitem{else:  \# Windows}
\sphinxAtStartPar
physical\_cores = logical\_cores // 2  \# Approximation

\end{description}

\sphinxAtStartPar
print(f”Logical cores: \{logical\_cores\}”)
print(f”Physical cores: \{physical\_cores\}”)
print(f”Hyperthreading: \{‘Enabled’ if logical\_cores \textgreater{} physical\_cores else ‘Disabled’\}”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{How Threading is Bounded by Physical Cores}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id5}}

\subsubsection{The Fundamental Constraint}
\label{\detokenize{cpu-concurrency/hardware_parallelism:the-fundamental-constraint}}
\sphinxAtStartPar
\sphinxstylestrong{No matter how many threads you create, true parallel execution is limited by physical cores.}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
System: 4 physical cores (8 logical with HT)

Scenario 1: 4 CPU\PYGZhy{}intensive threads
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
|Thread 1 | |Thread 2 | |Thread 3 | |Thread 4 |
|  100\PYGZpc{}   | |  100\PYGZpc{}   | |  100\PYGZpc{}   | |  100\PYGZpc{}   |
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
     down           down           down           down
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
| Core 0  | | Core 1  | | Core 2  | | Core 3  |
|  100\PYGZpc{}   | |  100\PYGZpc{}   | |  100\PYGZpc{}   | |  100\PYGZpc{}   |
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
Result: Perfect utilization, 4x speedup [OK]

Scenario 2: 8 CPU\PYGZhy{}intensive threads
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
| T1 || T2 || T3 || T4 || T5 || T6 || T7 || T8 |
+\PYGZhy{}+\PYGZhy{}\PYGZhy{}++\PYGZhy{}+\PYGZhy{}\PYGZhy{}++\PYGZhy{}+\PYGZhy{}\PYGZhy{}++\PYGZhy{}+\PYGZhy{}\PYGZhy{}++\PYGZhy{}+\PYGZhy{}\PYGZhy{}++\PYGZhy{}+\PYGZhy{}\PYGZhy{}++\PYGZhy{}+\PYGZhy{}\PYGZhy{}++\PYGZhy{}+\PYGZhy{}\PYGZhy{}+
  +\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}+\PYGZhy{}+
     down      down      down      down      down      down      down
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
| Core 0  | | Core 1  | | Core 2  | | Core 3  |
| T1 + T5 | | T2 + T6 | | T3 + T7 | | T4 + T8 |
| compete | | compete | | compete | | compete |
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
Result: \PYGZti{}4.5x speedup (not 8x!) [warning]

Scenario 3: 16 CPU\PYGZhy{}intensive threads
+\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}++\PYGZhy{}\PYGZhy{}+
|T1||T2||T3||T4||T5||T6||T7||T8||T9||10||11||12||13||14||15||16|
++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+++\PYGZhy{}+
 +\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}+
                          down
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
| Core 0  | | Core 1  | | Core 2  | | Core 3  |
| 4 threads| | 4 threads| | 4 threads| | 4 threads|
|time\PYGZhy{}slice| |time\PYGZhy{}slice| |time\PYGZhy{}slice| |time\PYGZhy{}slice|
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+ +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
Result: \PYGZti{}4x speedup (same as 4 threads!) + overhead [FAIL]
\end{sphinxVerbatim}


\subsubsection{Why More Threads != More Speed}
\label{\detokenize{cpu-concurrency/hardware_parallelism:why-more-threads-more-speed}}
\sphinxAtStartPar
\sphinxstylestrong{CPU\sphinxhyphen{}bound tasks} are limited by actual computation capacity:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Benchmark: Computing sum of squares}
\label{\detokenize{cpu-concurrency/hardware_parallelism:benchmark-computing-sum-of-squares}}
\sphinxAtStartPar
import time
from concurrent.futures import ProcessPoolExecutor
\begin{description}
\sphinxlineitem{def compute(n):}
\sphinxAtStartPar
return sum(i\_i for i in range(n))

\sphinxlineitem{def benchmark(num\_workers):}
\sphinxAtStartPar
start = time.perf\_counter()
with ProcessPoolExecutor(max\_workers=num\_workers) as executor:
\begin{quote}

\sphinxAtStartPar
tasks = {[}10*000*000{]} * num\_workers
list(executor.map(compute, tasks))
\end{quote}

\sphinxAtStartPar
return time.perf\_counter() \sphinxhyphen{} start

\end{description}


\section{Results on 4\sphinxhyphen{}core CPU:}
\label{\detokenize{cpu-concurrency/hardware_parallelism:results-on-4-core-cpu}}

\section{1 worker:  10.0s  (baseline)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:worker-10-0s-baseline}}

\section{2 workers:  5.1s  (1.96x speedup) {[}{[}OK{]}{]}}
\label{\detokenize{cpu-concurrency/hardware_parallelism:workers-5-1s-1-96x-speedup-ok}}

\section{4 workers:  2.6s  (3.85x speedup) {[}{[}OK{]}{]}}
\label{\detokenize{cpu-concurrency/hardware_parallelism:workers-2-6s-3-85x-speedup-ok}}

\section{8 workers:  2.8s  (3.57x speedup) {[}warning{]} (worse than 4!)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:workers-2-8s-3-57x-speedup-warning-worse-than-4}}

\section{16 workers: 3.2s  (3.13x speedup) {[}{[}FAIL{]}{]} (much worse!)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:workers-3-2s-3-13x-speedup-fail-much-worse}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Why} \PYG{n}{performance} \PYG{n}{degrades}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Context Switching Overhead}

\sphinxAtStartPar
OS must constantly switch between threads:
\sphinxhyphen{} Save thread state (registers, PC, stack pointer)
\sphinxhyphen{} Load next thread state
\sphinxhyphen{} Flush CPU caches
\sphinxhyphen{} Update memory mappings

\sphinxAtStartPar
Cost: \textasciitilde{}1\sphinxhyphen{}10 microseconds per switch
With many threads: Spends more time switching than computing!

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Cache Thrashing}

\sphinxAtStartPar
Each thread loads its data into cache:
Thread A: Loads data \sphinxhyphen{}\textgreater{} Evicts Thread B’s cache
Thread B: Loads data \sphinxhyphen{}\textgreater{} Evicts Thread C’s cache
Thread C: Loads data \sphinxhyphen{}\textgreater{} Evicts Thread A’s cache
Thread A: Needs data again \sphinxhyphen{}\textgreater{} Cache miss! (must reload)

\sphinxAtStartPar
Result: More memory access, slower execution

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource Contention}

\sphinxAtStartPar
Multiple threads compete for:
\sphinxhyphen{} Memory bandwidth
\sphinxhyphen{} Cache space
\sphinxhyphen{} TLB entries
\sphinxhyphen{} Execution units

\sphinxAtStartPar
More threads = More contention = Slower per\sphinxhyphen{}thread progress

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{For CPU\sphinxhyphen{}bound tasks}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import os


\section{Best practice:}
\label{\detokenize{cpu-concurrency/hardware_parallelism:best-practice}}
\sphinxAtStartPar
physical\_cores = os.cpu\_count() // 2  \# Approximate physical cores
optimal\_workers = physical\_cores


\section{Conservative (recommended for production):}
\label{\detokenize{cpu-concurrency/hardware_parallelism:conservative-recommended-for-production}}
\sphinxAtStartPar
optimal\_workers = max(1, physical\_cores \sphinxhyphen{} 1)  \# Leave one core for OS


\section{Or detect actual physical cores:}
\label{\detokenize{cpu-concurrency/hardware_parallelism:or-detect-actual-physical-cores}}
\sphinxAtStartPar
import psutil  \# pip install psutil
optimal\_workers = psutil.cpu\_count(logical=False)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Rule} \PYG{n}{of} \PYG{n}{thumb}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
CPU\sphinxhyphen{}bound tasks:
{\color{red}\bfseries{}|}\sphinxhyphen{} Optimal workers = Physical cores
{\color{red}\bfseries{}|}\sphinxhyphen{} Max useful workers = Physical cores + 1 or 2
+\sphinxhyphen{} More workers = Performance degradation

\sphinxAtStartPar
I/O\sphinxhyphen{}bound tasks:
{\color{red}\bfseries{}|}\sphinxhyphen{} Optimal workers = Much higher (100s\sphinxhyphen{}1000s with asyncio)
{\color{red}\bfseries{}|}\sphinxhyphen{} No physical core limit (threads are mostly waiting)
+\sphinxhyphen{} Limited by memory and file descriptors instead

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Matrix multiplication (CPU\sphinxhyphen{}intensive)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:matrix-multiplication-cpu-intensive}}
\sphinxAtStartPar
import numpy as np
from concurrent.futures import ProcessPoolExecutor
import time
\begin{description}
\sphinxlineitem{def multiply\_matrices(size):}
\sphinxAtStartPar
A = np.random.rand(size, size)
B = np.random.rand(size, size)
return np.dot(A, B)

\sphinxlineitem{def benchmark(num\_workers, num\_tasks=12):}
\sphinxAtStartPar
“””
Multiply 12 matrices of size 1000x1000
Each multiplication takes \textasciitilde{}1 second on one core
“””
start = time.perf\_counter()
\begin{description}
\sphinxlineitem{with ProcessPoolExecutor(max\_workers=num\_workers) as executor:}\begin{description}
\sphinxlineitem{futures = {[}executor.submit(multiply\_matrices, 1000)}
\sphinxAtStartPar
for * in range(num\_tasks){]}

\end{description}

\sphinxAtStartPar
results = {[}f.result() for f in futures{]}

\end{description}

\sphinxAtStartPar
elapsed = time.perf\_counter() \sphinxhyphen{} start
speedup = (num\_tasks * 1.0) / elapsed
efficiency = speedup / num\_workers * 100

\sphinxAtStartPar
return elapsed, speedup, efficiency

\end{description}


\section{Test on 4\sphinxhyphen{}core CPU:}
\label{\detokenize{cpu-concurrency/hardware_parallelism:test-on-4-core-cpu}}
\sphinxAtStartPar
print(“Workers | Time  | Speedup | Efficiency”)
print(”——\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———{\color{red}\bfseries{}|}————“)
for workers in {[}1, 2, 4, 6, 8{]}:
\begin{quote}

\sphinxAtStartPar
time, speedup, eff = benchmark(workers)
print(f”\{workers:7\} | \{time:5.1f\}s | \{speedup:5.2f\}x  | \{eff:6.1f\}\%”)
\end{quote}


\section{Typical output:}
\label{\detokenize{cpu-concurrency/hardware_parallelism:typical-output}}

\section{Workers | Time  | Speedup | Efficiency}
\label{\detokenize{cpu-concurrency/hardware_parallelism:workers-time-speedup-efficiency}}

\section{——\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———{\color{red}\bfseries{}|}————}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id18}}\begin{quote}

\sphinxAtStartPar
1 | 12.0s |  1.00x  |  100.0\%
\end{quote}
\begin{quote}

\sphinxAtStartPar
4 |  3.1s |  3.87x  |   96.8\%  \textless{}\sphinxhyphen{} Near perfect
\end{quote}
\begin{quote}

\sphinxAtStartPar
8 |  2.9s |  4.14x  |   51.8\%  \textless{}\sphinxhyphen{} Getting worse
\end{quote}

\sphinxAtStartPar
\sphinxstylestrong{Analysis}:
\sphinxhyphen{} \sphinxstylestrong{1\sphinxhyphen{}4 workers}: Nearly linear speedup (limited by 4 physical cores)
\sphinxhyphen{} \sphinxstylestrong{6 workers}: Still faster, but efficiency drops (HT helps a bit)
\sphinxhyphen{} \sphinxstylestrong{8 workers}: Performance plateau or degradation (overhead dominates)

\sphinxAtStartPar
—


\subsection{Why GPUs Excel at Parallel Computing}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id19}}

\subsubsection{CPU vs GPU: Different Design Philosophies}
\label{\detokenize{cpu-concurrency/hardware_parallelism:cpu-vs-gpu-different-design-philosophies}}
\sphinxAtStartPar
CPU (Latency\sphinxhyphen{}Optimized):
Goal: Execute single thread as fast as possible

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}
\PYG{o}{|}  \PYG{n}{Few} \PYG{n}{cores} \PYG{p}{(}\PYG{l+m+mi}{4}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{64}\PYG{p}{)}\PYG{p}{,} \PYG{n}{each} \PYG{n}{very} \PYG{n}{powerful}  \PYG{o}{|}
\PYG{o}{|}                                        \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}  \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|} \PYG{n}{Core} \PYG{l+m+mi}{0}  \PYG{o}{|}  \PYG{o}{|} \PYG{n}{Core} \PYG{l+m+mi}{1}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}         \PYG{o}{|}  \PYG{o}{|}         \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{ALU}\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{ALU}\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}        \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{|}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{FPU}\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{FPU}\PYG{o}{|}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{|}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{Big}\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{Big}\PYG{o}{|}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{L1} \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{L1} \PYG{o}{|}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{|}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{L2} \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{|}\PYG{n}{L2} \PYG{o}{|}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}  \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}  \PYG{o}{|}  \PYG{o}{|}  \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}  \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}         \PYG{o}{|}  \PYG{o}{|}         \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|} \PYG{n}{Complex} \PYG{o}{|}  \PYG{o}{|} \PYG{n}{Complex} \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|} \PYG{n}{Control} \PYG{o}{|}  \PYG{o}{|} \PYG{n}{Control} \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|}\PYG{n}{Out}\PYG{o}{\PYGZhy{}}\PYG{n}{order}\PYG{o}{|}  \PYG{o}{|}\PYG{n}{Out}\PYG{o}{\PYGZhy{}}\PYG{n}{order}\PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{|} \PYG{n}{Exec}    \PYG{o}{|}  \PYG{o}{|} \PYG{n}{Exec}    \PYG{o}{|}             \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}  \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}             \PYG{o}{|}
\PYG{o}{|}                                        \PYG{o}{|}
\PYG{o}{|}      \PYG{n}{Huge} \PYG{n}{L3} \PYG{n}{Cache} \PYG{p}{(}\PYG{l+m+mi}{32}\PYG{o}{+} \PYG{n}{MB}\PYG{p}{)}            \PYG{o}{|}
\PYG{o}{|}      \PYG{n}{Complex} \PYG{n}{Branch} \PYG{n}{Prediction}         \PYG{o}{|}
\PYG{o}{|}      \PYG{n}{Speculative} \PYG{n}{Execution}             \PYG{o}{|}
\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}
\end{sphinxVerbatim}

\sphinxAtStartPar
GPU (Throughput\sphinxhyphen{}Optimized):
Goal: Execute many threads simultaneously

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}
\PYG{o}{|}  \PYG{n}{Thousands} \PYG{n}{of} \PYG{n}{tiny} \PYG{n}{cores}               \PYG{o}{|}
\PYG{o}{|}                                        \PYG{o}{|}
\PYG{o}{|} \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}        \PYG{o}{|}
\PYG{o}{|} \PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}        \PYG{o}{|}
\PYG{o}{|} \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}        \PYG{o}{|}
\PYG{o}{|} \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}        \PYG{o}{|}
\PYG{o}{|} \PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}   \PYG{o}{|}
\PYG{o}{|} \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}        \PYG{o}{|}
\PYG{o}{|} \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}        \PYG{o}{|}
\PYG{o}{|} \PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}\PYG{o}{|}\PYG{n}{C}\PYG{o}{|}        \PYG{o}{|}
\PYG{o}{|} \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{+}        \PYG{o}{|}
\PYG{o}{|}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.} \PYG{p}{(}\PYG{n}{thousands} \PYG{n}{more}\PYG{p}{)} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}              \PYG{o}{|}
\PYG{o}{|}                                        \PYG{o}{|}
\PYG{o}{|}  \PYG{n}{Tiny} \PYG{n}{L1} \PYG{n}{caches}                        \PYG{o}{|}
\PYG{o}{|}  \PYG{n}{Simple} \PYG{n}{control} \PYG{n}{logic}                  \PYG{o}{|}
\PYG{o}{|}  \PYG{n}{No} \PYG{n}{branch} \PYG{n}{prediction}                  \PYG{o}{|}
\PYG{o}{|}  \PYG{n}{No} \PYG{n}{out}\PYG{o}{\PYGZhy{}}\PYG{n}{of}\PYG{o}{\PYGZhy{}}\PYG{n}{order} \PYG{n}{execution}             \PYG{o}{|}
\PYG{o}{|}  \PYG{n}{SIMD}\PYG{p}{:} \PYG{n}{Same} \PYG{n}{instruction}\PYG{p}{,} \PYG{n+nb}{all} \PYG{n}{cores}     \PYG{o}{|}
\PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{+}
\end{sphinxVerbatim}


\subsubsection{Key Differences}
\label{\detokenize{cpu-concurrency/hardware_parallelism:key-differences}}
\begin{DUlineblock}{0em}
\item[] Aspect | CPU | GPU |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}—\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| \sphinxstylestrong{Core Count} | 4\sphinxhyphen{}64 cores | 1,000s\sphinxhyphen{}10,000s of cores |
| \sphinxstylestrong{Core Speed} | 3\sphinxhyphen{}5 GHz | 1\sphinxhyphen{}2 GHz |
| \sphinxstylestrong{Core Complexity} | Very complex | Very simple |
| \sphinxstylestrong{Cache per Core} | 256KB\sphinxhyphen{}2MB L2 | 16\sphinxhyphen{}128KB L1 |
| \sphinxstylestrong{Control Logic} | 40\% of die | 5\% of die |
| \sphinxstylestrong{Compute Units} | 60\% of die | 95\% of die |
| \sphinxstylestrong{Best For} | Complex, branching code | Simple, repetitive operations |
| \sphinxstylestrong{Parallelism} | 4\sphinxhyphen{}64 tasks | 1,000\sphinxhyphen{}10,000+ tasks |


\subsubsection{SIMD and GPU Architecture}
\label{\detokenize{cpu-concurrency/hardware_parallelism:simd-and-gpu-architecture}}
\sphinxAtStartPar
\sphinxstylestrong{SIMD}: Single Instruction, Multiple Data

\sphinxAtStartPar
CPU executing 4 additions sequentially:
Time \sphinxhyphen{}\textgreater{}
Core 1: {[}A+B{]} \sphinxhyphen{}\textgreater{} {[}C+D{]} \sphinxhyphen{}\textgreater{} {[}E+F{]} \sphinxhyphen{}\textgreater{} {[}G+H{]}  (4 time units)

\sphinxAtStartPar
GPU executing 4 additions in parallel (SIMD):
Time \sphinxhyphen{}\textgreater{}
Core 1: {[}A+B{]}
Core 2: {[}C+D{]}  \} All execute simultaneously
Core 3: {[}E+F{]}    (1 time unit)
Core 4: {[}G+H{]}

\sphinxAtStartPar
Speedup: 4x for this simple case

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{GPU} \PYG{n}{Organization}\PYG{o}{*}\PYG{o}{*} \PYG{p}{(}\PYG{n}{NVIDIA} \PYG{n}{example}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
GPU (e.g., NVIDIA RTX 4090):
+—————————————————\textendash{}+
|  Streaming Multiprocessors (SMs): 128 units         |
|                                                     |
|  Each SM contains:                                  |
|  {\color{red}\bfseries{}|}\sphinxhyphen{} 128 CUDA cores (simple ALUs)                   |
|  {\color{red}\bfseries{}|}\sphinxhyphen{} 4 Tensor cores (matrix operations)             |
|  {\color{red}\bfseries{}|}\sphinxhyphen{} Shared memory (64\sphinxhyphen{}128 KB)                      |
|  +\sphinxhyphen{} Warp scheduler                                 |
|                                                     |
|  Total: 128 SMs x 128 cores = 16,384 CUDA cores    |
|                                                     |
|  All cores execute in lockstep (SIMD):             |
|  One instruction broadcast to 32 cores (1 warp)    |
+—————————————————\textendash{}+


\subsubsection{Why GPUs Excel at Compute\sphinxhyphen{}Intensive Tasks}
\label{\detokenize{cpu-concurrency/hardware_parallelism:why-gpus-excel-at-compute-intensive-tasks}}

\paragraph{1. Massive Parallelism}
\label{\detokenize{cpu-concurrency/hardware_parallelism:massive-parallelism}}
\sphinxAtStartPar
\sphinxstylestrong{Problem}: Add 1 million numbers

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{CPU approach (4 cores):}
\label{\detokenize{cpu-concurrency/hardware_parallelism:cpu-approach-4-cores}}

\section{Split into 4 chunks of 250,000 each}
\label{\detokenize{cpu-concurrency/hardware_parallelism:split-into-4-chunks-of-250-000-each}}

\section{Each core processes 250,000 additions sequentially}
\label{\detokenize{cpu-concurrency/hardware_parallelism:each-core-processes-250-000-additions-sequentially}}
\sphinxAtStartPar
Core 1: {[}sum 250,000 numbers{]}
Core 2: {[}sum 250,000 numbers{]}  \} Parallel
Core 3: {[}sum 250,000 numbers{]}
Core 4: {[}sum 250,000 numbers{]}

\sphinxAtStartPar
Time: 250,000 additions / core\_speed

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{.}\PYG{o}{.} \PYG{n}{code}\PYG{o}{\PYGZhy{}}\PYG{n}{block}\PYG{p}{:}\PYG{p}{:} \PYG{n}{python}
\end{sphinxVerbatim}


\section{GPU approach (16,384 cores):}
\label{\detokenize{cpu-concurrency/hardware_parallelism:gpu-approach-16-384-cores}}

\section{Split into 16,384 chunks of \textasciitilde{}61 each}
\label{\detokenize{cpu-concurrency/hardware_parallelism:split-into-16-384-chunks-of-61-each}}

\section{Each core processes \textasciitilde{}61 additions}
\label{\detokenize{cpu-concurrency/hardware_parallelism:each-core-processes-61-additions}}
\sphinxAtStartPar
Cores 1\sphinxhyphen{}16,384: {[}sum \textasciitilde{}61 numbers each{]}  \} All parallel

\sphinxAtStartPar
Time: 61 additions / core\_speed

\sphinxAtStartPar
Speedup: 250,000 / 61 \textasciitilde{} 4,000x faster!

\sphinxAtStartPar
\sphinxstylestrong{Data\sphinxhyphen{}parallel}: Same operation on different data elements

\sphinxAtStartPar
Example: Image processing \sphinxhyphen{} apply filter to each pixel

\sphinxAtStartPar
Image: 1920x1080 = 2,073,600 pixels
Operation: Apply blur filter to each pixel

\sphinxAtStartPar
CPU (4 cores):
{\color{red}\bfseries{}|}\sphinxhyphen{} Core 1: Process pixels 0\sphinxhyphen{}518,400
{\color{red}\bfseries{}|}\sphinxhyphen{} Core 2: Process pixels 518,401\sphinxhyphen{}1,036,800
{\color{red}\bfseries{}|}\sphinxhyphen{} Core 3: Process pixels 1,036,801\sphinxhyphen{}1,555,200
+\sphinxhyphen{} Core 4: Process pixels 1,555,201\sphinxhyphen{}2,073,600
Time: 518,400 pixels per core

\sphinxAtStartPar
GPU (8,192 cores):
{\color{red}\bfseries{}|}\sphinxhyphen{} Each core: Process \textasciitilde{}253 pixels
+\sphinxhyphen{} All cores work simultaneously
Time: \textasciitilde{}253 pixels per core

\sphinxAtStartPar
Speedup: 518,400 / 253 \textasciitilde{} 2,048x faster!

\sphinxAtStartPar
CPU Memory Bandwidth:
{\color{red}\bfseries{}|}\sphinxhyphen{} DDR4: \textasciitilde{}50 GB/s
+\sphinxhyphen{} DDR5: \textasciitilde{}80 GB/s

\sphinxAtStartPar
GPU Memory Bandwidth:
{\color{red}\bfseries{}|}\sphinxhyphen{} GDDR6: \textasciitilde{}600 GB/s
+\sphinxhyphen{} HBM2: \textasciitilde{}1,000 GB/s

\sphinxAtStartPar
Ratio: GPU has 10\sphinxhyphen{}20x more memory bandwidth!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Why} \PYG{n}{it} \PYG{n}{matters}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
Compute\sphinxhyphen{}intensive task often needs:
1. Read data from memory
2. Perform computation
3. Write result to memory

\sphinxAtStartPar
With 16,384 cores all reading/writing:
{\color{red}\bfseries{}|}\sphinxhyphen{} CPU bandwidth: Saturated immediately
+\sphinxhyphen{} GPU bandwidth: Designed for this workload

\sphinxAtStartPar
\sphinxstylestrong{Perfect for GPUs} (embarrassingly parallel):
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Matrix Operations}
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
\# Matrix multiplication: C = A x B
\# Each element of C can be computed independently

\sphinxAtStartPar
C{[}i,j{]} = sum(A{[}i,k{]} * B{[}k,j{]} for k in range(n))

\sphinxAtStartPar
\# For 1000x1000 matrices:
\# 1,000,000 independent calculations
\# Perfect for 16,384 GPU cores!

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Image/Video Processing}
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
\# Apply operation to each pixel
for each pixel in image:
\begin{quote}

\sphinxAtStartPar
output{[}pixel{]} = transform(input{[}pixel{]})
\end{quote}

\sphinxAtStartPar
\# 4K video frame: 8,294,400 pixels
\# All can be processed in parallel on GPU

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep Learning}
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
\# Neural network: mostly matrix multiplications
layer\_output = activation(weights @ inputs + bias)

\sphinxAtStartPar
\# Millions of multiply\sphinxhyphen{}add operations
\# All can run in parallel

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scientific Simulations}
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
\# N\sphinxhyphen{}body simulation: calculate force on each particle
for particle\_i in particles:
\begin{quote}
\begin{description}
\sphinxlineitem{force = sum(calculate\_force(particle\_i, particle\_j)}
\sphinxAtStartPar
for particle\_j in other\_particles)

\end{description}
\end{quote}

\sphinxAtStartPar
\# Each particle’s force is independent
\# GPU can calculate all simultaneously

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Poor for GPUs} (lots of branching/dependencies):
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Complex Control Flow}
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
\# Lots of if/else statements
def complex\_logic(data):
\begin{quote}
\begin{description}
\sphinxlineitem{if data.type == ‘A’:}\begin{description}
\sphinxlineitem{if data.value \textgreater{} threshold\_1:}
\sphinxAtStartPar
return process\_A1(data)

\sphinxlineitem{else:}
\sphinxAtStartPar
return process\_A2(data)

\end{description}

\sphinxlineitem{elif data.type == ‘B’:}
\sphinxAtStartPar
\# … more branching

\end{description}
\end{quote}

\sphinxAtStartPar
\# Different threads take different paths
\# GPU cores must wait for slowest path (divergence)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Recursive Algorithms}
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{def quicksort(arr):}\begin{description}
\sphinxlineitem{if len(arr) \textless{}= 1:}
\sphinxAtStartPar
return arr

\end{description}

\sphinxAtStartPar
pivot = arr{[}0{]}
left = quicksort({[}x for x in arr{[}1:{]} if x \textless{} pivot{]})
right = quicksort({[}x for x in arr{[}1:{]} if x \textgreater{}= pivot{]})
return left + {[}pivot{]} + right

\end{description}

\sphinxAtStartPar
\# Highly sequential, data\sphinxhyphen{}dependent
\# Cannot parallelize effectively on GPU

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Database Queries with Joins}
.. code\sphinxhyphen{}block:: sql

\sphinxAtStartPar
\textendash{} Complex joins with unpredictable data access patterns
SELECT * FROM table1
JOIN table2 ON table1.id = table2.foreign\_id
WHERE complex\_condition(table1.data)

\sphinxAtStartPar
\textendash{} Irregular memory access
\textendash{} Branch\sphinxhyphen{}heavy logic
\textendash{} CPU better suited

\end{enumerate}

\sphinxAtStartPar
—


\subsection{CPU vs GPU Architecture}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id40}}

\subsubsection{Silicon Real Estate Comparison}
\label{\detokenize{cpu-concurrency/hardware_parallelism:silicon-real-estate-comparison}}
\sphinxAtStartPar
CPU Die Layout (approximate):
+———————————————+
|                                             |
|  \#\#\#\#\#\#\#\#\#\#\#\# Control Logic (40\%)           |
|  Fetch, decode, branch predict, OoO, etc.  |
|                                             |
|  \#\#\#\#\#\#\#\# Cache (30\%)                       |
|  L1, L2, L3 caches                          |
|                                             |
|  \#\#\#\#\#\# Compute Units (20\%)                 |
|  ALUs, FPUs, actual computation             |
|                                             |
|  \#\# Other (10\%)                             |
|  Memory controller, I/O, etc.               |
+———————————————+

\sphinxAtStartPar
GPU Die Layout (approximate):
+———————————————+
|  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#      |
|  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#      |
|  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#      |
|  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#      |
|  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#      |
|  Compute Units (80\sphinxhyphen{}85\%)                     |
|  Thousands of simple ALUs                   |
|  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#      |
|  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#      |
|  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#      |
|  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#      |
|                                             |
|  \#\# Cache (5\sphinxhyphen{}10\%)                           |
|  \#\# Control (5\%)                            |
|  \# Other (5\%)                               |
+———————————————+


\subsubsection{Performance Comparison}
\label{\detokenize{cpu-concurrency/hardware_parallelism:performance-comparison}}
\sphinxAtStartPar
\sphinxstylestrong{Example: Matrix Multiplication (2048x2048)}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import numpy as np
import time


\section{Generate random matrices}
\label{\detokenize{cpu-concurrency/hardware_parallelism:generate-random-matrices}}
\sphinxAtStartPar
A = np.random.rand(2048, 2048)
B = np.random.rand(2048, 2048)


\section{CPU (NumPy with optimized BLAS)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:cpu-numpy-with-optimized-blas}}
\sphinxAtStartPar
start = time.time()
C\_cpu = np.dot(A, B)
cpu\_time = time.time() \sphinxhyphen{} start
print(f”CPU time: \{cpu\_time:.3f\}s”)


\section{GPU (using CuPy \sphinxhyphen{} CUDA arrays)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:gpu-using-cupy-cuda-arrays}}
\sphinxAtStartPar
import cupy as cp
A\_gpu = cp.array(A)
B\_gpu = cp.array(B)

\sphinxAtStartPar
start = time.time()
C\_gpu = cp.dot(A\_gpu, B\_gpu)
cp.cuda.Stream.null.synchronize()  \# Wait for GPU
gpu\_time = time.time() \sphinxhyphen{} start
print(f”GPU time: \{gpu\_time:.3f\}s”)

\sphinxAtStartPar
print(f”Speedup: \{cpu\_time / gpu\_time:.1f\}x”)


\section{Typical results:}
\label{\detokenize{cpu-concurrency/hardware_parallelism:typical-results}}

\section{CPU time: 0.850s (Intel i9, 8 cores)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:cpu-time-0-850s-intel-i9-8-cores}}

\section{GPU time: 0.012s (NVIDIA RTX 4090)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:gpu-time-0-012s-nvidia-rtx-4090}}

\section{Speedup: 70.8x}
\label{\detokenize{cpu-concurrency/hardware_parallelism:speedup-70-8x}}
\begin{DUlineblock}{0em}
\item[] Task | CPU (8\sphinxhyphen{}core i9) | GPU (RTX 4090) | Speedup |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}—————\sphinxhyphen{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———|
| Matrix Multiply (2048\textasciicircum{}2) | 850 ms | 12 ms | 70x |
| Image Convolution (4K) | 1200 ms | 8 ms | 150x |
| FFT (16M points) | 2500 ms | 15 ms | 166x |
| Neural Net Forward Pass | 5000 ms | 25 ms | 200x |
| Ray Tracing (1080p frame) | 45000 ms | 16 ms | 2800x |

\sphinxAtStartPar
—


\subsection{When to Use What}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id41}}

\subsubsection{Decision Matrix}
\label{\detokenize{cpu-concurrency/hardware_parallelism:decision-matrix}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{:}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{T}
\sphinxtoprule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
DECISION TREE
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}
\end{quote}

\sphinxAtStartPar
Is the problem parallelizable?
{\color{red}\bfseries{}|}\sphinxhyphen{} No (sequential dependencies)
|  +\sphinxhyphen{} Use: CPU (single core)
|     Examples: Parsing, tree traversal, complex algorithms
|
+\sphinxhyphen{} Yes (can be split into independent tasks)
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|}\sphinxhyphen{} What type of parallelism?
|
{\color{red}\bfseries{}|}\sphinxhyphen{} Task Parallelism (different operations)
|  {\color{red}\bfseries{}|}\sphinxhyphen{} CPU\sphinxhyphen{}intensive tasks
|  |  +\sphinxhyphen{} Use: CPU Multiprocessing (\# workers = physical cores)
|  |     Examples: Video encoding, data compression
|  |
|  +\sphinxhyphen{} I/O\sphinxhyphen{}intensive tasks
|     +\sphinxhyphen{} Use: Asyncio or Threading (\# workers = 100s\sphinxhyphen{}1000s)
|        Examples: Web scraping, API calls
|
+\sphinxhyphen{} Data Parallelism (same operation on different data)
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|}\sphinxhyphen{} Problem size?
|
{\color{red}\bfseries{}|}\sphinxhyphen{} Small (\textless{} 10,000 elements)
|  +\sphinxhyphen{} Use: CPU Multiprocessing
|     Reason: GPU overhead not worth it
|
{\color{red}\bfseries{}|}\sphinxhyphen{} Medium (10,000 \sphinxhyphen{} 1,000,000 elements)
|  {\color{red}\bfseries{}|}\sphinxhyphen{} Simple operations (add, multiply)
|  |  +\sphinxhyphen{} Use: GPU
|  +\sphinxhyphen{} Complex operations (lots of branches)
|     +\sphinxhyphen{} Use: CPU Multiprocessing
|
+\sphinxhyphen{} Large (\textgreater{} 1,000,000 elements)
\begin{quote}
\begin{description}
\sphinxlineitem{+\sphinxhyphen{} Use: GPU}
\sphinxAtStartPar
Reason: Massive parallelism advantage

\end{description}
\end{quote}
\end{quote}
\end{quote}


\subsubsection{Practical Guidelines}
\label{\detokenize{cpu-concurrency/hardware_parallelism:practical-guidelines}}

\paragraph{Use CPU When:}
\label{\detokenize{cpu-concurrency/hardware_parallelism:use-cpu-when}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{p}{[}\PYG{n}{OK}\PYG{p}{]}\PYG{p}{]} \PYG{n}{Complex} \PYG{n}{branching} \PYG{n}{logic}
\end{sphinxVerbatim}
\begin{quote}

\sphinxAtStartPar
if/else trees, switch statements, dynamic dispatch
\end{quote}
\begin{description}
\sphinxlineitem{{[}{[}OK{]}{]} Irregular memory access patterns}
\sphinxAtStartPar
Hash tables, tree structures, linked lists

\sphinxlineitem{{[}{[}OK{]}{]} Small datasets (\textless{} 10K elements)}
\sphinxAtStartPar
GPU transfer overhead \textgreater{} computation time

\sphinxlineitem{{[}{[}OK{]}{]} Frequent host\sphinxhyphen{}device communication}
\sphinxAtStartPar
Need to move data between CPU/GPU often

\sphinxlineitem{{[}{[}OK{]}{]} Sequential dependencies}
\sphinxAtStartPar
Each step depends on previous result

\sphinxlineitem{{[}{[}OK{]}{]} Debugging and development}
\sphinxAtStartPar
CPU tools more mature, easier to debug

\end{description}


\paragraph{Use GPU When:}
\label{\detokenize{cpu-concurrency/hardware_parallelism:use-gpu-when}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{p}{[}\PYG{n}{OK}\PYG{p}{]}\PYG{p}{]} \PYG{n}{Massive} \PYG{n}{data} \PYG{n}{parallelism}
\end{sphinxVerbatim}
\begin{quote}

\sphinxAtStartPar
Same operation on millions of data points
\end{quote}
\begin{description}
\sphinxlineitem{{[}{[}OK{]}{]} Matrix operations}
\sphinxAtStartPar
Linear algebra, transformations

\sphinxlineitem{{[}{[}OK{]}{]} Regular memory access patterns}
\sphinxAtStartPar
Arrays, grids, uniform data structures

\sphinxlineitem{{[}{[}OK{]}{]} Minimal branching}
\sphinxAtStartPar
Straight\sphinxhyphen{}line code, vectorizable operations

\sphinxlineitem{{[}{[}OK{]}{]} Large datasets (\textgreater{} 100K elements)}
\sphinxAtStartPar
Enough work to saturate GPU cores

\sphinxlineitem{{[}{[}OK{]}{]} Can keep data on GPU}
\sphinxAtStartPar
Minimize CPU\textless{}\sphinxhyphen{}\textgreater{}GPU transfers

\end{description}


\paragraph{Use Hyperthreading When:}
\label{\detokenize{cpu-concurrency/hardware_parallelism:use-hyperthreading-when}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{p}{[}\PYG{n}{OK}\PYG{p}{]}\PYG{p}{]} \PYG{n}{Workload} \PYG{n}{has} \PYG{n}{varied} \PYG{n}{resource} \PYG{n}{usage}
\end{sphinxVerbatim}
\begin{quote}

\sphinxAtStartPar
Some threads use ALU, others use FPU
\end{quote}
\begin{description}
\sphinxlineitem{{[}{[}OK{]}{]} Latency hiding}
\sphinxAtStartPar
Memory\sphinxhyphen{}bound code with cache misses

\sphinxlineitem{{[}{[}OK{]}{]} Server workloads}
\sphinxAtStartPar
Many small tasks with idle time

\sphinxlineitem{{[}{[}FAIL{]}{]} DON’T use for CPU\sphinxhyphen{}intensive Python}
\sphinxAtStartPar
GIL + context switching = slower

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Practical Examples}
\label{\detokenize{cpu-concurrency/hardware_parallelism:id58}}

\subsubsection{Example 1: Image Processing}
\label{\detokenize{cpu-concurrency/hardware_parallelism:example-1-image-processing}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Task: Apply Gaussian blur to 100 images (4K resolution)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:task-apply-gaussian-blur-to-100-images-4k-resolution}}

\section{Approach 1: Sequential (CPU, 1 core)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:approach-1-sequential-cpu-1-core}}
\sphinxAtStartPar
import cv2
for img\_path in image\_paths:
\begin{quote}

\sphinxAtStartPar
img = cv2.imread(img\_path)
blurred = cv2.GaussianBlur(img, (15, 15), 0)
cv2.imwrite(output\_path, blurred)
\end{quote}


\section{Time: \textasciitilde{}300 seconds}
\label{\detokenize{cpu-concurrency/hardware_parallelism:time-300-seconds}}

\section{Approach 2: CPU Multiprocessing (4 physical cores)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:approach-2-cpu-multiprocessing-4-physical-cores}}
\sphinxAtStartPar
from concurrent.futures import ProcessPoolExecutor
\begin{description}
\sphinxlineitem{def process\_image(img\_path):}
\sphinxAtStartPar
img = cv2.imread(img\_path)
blurred = cv2.GaussianBlur(img, (15, 15), 0)
cv2.imwrite(output\_path, blurred)

\sphinxlineitem{with ProcessPoolExecutor(max\_workers=4) as executor:}
\sphinxAtStartPar
executor.map(process\_image, image\_paths)

\end{description}


\section{Time: \textasciitilde{}80 seconds (3.75x speedup)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:time-80-seconds-3-75x-speedup}}

\section{Approach 3: GPU (CUDA)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:approach-3-gpu-cuda}}
\sphinxAtStartPar
import cupy as cp
import cupyx.scipy.ndimage as ndimage
\begin{description}
\sphinxlineitem{def process\_image*gpu(img\_path):}
\sphinxAtStartPar
img = cv2.imread(img\_path)
img\_gpu = cp.array(img)
blurred\_gpu = ndimage.gaussian\_filter(img\_gpu, sigma=3)
blurred = cp.asnumpy(blurred\_gpu)
cv2.imwrite(output\_path, blurred)

\sphinxlineitem{for img\_path in image\_paths:}
\sphinxAtStartPar
process\_image*gpu(img\_path)

\end{description}


\section{Time: \textasciitilde{}4 seconds (75x speedup!)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:time-4-seconds-75x-speedup}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Task: Estimate pi using Monte Carlo (1 billion points)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:task-estimate-pi-using-monte-carlo-1-billion-points}}
\sphinxAtStartPar
import numpy as np
import time


\section{Sequential CPU}
\label{\detokenize{cpu-concurrency/hardware_parallelism:sequential-cpu}}\begin{description}
\sphinxlineitem{def estimate\_pi*cpu(n):}
\sphinxAtStartPar
inside = 0
for * in range(n):
\begin{quote}

\sphinxAtStartPar
x, y = np.random.random(), np.random.random()
if x\_x + y\_y \textless{}= 1:
\begin{quote}

\sphinxAtStartPar
inside += 1
\end{quote}
\end{quote}

\sphinxAtStartPar
return 4 * inside / n

\end{description}

\sphinxAtStartPar
start = time.time()
pi\_estimate = estimate\_pi*cpu(1*000*000*000)
print(f”CPU time: \{time.time() \sphinxhyphen{} start:.2f\}s”)
Output: CPU time: 180.00s
=========================


\section{CPU Multiprocessing (4 cores)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:cpu-multiprocessing-4-cores}}
\sphinxAtStartPar
from concurrent.futures import ProcessPoolExecutor
\begin{description}
\sphinxlineitem{def estimate\_pi*parallel(n, num\_workers=4):}
\sphinxAtStartPar
chunk\_size = n // num\_workers
with ProcessPoolExecutor(max\_workers=num\_workers) as executor:
\begin{quote}

\sphinxAtStartPar
results = executor.map(estimate\_pi*cpu, {[}chunk\_size{]} * num\_workers)
\end{quote}

\sphinxAtStartPar
return sum(results) / num\_workers

\end{description}

\sphinxAtStartPar
start = time.time()
pi\_estimate = estimate\_pi*parallel(1*000*000*000, 4)
print(f”CPU parallel time: \{time.time() \sphinxhyphen{} start:.2f\}s”)
Output: CPU parallel time: 47.00s (3.8x speedup)
================================================


\section{GPU (CUDA)}
\label{\detokenize{cpu-concurrency/hardware_parallelism:gpu-cuda}}
\sphinxAtStartPar
import cupy as cp
\begin{description}
\sphinxlineitem{def estimate\_pi*gpu(n):}
\sphinxAtStartPar
\# Generate all random numbers at once on GPU
points = cp.random.random((n, 2))
\# Vectorized distance calculation
inside = cp.sum(cp.sum(points**2, axis=1) \textless{}= 1)
return 4 * float(inside) / n

\end{description}

\sphinxAtStartPar
start = time.time()
pi\_estimate = estimate\_pi*gpu(1*000*000*000)
cp.cuda.Stream.null.synchronize()
print(f”GPU time: \{time.time() \sphinxhyphen{} start:.2f\}s”)
Output: GPU time: 1.50s (120x speedup!)
=======================================

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Task: Train a neural network on MNIST dataset}
\label{\detokenize{cpu-concurrency/hardware_parallelism:task-train-a-neural-network-on-mnist-dataset}}
\sphinxAtStartPar
import torch
import torch.nn as nn
import time


\section{Define simple network}
\label{\detokenize{cpu-concurrency/hardware_parallelism:define-simple-network}}\begin{description}
\sphinxlineitem{model = nn.Sequential(}
\sphinxAtStartPar
nn.Flatten(),
nn.Linear(784, 128),
nn.ReLU(),
nn.Linear(128, 10)

\end{description}

\sphinxAtStartPar
)


\section{CPU Training}
\label{\detokenize{cpu-concurrency/hardware_parallelism:cpu-training}}
\sphinxAtStartPar
model\_cpu = model.to(‘cpu’)
optimizer = torch.optim.Adam(model\_cpu.parameters())

\sphinxAtStartPar
start = time.time()
for epoch in range(10):
\begin{quote}
\begin{description}
\sphinxlineitem{for batch in train\_loader:}
\sphinxAtStartPar
data, target = batch
data, target = data.to(‘cpu’), target.to(‘cpu’)

\sphinxAtStartPar
optimizer.zero\_grad()
output = model\_cpu(data)
loss = nn.functional.cross\_entropy(output, target)
loss.backward()
optimizer.step()

\end{description}
\end{quote}

\sphinxAtStartPar
cpu\_time = time.time() \sphinxhyphen{} start
print(f”CPU training time: \{cpu\_time:.2f\}s”)
Output: CPU training time: 450.00s
==================================


\section{GPU Training}
\label{\detokenize{cpu-concurrency/hardware_parallelism:gpu-training}}
\sphinxAtStartPar
model\_gpu = model.to(‘cuda’)
optimizer = torch.optim.Adam(model\_gpu.parameters())

\sphinxAtStartPar
start = time.time()
for epoch in range(10):
\begin{quote}
\begin{description}
\sphinxlineitem{for batch in train\_loader:}
\sphinxAtStartPar
data, target = batch
data, target = data.to(‘cuda’), target.to(‘cuda’)

\sphinxAtStartPar
optimizer.zero\_grad()
output = model\_gpu(data)
loss = nn.functional.cross\_entropy(output, target)
loss.backward()
optimizer.step()

\end{description}
\end{quote}

\sphinxAtStartPar
torch.cuda.synchronize()
gpu\_time = time.time() \sphinxhyphen{} start
print(f”GPU training time: \{gpu\_time:.2f\}s”)
print(f”Speedup: \{cpu\_time / gpu\_time:.1f\}x”)
Output: GPU training time: 15.00s
=================================
\begin{quote}

\sphinxAtStartPar
Speedup: 30.0x
\end{quote}

\sphinxAtStartPar
—


\subsection{Summary}
\label{\detokenize{cpu-concurrency/hardware_parallelism:summary}}

\subsubsection{Core Principles}
\label{\detokenize{cpu-concurrency/hardware_parallelism:core-principles}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Physical Cores = True Parallelism}
\sphinxhyphen{} Only physical cores provide true simultaneous execution
\sphinxhyphen{} Thread count beyond physical cores = diminishing returns
\sphinxhyphen{} For CPU\sphinxhyphen{}bound: optimal workers = physical cores

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hyperthreading = Resource Sharing}
\sphinxhyphen{} 2 threads share 1 physical core’s execution units
\sphinxhyphen{} \textasciitilde{}20\sphinxhyphen{}40\% improvement in best case (not 2x)
\sphinxhyphen{} Can hurt performance for CPU\sphinxhyphen{}intensive tasks
\sphinxhyphen{} Best for varied workloads with different resource needs

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GPUs = Massive Parallelism}
\sphinxhyphen{} 1000s of simple cores vs few complex cores
\sphinxhyphen{} Optimized for throughput, not latency
\sphinxhyphen{} Perfect for data\sphinxhyphen{}parallel workloads
\sphinxhyphen{} Trade\sphinxhyphen{}off: Simple operations, massive scale

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Architecture Determines Use Case}

\sphinxAtStartPar
CPU: Complex tasks, few parallel streams
GPU: Simple tasks, massive parallel streams
HT:  Resource sharing within a core

\end{enumerate}


\subsubsection{Quick Reference}
\label{\detokenize{cpu-concurrency/hardware_parallelism:quick-reference}}
\begin{DUlineblock}{0em}
\item[] Workload | Best Solution | Why |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}————\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| Web server (1000s connections) | CPU + Asyncio | I/O\sphinxhyphen{}bound, need async |
| Video encoding (1 file) | CPU + multiprocessing | CPU\sphinxhyphen{}bound, need all cores |
| Deep learning training | GPU | Data\sphinxhyphen{}parallel, matrix ops |
| Database query | CPU | Complex branching, irregular access |
| Image batch processing | GPU | Same operation, many images |
| Code compilation | CPU + multiprocessing | Complex, parallelizable |
| Scientific simulation | GPU | Numerical computation, data\sphinxhyphen{}parallel |
| Game physics | GPU | Many objects, same equations |


\subsubsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/hardware_parallelism:key-takeaways}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Know your hardware}: Physical cores determine max CPU parallelism

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Match tool to task}: CPU for complex, GPU for simple\sphinxhyphen{}but\sphinxhyphen{}massive

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hyperthreading helps} when threads use different resources

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Profile first}: Measure before choosing architecture

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Consider transfer costs}: CPU\textless{}\sphinxhyphen{}\textgreater{}GPU data movement is expensive

\end{itemize}

\sphinxstepscope


\section{Threading Basics: start() and join()}
\label{\detokenize{cpu-concurrency/threading_basics:threading-basics-start-and-join}}\label{\detokenize{cpu-concurrency/threading_basics::doc}}

\subsection{What is start()?}
\label{\detokenize{cpu-concurrency/threading_basics:what-is-start}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{start()}} \sphinxstylestrong{launches the thread and begins execution}. It’s the method that actually creates and starts running the thread.


\subsubsection{Key Points About start()}
\label{\detokenize{cpu-concurrency/threading_basics:key-points-about-start}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{It doesn’t run the function immediately} \sphinxhyphen{} Instead, it schedules the thread to run

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{The main thread continues immediately} \sphinxhyphen{} It doesn’t wait for the worker thread to finish

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Without start(), the thread never runs} \sphinxhyphen{} Creating a Thread object does nothing by itself

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Can only call start() once} \sphinxhyphen{} Calling it twice raises an error

\end{itemize}


\subsubsection{Example}
\label{\detokenize{cpu-concurrency/threading_basics:example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import threading
\begin{description}
\sphinxlineitem{def worker():}
\sphinxAtStartPar
print(“Worker running”)

\end{description}


\section{Create thread object (doesn’t start yet)}
\label{\detokenize{cpu-concurrency/threading_basics:create-thread-object-doesn-t-start-yet}}
\sphinxAtStartPar
thread = threading.Thread(target=worker)


\section{Call start() to actually begin execution}
\label{\detokenize{cpu-concurrency/threading_basics:call-start-to-actually-begin-execution}}
\sphinxAtStartPar
thread.start()

\sphinxAtStartPar
print(“Main continues here”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Worker running
Main continues here

\sphinxAtStartPar
Notice: Both print statements execute, showing that main doesn’t wait.

\sphinxAtStartPar
—


\subsection{What is join()?}
\label{\detokenize{cpu-concurrency/threading_basics:what-is-join}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{join()}} \sphinxstylestrong{makes the main thread wait until the worker thread finishes}. It’s a blocking operation.


\subsubsection{Key Points About join()}
\label{\detokenize{cpu-concurrency/threading_basics:key-points-about-join}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{The main thread blocks/pauses} at the join() call

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Waits for the worker thread to complete} before continuing

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Without join(), main might exit before worker finishes} (and the work is lost)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{You can join() multiple threads} to wait for all of them

\end{itemize}


\subsubsection{Example}
\label{\detokenize{cpu-concurrency/threading_basics:id1}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import threading
import time
\begin{description}
\sphinxlineitem{def worker():}
\sphinxAtStartPar
print(“Worker starting”)
time.sleep(1)
print(“Worker done”)

\end{description}

\sphinxAtStartPar
thread = threading.Thread(target=worker)
thread.start()

\sphinxAtStartPar
print(“Main: waiting for worker…”)
thread.join()  \# Main thread BLOCKS here
print(“Main: worker finished, I can continue”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Worker starting
Main: waiting for worker…
Worker done
Main: worker finished, I can continue
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Comparison: With vs Without join()}
\label{\detokenize{cpu-concurrency/threading_basics:comparison-with-vs-without-join}}

\subsubsection{WITHOUT join() \sphinxhyphen{} DANGER!}
\label{\detokenize{cpu-concurrency/threading_basics:without-join-danger}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import threading
import time
\begin{description}
\sphinxlineitem{def download\_file():}
\sphinxAtStartPar
print(”  Download starting…”)
time.sleep(2)
print(”  Download complete!”)

\end{description}

\sphinxAtStartPar
thread = threading.Thread(target=download\_file)
thread.start()

\sphinxAtStartPar
print(“Download started! I’m moving on…”)
print(“Program exiting”)
Program exits before download finishes!
=======================================
Download is interrupted!
========================

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Download started! I’m moving on…
Program exiting

\sphinxAtStartPar
The download never completes because the program exits!


\subsubsection{WITH join() \sphinxhyphen{} CORRECT!}
\label{\detokenize{cpu-concurrency/threading_basics:with-join-correct}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import threading
import time
\begin{description}
\sphinxlineitem{def download\_file():}
\sphinxAtStartPar
print(”  Download starting…”)
time.sleep(2)
print(”  Download complete!”)

\end{description}

\sphinxAtStartPar
thread = threading.Thread(target=download\_file)
thread.start()

\sphinxAtStartPar
print(“Download started! Waiting…”)
thread.join()  \# Main blocks here until download finishes
print(“Download is done! Safely exiting”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{Download started! Waiting…}
\sphinxAtStartPar
Download starting…
Download complete!

\end{description}

\sphinxAtStartPar
Download is done! Safely exiting

\sphinxAtStartPar
Now the download completes before the program exits.

\sphinxAtStartPar
—


\subsection{Thread Lifecycle Diagram}
\label{\detokenize{cpu-concurrency/threading_basics:thread-lifecycle-diagram}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
| Thread Object Created                                   |
| thread = Thread(target=worker\PYGZus{}func)                     |
| (Thread exists but doesn\PYGZsq{}t run)                         |
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
|
| thread.start()
[v]
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
| Thread Running (in parallel with main)                  |
| \PYGZhy{} Main thread continues immediately                     |
| \PYGZhy{} Worker thread executes target function               |
| \PYGZhy{} Both run concurrently                                |
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
|
| Worker thread finishes
[v]
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
| Thread Finished                                         |
| (if no join(), main might exit before here)            |
| (with join(), main waits here)                         |
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Typical Pattern for Multiple Threads}
\label{\detokenize{cpu-concurrency/threading_basics:typical-pattern-for-multiple-threads}}
\sphinxAtStartPar
The standard pattern for managing multiple threads:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import threading


\section{Step 1: Create all threads}
\label{\detokenize{cpu-concurrency/threading_basics:step-1-create-all-threads}}
\sphinxAtStartPar
threads = {[}{]}
for task in tasks:
\begin{quote}

\sphinxAtStartPar
thread = threading.Thread(target=do\_task, args=(task,))
thread.start()      \# Start each thread
threads.append(thread)
\end{quote}


\section{Step 2: Wait for all to finish}
\label{\detokenize{cpu-concurrency/threading_basics:step-2-wait-for-all-to-finish}}\begin{description}
\sphinxlineitem{for thread in threads:}
\sphinxAtStartPar
thread.join()       \# Wait for each one

\end{description}

\sphinxAtStartPar
print(“All tasks complete!”)

\sphinxAtStartPar
This ensures:
\sphinxhyphen{} All work starts in parallel
\sphinxhyphen{} Main waits for all work to complete
\sphinxhyphen{} You only continue when everything is done

\sphinxAtStartPar
—


\subsection{Common Mistakes}
\label{\detokenize{cpu-concurrency/threading_basics:common-mistakes}}

\subsubsection{Mistake 1: Calling the function directly instead of start()}
\label{\detokenize{cpu-concurrency/threading_basics:mistake-1-calling-the-function-directly-instead-of-start}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{{[}{[}FAIL{]}{]} WRONG \sphinxhyphen{} runs worker in MAIN thread}
\label{\detokenize{cpu-concurrency/threading_basics:fail-wrong-runs-worker-in-main-thread}}
\sphinxAtStartPar
thread = threading.Thread(target=worker)
worker()  \# This blocks main! Not parallel!


\section{{[}{[}OK{]}{]} CORRECT \sphinxhyphen{} runs worker in NEW thread}
\label{\detokenize{cpu-concurrency/threading_basics:ok-correct-runs-worker-in-new-thread}}
\sphinxAtStartPar
thread = threading.Thread(target=worker)
thread.start()  \# Non\sphinxhyphen{}blocking, parallel execution

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{{[}{[}FAIL{]}{]} WRONG \sphinxhyphen{} main exits before worker finishes}
\label{\detokenize{cpu-concurrency/threading_basics:fail-wrong-main-exits-before-worker-finishes}}
\sphinxAtStartPar
thread = threading.Thread(target=save\_to*database)
thread.start()
print(“Saved!”)  \# Exits immediately, database never saves


\section{{[}{[}OK{]}{]} CORRECT \sphinxhyphen{} wait for worker to finish}
\label{\detokenize{cpu-concurrency/threading_basics:ok-correct-wait-for-worker-to-finish}}
\sphinxAtStartPar
thread = threading.Thread(target=save\_to*database)
thread.start()
thread.join()  \# Wait for it
print(“Saved!”)  \# Now safe to continue

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{{[}{[}FAIL{]}{]} NOT SAFE \sphinxhyphen{} race condition}
\label{\detokenize{cpu-concurrency/threading_basics:fail-not-safe-race-condition}}
\sphinxAtStartPar
counter = 0
def increment():
\begin{quote}

\sphinxAtStartPar
global counter
counter += 1  \# Not atomic! Can have race condition
\end{quote}

\sphinxAtStartPar
threads = {[}threading.Thread(target=increment) for * in range(100){]}
for t in threads:
\begin{quote}

\sphinxAtStartPar
t.start()
\end{quote}
\begin{description}
\sphinxlineitem{for t in threads:}
\sphinxAtStartPar
t.join()

\end{description}

\sphinxAtStartPar
print(counter)  \# Might not be 100!


\section{{[}{[}OK{]}{]} SAFE \sphinxhyphen{} use lock}
\label{\detokenize{cpu-concurrency/threading_basics:ok-safe-use-lock}}
\sphinxAtStartPar
counter = 0
lock = threading.Lock()
def increment():
\begin{quote}

\sphinxAtStartPar
global counter
with lock:  \# Acquire lock
\begin{quote}

\sphinxAtStartPar
counter += 1  \# Now safe
\# Release lock automatically
\end{quote}
\end{quote}

\sphinxAtStartPar
threads = {[}threading.Thread(target=increment) for * in range(100){]}
for t in threads:
\begin{quote}

\sphinxAtStartPar
t.start()
\end{quote}
\begin{description}
\sphinxlineitem{for t in threads:}
\sphinxAtStartPar
t.join()

\end{description}

\sphinxAtStartPar
print(counter)  \# Guaranteed to be 100
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Practical Example: Web Scraper}
\label{\detokenize{cpu-concurrency/threading_basics:practical-example-web-scraper}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import threading
import time
\begin{description}
\sphinxlineitem{def fetch\_url(url, results):}
\sphinxAtStartPar
“””Fetch a URL and store result.”””
print(f”  Fetching \{url\}…”)
time.sleep(1)  \# Simulate network request
results.append(f”Data from \{url\}”)
print(f”  Done: \{url\}”)

\end{description}


\section{URLs to fetch}
\label{\detokenize{cpu-concurrency/threading_basics:urls-to-fetch}}
\sphinxAtStartPar
urls = {[}”\sphinxurl{http://example.com/1}”, “\sphinxurl{http://example.com/2}”, “\sphinxurl{http://example.com/3}”{]}
results = {[}{]}


\section{Create and start threads}
\label{\detokenize{cpu-concurrency/threading_basics:create-and-start-threads}}
\sphinxAtStartPar
threads = {[}{]}
for url in urls:
\begin{quote}

\sphinxAtStartPar
thread = threading.Thread(target=fetch\_url, args=(url, results))
thread.start()      \# Launch the thread
threads.append(thread)
\end{quote}

\sphinxAtStartPar
print(f”Started fetching \{len(threads)\} URLs…”)


\section{Wait for all to complete}
\label{\detokenize{cpu-concurrency/threading_basics:wait-for-all-to-complete}}\begin{description}
\sphinxlineitem{for thread in threads:}
\sphinxAtStartPar
thread.join()       \# Wait for each thread

\end{description}

\sphinxAtStartPar
print(f”All done! Results: \{results\}”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{Started fetching 3 URLs…}
\sphinxAtStartPar
Fetching \sphinxurl{http://example.com/1}…
Fetching \sphinxurl{http://example.com/2}…
Fetching \sphinxurl{http://example.com/3}…
Done: \sphinxurl{http://example.com/1}
Done: \sphinxurl{http://example.com/2}
Done: \sphinxurl{http://example.com/3}

\end{description}

\sphinxAtStartPar
All done! Results: {[}‘Data from \sphinxurl{http://example.com/1}’, …{]}

\sphinxAtStartPar
All three URLs fetched in parallel. Without \sphinxcode{\sphinxupquote{join()}}, the program would exit before results were ready.

\sphinxAtStartPar
—


\subsection{Summary Table}
\label{\detokenize{cpu-concurrency/threading_basics:summary-table}}
\begin{DUlineblock}{0em}
\item[] Method | What it does | When to use | Important notes |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}————{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}—————\textendash{}|
| \sphinxcode{\sphinxupquote{start()}} | Launches the thread | Always, right after creating Thread object | Can only call once per thread; doesn’t block |
| \sphinxcode{\sphinxupquote{join()}} | Waits for thread to finish | Always, before assuming work is done | Blocks execution; use with multiple threads for parallel work |
| Neither | Thread created but never runs | Never (waste of memory) | You MUST call start() or thread does nothing |

\sphinxAtStartPar
—


\subsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/threading_basics:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{start() launches}, \sphinxstylestrong{join() waits}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Without start()} \sphinxhyphen{} thread never runs (dead code)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Without join()} \sphinxhyphen{} main exits before worker finishes (lost work)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Proper pattern} \sphinxhyphen{} start all threads, then join all threads

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Always use join()} before assuming work is complete

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Threads run in parallel} \sphinxhyphen{} multiple tasks execute simultaneously

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Be careful with shared data} \sphinxhyphen{} use locks to prevent race conditions

\end{enumerate}

\sphinxstepscope


\section{Asyncio Event Loop}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:asyncio-event-loop}}\label{\detokenize{cpu-concurrency/asyncio_event_loop::doc}}

\subsection{Overview}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:overview}}
\sphinxAtStartPar
This script demonstrates asyncio’s event loop by implementing a simple task scheduling system where three tasks (A, B, C) call each other in rotation for 60 seconds.


\subsection{File Location}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:file-location}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{basics/07*asyncio\_event*loop.py}}


\subsection{Key Concepts}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:key-concepts}}

\subsubsection{Asyncio Event Loop}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:id1}}
\sphinxAtStartPar
The event loop is the core of asyncio’s asynchronous execution model. It manages and schedules coroutines, handles I/O operations, and coordinates concurrent tasks.


\subsubsection{Async/Await Pattern}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:async-await-pattern}}
\sphinxAtStartPar
Modern Python uses \sphinxcode{\sphinxupquote{async def}} to define coroutines and \sphinxcode{\sphinxupquote{await}} to yield control back to the event loop while waiting for operations to complete.


\subsection{Code Breakdown}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:code-breakdown}}

\subsubsection{Task Functions}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:task-functions}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def task\_A(end\_time):}
\sphinxAtStartPar
print(“task\_A called”)
await asyncio.sleep(random.randint(0, 5))
if (asyncio.get\_event*loop().time() + 1.0) \textless{} end\_time:
\begin{quote}

\sphinxAtStartPar
await asyncio.sleep(1)
await task\_B(end\_time)
\end{quote}

\end{description}

\sphinxAtStartPar
Each task:
1. Prints its name
2. Sleeps for a random duration (0\sphinxhyphen{}5 seconds)
3. Checks if there’s time remaining before end\_time
4. If time remains, sleeps for 1 more second and calls the next task
5. Creates a chain: A \sphinxhyphen{}\textgreater{} B \sphinxhyphen{}\textgreater{} C \sphinxhyphen{}\textgreater{} A (repeating)


\subsubsection{Main Function}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:main-function}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def main():}
\sphinxAtStartPar
loop = asyncio.get\_event*loop()
end\_loop = loop.time() + 60
await task\_A(end\_loop)

\end{description}

\sphinxAtStartPar
Sets up the event loop to run for 60 seconds and initiates the task chain.


\subsubsection{Entry Point}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:entry-point}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{if \sphinxstylestrong{name} == ‘\sphinxstylestrong{main}’:}
\sphinxAtStartPar
asyncio.run(main())

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
``asyncio.run()`` is the modern way to execute the top\PYGZhy{}level async function, handling event loop creation and cleanup automatically.
\end{sphinxVerbatim}


\subsection{Python 3.12 Updates}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:python-3-12-updates}}

\subsubsection{Changes Made}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:changes-made}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced blocking \textasciigrave{}\textasciigrave{}time.sleep()\textasciigrave{}\textasciigrave{} with \textasciigrave{}\textasciigrave{}await asyncio.sleep()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{time.sleep()}} blocks the entire event loop
\sphinxhyphen{} \sphinxcode{\sphinxupquote{asyncio.sleep()}} yields control, allowing other tasks to run

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Converted to async/await syntax}
\sphinxhyphen{} Replaced callback\sphinxhyphen{}based event loop manipulation (\sphinxcode{\sphinxupquote{loop.call\_later}}, \sphinxcode{\sphinxupquote{loop.call\_soon}})
\sphinxhyphen{} Used modern async function chaining instead

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Updated to \textasciigrave{}\textasciigrave{}asyncio.run()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Replaced manual event loop management (\sphinxcode{\sphinxupquote{get\_event*loop()}}, \sphinxcode{\sphinxupquote{run\_forever()}}, \sphinxcode{\sphinxupquote{close()}})
\sphinxhyphen{} \sphinxcode{\sphinxupquote{asyncio.run()}} is simpler and handles cleanup automatically

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Removed \textasciigrave{}\textasciigrave{}loop.stop()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} No longer needed with async/await pattern
\sphinxhyphen{} Function naturally completes when time expires

\end{enumerate}


\subsection{Execution Flow}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:execution-flow}}\begin{description}
\sphinxlineitem{main() starts}
\sphinxAtStartPar
down

\sphinxlineitem{task\_A() executes}
\sphinxAtStartPar
down (sleeps 0\sphinxhyphen{}5 seconds)
down (sleeps 1 second)
down

\sphinxlineitem{task\_B() executes}
\sphinxAtStartPar
down (sleeps 0\sphinxhyphen{}5 seconds)
down (sleeps 1 second)
down

\sphinxlineitem{task\_C() executes}
\sphinxAtStartPar
down (sleeps 0\sphinxhyphen{}5 seconds)
down (sleeps 1 second)
down

\sphinxlineitem{task\_A() executes again}
\sphinxAtStartPar
down

\sphinxlineitem{… (repeats for \textasciitilde{}60 seconds)}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
Time expires, chain stops


\subsection{Usage}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:usage}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
python3 07*asyncio\_event*loop.py

\sphinxAtStartPar
The script will run for approximately 60 seconds, printing task names as they execute.


\subsection{Output Example}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:output-example}}
\sphinxAtStartPar
task\_A called
task\_B called
task\_C called
task\_A called
task\_B called
task\_C called
…


\subsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/asyncio_event_loop:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Non\sphinxhyphen{}blocking Sleep}: Always use \sphinxcode{\sphinxupquote{asyncio.sleep()}} in async code, never \sphinxcode{\sphinxupquote{time.sleep()}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Event Loop Time}: Use \sphinxcode{\sphinxupquote{loop.time()}} for precise timing within the event loop

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Async Chains}: Coroutines can call other coroutines using \sphinxcode{\sphinxupquote{await}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Modern asyncio}: \sphinxcode{\sphinxupquote{asyncio.run()}} is the preferred way to run async code in Python 3.7+

\end{enumerate}

\sphinxstepscope


\section{Asyncio Coroutine \sphinxhyphen{} Finite State Machine}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:asyncio-coroutine-finite-state-machine}}\label{\detokenize{cpu-concurrency/asyncio_coroutine::doc}}

\subsection{Overview}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:overview}}
\sphinxAtStartPar
This script demonstrates asyncio coroutines by implementing a finite state machine (FSM) that randomly transitions between states until reaching an end state.


\subsection{File Location}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:file-location}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{basics/08*asyncio\_coroutine.py}}


\subsection{Key Concepts}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:key-concepts}}

\subsubsection{Finite State Machine (FSM)}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:finite-state-machine-fsm}}
\sphinxAtStartPar
A computational model consisting of states and transitions between those states. The system is always in exactly one state at a time.


\subsubsection{Coroutines}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:coroutines}}
\sphinxAtStartPar
Special functions defined with \sphinxcode{\sphinxupquote{async def}} that can pause execution and yield control back to the event loop using \sphinxcode{\sphinxupquote{await}}.


\subsubsection{State Transitions}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:state-transitions}}
\sphinxAtStartPar
The FSM randomly chooses the next state based on a random input value (0 or 1), simulating decision\sphinxhyphen{}making logic.


\subsection{Code Breakdown}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:code-breakdown}}

\subsubsection{State Machine Structure}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:state-machine-structure}}\begin{description}
\sphinxlineitem{Start State}
\sphinxAtStartPar
down
{\color{red}\bfseries{}|}\textendash{}\textgreater{} State 1 \sphinxhyphen{}+\textendash{}\textgreater{} State 2
|            +\textendash{}\textgreater{} State 3
+\textendash{}\textgreater{} State 2 \sphinxhyphen{}+\textendash{}\textgreater{} State 1
\begin{quote}

\sphinxAtStartPar
+\textendash{}\textgreater{} State 3
\end{quote}

\sphinxlineitem{State 3 \sphinxhyphen{}+\textendash{}\textgreater{} State 1}
\sphinxAtStartPar
+\textendash{}\textgreater{} End State

\end{description}


\subsubsection{Start State}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:start-state}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def start\_state():}
\sphinxAtStartPar
print(‘Start State calledn’)
input\_value = randint(0, 1)
await asyncio.sleep(1)
\begin{description}
\sphinxlineitem{if input\_value == 0:}
\sphinxAtStartPar
result = await state2(input\_value)

\sphinxlineitem{else:}
\sphinxAtStartPar
result = await state1(input\_value)

\end{description}

\sphinxAtStartPar
print(f’Resume of the Transition : nStart State calling \{result\}’)

\end{description}

\sphinxAtStartPar
Entry point of the FSM that randomly transitions to State 1 or State 2.


\subsubsection{Intermediate States (State 1, 2, 3)}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:intermediate-states-state-1-2-3}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def state1(transition\_value):}
\sphinxAtStartPar
output\_value = f’State 1 with transition value = \{transition\_value\}n’
input\_value = randint(0, 1)
await asyncio.sleep(1)

\sphinxAtStartPar
print(’…evaluating…’)
if input\_value == 0:
\begin{quote}

\sphinxAtStartPar
result = await state3(input\_value)
\end{quote}
\begin{description}
\sphinxlineitem{else:}
\sphinxAtStartPar
result = await state2(input\_value)

\end{description}

\sphinxAtStartPar
return output\_value + f’State 1 calling \{result\}’

\end{description}

\sphinxAtStartPar
Each state:
1. Records its transition value
2. Waits asynchronously (simulates processing)
3. Randomly chooses the next state
4. Returns a string tracking the transition path


\subsubsection{End State}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:end-state}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def end\_state(transition\_value):}
\sphinxAtStartPar
output\_value = f’End State with transition value = \{transition\_value\}n’
print(’…stop computation…’)
return output\_value

\end{description}

\sphinxAtStartPar
Terminal state that ends the computation and returns the final result.


\subsection{Python 3.12 Updates}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:python-3-12-updates}}

\subsubsection{Changes Made}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:changes-made}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced \textasciigrave{}\textasciigrave{}@asyncio.coroutine\textasciigrave{}\textasciigrave{} decorator with \textasciigrave{}\textasciigrave{}async def\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Old: \sphinxcode{\sphinxupquote{@asyncio.coroutine}} + \sphinxcode{\sphinxupquote{def function()}}
\sphinxhyphen{} New: \sphinxcode{\sphinxupquote{async def function()}}
\sphinxhyphen{} The decorator was deprecated in Python 3.8

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced \textasciigrave{}\textasciigrave{}yield from\textasciigrave{}\textasciigrave{} with \textasciigrave{}\textasciigrave{}await\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Old: \sphinxcode{\sphinxupquote{result = yield from state2(input\_value)}}
\sphinxhyphen{} New: \sphinxcode{\sphinxupquote{result = await state2(input\_value)}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{await}} is the modern, cleaner syntax

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced \textasciigrave{}\textasciigrave{}time.sleep()\textasciigrave{}\textasciigrave{} with \textasciigrave{}\textasciigrave{}await asyncio.sleep()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Prevents blocking the event loop
\sphinxhyphen{} Allows proper asynchronous execution

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Updated to \textasciigrave{}\textasciigrave{}asyncio.run()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Old: \sphinxcode{\sphinxupquote{loop = asyncio.get\_event*loop(); loop.run\_until*complete()}}
\sphinxhyphen{} New: \sphinxcode{\sphinxupquote{asyncio.run(start\_state())}}
\sphinxhyphen{} Simpler and handles cleanup automatically

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Modernized string formatting}
\sphinxhyphen{} Replaced \sphinxcode{\sphinxupquote{\%}} formatting with f\sphinxhyphen{}strings
\sphinxhyphen{} More readable and Pythonic

\end{enumerate}


\subsection{Execution Flow Example}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:execution-flow-example}}\begin{description}
\sphinxlineitem{start\_state() \sphinxhyphen{}\textgreater{} (random: 1)}
\sphinxAtStartPar
down

\sphinxlineitem{state1(1) \sphinxhyphen{}\textgreater{} (random: 0)}
\sphinxAtStartPar
down

\sphinxlineitem{state3(0) \sphinxhyphen{}\textgreater{} (random: 1)}
\sphinxAtStartPar
down

\sphinxlineitem{end\_state(1)}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
Returns complete transition path


\subsection{Usage}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:usage}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
python3 08*asyncio\_coroutine.py


\subsection{Output Example}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:output-example}}
\sphinxAtStartPar
Finite State Machine simulation with Asyncio Coroutine
Start State called

\sphinxAtStartPar
…evaluating…
…evaluating…
…evaluating…
…stop computation…
Resume of the Transition :
Start State calling State 2 with transition value = 0
State 2 calling State 1 with transition value = 0
State 1 calling State 3 with transition value = 0
State 3 calling End State with transition value = 1


\subsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Async Recursion}: Coroutines can call other coroutines recursively using \sphinxcode{\sphinxupquote{await}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Return Values}: Async functions can return values just like regular functions

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{State Pattern}: FSMs are naturally expressed with async/await

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Path Tracking}: String concatenation allows tracking the execution path through states

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Non\sphinxhyphen{}deterministic}: Random transitions create different paths each run

\end{enumerate}


\subsection{Use Cases}
\label{\detokenize{cpu-concurrency/asyncio_coroutine:use-cases}}
\sphinxAtStartPar
This pattern is useful for:
\sphinxhyphen{} Workflow engines
\sphinxhyphen{} Game state management
\sphinxhyphen{} Protocol implementations
\sphinxhyphen{} Decision trees
\sphinxhyphen{} Process automation

\sphinxstepscope


\section{Asyncio and Futures}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:asyncio-and-futures}}\label{\detokenize{cpu-concurrency/asyncio_and_futures::doc}}

\subsection{Overview}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:overview}}
\sphinxAtStartPar
This script demonstrates asyncio with command\sphinxhyphen{}line arguments, executing two mathematical computations (sum and factorial) concurrently and returning their results.


\subsection{File Location}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:file-location}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{basics/10*asyncio\_and*futures.py}}


\subsection{Key Concepts}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:key-concepts}}

\subsubsection{Command\sphinxhyphen{}Line Arguments}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:command-line-arguments}}
\sphinxAtStartPar
Uses \sphinxcode{\sphinxupquote{sys.argv}} to accept input values from the command line, making the script configurable.


\subsubsection{Async Task Results}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:async-task-results}}
\sphinxAtStartPar
Coroutines can return values that can be collected and used by the caller.


\subsubsection{Concurrent Computation}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:concurrent-computation}}
\sphinxAtStartPar
Two independent computations run concurrently, each with their own delay.


\subsection{Code Breakdown}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:code-breakdown}}

\subsubsection{First Coroutine \sphinxhyphen{} Sum of N Integers}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:first-coroutine-sum-of-n-integers}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def first\_coroutine(num):}
\sphinxAtStartPar
count = 0
for i in range(1, num + 1):
\begin{quote}

\sphinxAtStartPar
count += 1
\end{quote}

\sphinxAtStartPar
await asyncio.sleep(4)
result = f’First coroutine (sum of N ints) result = \{count\}’
print(result)
return result

\end{description}

\sphinxAtStartPar
Computes a simple sum (which equals \sphinxcode{\sphinxupquote{num}}) and waits 4 seconds before returning.


\subsubsection{Second Coroutine \sphinxhyphen{} Factorial}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:second-coroutine-factorial}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def second\_coroutine(num):}
\sphinxAtStartPar
count = 1
for i in range(2, num + 1):
\begin{quote}

\sphinxAtStartPar
count {\color{red}\bfseries{}*}= i
\end{quote}

\sphinxAtStartPar
await asyncio.sleep(4)
result = f’Second coroutine (factorial) result = \{count\}’
print(result)
return result

\end{description}

\sphinxAtStartPar
Computes factorial and also waits 4 seconds before returning.


\subsubsection{Main Function}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:main-function}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def main():}
\sphinxAtStartPar
num1 = int(sys.argv{[}1{]})
num2 = int(sys.argv{[}2{]})
\begin{description}
\sphinxlineitem{tasks = {[}}
\sphinxAtStartPar
asyncio.create\_task(first\_coroutine(num1)),
asyncio.create\_task(second\_coroutine(num2))

\end{description}

\sphinxAtStartPar
{]}

\sphinxAtStartPar
await asyncio.gather({\color{red}\bfseries{}*}tasks)

\end{description}

\sphinxAtStartPar
Reads command\sphinxhyphen{}line arguments, creates two tasks, and runs them concurrently.


\subsection{Python 3.12 Updates}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:python-3-12-updates}}

\subsubsection{Changes Made}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:changes-made}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Removed \textasciigrave{}\textasciigrave{}asyncio.Future()\textasciigrave{}\textasciigrave{} pattern}
\sphinxhyphen{} Old approach used explicit Future objects and callbacks
\sphinxhyphen{} Modern approach: tasks return values directly
\sphinxhyphen{} Simpler and more Pythonic

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced \textasciigrave{}\textasciigrave{}@asyncio.coroutine\textasciigrave{}\textasciigrave{} with \textasciigrave{}\textasciigrave{}async def\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Modern syntax for defining coroutines

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced \textasciigrave{}\textasciigrave{}yield from\textasciigrave{}\textasciigrave{} with \textasciigrave{}\textasciigrave{}await\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Standard async/await syntax

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Removed \textasciigrave{}\textasciigrave{}add\_done*callback()\textasciigrave{}\textasciigrave{} pattern}
\sphinxhyphen{} Old: \sphinxcode{\sphinxupquote{future.add\_done*callback(got\_result)}}
\sphinxhyphen{} New: Direct return values and \sphinxcode{\sphinxupquote{await}}
\sphinxhyphen{} Callbacks are unnecessary with modern async/await

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Removed \textasciigrave{}\textasciigrave{}future.set\_result()\textasciigrave{}\textasciigrave{} pattern}
\sphinxhyphen{} Old: Manually setting future results
\sphinxhyphen{} New: Simply return values from async functions

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simplified to \textasciigrave{}\textasciigrave{}asyncio.create\_task()\textasciigrave{}\textasciigrave{} and \textasciigrave{}\textasciigrave{}asyncio.gather()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Cleaner task management
\sphinxhyphen{} Results can be captured if needed

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Updated to \textasciigrave{}\textasciigrave{}asyncio.run()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Automatic event loop management

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Modernized string formatting}
\sphinxhyphen{} f\sphinxhyphen{}strings throughout

\end{enumerate}


\subsection{Before vs After Comparison}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:before-vs-after-comparison}}

\subsubsection{Old Pattern (Deprecated)}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:old-pattern-deprecated}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
future = asyncio.Future()
future.add\_done*callback(callback\_function)
await coroutine(future, value)
future.set\_result(result)


\subsubsection{New Pattern (Modern)}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:new-pattern-modern}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
result = await coroutine(value)
Or with tasks:
==============
task = asyncio.create\_task(coroutine(value))
result = await task


\subsection{Execution Flow}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:execution-flow}}\begin{description}
\sphinxlineitem{main() starts}
\sphinxAtStartPar
down

\sphinxlineitem{Reads command\sphinxhyphen{}line arguments}
\sphinxAtStartPar
down

\sphinxlineitem{Creates 2 tasks:}
\sphinxAtStartPar
{\color{red}\bfseries{}|}\textendash{}\textgreater{} first\_coroutine(num1)
+\textendash{}\textgreater{} second\_coroutine(num2)

\sphinxlineitem{Both tasks execute concurrently:}
\sphinxAtStartPar
{\color{red}\bfseries{}|}\textendash{}\textgreater{} first: computes sum, sleeps 4s, prints/returns result
+\textendash{}\textgreater{} second: computes factorial, sleeps 4s, prints/returns result

\sphinxlineitem{gather() waits for both}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
main() completes


\subsection{Usage}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:usage}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
python3 10*asyncio\_and*futures.py \textless{}num1\textgreater{} \textless{}num2\textgreater{}


\subsection{Examples}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:examples}}

\subsubsection{Example 1}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:example-1}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
python3 10*asyncio\_and*futures.py 10 5

\sphinxAtStartPar
Output:

\sphinxAtStartPar
First coroutine (sum of N ints) result = 10
Second coroutine (factorial) result = 120


\subsubsection{Example 2}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:example-2}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
python3 10*asyncio\_and*futures.py 100 7

\sphinxAtStartPar
Output:

\sphinxAtStartPar
First coroutine (sum of N ints) result = 100
Second coroutine (factorial) result = 5040


\subsection{Performance}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:performance}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sequential execution}: Would take 8 seconds (4 + 4)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Concurrent execution}: Takes only 4 seconds (both run together)

\item {} 
\sphinxAtStartPar
Both coroutines sleep simultaneously

\item {} 
\sphinxAtStartPar
Demonstrates time savings from concurrency

\end{itemize}


\subsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Direct Returns}: Modern async functions return values directly, no need for Future objects

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Task Results}: \sphinxcode{\sphinxupquote{asyncio.gather()}} can collect return values: \sphinxcode{\sphinxupquote{results = await asyncio.gather(*tasks)}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simplicity}: Callback\sphinxhyphen{}based code replaced with simple await expressions

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Command\sphinxhyphen{}Line Integration}: Async programs can easily accept CLI arguments

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Concurrent I/O}: Multiple operations run together during wait times

\end{enumerate}


\subsection{Modern Best Practices}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:modern-best-practices}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Avoid explicit Future objects} in application code

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use \textasciigrave{}\textasciigrave{}asyncio.create\_task()\textasciigrave{}\textasciigrave{}} to schedule coroutines

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use \textasciigrave{}\textasciigrave{}asyncio.gather()\textasciigrave{}\textasciigrave{}} to wait for multiple tasks

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Return values directly} from async functions

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use \textasciigrave{}\textasciigrave{}asyncio.run()\textasciigrave{}\textasciigrave{}} as the entry point

\end{enumerate}


\subsection{When to Use This Pattern}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:when-to-use-this-pattern}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Multiple independent async operations

\item {} 
\sphinxAtStartPar
Need to collect results from concurrent tasks

\item {} 
\sphinxAtStartPar
Operations have waiting periods (I/O, sleep, network)

\item {} 
\sphinxAtStartPar
Want clean, readable async code without callbacks

\end{itemize}


\subsection{Capturing Results}
\label{\detokenize{cpu-concurrency/asyncio_and_futures:capturing-results}}
\sphinxAtStartPar
If you need the return values:
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{async def main():}\begin{description}
\sphinxlineitem{tasks = {[}}
\sphinxAtStartPar
asyncio.create\_task(first\_coroutine(num1)),
asyncio.create\_task(second\_coroutine(num2))

\end{description}

\sphinxAtStartPar
{]}
results = await asyncio.gather({\color{red}\bfseries{}*}tasks)
\# results{[}0{]} = first coroutine result
\# results{[}1{]} = second coroutine result

\end{description}

\sphinxstepscope


\section{Asyncio Task Manipulation}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:asyncio-task-manipulation}}\label{\detokenize{cpu-concurrency/asyncio_task_manipulation::doc}}

\subsection{Overview}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:overview}}
\sphinxAtStartPar
This script demonstrates how to run multiple asyncio tasks in parallel by executing three mathematical computations (factorial, fibonacci, binomial coefficient) concurrently.


\subsection{File Location}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:file-location}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{basics/09*asyncio\_task*manipulation.py}}


\subsection{Key Concepts}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:key-concepts}}

\subsubsection{Asyncio Tasks}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:asyncio-tasks}}
\sphinxAtStartPar
Tasks are used to schedule coroutines concurrently. When a coroutine is wrapped in a Task, it starts running in the background as soon as possible.


\subsubsection{Concurrent Execution}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:concurrent-execution}}
\sphinxAtStartPar
Multiple tasks can run concurrently within a single thread, interleaving execution when one task is waiting (e.g., during \sphinxcode{\sphinxupquote{await asyncio.sleep()}}).


\subsubsection{asyncio.gather()}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:asyncio-gather}}
\sphinxAtStartPar
Runs multiple awaitables concurrently and waits for all to complete, maintaining order of results.


\subsection{Code Breakdown}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:code-breakdown}}

\subsubsection{Factorial Computation}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:factorial-computation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def factorial(number):}
\sphinxAtStartPar
fact = 1
for i in range(2, number + 1):
\begin{quote}

\sphinxAtStartPar
print(f’Asyncio.Task: Compute factorial(\{i\})’)
await asyncio.sleep(1)
fact {\color{red}\bfseries{}*}= i
\end{quote}

\sphinxAtStartPar
print(f’Asyncio.Task \sphinxhyphen{} factorial(\{number\}) = \{fact\}’)

\end{description}

\sphinxAtStartPar
Computes factorial iteratively, sleeping 1 second between each step to simulate work.


\subsubsection{Fibonacci Computation}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:fibonacci-computation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def fibonacci(number):}
\sphinxAtStartPar
a, b = 0, 1
for i in range(number):
\begin{quote}

\sphinxAtStartPar
print(f’Asyncio.Task: Compute fibonacci(\{i\})’)
await asyncio.sleep(1)
a, b = b, a + b
\end{quote}

\sphinxAtStartPar
print(f’Asyncio.Task \sphinxhyphen{} fibonacci(\{number\}) = \{a\}’)

\end{description}

\sphinxAtStartPar
Computes Fibonacci number iteratively with 1 second delay per iteration.


\subsubsection{Binomial Coefficient Computation}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:binomial-coefficient-computation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def binomial\_coefficient(n, k):}
\sphinxAtStartPar
result = 1
for i in range(1, k + 1):
\begin{quote}

\sphinxAtStartPar
result = result * (n \sphinxhyphen{} i + 1) / i
print(f’Asyncio.Task: Compute binomial\_coefficient(\{i\})’)
await asyncio.sleep(1)
\end{quote}

\sphinxAtStartPar
print(f’Asyncio.Task \sphinxhyphen{} binomial\_coefficient(\{n\}, \{k\}) = \{result\}’)

\end{description}

\sphinxAtStartPar
Computes binomial coefficient C(n,k) iteratively.


\subsubsection{Main Function}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:main-function}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def main():}\begin{description}
\sphinxlineitem{task\_list = {[}}
\sphinxAtStartPar
asyncio.create\_task(factorial(10)),
asyncio.create\_task(fibonacci(10)),
asyncio.create\_task(binomial\_coefficient(20, 10))

\end{description}

\sphinxAtStartPar
{]}
await asyncio.gather({\color{red}\bfseries{}*}task\_list)

\end{description}

\sphinxAtStartPar
Creates three tasks and runs them concurrently using \sphinxcode{\sphinxupquote{gather()}}.


\subsection{Python 3.12 Updates}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:python-3-12-updates}}

\subsubsection{Changes Made}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:changes-made}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced \textasciigrave{}\textasciigrave{}@asyncio.coroutine\textasciigrave{}\textasciigrave{} with \textasciigrave{}\textasciigrave{}async def\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Modern coroutine syntax
\sphinxhyphen{} Deprecated decorator removed

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced \textasciigrave{}\textasciigrave{}yield from\textasciigrave{}\textasciigrave{} with \textasciigrave{}\textasciigrave{}await\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Cleaner, more explicit syntax
\sphinxhyphen{} Standard since Python 3.5

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced \textasciigrave{}\textasciigrave{}asyncio.Task()\textasciigrave{}\textasciigrave{} with \textasciigrave{}\textasciigrave{}asyncio.create\_task()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Old: \sphinxcode{\sphinxupquote{asyncio.Task(factorial(10))}}
\sphinxhyphen{} New: \sphinxcode{\sphinxupquote{asyncio.create\_task(factorial(10))}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{asyncio.Task()}} constructor was deprecated
\sphinxhyphen{} \sphinxcode{\sphinxupquote{create\_task()}} is the recommended way to schedule coroutines

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replaced \textasciigrave{}\textasciigrave{}asyncio.wait()\textasciigrave{}\textasciigrave{} with \textasciigrave{}\textasciigrave{}asyncio.gather()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Old: \sphinxcode{\sphinxupquote{asyncio.wait(task\_list)}}
\sphinxhyphen{} New: \sphinxcode{\sphinxupquote{asyncio.gather(*task\_list)}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{gather()}} is simpler and returns results in order
\sphinxhyphen{} Better error handling and more Pythonic

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Updated to \textasciigrave{}\textasciigrave{}asyncio.run()\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Replaces manual event loop management
\sphinxhyphen{} Automatic cleanup

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Modernized string formatting}
\sphinxhyphen{} f\sphinxhyphen{}strings instead of \sphinxcode{\sphinxupquote{\%}} formatting

\end{enumerate}


\subsection{Execution Flow}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:execution-flow}}\begin{description}
\sphinxlineitem{main() starts}
\sphinxAtStartPar
down

\sphinxlineitem{Creates 3 tasks simultaneously:}
\sphinxAtStartPar
{\color{red}\bfseries{}|}\textendash{}\textgreater{} factorial(10) task
{\color{red}\bfseries{}|}\textendash{}\textgreater{} fibonacci(10) task
+\textendash{}\textgreater{} binomial\_coefficient(20, 10) task

\sphinxlineitem{All tasks run concurrently:}\begin{description}
\sphinxlineitem{Each task:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Prints computation step

\item {} 
\sphinxAtStartPar
Sleeps 1 second (yields to other tasks)

\item {} 
\sphinxAtStartPar
Continues computation

\item {} 
\sphinxAtStartPar
Repeats until done

\end{itemize}

\end{description}

\sphinxlineitem{gather() waits for all tasks to complete}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
main() completes


\subsection{Concurrency vs Parallelism}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:concurrency-vs-parallelism}}
\sphinxAtStartPar
This demonstrates \sphinxstylestrong{concurrency} (not parallelism):
\sphinxhyphen{} Tasks interleave execution in a single thread
\sphinxhyphen{} When one task {\color{red}\bfseries{}\textasciigrave{}\textasciigrave{}}await\textasciigrave{}\textasciigrave{}s, another can run
\sphinxhyphen{} Not true parallel execution (no multiple CPU cores used)
\sphinxhyphen{} Perfect for I/O\sphinxhyphen{}bound operations


\subsection{Usage}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:usage}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
python3 09*asyncio\_task*manipulation.py


\subsection{Output Example}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:output-example}}
\sphinxAtStartPar
Asyncio.Task: Compute factorial(2)
Asyncio.Task: Compute fibonacci(0)
Asyncio.Task: Compute binomial\_coefficient(1)
Asyncio.Task: Compute factorial(3)
Asyncio.Task: Compute fibonacci(1)
Asyncio.Task: Compute binomial\_coefficient(2)
…
Asyncio.Task \sphinxhyphen{} factorial(10) = 3628800
…
Asyncio.Task \sphinxhyphen{} fibonacci(10) = 55
Asyncio.Task \sphinxhyphen{} binomial\_coefficient(20, 10) = 184756.0

\sphinxAtStartPar
Notice how the output interleaves between tasks, showing concurrent execution.


\subsection{Performance}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:performance}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sequential execution}: Would take \textasciitilde{}30 seconds (10 + 10 + 10)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Concurrent execution}: Takes \textasciitilde{}10 seconds (all run together)

\item {} 
\sphinxAtStartPar
Each task sleeps 1 second per iteration

\item {} 
\sphinxAtStartPar
While one sleeps, others continue

\end{itemize}


\subsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Task Creation}: Use \sphinxcode{\sphinxupquote{asyncio.create\_task()}} to schedule coroutines

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Concurrent Execution}: Multiple tasks can run together in one thread

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Gather Pattern}: \sphinxcode{\sphinxupquote{asyncio.gather()}} waits for all tasks and collects results

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Interleaving}: Tasks yield control during \sphinxcode{\sphinxupquote{await}}, allowing others to run

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Efficiency}: Great for I/O\sphinxhyphen{}bound operations where waiting dominates

\end{enumerate}


\subsection{When to Use}
\label{\detokenize{cpu-concurrency/asyncio_task_manipulation:when-to-use}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Multiple I/O operations (network requests, file operations)

\item {} 
\sphinxAtStartPar
Operations that involve waiting

\item {} 
\sphinxAtStartPar
Need concurrent execution without multi\sphinxhyphen{}processing overhead

\item {} 
\sphinxAtStartPar
Tasks are independent and can run in any order

\end{itemize}

\sphinxstepscope


\section{Concurrent Futures Pooling}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:concurrent-futures-pooling}}\label{\detokenize{cpu-concurrency/concurrent_futures_pooling::doc}}

\subsection{Overview}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:overview}}
\sphinxAtStartPar
This script demonstrates the performance differences between sequential execution, thread pool execution, and process pool execution using Python’s \sphinxcode{\sphinxupquote{concurrent.futures}} module.


\subsection{File Location}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:file-location}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{basics/06*concurrent\_futures*pooling.py}}


\subsection{Key Concepts}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:key-concepts}}

\subsubsection{Executor Pattern}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:executor-pattern}}
\sphinxAtStartPar
The script uses the executor pattern from \sphinxcode{\sphinxupquote{concurrent.futures}} which provides a high\sphinxhyphen{}level interface for asynchronously executing callables.


\subsubsection{Execution Models}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:execution-models}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sequential Execution}: Tasks run one after another on a single thread

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ThreadPoolExecutor}: Tasks run concurrently using multiple threads (limited by GIL for CPU\sphinxhyphen{}bound tasks)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ProcessPoolExecutor}: Tasks run in parallel using multiple processes (true parallelism for CPU\sphinxhyphen{}bound tasks)

\end{enumerate}


\subsection{Code Breakdown}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:code-breakdown}}

\subsubsection{CPU\sphinxhyphen{}Intensive Task}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:cpu-intensive-task}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def count(number):}\begin{description}
\sphinxlineitem{for i in range(0, 10000000):}
\sphinxAtStartPar
i += 1

\end{description}

\sphinxAtStartPar
return i * number

\end{description}

\sphinxAtStartPar
A CPU\sphinxhyphen{}bound operation that performs 10 million iterations to demonstrate performance differences.


\subsubsection{Evaluation Function}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:evaluation-function}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def evaluate(item):}
\sphinxAtStartPar
result\_item = count(item)
print(f’Item \{item\}, result \{result\_item\}’)

\end{description}

\sphinxAtStartPar
Wrapper function that executes the CPU\sphinxhyphen{}intensive task and prints the result.


\subsubsection{Sequential Execution}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:sequential-execution}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
start\_time = time.perf\_counter()
for item in number\_list:
\begin{quote}

\sphinxAtStartPar
evaluate(item)
\end{quote}

\sphinxAtStartPar
print(f’Sequential Execution in \{time.perf\_counter() \sphinxhyphen{} start\_time\} seconds’)

\sphinxAtStartPar
Baseline performance \sphinxhyphen{} executes all tasks sequentially in the main thread.


\subsubsection{Thread Pool Execution}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:thread-pool-execution}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{with concurrent.futures.ThreadPoolExecutor(max\_workers=5) as executor:}\begin{description}
\sphinxlineitem{for item in number\_list:}
\sphinxAtStartPar
executor.submit(evaluate, item)

\end{description}

\end{description}

\sphinxAtStartPar
Uses a pool of 5 threads. Due to Python’s Global Interpreter Lock (GIL), CPU\sphinxhyphen{}bound tasks don’t see much performance improvement with threads.


\subsubsection{Process Pool Execution}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:process-pool-execution}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{with concurrent.futures.ProcessPoolExecutor(max\_workers=5) as executor:}\begin{description}
\sphinxlineitem{for item in number\_list:}
\sphinxAtStartPar
executor.submit(evaluate, item)

\end{description}

\end{description}

\sphinxAtStartPar
Uses a pool of 5 separate processes. This bypasses the GIL and provides true parallel execution for CPU\sphinxhyphen{}bound tasks.


\subsection{Performance Expectations}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:performance-expectations}}
\sphinxAtStartPar
For CPU\sphinxhyphen{}bound tasks:
\sphinxhyphen{} \sphinxstylestrong{Sequential}: Baseline performance
\sphinxhyphen{} \sphinxstylestrong{ThreadPool}: Similar or slightly slower than sequential (due to GIL and threading overhead)
\sphinxhyphen{} \sphinxstylestrong{ProcessPool}: Significantly faster (near\sphinxhyphen{}linear speedup based on number of cores)


\subsection{Python 3.12 Updates}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:python-3-12-updates}}

\subsubsection{Changes Made}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:changes-made}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Replaced deprecated \sphinxcode{\sphinxupquote{time.clock()}} with \sphinxcode{\sphinxupquote{time.perf\_counter()}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{time.clock()}} was removed in Python 3.8
\sphinxhyphen{} \sphinxcode{\sphinxupquote{time.perf\_counter()}} provides higher resolution and is the recommended timing function

\item {} 
\sphinxAtStartPar
Updated string formatting from \sphinxcode{\sphinxupquote{\%}} to f\sphinxhyphen{}strings
\sphinxhyphen{} More readable and Pythonic
\sphinxhyphen{} Better performance

\end{enumerate}


\subsection{Usage}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:usage}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
python3 06*concurrent\_futures\_pooling.py


\subsection{Output Example}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:output-example}}
\sphinxAtStartPar
Item 1, result 10000000
Item 2, result 20000000
…
Sequential Execution in 3.47 seconds
…
Thread Pool Execution in 3.31 seconds
…
Process Pool Execution in 1.23 seconds


\subsection{When to Use Each Approach}
\label{\detokenize{cpu-concurrency/concurrent_futures_pooling:when-to-use-each-approach}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sequential}: Simple tasks, I/O\sphinxhyphen{}bound operations, or when order matters

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ThreadPool}: I/O\sphinxhyphen{}bound tasks (network requests, file operations)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ProcessPool}: CPU\sphinxhyphen{}bound tasks that need true parallelism

\end{itemize}

\sphinxstepscope


\section{Queue Explained: Thread\sphinxhyphen{}Safe Data Structure \& Locking}
\label{\detokenize{cpu-concurrency/queue_explained:queue-explained-thread-safe-data-structure-locking}}\label{\detokenize{cpu-concurrency/queue_explained::doc}}

\subsection{What is a Queue?}
\label{\detokenize{cpu-concurrency/queue_explained:what-is-a-queue}}
\sphinxAtStartPar
A \sphinxstylestrong{queue} is a \sphinxstylestrong{First\sphinxhyphen{}In\sphinxhyphen{}First\sphinxhyphen{}Out (FIFO)} data structure with \sphinxstylestrong{built\sphinxhyphen{}in thread safety}. It handles synchronization automatically, so you don’t have to manage locks yourself.


\subsubsection{Key Concept}
\label{\detokenize{cpu-concurrency/queue_explained:key-concept}}\begin{description}
\sphinxlineitem{put(item)      get()}
\begin{DUlineblock}{0em}
\item[] {\color{red}\bfseries{}|}
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}             {[}v{]}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}} \PYG{n}{Items} \PYG{n}{added} \PYG{n}{to} \PYG{n}{the} \PYG{o}{*}\PYG{o}{*}\PYG{n}{back}\PYG{o}{*}\PYG{o}{*}
\PYG{o}{\PYGZhy{}} \PYG{n}{Items} \PYG{n}{removed} \PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{the} \PYG{o}{*}\PYG{o}{*}\PYG{n}{front}\PYG{o}{*}\PYG{o}{*}
\PYG{o}{\PYGZhy{}} \PYG{o}{*}\PYG{o}{*}\PYG{n}{FIFO}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{First} \PYG{n}{item} \PYG{o+ow}{in} \PYG{o+ow}{is} \PYG{n}{first} \PYG{n}{item} \PYG{n}{out}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{How Queue Works Internally}
\label{\detokenize{cpu-concurrency/queue_explained:how-queue-works-internally}}

\subsubsection{Internal Structure}
\label{\detokenize{cpu-concurrency/queue_explained:internal-structure}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue

\sphinxAtStartPar
q = Queue()

\sphinxAtStartPar
Queue internally looks like this:

\sphinxAtStartPar
Queue object:
+————————————\sphinxhyphen{}+
| items: {[}1, 2, 3, 4, 5{]}             | \textless{}\sphinxhyphen{} Internal list
| mutex: Lock()                       | \textless{}\sphinxhyphen{} Built\sphinxhyphen{}in lock!
| not\_empty: Condition()              | \textless{}\sphinxhyphen{} Signals when items available
| not\_full: Condition()               | \textless{}\sphinxhyphen{} Signals when space available
| maxsize: 0 (unlimited)              |
+————————————\sphinxhyphen{}+


\subsubsection{Automatic Locking}
\label{\detokenize{cpu-concurrency/queue_explained:automatic-locking}}
\sphinxAtStartPar
When you call \sphinxcode{\sphinxupquote{put()}} or \sphinxcode{\sphinxupquote{get()}}, the Queue \sphinxstylestrong{automatically}:
1. \sphinxstylestrong{Acquires the lock} \sphinxhyphen{} Only one thread can access at a time
2. \sphinxstylestrong{Modifies the list} \sphinxhyphen{} Safely adds/removes items
3. \sphinxstylestrong{Releases the lock} \sphinxhyphen{} Other threads can now access
4. \sphinxstylestrong{Signals waiting threads} \sphinxhyphen{} Wakes up threads waiting for data/space

\sphinxAtStartPar
—


\subsection{Queue Operations}
\label{\detokenize{cpu-concurrency/queue_explained:queue-operations}}

\subsubsection{\sphinxstyleliteralintitle{\sphinxupquote{put(item)}}}
\label{\detokenize{cpu-concurrency/queue_explained:put-item}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Adds item to the back}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Blocks if queue is full} (maxsize reached)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Thread\sphinxhyphen{}safe} \sphinxhyphen{} internally uses lock

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
q = Queue(maxsize=3)

\sphinxAtStartPar
q.put(1)  \# Adds 1, queue: {[}1{]}
q.put(2)  \# Adds 2, queue: {[}1, 2{]}
q.put(3)  \# Adds 3, queue: {[}1, 2, 3{]}
q.put(4)  \# BLOCKS! Queue is full
\begin{quote}

\sphinxAtStartPar
\# Waits until something is removed
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
``get()``
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Thread\sphinxhyphen{}safe} \sphinxhyphen{} internally uses lock

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
q = Queue()
q.put(1)
q.put(2)
q.put(3)

\sphinxAtStartPar
item = q.get()  \# Returns 1, queue: {[}2, 3{]}
item = q.get()  \# Returns 2, queue: {[}3{]}
item = q.get()  \# Returns 3, queue: {[}{]}
item = q.get()  \# BLOCKS! Queue is empty
\begin{quote}

\sphinxAtStartPar
\# Waits until something is added
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
``task\PYGZus{}done()``
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Decrements internal counter}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
q.put(1)
item = q.get()      \# Remove item from queue
Process item…
===============
q.task\_done()       \# Tell queue you’re done with this item

\begin{sphinxVerbatim}[commandchars=\\\{\}]
``join()``
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
q.put(1)
q.put(2)
q.put(3)

\sphinxAtStartPar
thread.get()  \# Got item
thread.get()  \# Got item
thread.get()  \# Got item
thread.task\_done()  \# Done with first
thread.task\_done()  \# Done with second
thread.task\_done()  \# Done with third

\sphinxAtStartPar
q.join()  \# Now returns (all items processed)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Why Queue is Best Practice}
\label{\detokenize{cpu-concurrency/queue_explained:why-queue-is-best-practice}}

\subsubsection{Problem with Manual Locks}
\label{\detokenize{cpu-concurrency/queue_explained:problem-with-manual-locks}}
\sphinxAtStartPar
\sphinxstylestrong{Without Queue \sphinxhyphen{} Manual Locking (Error\sphinxhyphen{}prone):}
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
import threading

\sphinxAtStartPar
data = {[}{]}
lock = threading.Lock()
\begin{description}
\sphinxlineitem{def producer():}
\sphinxAtStartPar
item = produce()
with lock:
\begin{quote}

\sphinxAtStartPar
data.append(item)  \# Must remember to use lock!
\end{quote}

\sphinxlineitem{def consumer():}\begin{description}
\sphinxlineitem{with lock:}\begin{description}
\sphinxlineitem{if not data:}
\sphinxAtStartPar
\# What do we do here? Busy wait? Return None?
return None

\end{description}

\sphinxAtStartPar
item = data.pop(0)

\end{description}

\sphinxAtStartPar
process(item)

\end{description}


\section{Problems:}
\label{\detokenize{cpu-concurrency/queue_explained:problems}}

\section{1. Easy to forget lock somewhere}
\label{\detokenize{cpu-concurrency/queue_explained:easy-to-forget-lock-somewhere}}

\section{2. Busy\sphinxhyphen{}waiting wastes CPU}
\label{\detokenize{cpu-concurrency/queue_explained:busy-waiting-wastes-cpu}}

\section{3. No synchronization between threads}
\label{\detokenize{cpu-concurrency/queue_explained:no-synchronization-between-threads}}

\section{4. Race conditions possible}
\label{\detokenize{cpu-concurrency/queue_explained:race-conditions-possible}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{With} \PYG{n}{Queue} \PYG{o}{\PYGZhy{}} \PYG{n}{Automatic} \PYG{n}{Locking} \PYG{p}{(}\PYG{n}{Correct}\PYG{p}{)}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue

\sphinxAtStartPar
q = Queue()
\begin{description}
\sphinxlineitem{def producer():}
\sphinxAtStartPar
item = produce()
q.put(item)  \# Automatically thread\sphinxhyphen{}safe!

\sphinxlineitem{def consumer():}
\sphinxAtStartPar
item = q.get()  \# Blocks if empty, no busy\sphinxhyphen{}wait
process(item)   \# No lock management needed!

\end{description}


\section{Benefits:}
\label{\detokenize{cpu-concurrency/queue_explained:benefits}}

\section{1. Synchronization automatic}
\label{\detokenize{cpu-concurrency/queue_explained:synchronization-automatic}}

\section{2. No busy\sphinxhyphen{}waiting}
\label{\detokenize{cpu-concurrency/queue_explained:no-busy-waiting}}

\section{3. Blocks correctly when empty/full}
\label{\detokenize{cpu-concurrency/queue_explained:blocks-correctly-when-empty-full}}

\section{4. Task tracking with task\_done()}
\label{\detokenize{cpu-concurrency/queue_explained:task-tracking-with-task-done}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Queue as Locking Mechanism}
\label{\detokenize{cpu-concurrency/queue_explained:queue-as-locking-mechanism}}
\sphinxAtStartPar
Queue provides \sphinxstylestrong{implicit locking} without you having to manage locks directly.


\subsubsection{How Queue Does Locking}
\label{\detokenize{cpu-concurrency/queue_explained:how-queue-does-locking}}
\sphinxAtStartPar
Inside Queue:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{class Queue:}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self):}
\sphinxAtStartPar
self.mutex = Lock()           \# Internal lock
self.not\_empty = Condition()  \# Wait for items
self.not\_full = Condition()   \# Wait for space
self.items = {[}{]}

\sphinxlineitem{def put(self, item):}\begin{description}
\sphinxlineitem{with self.mutex:              \# AUTO\sphinxhyphen{}LOCK HERE}\begin{description}
\sphinxlineitem{if self.maxsize \textgreater{} 0 and len(self.items) \textgreater{}= self.maxsize:}
\sphinxAtStartPar
self.not\_full.wait()  \# Wait for space

\end{description}

\sphinxAtStartPar
self.items.append(item)
self.not\_empty.notify()   \# Wake consumer waiting for items

\end{description}

\sphinxlineitem{def get(self):}\begin{description}
\sphinxlineitem{with self.mutex:              \# AUTO\sphinxhyphen{}LOCK HERE}\begin{description}
\sphinxlineitem{while not self.items:}
\sphinxAtStartPar
self.not\_empty.wait() \# Wait for items

\end{description}

\sphinxAtStartPar
item = self.items.pop(0)
self.not\_full.notify()    \# Wake producer if waiting
return item

\end{description}

\end{description}

\end{description}


\subsubsection{Locking Benefits}
\label{\detokenize{cpu-concurrency/queue_explained:locking-benefits}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Automatic} \sphinxhyphen{} No manual lock management

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Condition variables} \sphinxhyphen{} Threads wait efficiently (no busy\sphinxhyphen{}wait)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Atomic operations} \sphinxhyphen{} Add/remove is indivisible

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fair} \sphinxhyphen{} FIFO order for waiting threads

\end{enumerate}

\sphinxAtStartPar
—


\subsection{The Code Explained: Producer\sphinxhyphen{}Consumer}
\label{\detokenize{cpu-concurrency/queue_explained:the-code-explained-producer-consumer}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue
from threading import Thread
\begin{description}
\sphinxlineitem{class Producer(Thread):}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self, queue):}
\sphinxAtStartPar
Thread.**init**(self)
self.queue = queue

\sphinxlineitem{def run(self):}\begin{description}
\sphinxlineitem{for i in range(5):}
\sphinxAtStartPar
item = random.randint(0, 256)
self.queue.put(item)        \# Safely add to queue
print(f”producer: added \{item\}”)
time.sleep(1)

\end{description}

\end{description}

\sphinxlineitem{class Consumer(Thread):}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self, queue):}
\sphinxAtStartPar
Thread.**init**(self)
self.queue = queue

\sphinxlineitem{def run(self):}\begin{description}
\sphinxlineitem{while True:}
\sphinxAtStartPar
item = self.queue.get()     \# Safely remove from queue
print(f”consumer: got \{item\}”)
self.queue.task\_done()      \# Mark as processed

\end{description}

\end{description}

\end{description}


\section{Create queue}
\label{\detokenize{cpu-concurrency/queue_explained:create-queue}}
\sphinxAtStartPar
queue = Queue()


\section{Create threads}
\label{\detokenize{cpu-concurrency/queue_explained:create-threads}}
\sphinxAtStartPar
producer = Producer(queue)   \# 1 producer
consumer1 = Consumer(queue)  \# 3 consumers
consumer2 = Consumer(queue)
consumer3 = Consumer(queue)


\section{Start all}
\label{\detokenize{cpu-concurrency/queue_explained:start-all}}
\sphinxAtStartPar
producer.start()
consumer1.start()
consumer2.start()
consumer3.start()

\sphinxAtStartPar
producer.join()   \# Wait for producer to finish
consumer1.join()  \# Wait for consumers (infinite loop!)
consumer2.join()
consumer3.join()


\subsection{Time  Producer               Queue              Consumer1/2/3}
\label{\detokenize{cpu-concurrency/queue_explained:time-producer-queue-consumer1-2-3}}\begin{description}
\sphinxlineitem{0     produces item 42      {[}{]}                 waiting…}
\sphinxAtStartPar
put(42)               {[}42{]}               get() wakes up!

\sphinxlineitem{0     put(42)               {[}42{]}               consumer1: got 42}
\sphinxAtStartPar
task\_done()

\sphinxlineitem{1     produces item 7       {[}{]}                 get() blocks (empty)}
\sphinxAtStartPar
put(7)                {[}7{]}                consumer2 wakes up

\sphinxlineitem{1     put(7)                {[}7{]}                consumer2: got 7}
\sphinxAtStartPar
task\_done()

\sphinxlineitem{2     produces item 199     {[}{]}                 get() blocks (empty)}
\sphinxAtStartPar
put(199)              {[}199{]}              consumer3 wakes up

\sphinxlineitem{2     put(199)              {[}199{]}              consumer3: got 199}
\sphinxAtStartPar
task\_done()

\end{description}

\sphinxAtStartPar
…
5     Finishes (join done)   {[}waiting…{]}      Still running (while True)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Queue vs Lock vs Semaphore vs RLock}
\label{\detokenize{cpu-concurrency/queue_explained:queue-vs-lock-vs-semaphore-vs-rlock}}
\begin{DUlineblock}{0em}
\item[] Feature | Lock | RLock | Semaphore | Queue |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| \sphinxstylestrong{Purpose} | Mutual exclusion | Reentrant locking | Signaling/Resource pool | Data passing |
| \sphinxstylestrong{Thread\sphinxhyphen{}safe data} | {[}FAIL{]} | {[}FAIL{]} | {[}FAIL{]} | {[}OK{]} (built\sphinxhyphen{}in) |
| \sphinxstylestrong{Built\sphinxhyphen{}in blocking} | {[}FAIL{]} | {[}FAIL{]} | {[}OK{]} | {[}OK{]} |
| \sphinxstylestrong{FIFO ordering} | {[}FAIL{]} | {[}FAIL{]} | {[}FAIL{]} | {[}OK{]} |
| \sphinxstylestrong{Task tracking} | {[}FAIL{]} | {[}FAIL{]} | {[}FAIL{]} | {[}OK{]} (task\_done) |
| \sphinxstylestrong{Condition vars} | {[}FAIL{]} | {[}FAIL{]} | {[}OK{]} | {[}OK{]} (internal) |
| \sphinxstylestrong{Best for} | Critical sections | Nested locks | Signaling | Producer\sphinxhyphen{}consumer |

\sphinxAtStartPar
—


\subsection{Queue Benefits Summary}
\label{\detokenize{cpu-concurrency/queue_explained:queue-benefits-summary}}

\subsubsection{1. \sphinxstylestrong{Thread\sphinxhyphen{}Safe Data Structure}}
\label{\detokenize{cpu-concurrency/queue_explained:thread-safe-data-structure}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Without Queue \sphinxhyphen{} Need to manage locking}
\label{\detokenize{cpu-concurrency/queue_explained:without-queue-need-to-manage-locking}}
\sphinxAtStartPar
data = {[}{]}
lock = Lock()
with lock:
\begin{quote}

\sphinxAtStartPar
data.append(item)  \# Manual lock
\end{quote}


\section{With Queue \sphinxhyphen{} Automatic}
\label{\detokenize{cpu-concurrency/queue_explained:with-queue-automatic}}
\sphinxAtStartPar
q = Queue()
q.put(item)  \# No lock needed

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Without Queue \sphinxhyphen{} Need condition variables}
\label{\detokenize{cpu-concurrency/queue_explained:without-queue-need-condition-variables}}\begin{description}
\sphinxlineitem{if not data:}
\sphinxAtStartPar
\# How do we block here? Spin? Sleep?
while not data:
\begin{quote}

\sphinxAtStartPar
time.sleep(0.01)  \# Busy\sphinxhyphen{}wait, wastes CPU
\end{quote}

\end{description}


\section{With Queue \sphinxhyphen{} Blocks automatically}
\label{\detokenize{cpu-concurrency/queue_explained:with-queue-blocks-automatically}}
\sphinxAtStartPar
item = q.get()  \# Blocks if empty, efficient

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Bad \sphinxhyphen{} Busy\sphinxhyphen{}wait (wastes CPU)}
\label{\detokenize{cpu-concurrency/queue_explained:bad-busy-wait-wastes-cpu}}\begin{description}
\sphinxlineitem{while queue\_is*empty:}
\sphinxAtStartPar
time.sleep(0.01)

\end{description}

\sphinxAtStartPar
item = get\_item()


\section{Good \sphinxhyphen{} Queue blocks efficiently}
\label{\detokenize{cpu-concurrency/queue_explained:good-queue-blocks-efficiently}}
\sphinxAtStartPar
item = q.get()  \# No CPU waste, thread sleeps

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
q.put(item1)
q.put(item2)
q.put(item3)


\section{Process items}
\label{\detokenize{cpu-concurrency/queue_explained:process-items}}\begin{description}
\sphinxlineitem{for * in range(3):}
\sphinxAtStartPar
item = q.get()
process(item)
q.task\_done()

\end{description}

\sphinxAtStartPar
q.join()  \# Wait until all items processed

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
q = Queue()


\section{Multiple producers}
\label{\detokenize{cpu-concurrency/queue_explained:multiple-producers}}
\sphinxAtStartPar
producer1 = Producer(q)
producer2 = Producer(q)
producer3 = Producer(q)


\section{Multiple consumers}
\label{\detokenize{cpu-concurrency/queue_explained:multiple-consumers}}
\sphinxAtStartPar
consumer1 = Consumer(q)
consumer2 = Consumer(q)
consumer3 = Consumer(q)


\section{All safe! Queue handles synchronization}
\label{\detokenize{cpu-concurrency/queue_explained:all-safe-queue-handles-synchronization}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Internal Queue Locking Mechanism}
\label{\detokenize{cpu-concurrency/queue_explained:internal-queue-locking-mechanism}}

\subsubsection{Step\sphinxhyphen{}by\sphinxhyphen{}Step: What Happens in \sphinxstyleliteralintitle{\sphinxupquote{put()}}}
\label{\detokenize{cpu-concurrency/queue_explained:step-by-step-what-happens-in-put}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
q.put(42)


\section{Internally in Queue:}
\label{\detokenize{cpu-concurrency/queue_explained:internally-in-queue}}

\section{1. Acquire mutex (lock)}
\label{\detokenize{cpu-concurrency/queue_explained:acquire-mutex-lock}}\begin{quote}

\sphinxAtStartPar
with self.mutex:
\end{quote}


\section{2. Check if full}
\label{\detokenize{cpu-concurrency/queue_explained:check-if-full}}\begin{quote}

\sphinxAtStartPar
if queue is full:
\end{quote}
\begin{quote}

\sphinxAtStartPar
down
\end{quote}
\begin{quote}

\sphinxAtStartPar
self.items.append(42)
\end{quote}


\section{4. Notify consumers}
\label{\detokenize{cpu-concurrency/queue_explained:notify-consumers}}\begin{quote}

\sphinxAtStartPar
not\_empty.notify()  \# Wake one sleeping consumer
\end{quote}


\section{5. Release mutex}
\label{\detokenize{cpu-concurrency/queue_explained:release-mutex}}\begin{quote}

\sphinxAtStartPar
(with block ends)
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
item = q.get()


\section{Internally in Queue:}
\label{\detokenize{cpu-concurrency/queue_explained:id9}}

\section{1. Acquire mutex (lock)}
\label{\detokenize{cpu-concurrency/queue_explained:id10}}\begin{quote}

\sphinxAtStartPar
with self.mutex:
\end{quote}


\section{2. Check if empty}
\label{\detokenize{cpu-concurrency/queue_explained:check-if-empty}}\begin{quote}

\sphinxAtStartPar
while not self.items:
\end{quote}
\begin{quote}

\sphinxAtStartPar
down
\end{quote}
\begin{quote}

\sphinxAtStartPar
item = self.items.pop(0)
\end{quote}


\section{4. Notify producers}
\label{\detokenize{cpu-concurrency/queue_explained:notify-producers}}\begin{quote}

\sphinxAtStartPar
not\_full.notify()  \# Wake one sleeping producer
\end{quote}


\section{5. Release mutex}
\label{\detokenize{cpu-concurrency/queue_explained:id11}}\begin{quote}

\sphinxAtStartPar
return item
\end{quote}

\sphinxAtStartPar
—


\subsection{Why Queue is Safer Than Manual Locking}
\label{\detokenize{cpu-concurrency/queue_explained:why-queue-is-safer-than-manual-locking}}

\subsubsection{Mistake 1: Forgetting Lock}
\label{\detokenize{cpu-concurrency/queue_explained:mistake-1-forgetting-lock}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{{[}{[}FAIL{]}{]} WRONG \sphinxhyphen{} No lock on append}
\label{\detokenize{cpu-concurrency/queue_explained:fail-wrong-no-lock-on-append}}
\sphinxAtStartPar
data = {[}{]}
\begin{description}
\sphinxlineitem{def producer():}
\sphinxAtStartPar
data.append(item)  \# No lock! Race condition!

\sphinxlineitem{def consumer():}
\sphinxAtStartPar
item = data.pop(0)  \# Not synchronized!

\end{description}


\section{{[}{[}OK{]}{]} CORRECT \sphinxhyphen{} Queue is always locked}
\label{\detokenize{cpu-concurrency/queue_explained:ok-correct-queue-is-always-locked}}
\sphinxAtStartPar
q = Queue()
\begin{description}
\sphinxlineitem{def producer():}
\sphinxAtStartPar
q.put(item)  \# Always thread\sphinxhyphen{}safe

\sphinxlineitem{def consumer():}
\sphinxAtStartPar
item = q.get()  \# Always thread\sphinxhyphen{}safe

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{{[}{[}FAIL{]}{]} WRONG \sphinxhyphen{} Wastes CPU}
\label{\detokenize{cpu-concurrency/queue_explained:fail-wrong-wastes-cpu}}\begin{description}
\sphinxlineitem{while not data:}
\sphinxAtStartPar
time.sleep(0.01)  \# Spin loop, bad!

\end{description}

\sphinxAtStartPar
item = data.pop(0)


\section{{[}{[}OK{]}{]} CORRECT \sphinxhyphen{} Queue blocks efficiently}
\label{\detokenize{cpu-concurrency/queue_explained:ok-correct-queue-blocks-efficiently}}
\sphinxAtStartPar
item = q.get()  \# Sleeps without spinning

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{{[}{[}FAIL{]}{]} WRONG \sphinxhyphen{} Check and pop not atomic}
\label{\detokenize{cpu-concurrency/queue_explained:fail-wrong-check-and-pop-not-atomic}}\begin{description}
\sphinxlineitem{with lock:}\begin{description}
\sphinxlineitem{if len(data) \textgreater{} 0:  \# Check}
\sphinxAtStartPar
\# Lock released here!
item = data.pop(0)  \# Another thread might have removed it!

\end{description}

\end{description}


\section{{[}{[}OK{]}{]} CORRECT \sphinxhyphen{} Queue makes check and pop atomic}
\label{\detokenize{cpu-concurrency/queue_explained:ok-correct-queue-makes-check-and-pop-atomic}}
\sphinxAtStartPar
item = q.get()  \# Entire operation is atomic

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Practical Example: Work Queue}
\label{\detokenize{cpu-concurrency/queue_explained:practical-example-work-queue}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue
from threading import Thread
import time

\sphinxAtStartPar
task\_queue = Queue()
results = {[}{]}
\begin{description}
\sphinxlineitem{def worker(worker\_id):}\begin{description}
\sphinxlineitem{while True:}
\sphinxAtStartPar
task = task\_queue.get()  \# Blocks if no tasks
\begin{description}
\sphinxlineitem{if task is None:  \# Poison pill (signal to exit)}
\sphinxAtStartPar
break

\end{description}

\sphinxAtStartPar
\# Do work
print(f”Worker \{worker\_id\} processing \{task\}”)
time.sleep(1)
results.append(f”Completed \{task\}”)

\sphinxAtStartPar
task\_queue.task\_done()

\end{description}

\end{description}


\section{Create workers}
\label{\detokenize{cpu-concurrency/queue_explained:create-workers}}
\sphinxAtStartPar
workers = {[}Thread(target=worker, args=(i,)) for i in range(3){]}
for w in workers:
\begin{quote}

\sphinxAtStartPar
w.start()
\end{quote}


\section{Add tasks}
\label{\detokenize{cpu-concurrency/queue_explained:add-tasks}}\begin{description}
\sphinxlineitem{for i in range(10):}
\sphinxAtStartPar
task\_queue.put(f”Task \{i\}”)

\end{description}


\section{Wait for all tasks to be processed}
\label{\detokenize{cpu-concurrency/queue_explained:wait-for-all-tasks-to-be-processed}}
\sphinxAtStartPar
task\_queue.join()


\section{Stop workers}
\label{\detokenize{cpu-concurrency/queue_explained:stop-workers}}\begin{description}
\sphinxlineitem{for * in range(3):}
\sphinxAtStartPar
task\_queue.put(None)  \# Poison pill

\sphinxlineitem{for w in workers:}
\sphinxAtStartPar
w.join()

\end{description}

\sphinxAtStartPar
print(f”All done! Results: \{results\}”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\end{sphinxVerbatim}


\subsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/queue_explained:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Queue = Thread\sphinxhyphen{}Safe Data Structure}
\sphinxhyphen{} Built\sphinxhyphen{}in locks and condition variables
\sphinxhyphen{} No manual synchronization needed

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Automatic Locking}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{put()}} and \sphinxcode{\sphinxupquote{get()}} handle all locking
\sphinxhyphen{} Prevents race conditions automatically

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Efficient Blocking}
\sphinxhyphen{} No busy\sphinxhyphen{}waiting
\sphinxhyphen{} Threads sleep efficiently until data available

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Task Tracking}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{task\_done()}} marks completion
\sphinxhyphen{} \sphinxcode{\sphinxupquote{join()}} waits for all tasks

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Best Practice for Producer\sphinxhyphen{}Consumer}
\sphinxhyphen{} Cleaner than manual locks
\sphinxhyphen{} Less error\sphinxhyphen{}prone
\sphinxhyphen{} Better performance

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multiple Threads}
\sphinxhyphen{} Works safely with many producers/consumers
\sphinxhyphen{} FIFO ordering guaranteed
\sphinxhyphen{} All synchronization automatic

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{In the Code}
\sphinxhyphen{} Producer calls \sphinxcode{\sphinxupquote{put()}} to add items
\sphinxhyphen{} Consumers call \sphinxcode{\sphinxupquote{get()}} to remove items
\sphinxhyphen{} Queue handles all locking internally
\sphinxhyphen{} \sphinxcode{\sphinxupquote{task\_done()}} signals completion
\sphinxhyphen{} All thread\sphinxhyphen{}safe with zero manual locking!

\end{enumerate}

\sphinxstepscope


\section{Queue Internal Mechanics: Condition Variables \& Events}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:queue-internal-mechanics-condition-variables-events}}\label{\detokenize{cpu-concurrency/queue_internal_mechanics::doc}}

\subsection{The Answer}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:the-answer}}
\sphinxAtStartPar
\sphinxstylestrong{No, the item itself doesn’t get a condition/event with it.}

\sphinxAtStartPar
Instead, the \sphinxstylestrong{Queue object maintains condition variables} that signal \sphinxstylestrong{all waiting threads} when items are added or removed.

\sphinxAtStartPar
—


\subsection{Queue’s Internal Structure}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:queue-s-internal-structure}}
\sphinxAtStartPar
When you create a Queue, here’s what’s inside:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue

\sphinxAtStartPar
q = Queue()


\section{Inside the Queue object:}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:inside-the-queue-object}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{}
\sphinxtoprule
\sphinxtableatstartofbodyhook\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\section{{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:id1}}
\begin{DUlineblock}{0em}
\item[] mutex: Lock()                           |
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] {\color{red}\bfseries{}|}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] \sphinxhyphen{} Signals when items added            |
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] {\color{red}\bfseries{}|}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] \sphinxhyphen{} Signals when items removed          |
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] {\color{red}\bfseries{}|}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] \sphinxhyphen{} Actual items (without conditions)   |
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] task\_counter: 0                         |
\end{DUlineblock}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{}
\sphinxtoprule
\sphinxtableatstartofbodyhook\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{When you put an item}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:when-you-put-an-item}}
\sphinxAtStartPar
q.put(42)


\section{Inside queue:}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:inside-queue}}

\section{items = {[}42{]}  \textless{}\sphinxhyphen{} Just the number, no condition attached}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:items-42-just-the-number-no-condition-attached}}

\section{not\_empty condition wakes up consumers}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:not-empty-condition-wakes-up-consumers}}

\section{not\_full condition might wake up producers}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:not-full-condition-might-wake-up-producers}}
\sphinxAtStartPar
The \sphinxstylestrong{item} is just data. The \sphinxstylestrong{conditions} belong to the \sphinxstylestrong{Queue}, not the item.

\sphinxAtStartPar
—


\subsection{How Conditions Work in Queue}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:how-conditions-work-in-queue}}

\subsubsection{Condition Variables Explained}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:condition-variables-explained}}
\sphinxAtStartPar
A \sphinxstylestrong{Condition variable} is like a \sphinxstylestrong{notification system}:
\sphinxhyphen{} Threads can \sphinxstylestrong{wait} on a condition
\sphinxhyphen{} Threads can \sphinxstylestrong{notify} waiting threads

\sphinxAtStartPar
Condition Variable (not\_empty):
+——————————\textendash{}+
| Waiting Threads Queue:         |
| {[}Consumer1{]} {[}Consumer2{]} {[}C3{]}   | \textless{}\sphinxhyphen{} Sleeping, waiting for items
+——————————\textendash{}+
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\item[] when put() is called
\item[] notify() is called
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}

\sphinxAtStartPar
One waiting thread wakes up!
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{put() \sphinxhyphen{} What Actually Happens}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:put-what-actually-happens}}

\subsubsection{Step\sphinxhyphen{}by\sphinxhyphen{}Step Execution}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:step-by-step-execution}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
q.put(item)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Internal} \PYG{n}{Queue} \PYG{n}{code}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def put(self, item):}
\sphinxAtStartPar
\# 1. Acquire the mutex (lock)
with self.mutex:
\begin{quote}

\sphinxAtStartPar
\# 2. Wait if queue is full
while len(self.items) \textgreater{}= self.maxsize:
\begin{quote}

\sphinxAtStartPar
self.not\_full.wait(self.mutex)
\# Thread blocks here, waiting for space
\# Releases mutex while waiting
\end{quote}

\sphinxAtStartPar
\# 3. Queue has space now, add item
self.items.append(item)
\# items = {[}42{]}  \textless{}\sphinxhyphen{} Item added (no condition on item)

\sphinxAtStartPar
\# 4. Notify ONE consumer that item is available
self.not\_empty.notify()
\# Wakes one consumer waiting on not\_empty
\end{quote}

\sphinxAtStartPar
\# 5. Mutex released here (with block ends)

\end{description}


\subsubsection{Timeline: put() with Condition Variable}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:timeline-put-with-condition-variable}}

\subsection{Time  Queue                    Condition Variables    Consumers}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:time-queue-condition-variables-consumers}}\begin{description}
\sphinxlineitem{0     put(42)                                         waiting…}
\sphinxAtStartPar
acquire mutex

\sphinxlineitem{1                              not\_empty signal       WAKE UP!}
\sphinxAtStartPar
items.append(42)

\end{description}

\sphinxAtStartPar
2     notify()
\begin{description}
\sphinxlineitem{3                                                     get() continues}
\sphinxAtStartPar
release mutex

\end{description}

\sphinxAtStartPar
4     (done)
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{get() \sphinxhyphen{} What Actually Happens}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:get-what-actually-happens}}

\subsubsection{Step\sphinxhyphen{}by\sphinxhyphen{}Step Execution}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:id8}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
item = q.get()

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Internal} \PYG{n}{Queue} \PYG{n}{code}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def get(self):}
\sphinxAtStartPar
\# 1. Acquire the mutex (lock)
with self.mutex:
\begin{quote}

\sphinxAtStartPar
\# 2. Wait if queue is empty
while len(self.items) == 0:
\begin{quote}

\sphinxAtStartPar
self.not\_empty.wait(self.mutex)
\# Thread blocks here, waiting for items
\# Releases mutex while waiting
\# When notified, reacquires mutex and continues
\end{quote}

\sphinxAtStartPar
\# 3. Queue has items, remove one
item = self.items.pop(0)
\# Just the value, no condition with it

\sphinxAtStartPar
\# 4. Notify ONE producer that space is available
self.not\_full.notify()
\# Wakes one producer waiting on not\_full

\sphinxAtStartPar
\# 5. Return the item (just the value)
return item
\end{quote}

\sphinxAtStartPar
\# 6. Mutex released here (with block ends)

\end{description}


\subsubsection{Timeline: get() with Condition Variable}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:timeline-get-with-condition-variable}}

\subsection{Time  Queue                    Condition Variables    Producers}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:time-queue-condition-variables-producers}}\begin{description}
\sphinxlineitem{0     get()                                           put() blocked}
\sphinxAtStartPar
acquire mutex                                   (queue full)

\sphinxlineitem{1     items = {[}1,2,3{]}}
\sphinxAtStartPar
items.pop(0) \sphinxhyphen{}\textgreater{} returns 1  not\_full signal       WAKE UP!

\end{description}

\sphinxAtStartPar
2     notify()
\begin{description}
\sphinxlineitem{3     release mutex                                   put() continues}
\sphinxAtStartPar
return 1

\end{description}

\sphinxAtStartPar
4     (done)
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Visual: No Condition On Items}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:visual-no-condition-on-items}}
\sphinxAtStartPar
Queue Structure:
\begin{description}
\sphinxlineitem{KEY: The conditions are properties of the QUEUE,}
\sphinxAtStartPar
not attached to individual items!

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Complete put() and get() Timeline}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:complete-put-and-get-timeline}}

\subsubsection{Scenario: Queue with maxsize=2, multiple producers/consumers}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:scenario-queue-with-maxsize-2-multiple-producers-consumers}}
\sphinxAtStartPar
Initial:
Queue: {[}{]}
not\_empty: {[}Consumer1, Consumer2{]}  \textless{}\sphinxhyphen{} Waiting for items
not\_full: {[}{]}                       \textless{}\sphinxhyphen{} No one waiting

\sphinxAtStartPar
Time 0: Producer1.put(42)
{\color{red}\bfseries{}|}\sphinxhyphen{} Acquire mutex
{\color{red}\bfseries{}|}\sphinxhyphen{} Queue not full, add item
{\color{red}\bfseries{}|}\sphinxhyphen{} items = {[}42{]}
{\color{red}\bfseries{}|}\sphinxhyphen{} Call not\_empty.notify()
|  +\sphinxhyphen{} Consumer1 wakes up!
{\color{red}\bfseries{}|}\sphinxhyphen{} Release mutex
+\sphinxhyphen{} (put done)

\sphinxAtStartPar
Queue: {[}42{]}
not\_empty: {[}Consumer2{]}  \textless{}\sphinxhyphen{} Consumer1 woke up
not\_full: {[}{]}

\sphinxAtStartPar
Time 1: Producer2.put(7)
{\color{red}\bfseries{}|}\sphinxhyphen{} Acquire mutex
{\color{red}\bfseries{}|}\sphinxhyphen{} Queue not full, add item
{\color{red}\bfseries{}|}\sphinxhyphen{} items = {[}42, 7{]}
{\color{red}\bfseries{}|}\sphinxhyphen{} Call not\_empty.notify()
|  +\sphinxhyphen{} Consumer2 wakes up!
{\color{red}\bfseries{}|}\sphinxhyphen{} Release mutex
+\sphinxhyphen{} (put done)

\sphinxAtStartPar
Queue: {[}42, 7{]}  \textless{}\sphinxhyphen{} FULL (maxsize=2)
not\_empty: {[}{]}   \textless{}\sphinxhyphen{} All consumers woke
not\_full: {[}{]}

\sphinxAtStartPar
Time 2: Producer3.put(199)
{\color{red}\bfseries{}|}\sphinxhyphen{} Acquire mutex
{\color{red}\bfseries{}|}\sphinxhyphen{} Queue IS FULL! len(items)=2 \textgreater{}= maxsize=2
{\color{red}\bfseries{}|}\sphinxhyphen{} Call not\_full.wait(mutex)
|  +\sphinxhyphen{} Producer3 BLOCKS here
|  +\sphinxhyphen{} Releases mutex
+\sphinxhyphen{} (waiting for space)

\sphinxAtStartPar
Queue: {[}42, 7{]}
not\_empty: {[}{]}
not\_full: {[}Producer3{]}  \textless{}\sphinxhyphen{} Waiting for space

\sphinxAtStartPar
Time 3: Consumer1.get()
{\color{red}\bfseries{}|}\sphinxhyphen{} Acquire mutex
{\color{red}\bfseries{}|}\sphinxhyphen{} items = {[}42, 7{]}, not empty
{\color{red}\bfseries{}|}\sphinxhyphen{} item = items.pop(0) = 42  \textless{}\sphinxhyphen{} Returns just 42, no condition
{\color{red}\bfseries{}|}\sphinxhyphen{} Call not\_full.notify()
|  +\sphinxhyphen{} Producer3 wakes up!
{\color{red}\bfseries{}|}\sphinxhyphen{} Release mutex
+\sphinxhyphen{} Return 42

\sphinxAtStartPar
Queue: {[}7{]}
not\_empty: {[}{]}
not\_full: {[}{]}  \textless{}\sphinxhyphen{} Producer3 woke up

\sphinxAtStartPar
Time 4: Producer3 continues from where it blocked
{\color{red}\bfseries{}|}\sphinxhyphen{} Acquire mutex (had released it, now reacquires)
{\color{red}\bfseries{}|}\sphinxhyphen{} Queue not full anymore, add item
{\color{red}\bfseries{}|}\sphinxhyphen{} items = {[}7, 199{]}
{\color{red}\bfseries{}|}\sphinxhyphen{} Call not\_empty.notify() (no one waiting)
{\color{red}\bfseries{}|}\sphinxhyphen{} Release mutex
+\sphinxhyphen{} (put done)

\sphinxAtStartPar
Queue: {[}7, 199{]}  \textless{}\sphinxhyphen{} FULL again
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Key Insight: Conditions Are On Queue, Not Items}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:key-insight-conditions-are-on-queue-not-items}}

\subsubsection{What People Might Think (WRONG):}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:what-people-might-think-wrong}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{{[}{[}FAIL{]}{]} WRONG MENTAL MODEL}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:fail-wrong-mental-model}}
\sphinxAtStartPar
q.put(42)  \# Does the item 42 carry an event/condition with it?
item = q.get()  \# Does the returned item have a condition?

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{{[}OK{]} CORRECT MENTAL MODEL}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:ok-correct-mental-model}}\begin{description}
\sphinxlineitem{q.put(42)  \# 42 goes into queue}
\sphinxAtStartPar
\# Queue’s not\_empty condition is signaled
\# The number 42 itself has NO condition

\sphinxlineitem{item = q.get()  \# 42 is returned as plain data}
\sphinxAtStartPar
\# Queue’s not\_full condition is signaled
\# The number 42 returned has NO condition

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Why Queue Uses Shared Conditions}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:why-queue-uses-shared-conditions}}

\subsubsection{Instead of Per\sphinxhyphen{}Item Events}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:instead-of-per-item-events}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{{[}{[}FAIL{]}{]} This would be inefficient (if items had events)}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:fail-this-would-be-inefficient-if-items-had-events}}\begin{description}
\sphinxlineitem{class BadQueue:}\begin{description}
\sphinxlineitem{def put(self, item):}
\sphinxAtStartPar
event = Event()
self.items.append((item, event))
\# Every item carries its own event \sphinxhyphen{} wastes memory!

\sphinxlineitem{def get(self):}
\sphinxAtStartPar
item, event = self.items.pop(0)
\# What do we do with the event?
\# Consumer doesn’t care about one item’s event
return item

\end{description}

\end{description}


\section{{[}OK{]} This is efficient (what Queue actually does)}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:ok-this-is-efficient-what-queue-actually-does}}\begin{description}
\sphinxlineitem{class GoodQueue:}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self):}
\sphinxAtStartPar
self.items = {[}{]}
self.not\_empty = Condition()  \# Single condition for ALL items

\sphinxlineitem{def put(self, item):}
\sphinxAtStartPar
self.items.append(item)  \# Item is just data
self.not\_empty.notify()  \# Signal all waiting consumers

\sphinxlineitem{def get(self):}
\sphinxAtStartPar
item = self.items.pop(0)  \# Item is just data
return item

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{How Conditions Work: The Mechanism}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:how-conditions-work-the-mechanism}}

\subsubsection{not\_empty.notify() \sphinxhyphen{} What It Does}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:not-empty-notify-what-it-does}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{When put() is called}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:when-put-is-called}}
\sphinxAtStartPar
q.put(42)


\section{Inside put():}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:inside-put}}
\sphinxAtStartPar
self.not\_empty.notify()


\section{This tells the condition variable:}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:this-tells-the-condition-variable}}

\section{“Someone just added an item”}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:someone-just-added-an-item}}
\sphinxAtStartPar
\#
The condition variable wakes ONE waiting thread
===============================================
(the one sleeping on not\_empty.wait())
======================================
\#
That thread checks: “Is there data now?”
========================================
If yes, it continues
====================
If no, it goes back to sleep
============================

\sphinxAtStartPar
BEFORE put():
+————————\sphinxhyphen{}+
| not\_empty Condition     |
| Waiting threads:        |
| {[}Consumer1{]}             | \textless{}\sphinxhyphen{} Sleeping, waiting for items
| {[}Consumer2{]}             |
| {[}Consumer3{]}             |
| Items: {[}{]}               |
+————————\sphinxhyphen{}+

\sphinxAtStartPar
put(42) called:
|
{\color{red}\bfseries{}|}\sphinxhyphen{} items.append(42)
|
{\color{red}\bfseries{}|}\sphinxhyphen{} not\_empty.notify()
|  +\sphinxhyphen{} Sends signal!
|
+\sphinxhyphen{} (mutex released)

\sphinxAtStartPar
AFTER notify():
+————————\sphinxhyphen{}+
| not\_empty Condition     |
| Waiting threads:        |
| {[}Consumer1{]} \textless{}\sphinxhyphen{} WOKEN UP! |
| {[}Consumer2{]}             |
| {[}Consumer3{]}             |
| Items: {[}42{]}             |
+————————\sphinxhyphen{}+

\sphinxAtStartPar
Consumer1 wakes up:
{\color{red}\bfseries{}|}\sphinxhyphen{} Reacquires mutex
{\color{red}\bfseries{}|}\sphinxhyphen{} Checks: is items not empty? YES!
{\color{red}\bfseries{}|}\sphinxhyphen{} Proceeds to pop item
+\sphinxhyphen{} (gets the 42)
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Back to Your Original Question}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:back-to-your-original-question}}

\subsubsection{“Does each item get a condition?”}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:does-each-item-get-a-condition}}
\sphinxAtStartPar
\sphinxstylestrong{No.} Here’s why:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Items are just data} \sphinxhyphen{} 42, “hello”, {[}1,2,3{]}, etc.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Queue has conditions} \sphinxhyphen{} not\_empty, not\_full

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Conditions signal all threads} \sphinxhyphen{} not individual items

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{More efficient} \sphinxhyphen{} One pair of conditions for all items

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Correct semantics} \sphinxhyphen{} Consumer doesn’t care which item, just needs one

\end{enumerate}


\subsubsection{The Flow}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:the-flow}}\begin{description}
\sphinxlineitem{put(42)}
\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

\sphinxAtStartPar
+\sphinxhyphen{} Add 42 to items list
+\sphinxhyphen{} Signal not\_empty
\begin{quote}

\sphinxAtStartPar
+\sphinxhyphen{} Wakes one consumer
\end{quote}

\sphinxlineitem{get()}
\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

\sphinxAtStartPar
+\sphinxhyphen{} Wait on not\_empty (if empty)
+\sphinxhyphen{} Remove 42 from items
+\sphinxhyphen{} Return 42 (just the value, no condition)
+\sphinxhyphen{} Signal not\_full
\begin{quote}

\sphinxAtStartPar
+\sphinxhyphen{} Wakes one producer
\end{quote}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Real Code Example}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:real-code-example}}

\subsubsection{What Queue.put() Actually Does}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:what-queue-put-actually-does}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from threading import Lock, Condition
\begin{description}
\sphinxlineitem{class MyQueue:}\begin{description}
\sphinxlineitem{def \_\_init**(self, maxsize=0):}
\sphinxAtStartPar
self.mutex = Lock()
self.not\_empty = Condition(self.mutex)
self.not\_full = Condition(self.mutex)
self.items = {[}{]}
self.maxsize = maxsize
self.unfinished\_tasks = 0

\sphinxlineitem{def put(self, item):}\begin{description}
\sphinxlineitem{with self.not\_full:  \# Acquire lock via condition}
\sphinxAtStartPar
\# Wait if full
while len(self.items) \textgreater{}= self.maxsize and self.maxsize:
\begin{quote}

\sphinxAtStartPar
self.not\_full.wait()
\end{quote}

\sphinxAtStartPar
\# Add item (just the value, NO condition)
self.items.append(item)
self.unfinished\_tasks += 1

\sphinxAtStartPar
\# Notify one consumer
self.not\_empty.notify()

\end{description}

\sphinxlineitem{def get(self):}\begin{description}
\sphinxlineitem{with self.not\_empty:  \# Acquire lock via condition}
\sphinxAtStartPar
\# Wait if empty
while not self.items:
\begin{quote}

\sphinxAtStartPar
self.not\_empty.wait()
\end{quote}

\sphinxAtStartPar
\# Remove and return (just the value)
item = self.items.pop(0)

\sphinxAtStartPar
\# Notify one producer
self.not\_full.notify()

\sphinxAtStartPar
return item

\end{description}

\sphinxlineitem{def task\_done(self):}\begin{description}
\sphinxlineitem{with self.mutex:}
\sphinxAtStartPar
self.unfinished\_tasks \sphinxhyphen{}= 1
if self.unfinished\_tasks == 0:
\begin{quote}

\sphinxAtStartPar
\# All tasks done, could notify here
pass
\end{quote}

\end{description}

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Summary}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:summary}}
\begin{DUlineblock}{0em}
\item[] Question | Answer |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——\textendash{}|
| \sphinxstylestrong{Do items have conditions?} | No, items are just data |
| \sphinxstylestrong{Where are conditions?} | In the Queue object itself |
| \sphinxstylestrong{How many conditions?} | Two: not\_empty and not\_full |
| \sphinxstylestrong{What do conditions do?} | Signal all waiting threads when state changes |
| \sphinxstylestrong{Is condition per\sphinxhyphen{}item?} | No, shared for all items |
| \sphinxstylestrong{When does notification happen?} | Every put() and get() |
| \sphinxstylestrong{Who receives notification?} | One waiting thread (if any) |

\sphinxAtStartPar
—


\subsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/queue_internal_mechanics:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Items are just data} \sphinxhyphen{} No conditions attached

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Queue owns the conditions} \sphinxhyphen{} not\_empty and not\_full

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Conditions are shared} \sphinxhyphen{} Signal all consumers/producers

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Efficient design} \sphinxhyphen{} One pair of conditions for unlimited items

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{put() signals not\_empty} \sphinxhyphen{} “Item added, consumers can proceed”

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{get() signals not\_full} \sphinxhyphen{} “Space freed, producers can proceed”

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Thread\sphinxhyphen{}safe} \sphinxhyphen{} Mutex protects all modifications

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{No per\sphinxhyphen{}item overhead} \sphinxhyphen{} Conditions don’t travel with items

\end{enumerate}

\sphinxAtStartPar
The magic is that Queue uses \sphinxstylestrong{shared conditions} at the Queue level, not individual conditions per item. This is more efficient and achieves perfect synchronization!

\sphinxstepscope


\section{task\_done() and Queue Explained: Counter \& Condition Variables}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:task-done-and-queue-explained-counter-condition-variables}}\label{\detokenize{cpu-concurrency/task_done_queue_explained::doc}}

\subsection{Quick Answer}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:quick-answer}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{task\_done()}} is used to \sphinxstylestrong{track when you’ve finished processing an item}, so that \sphinxcode{\sphinxupquote{queue.join()}} knows when ALL items have been processed.

\sphinxAtStartPar
\sphinxstylestrong{Yes, \textasciigrave{}\textasciigrave{}task\_done()\textasciigrave{}\textasciigrave{} is on the Queue and does TWO things:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Decrements the task counter} \sphinxhyphen{} Tracks unfinished tasks

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Notifies the join() condition} \sphinxhyphen{} Wakes up \sphinxcode{\sphinxupquote{queue.join()}} if counter reaches 0

\end{enumerate}

\sphinxAtStartPar
—


\subsection{The Problem Without task\_done()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:the-problem-without-task-done}}

\subsubsection{Without task\_done() \sphinxhyphen{} Can’t Track Completion}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:without-task-done-can-t-track-completion}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue
from threading import Thread
import time

\sphinxAtStartPar
queue = Queue()
\begin{description}
\sphinxlineitem{def producer():}\begin{description}
\sphinxlineitem{for i in range(3):}
\sphinxAtStartPar
queue.put(f”Task \{i\}”)
print(f”Produced Task \{i\}”)

\end{description}

\sphinxlineitem{def consumer():}\begin{description}
\sphinxlineitem{for i in range(3):}
\sphinxAtStartPar
item = queue.get()
print(f”Processing \{item\}”)
time.sleep(1)  \# Simulate work
print(f”Finished \{item\}”)
\# NO task\_done() here!

\end{description}

\end{description}

\sphinxAtStartPar
producer\_thread = Thread(target=producer)
consumer\_thread = Thread(target=consumer)

\sphinxAtStartPar
producer\_thread.start()
consumer\_thread.start()

\sphinxAtStartPar
producer\_thread.join()
consumer\_thread.join()


\section{How do we know if ALL items have been processed?}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:how-do-we-know-if-all-items-have-been-processed}}

\section{WE DON’T! Consumer might still be working!}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:we-don-t-consumer-might-still-be-working}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue
from threading import Thread
import time

\sphinxAtStartPar
queue = Queue()
\begin{description}
\sphinxlineitem{def producer():}\begin{description}
\sphinxlineitem{for i in range(3):}
\sphinxAtStartPar
queue.put(f”Task \{i\}”)
print(f”Produced Task \{i\}”)

\end{description}

\sphinxlineitem{def consumer():}\begin{description}
\sphinxlineitem{for i in range(3):}
\sphinxAtStartPar
item = queue.get()
print(f”Processing \{item\}”)
time.sleep(1)  \# Simulate work
print(f”Finished \{item\}”)
queue.task\_done()  \# Tell queue we’re done!

\end{description}

\end{description}

\sphinxAtStartPar
producer\_thread = Thread(target=producer)
consumer\_thread = Thread(target=consumer)

\sphinxAtStartPar
producer\_thread.start()
consumer\_thread.start()

\sphinxAtStartPar
producer\_thread.join()
consumer\_thread.join()

\sphinxAtStartPar
queue.join()  \# NOW we can safely wait for all items to be processed!
print(“All tasks complete!”)
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{How task\_done() Works}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:how-task-done-works}}

\subsubsection{Internal Counter System}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:internal-counter-system}}
\sphinxAtStartPar
Queue maintains an \sphinxstylestrong{internal task counter}:

\sphinxAtStartPar
Initial state: task\_counter = 0

\sphinxAtStartPar
put(item1)    \sphinxhyphen{}\textgreater{} task\_counter = 1
put(item2)    \sphinxhyphen{}\textgreater{} task\_counter = 2
put(item3)    \sphinxhyphen{}\textgreater{} task\_counter = 3

\sphinxAtStartPar
get()         \sphinxhyphen{}\textgreater{} Returns item1 (counter still 3)
task\_done()   \sphinxhyphen{}\textgreater{} task\_counter = 2 (decrement)

\sphinxAtStartPar
get()         \sphinxhyphen{}\textgreater{} Returns item2 (counter still 2)
task\_done()   \sphinxhyphen{}\textgreater{} task\_counter = 1 (decrement)

\sphinxAtStartPar
get()         \sphinxhyphen{}\textgreater{} Returns item3 (counter still 1)
task\_done()   \sphinxhyphen{}\textgreater{} task\_counter = 0 (decrement)

\sphinxAtStartPar
join()        \sphinxhyphen{}\textgreater{} Returns immediately (counter = 0)


\subsubsection{Visual Timeline}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:visual-timeline}}

\subsection{Producer         Queue           Consumer          Main}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:producer-queue-consumer-main}}\begin{description}
\sphinxlineitem{put(1)          {[}1{]}}
\sphinxAtStartPar
counter=1
\begin{quote}

\sphinxAtStartPar
get() \sphinxhyphen{}\textgreater{} gets 1
counter=1 (unchanged)
processing…
task\_done()
counter=0
\begin{quote}

\sphinxAtStartPar
join()
BLOCKED
(counter=0)
CONTINUES!
\end{quote}
\end{quote}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{What task\_done() Actually Does}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:what-task-done-actually-does}}

\subsubsection{Code (Simplified)}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:code-simplified}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def task\_done(self):}\begin{description}
\sphinxlineitem{with self.mutex:}\begin{description}
\sphinxlineitem{if self.unfinished\_tasks \textless{}= 0:}
\sphinxAtStartPar
raise ValueError(‘task\_done() called too many times’)

\end{description}

\sphinxAtStartPar
self.unfinished\_tasks \sphinxhyphen{}= 1  \# Decrement counter
\begin{description}
\sphinxlineitem{if self.unfinished\_tasks == 0:}
\sphinxAtStartPar
self.all\_tasks*done.notify\_all()  \# Wake up join()

\end{description}

\end{description}

\end{description}


\subsubsection{Two Operations}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:two-operations}}
\sphinxAtStartPar
task\_done() does:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Decrement counter
+————\textendash{}+
| counter: 3   |
+——+——\sphinxhyphen{}+
\begin{quote}

\begin{DUlineblock}{0em}
\item[] task\_done()
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}

\item {} 
\sphinxAtStartPar
Check if counter = 0, then notify all\_tasks*done
+————\textendash{}+
| counter: 1   |
+——+——\sphinxhyphen{}+
\begin{quote}

\begin{DUlineblock}{0em}
\item[] task\_done()
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{The Third Condition Variable: all\_tasks*done}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:the-third-condition-variable-all-tasks-done}}
\sphinxAtStartPar
Queue has \sphinxstylestrong{THREE} condition variables:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{class Queue:}\begin{description}
\sphinxlineitem{def \_\_init**(self):}
\sphinxAtStartPar
self.mutex = Lock()
self.not\_empty = Condition()      \# put() signals, get() waits
self.not\_full = Condition()       \# get() signals, put() waits
self.all\_tasks*done = Condition() \# task\_done() signals, join() waits
self.items = {[}{]}
self.unfinished\_tasks = 0

\end{description}

\end{description}


\subsubsection{Timeline: All Three Conditions}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:timeline-all-three-conditions}}
\sphinxAtStartPar
Queue Object:
+————————————\sphinxhyphen{}+
| Condition: not\_empty                |
| {\color{red}\bfseries{}|}\sphinxhyphen{} Signaled by: put()              |
| {\color{red}\bfseries{}|}\sphinxhyphen{} Waited on by: get()             |
| +\sphinxhyphen{} Purpose: “Data available”       |
|                                     |
| Condition: not\_full                 |
| {\color{red}\bfseries{}|}\sphinxhyphen{} Signaled by: get()              |
| {\color{red}\bfseries{}|}\sphinxhyphen{} Waited on by: put()             |
| +\sphinxhyphen{} Purpose: “Space available”      |
|                                     |
| Condition: all\_tasks*done           |
| {\color{red}\bfseries{}|}\sphinxhyphen{} Signaled by: task\_done()        | \textless{}\sphinxhyphen{} YOUR QUESTION!
| {\color{red}\bfseries{}|}\sphinxhyphen{} Waited on by: join()            |
| +\sphinxhyphen{} Purpose: “All work processed”   |
|                                     |
| Counter: unfinished\_tasks           |
| {\color{red}\bfseries{}|}\sphinxhyphen{} Incremented by: put()           |
| {\color{red}\bfseries{}|}\sphinxhyphen{} Decremented by: task\_done()     |
| +\sphinxhyphen{} Checked by: join()              |
+————————————\sphinxhyphen{}+
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Complete Picture: All Operations}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:complete-picture-all-operations}}

\subsubsection{put()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:put}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def put(self, item):}\begin{description}
\sphinxlineitem{with self.not\_full:}\begin{description}
\sphinxlineitem{while len(self.items) \textgreater{}= self.maxsize:}
\sphinxAtStartPar
self.not\_full.wait()

\end{description}

\sphinxAtStartPar
self.items.append(item)
self.unfinished\_tasks += 1  \# \textless{}\sphinxhyphen{} Increment here!
self.not\_empty.notify()

\end{description}

\end{description}


\subsubsection{get()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:get}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def get(self):}\begin{description}
\sphinxlineitem{with self.not\_empty:}\begin{description}
\sphinxlineitem{while not self.items:}
\sphinxAtStartPar
self.not\_empty.wait()

\end{description}

\sphinxAtStartPar
item = self.items.pop(0)
self.not\_full.notify()

\sphinxAtStartPar
return item  \# \textless{}\sphinxhyphen{} Returns just the item

\end{description}

\end{description}


\subsubsection{task\_done()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:task-done}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def task\_done(self):}\begin{description}
\sphinxlineitem{with self.mutex:}
\sphinxAtStartPar
self.unfinished\_tasks \sphinxhyphen{}= 1  \# \textless{}\sphinxhyphen{} Decrement here!
\begin{description}
\sphinxlineitem{if self.unfinished\_tasks == 0:}
\sphinxAtStartPar
self.all\_tasks*done.notify\_all()  \# \textless{}\sphinxhyphen{} Notify join()!

\end{description}

\end{description}

\end{description}


\subsubsection{join()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:join}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def join(self):}\begin{description}
\sphinxlineitem{with self.all\_tasks*done:}\begin{description}
\sphinxlineitem{while self.unfinished\_tasks:}
\sphinxAtStartPar
self.all\_tasks*done.wait()  \# \textless{}\sphinxhyphen{} Waits here!

\end{description}

\sphinxAtStartPar
\# When counter reaches 0, task\_done() wakes this up

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{The Complete Flow with All Three Conditions}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:the-complete-flow-with-all-three-conditions}}

\subsubsection{Scenario: 1 Producer, 1 Consumer}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:scenario-1-producer-1-consumer}}

\subsection{Time  Producer        get()           task\_done()      Consumer Waits}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:time-producer-get-task-done-consumer-waits}}\begin{description}
\sphinxlineitem{0     put(1)}
\sphinxAtStartPar
{\color{red}\bfseries{}|}\sphinxhyphen{} items.append(1)
{\color{red}\bfseries{}|}\sphinxhyphen{} unfinished=1
+\sphinxhyphen{} not\_empty.notify()
\begin{quote}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}

\sphinxAtStartPar
get() wakes up
|
{\color{red}\bfseries{}|}\sphinxhyphen{} items.pop(0)
+\sphinxhyphen{} not\_full.notify()
\end{quote}

\end{description}

\sphinxAtStartPar
1                                    Processing item…
\begin{description}
\sphinxlineitem{2                                    task\_done()}
\sphinxAtStartPar
{\color{red}\bfseries{}|}\sphinxhyphen{} unfinished=0
+\sphinxhyphen{} all\_tasks*done.notify\_all()
\begin{quote}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}

\sphinxAtStartPar
join() wakes up!
Returns!
\end{quote}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Detailed Breakdown: task\_done() with Counter}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:detailed-breakdown-task-done-with-counter}}

\subsubsection{Step 1: put() \sphinxhyphen{} Increment Counter}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:step-1-put-increment-counter}}
\sphinxAtStartPar
Initial state:
+———————\sphinxhyphen{}+
| unfinished\_tasks: 0  |
| items: {[}{]}            |
+———————\sphinxhyphen{}+

\sphinxAtStartPar
put(1) called:
|
{\color{red}\bfseries{}|}\sphinxhyphen{} Acquire mutex
{\color{red}\bfseries{}|}\sphinxhyphen{} items.append(1)
{\color{red}\bfseries{}|}\sphinxhyphen{} unfinished\_tasks += 1  \textless{}\sphinxhyphen{} Counter incremented!
|
{\color{red}\bfseries{}|}\sphinxhyphen{} State now:
|  +———————\sphinxhyphen{}+
|  | unfinished\_tasks: 1  | \textless{}\sphinxhyphen{} Task added
|  | items: {[}1{]}           |
|  +———————\sphinxhyphen{}+
|
{\color{red}\bfseries{}|}\sphinxhyphen{} not\_empty.notify()
+\sphinxhyphen{} Release mutex
.. code\sphinxhyphen{}block:: text


\subsubsection{Step 2: get() \sphinxhyphen{} Item Removed, Counter Unchanged}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:step-2-get-item-removed-counter-unchanged}}
\sphinxAtStartPar
Before get():
+———————\sphinxhyphen{}+
| unfinished\_tasks: 1  | \textless{}\sphinxhyphen{} Still 1!
| items: {[}1{]}           |
+———————\sphinxhyphen{}+

\sphinxAtStartPar
get() called:
|
{\color{red}\bfseries{}|}\sphinxhyphen{} Acquire mutex
{\color{red}\bfseries{}|}\sphinxhyphen{} while not items:
|   +\sphinxhyphen{} not\_empty.wait()  (not called, has items)
|
{\color{red}\bfseries{}|}\sphinxhyphen{} item = items.pop(0)  \textless{}\sphinxhyphen{} Get the item
|
{\color{red}\bfseries{}|}\sphinxhyphen{} State now:
|  +———————\sphinxhyphen{}+
|  | unfinished\_tasks: 1  | \textless{}\sphinxhyphen{} STILL 1!
|  | items: {[}{]}            |   (not decremented by get!)
|  +———————\sphinxhyphen{}+
|
{\color{red}\bfseries{}|}\sphinxhyphen{} not\_full.notify()
+\sphinxhyphen{} Release mutex \& return 1
.. code\sphinxhyphen{}block:: text


\subsubsection{Step 3: task\_done() \sphinxhyphen{} Decrement Counter}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:step-3-task-done-decrement-counter}}
\sphinxAtStartPar
Before task\_done():
+———————\sphinxhyphen{}+
| unfinished\_tasks: 1  |
| items: {[}{]}            |
+———————\sphinxhyphen{}+

\sphinxAtStartPar
task\_done() called:
|
{\color{red}\bfseries{}|}\sphinxhyphen{} Acquire mutex
{\color{red}\bfseries{}|}\sphinxhyphen{} if unfinished\_tasks \textless{}= 0:
|   +\sphinxhyphen{} raise ValueError (not true here)
|
{\color{red}\bfseries{}|}\sphinxhyphen{} unfinished\_tasks \sphinxhyphen{}= 1  \textless{}\sphinxhyphen{} Counter decremented!
|
{\color{red}\bfseries{}|}\sphinxhyphen{} if unfinished\_tasks == 0:
|   {\color{red}\bfseries{}|}\sphinxhyphen{} YES! (now 0)
|   +\sphinxhyphen{} all\_tasks*done.notify\_all()  \textless{}\sphinxhyphen{} Signal join()!
|
{\color{red}\bfseries{}|}\sphinxhyphen{} State now:
|  +———————\sphinxhyphen{}+
|  | unfinished\_tasks: 0  | \textless{}\sphinxhyphen{} NOW 0!
|  | items: {[}{]}            |
|  +———————\sphinxhyphen{}+
|
+\sphinxhyphen{} Release mutex
.. code\sphinxhyphen{}block:: text


\subsubsection{Step 4: join() \sphinxhyphen{} Wait, Then Return}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:step-4-join-wait-then-return}}
\sphinxAtStartPar
join() called (from main thread):

\sphinxAtStartPar
{\color{red}\bfseries{}|}\sphinxhyphen{} Acquire all\_tasks*done condition
|
{\color{red}\bfseries{}|}\sphinxhyphen{} while unfinished\_tasks != 0:
|   +\sphinxhyphen{} all\_tasks*done.wait()  (waiting…)
|       {[}Paused, waiting for task\_done(){]}
|
{[}task\_done() is called from consumer thread{]}
{[}unfinished\_tasks becomes 0{]}
{[}task\_done() calls notify\_all(){]}
|
{\color{red}\bfseries{}|}\sphinxhyphen{} Woken up! Check condition again
|
{\color{red}\bfseries{}|}\sphinxhyphen{} while unfinished\_tasks != 0:
|   +\sphinxhyphen{} False! (unfinished\_tasks = 0)
|
{\color{red}\bfseries{}|}\sphinxhyphen{} Continue past the loop
|
+\sphinxhyphen{} Return from join()
\begin{quote}

\sphinxAtStartPar
Main thread can continue!
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{The Counter Lifecycle}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:the-counter-lifecycle}}

\subsubsection{Diagram: Tracking One Task}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:diagram-tracking-one-task}}
\sphinxAtStartPar
Lifecycle of Task \#1:
\begin{description}
\sphinxlineitem{put(task1)}
\sphinxAtStartPar
down

\sphinxlineitem{unfinished\_tasks = 1}
\sphinxAtStartPar
down

\sphinxlineitem{get() \sphinxhyphen{}\textgreater{} task1}
\sphinxAtStartPar
down

\sphinxlineitem{unfinished\_tasks = 1 (unchanged)}
\sphinxAtStartPar
down

\sphinxlineitem{{[}Processing task1…{]}}
\sphinxAtStartPar
down

\sphinxlineitem{task\_done()}
\sphinxAtStartPar
down

\sphinxlineitem{unfinished\_tasks = 0 \textless{}\sphinxhyphen{} Counter decremented!}
\sphinxAtStartPar
down

\sphinxlineitem{all\_tasks*done.notify\_all()}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
join() wakes up (if counter = 0)


\subsubsection{Diagram: Multiple Tasks}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:diagram-multiple-tasks}}
\sphinxAtStartPar
Multiple tasks lifecycle:

\sphinxAtStartPar
put(t1) \sphinxhyphen{}\textgreater{} unfinished=1
put(t2) \sphinxhyphen{}\textgreater{} unfinished=2
put(t3) \sphinxhyphen{}\textgreater{} unfinished=3
\begin{description}
\sphinxlineitem{get()\sphinxhyphen{}\textgreater{}t1, get()\sphinxhyphen{}\textgreater{}t2, get()\sphinxhyphen{}\textgreater{}t3}
\sphinxAtStartPar
unfinished=3 (unchanged)

\end{description}

\sphinxAtStartPar
{[}Processing all…{]}

\sphinxAtStartPar
task\_done()  \sphinxhyphen{}\textgreater{} unfinished=2
task\_done()  \sphinxhyphen{}\textgreater{} unfinished=1
task\_done()  \sphinxhyphen{}\textgreater{} unfinished=0 \textless{}\sphinxhyphen{} Reached 0!
\begin{quote}

\sphinxAtStartPar
all\_tasks*done.notify\_all()
\end{quote}

\sphinxAtStartPar
join() was waiting…
Now wakes up! \textless{}\sphinxhyphen{} Can continue
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Key Insight: Two Data Flows}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:key-insight-two-data-flows}}
\sphinxAtStartPar
Queue has TWO independent data flows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
ITEM FLOW:
put() \sphinxhyphen{}\textgreater{} {[}items list{]} \sphinxhyphen{}\textgreater{} get()
Signal: not\_empty (consumer waits for items)
Signal: not\_full (producer waits for space)

\item {} 
\sphinxAtStartPar
COMPLETION FLOW:
put() \sphinxhyphen{}\textgreater{} {[}unfinished\_tasks counter{]} \sphinxhyphen{}\textgreater{} task\_done()
Signal: all\_tasks*done (join() waits for completion)

\end{enumerate}

\sphinxAtStartPar
These are INDEPENDENT:
\sphinxhyphen{} get() doesn’t care if task\_done() was called
\sphinxhyphen{} task\_done() doesn’t remove items from list
\sphinxhyphen{} They work in parallel
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Why BOTH Counter AND Condition Variable?}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:why-both-counter-and-condition-variable}}

\subsubsection{Counter}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:counter}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
unfinished\_tasks: int


\section{Tracks how many items haven’t been marked done yet}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:tracks-how-many-items-haven-t-been-marked-done-yet}}

\section{Used to know WHEN to signal}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:used-to-know-when-to-signal}}

\section{Checked by join()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:checked-by-join}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
all\_tasks*done: Condition()


\section{Efficiently wakes up waiting threads}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:efficiently-wakes-up-waiting-threads}}

\section{Avoids busy\sphinxhyphen{}waiting}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:avoids-busy-waiting}}

\section{Used by join() to sleep instead of spinning}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:used-by-join-to-sleep-instead-of-spinning}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def task\_done(self):}\begin{description}
\sphinxlineitem{with self.mutex:}
\sphinxAtStartPar
self.unfinished\_tasks \sphinxhyphen{}= 1  \# \textless{}\sphinxhyphen{} Update counter
\begin{description}
\sphinxlineitem{if self.unfinished\_tasks == 0:  \# \textless{}\sphinxhyphen{} Check counter}
\sphinxAtStartPar
self.all\_tasks*done.notify\_all()  \# \textless{}\sphinxhyphen{} Use condition

\end{description}

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Counter} \PYG{n}{answers} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{when?}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{*}\PYG{o}{*}
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Condition} \PYG{n}{variable} \PYG{n}{answers} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{how to signal efficiently?}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
If Queue only had counter but NO condition variable:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{WRONG \sphinxhyphen{} Wastes CPU}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:wrong-wastes-cpu}}\begin{description}
\sphinxlineitem{def join\_without*condition(self):}\begin{description}
\sphinxlineitem{while self.unfinished\_tasks \textgreater{} 0:}
\sphinxAtStartPar
time.sleep(0.01)  \# Busy\sphinxhyphen{}wait, bad!
\# Keeps waking up to check, wastes CPU

\end{description}

\end{description}


\section{RIGHT \sphinxhyphen{} Uses condition variable}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:right-uses-condition-variable}}\begin{description}
\sphinxlineitem{def join\_with*condition(self):}\begin{description}
\sphinxlineitem{with self.all\_tasks*done:}\begin{description}
\sphinxlineitem{while self.unfinished\_tasks \textgreater{} 0:}
\sphinxAtStartPar
self.all\_tasks*done.wait()  \# Sleep efficiently
\# Only wakes up when task\_done() signals

\end{description}

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Visual: Counter and Condition Together}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:visual-counter-and-condition-together}}
\sphinxAtStartPar
Queue internals:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Complete Operation Table}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:complete-operation-table}}
\begin{DUlineblock}{0em}
\item[] Operation | What it touches | Effect |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}—————\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| \sphinxcode{\sphinxupquote{put(item)}} | items list | Appends item |
| \sphinxcode{\sphinxupquote{put(item)}} | unfinished\_tasks | Increments |
| \sphinxcode{\sphinxupquote{put(item)}} | not\_empty | Notifies consumer |
| \sphinxcode{\sphinxupquote{get()}} | items list | Pops item |
| \sphinxcode{\sphinxupquote{get()}} | unfinished\_tasks | No change! |
| \sphinxcode{\sphinxupquote{get()}} | not\_full | Notifies producer |
| \sphinxcode{\sphinxupquote{task\_done()}} | unfinished\_tasks | Decrements |
| \sphinxcode{\sphinxupquote{task\_done()}} | all\_tasks*done | Notifies join() |
| \sphinxcode{\sphinxupquote{join()}} | unfinished\_tasks | Checks if 0 |
| \sphinxcode{\sphinxupquote{join()}} | all\_tasks*done | Waits on it |

\sphinxAtStartPar
—


\subsection{What Happens WITHOUT task\_done()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:what-happens-without-task-done}}

\subsubsection{Queue.join() Without task\_done()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:queue-join-without-task-done}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
queue = Queue()

\sphinxAtStartPar
queue.put(1)
queue.put(2)
queue.put(3)

\sphinxAtStartPar
item = queue.get()  \# Get all items
item = queue.get()
item = queue.get()


\section{NO task\_done() calls!}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:no-task-done-calls}}\begin{description}
\sphinxlineitem{queue.join()  \# BLOCKS FOREVER!}
\sphinxAtStartPar
\# Queue still thinks 3 items are “unprocessed”
\# Counter never decrements to 0

\end{description}

\sphinxAtStartPar
Queue state:
+—————————\textendash{}+
| items: {[}{]}                   |  \textless{}\sphinxhyphen{} All removed
| task\_counter: 3             |  \textless{}\sphinxhyphen{} Still 3! Never decremented
+—————————\textendash{}+
\begin{description}
\sphinxlineitem{join() checks: if task\_counter != 0: wait()}
\sphinxAtStartPar
3 != 0, so WAIT FOREVER

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{What Happens WITH task\_done()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:what-happens-with-task-done}}

\subsubsection{Queue.join() With task\_done()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:queue-join-with-task-done}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
queue = Queue()

\sphinxAtStartPar
queue.put(1)      \# task\_counter = 1
queue.put(2)      \# task\_counter = 2
queue.put(3)      \# task\_counter = 3

\sphinxAtStartPar
item = queue.get()  \# task\_counter = 3 (unchanged)
queue.task\_done()   \# task\_counter = 2 (decremented!)

\sphinxAtStartPar
item = queue.get()  \# task\_counter = 2 (unchanged)
queue.task\_done()   \# task\_counter = 1 (decremented!)

\sphinxAtStartPar
item = queue.get()  \# task\_counter = 1 (unchanged)
queue.task\_done()   \# task\_counter = 0 (decremented!)

\sphinxAtStartPar
queue.join()  \# task\_counter = 0, so continues immediately!
print(“Done!”)


\subsubsection{Why It Works}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:why-it-works}}
\sphinxAtStartPar
Queue state after all task\_done():
+—————————\textendash{}+
| items: {[}{]}                   |  \textless{}\sphinxhyphen{} All removed
| task\_counter: 0             |  \textless{}\sphinxhyphen{} All decremented to 0
+—————————\textendash{}+
\begin{description}
\sphinxlineitem{join() checks: if task\_counter != 0: wait()}
\sphinxAtStartPar
0 == 0, so CONTINUE (don’t wait)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{The Three\sphinxhyphen{}Step Cycle}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:the-three-step-cycle}}

\subsubsection{Step 1: Put Item}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:step-1-put-item}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
queue.put(item)
task\_counter increments
=======================


\subsubsection{Step 2: Get and Process}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:step-2-get-and-process}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
item = queue.get()  \# Removes from queue but…
task\_counter STAYS the same
===========================
(item is “in flight”, being processed)
======================================
process(item)       \# Do actual work


\subsubsection{Step 3: Mark Done}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:step-3-mark-done}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
queue.task\_done()
task\_counter decrements
=======================
Tells queue: “I’m done with this item”
======================================


\subsubsection{Flow Diagram}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:flow-diagram}}\begin{description}
\sphinxlineitem{put(A)       get()      task\_done()    queue state}
\sphinxAtStartPar
down            down            down

\sphinxlineitem{{[}A{]}          (processing)   {[}OK{]}          counter: 1\sphinxhyphen{}\textgreater{}0}
\sphinxAtStartPar
down            down            down

\end{description}

\sphinxAtStartPar
counter=1   counter=1   counter=0
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Real Example: Work Queue Pattern}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:real-example-work-queue-pattern}}

\subsubsection{Without task\_done() \sphinxhyphen{} PROBLEMATIC}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:without-task-done-problematic}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue
from threading import Thread

\sphinxAtStartPar
tasks = Queue()


\section{Add tasks}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:add-tasks}}\begin{description}
\sphinxlineitem{for i in range(5):}
\sphinxAtStartPar
tasks.put(f”Task \{i\}”)

\sphinxlineitem{def worker():}\begin{description}
\sphinxlineitem{while True:}
\sphinxAtStartPar
task = tasks.get()
if task is None:
\begin{quote}

\sphinxAtStartPar
break
\end{quote}

\sphinxAtStartPar
print(f”Working on \{task\}”)
\# Do work here…
\# NO task\_done()!

\end{description}

\end{description}


\section{Start worker}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:start-worker}}
\sphinxAtStartPar
t = Thread(target=worker)
t.start()


\section{Main thread wants to know when all work is done}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:main-thread-wants-to-know-when-all-work-is-done}}
\sphinxAtStartPar
tasks.join()  \# BLOCKS FOREVER!
print(“All done!”)  \# Never prints!


\section{Send poison pill to stop worker}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:send-poison-pill-to-stop-worker}}
\sphinxAtStartPar
tasks.put(None)
t.join()

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Working on Task 0
Working on Task 1
Working on Task 2
Working on Task 3
Working on Task 4
{[}Program hangs, never says “All done!”{]}
.. code\sphinxhyphen{}block:: text

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue
from threading import Thread

\sphinxAtStartPar
tasks = Queue()


\section{Add tasks}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:id67}}\begin{description}
\sphinxlineitem{for i in range(5):}
\sphinxAtStartPar
tasks.put(f”Task \{i\}”)

\sphinxlineitem{def worker():}\begin{description}
\sphinxlineitem{while True:}
\sphinxAtStartPar
task = tasks.get()
if task is None:
\begin{quote}

\sphinxAtStartPar
break
\end{quote}

\sphinxAtStartPar
print(f”Working on \{task\}”)
\# Do work here…
tasks.task\_done()  \# IMPORTANT!

\end{description}

\end{description}


\section{Start worker}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:id68}}
\sphinxAtStartPar
t = Thread(target=worker)
t.start()


\section{Main thread waits for all work to be done}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:main-thread-waits-for-all-work-to-be-done}}
\sphinxAtStartPar
tasks.join()  \# Returns when all task\_done() called
print(“All done!”)  \# Now prints!


\section{Send poison pill to stop worker}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:id69}}
\sphinxAtStartPar
tasks.put(None)
t.join()

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Working on Task 0
Working on Task 1
Working on Task 2
Working on Task 3
Working on Task 4
All done!
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Why Is This Useful?}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:why-is-this-useful}}

\subsubsection{Use Case 1: Verify All Work Complete}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:use-case-1-verify-all-work-complete}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Main thread}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:main-thread}}\begin{description}
\sphinxlineitem{for task in tasks:}
\sphinxAtStartPar
queue.put(task)

\end{description}


\section{Wait for workers to finish ALL tasks}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:wait-for-workers-to-finish-all-tasks}}
\sphinxAtStartPar
queue.join()


\section{NOW we know all work is done}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:now-we-know-all-work-is-done}}
\sphinxAtStartPar
print(“All tasks processed successfully!”)
save\_results()
shutdown\_workers()

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
queue = Queue()
for i in range(100):
\begin{quote}

\sphinxAtStartPar
queue.put(i)
\end{quote}
\begin{description}
\sphinxlineitem{def worker():}\begin{description}
\sphinxlineitem{while True:}
\sphinxAtStartPar
task = queue.get()
if task is None:
\begin{quote}

\sphinxAtStartPar
break
\end{quote}

\sphinxAtStartPar
process(task)
queue.task\_done()

\end{description}

\end{description}

\sphinxAtStartPar
workers = {[}Thread(target=worker) for * in range(4){]}
for w in workers:
\begin{quote}

\sphinxAtStartPar
w.start()
\end{quote}


\section{Wait for completion}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:wait-for-completion}}
\sphinxAtStartPar
queue.join()
print(f”All 100 tasks completed!”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
batch\_queue = Queue()
\begin{description}
\sphinxlineitem{def add\_to*batch(item):}
\sphinxAtStartPar
batch\_queue.put(item)

\sphinxlineitem{def process\_batch():}
\sphinxAtStartPar
batch = {[}{]}
while True:
\begin{quote}

\sphinxAtStartPar
item = batch\_queue.get()
if item is None:
\begin{quote}

\sphinxAtStartPar
process\_batch*now(batch)
break
\end{quote}

\sphinxAtStartPar
batch.append(item)
batch\_queue.task\_done()
\begin{description}
\sphinxlineitem{if len(batch) == 10:}
\sphinxAtStartPar
process\_batch*now(batch)
batch = {[}{]}

\end{description}
\end{quote}

\end{description}


\section{Main}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:main}}\begin{description}
\sphinxlineitem{for i in range(50):}
\sphinxAtStartPar
add\_to*batch(i)

\end{description}


\section{Wait until all batches processed}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:wait-until-all-batches-processed}}
\sphinxAtStartPar
batch\_queue.join()
print(“All batches processed!”)
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Comparison: With vs Without task\_done()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:comparison-with-vs-without-task-done}}

\subsubsection{Scenario: Main thread needs to know when workers finish}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:scenario-main-thread-needs-to-know-when-workers-finish}}
\begin{DUlineblock}{0em}
\item[] Without task\_done() | With task\_done() |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}—————\textendash{}|
| \sphinxcode{\sphinxupquote{queue.join()}} blocks forever | \sphinxcode{\sphinxupquote{queue.join()}} returns when done |
| Can’t track progress | Can track completion |
| Unpredictable behavior | Predictable, reliable |
| No way to know if work finished | Clear completion signal |

\sphinxAtStartPar
—


\subsection{The Flow in Your Code (05*queue.py)}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:the-flow-in-your-code-05-queue-py}}
\sphinxAtStartPar
Looking back at the code:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{class Consumer(Thread):}\begin{description}
\sphinxlineitem{def \_\_init**(self, queue):}
\sphinxAtStartPar
Thread.**init**(self)
self.queue = queue

\sphinxlineitem{def run(self):}\begin{description}
\sphinxlineitem{while True:}
\sphinxAtStartPar
item = self.queue.get()           \# Step 1: Remove from queue
print(f”consumer: got \{item\}”)    \# Step 2: Do work
self.queue.task\_done()            \# Step 3: Mark as done

\end{description}

\end{description}

\end{description}


\subsubsection{Timeline}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:timeline}}
\sphinxAtStartPar
Producer puts 5 items in 5 seconds
{\color{red}\bfseries{}|}\sphinxhyphen{} 0s: put(42)   \sphinxhyphen{}\textgreater{} counter = 1
{\color{red}\bfseries{}|}\sphinxhyphen{} 1s: put(7)    \sphinxhyphen{}\textgreater{} counter = 2
{\color{red}\bfseries{}|}\sphinxhyphen{} 2s: put(199)  \sphinxhyphen{}\textgreater{} counter = 3
{\color{red}\bfseries{}|}\sphinxhyphen{} 3s: put(88)   \sphinxhyphen{}\textgreater{} counter = 4
+\sphinxhyphen{} 4s: put(12)   \sphinxhyphen{}\textgreater{} counter = 5

\sphinxAtStartPar
Meanwhile, 3 consumers get and process:
Consumer1: get(42) \sphinxhyphen{}\textgreater{} task\_done() \sphinxhyphen{}\textgreater{} counter = 4
Consumer2: get(7) \sphinxhyphen{}\textgreater{} task\_done() \sphinxhyphen{}\textgreater{} counter = 3
Consumer3: get(199) \sphinxhyphen{}\textgreater{} task\_done() \sphinxhyphen{}\textgreater{} counter = 2
Consumer1: get(88) \sphinxhyphen{}\textgreater{} task\_done() \sphinxhyphen{}\textgreater{} counter = 1
Consumer2: get(12) \sphinxhyphen{}\textgreater{} task\_done() \sphinxhyphen{}\textgreater{} counter = 0

\sphinxAtStartPar
Main thread: producer.join() completes at 4s
Main thread: But consumers are in infinite loop (while True)
Main thread: Tries to join consumers (would block forever)
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{The Missing Piece in the Code}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:the-missing-piece-in-the-code}}
\sphinxAtStartPar
The code has an issue \sphinxhyphen{} consumers have \sphinxcode{\sphinxupquote{while True}} (infinite loop):

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def run(self):}\begin{description}
\sphinxlineitem{while True:  \# Never exits!}
\sphinxAtStartPar
item = self.queue.get()
print(f”consumer notify: item popped from queue by \{item, self.name\}”)
self.queue.task\_done()

\end{description}

\end{description}


\subsubsection{Better Code Pattern}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:better-code-pattern}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def run(self):}\begin{description}
\sphinxlineitem{while True:}
\sphinxAtStartPar
item = self.queue.get()
\begin{description}
\sphinxlineitem{if item is None:  \# Poison pill \sphinxhyphen{} signal to exit}
\sphinxAtStartPar
self.queue.task\_done()
break

\end{description}

\sphinxAtStartPar
print(f”consumer: processing \{item\}”)
self.queue.task\_done()

\end{description}

\end{description}


\section{In main}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:in-main}}
\sphinxAtStartPar
producer.join()
queue.join()  \# All items processed


\section{Stop consumers}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:stop-consumers}}\begin{description}
\sphinxlineitem{for * in range(3):}
\sphinxAtStartPar
queue.put(None)  \# Poison pill

\end{description}

\sphinxAtStartPar
consumer1.join()
consumer2.join()
consumer3.join()
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Summary: What task\_done() Does}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:summary-what-task-done-does}}
\sphinxAtStartPar
\sphinxstylestrong{YES, task\_done() is on Queue and does:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Decrements unfinished\_tasks counter} by 1
\sphinxhyphen{} Tracks how many items still need processing

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Checks if counter reached 0}
\sphinxhyphen{} If yes: ALL items processed

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Notifies all\_tasks*done condition variable} (if counter = 0)
\sphinxhyphen{} Wakes up any threads waiting on \sphinxcode{\sphinxupquote{join()}}
\sphinxhyphen{} Uses \sphinxcode{\sphinxupquote{notify\_all()}} to wake ALL waiters

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{The counter tracks WHAT, the condition signals HOW.}


\subsubsection{Why We Need task\_done()}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:why-we-need-task-done}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tracks Completion} \sphinxhyphen{} Queue knows when items are processed

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Enables join()} \sphinxhyphen{} \sphinxcode{\sphinxupquote{queue.join()}} knows when to return

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Prevents Hanging} \sphinxhyphen{} Without it, \sphinxcode{\sphinxupquote{join()}} blocks forever

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Progress Tracking} \sphinxhyphen{} Count which items are done

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Synchronization} \sphinxhyphen{} Coordinate main thread with workers

\end{enumerate}


\subsubsection{The Pattern}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:the-pattern}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{put(item)      \sphinxhyphen{}\textgreater{} “Item added to queue”}
\sphinxAtStartPar
down

\sphinxlineitem{get(item)      \sphinxhyphen{}\textgreater{} “Someone took the item”}
\sphinxAtStartPar
down

\sphinxlineitem{task\_done()    \sphinxhyphen{}\textgreater{} “Item was PROCESSED”}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
join()         \sphinxhyphen{}\textgreater{} “All items PROCESSED?” \sphinxhyphen{}\textgreater{} Yes \sphinxhyphen{}\textgreater{} Continue

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Without ``task\PYGZus{}done()``, ``join()`` can never verify all items are truly processed!**
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Real Code Example}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:real-code-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from queue import Queue
from threading import Thread
import time

\sphinxAtStartPar
q = Queue()
\begin{description}
\sphinxlineitem{def producer():}\begin{description}
\sphinxlineitem{for i in range(3):}
\sphinxAtStartPar
q.put(f”Task \{i\}”)
print(f”Put Task \{i\}, unfinished\_tasks now = \{q.unfinished\_tasks\}”)

\end{description}

\sphinxlineitem{def consumer():}\begin{description}
\sphinxlineitem{while True:}
\sphinxAtStartPar
item = q.get()
if item is None:
\begin{quote}

\sphinxAtStartPar
break
\end{quote}

\sphinxAtStartPar
print(f”Got \{item\}, unfinished\_tasks still = \{q.unfinished\_tasks\}”)
time.sleep(0.5)
q.task\_done()
print(f”Marked \{item\} done, unfinished\_tasks now = \{q.unfinished\_tasks\}”)

\end{description}

\end{description}

\sphinxAtStartPar
t1 = Thread(target=producer)
t2 = Thread(target=consumer)

\sphinxAtStartPar
t1.start()
t2.start()

\sphinxAtStartPar
t1.join()


\section{Now wait for consumer to finish processing ALL items}
\label{\detokenize{cpu-concurrency/task_done_queue_explained:now-wait-for-consumer-to-finish-processing-all-items}}
\sphinxAtStartPar
print(“Waiting for all tasks to be processed…”)
q.join()  \# Waits here until counter reaches 0
print(“All tasks processed!”)

\sphinxAtStartPar
q.put(None)  \# Stop consumer
t2.join()

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Put Task 0, unfinished\_tasks now = 1
Put Task 1, unfinished\_tasks now = 2
Put Task 2, unfinished\_tasks now = 3
Got Task 0, unfinished\_tasks still = 3
Marked Task 0 done, unfinished\_tasks now = 2
Got Task 1, unfinished\_tasks still = 2
Marked Task 1 done, unfinished\_tasks now = 1
Got Task 2, unfinished\_tasks still = 1
Marked Task 2 done, unfinished\_tasks now = 0
Waiting for all tasks to be processed…
All tasks processed!

\sphinxAtStartPar
Notice how \sphinxcode{\sphinxupquote{task\_done()}} decrements the counter, and when it reaches 0, \sphinxcode{\sphinxupquote{join()}} returns!

\sphinxstepscope


\section{RLock (Reentrant Lock) Explained}
\label{\detokenize{cpu-concurrency/rlock_explained:rlock-reentrant-lock-explained}}\label{\detokenize{cpu-concurrency/rlock_explained::doc}}

\subsection{What is RLock?}
\label{\detokenize{cpu-concurrency/rlock_explained:what-is-rlock}}
\sphinxAtStartPar
\sphinxstylestrong{RLock} stands for \sphinxstylestrong{Reentrant Lock}. It’s a special type of lock that allows the \sphinxstylestrong{same thread to acquire it multiple times} without deadlocking.


\subsubsection{Regular Lock vs RLock}
\label{\detokenize{cpu-concurrency/rlock_explained:regular-lock-vs-rlock}}
\sphinxAtStartPar
\sphinxstylestrong{Regular Lock (threading.Lock):}
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
lock = threading.Lock()
\begin{description}
\sphinxlineitem{with lock:           \# Thread acquires lock}\begin{description}
\sphinxlineitem{with lock:       \# Same thread tries to acquire again}
\sphinxAtStartPar
pass         \# DEADLOCK! Thread waits for itself!

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{RLock} \PYG{p}{(}\PYG{n}{threading}\PYG{o}{.}\PYG{n}{RLock}\PYG{p}{)}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
rlock = threading.RLock()
\begin{description}
\sphinxlineitem{with rlock:          \# Thread acquires lock (count = 1)}\begin{description}
\sphinxlineitem{with rlock:      \# Same thread acquires again (count = 2)}\begin{description}
\sphinxlineitem{pass         \# No deadlock! Just increments counter}
\sphinxAtStartPar
\# Releases when count goes back to 0

\end{description}

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{How RLock Works Internally}
\label{\detokenize{cpu-concurrency/rlock_explained:how-rlock-works-internally}}
\sphinxAtStartPar
RLock uses a \sphinxstylestrong{counter system}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{First acquire} \sphinxhyphen{} Counter = 1, lock is held

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Same thread acquires again} \sphinxhyphen{} Counter = 2, lock still held

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Same thread acquires again} \sphinxhyphen{} Counter = 3, lock still held

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{First release} \sphinxhyphen{} Counter = 2, lock still held

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Second release} \sphinxhyphen{} Counter = 1, lock still held

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Third release} \sphinxhyphen{} Counter = 0, lock released

\end{enumerate}

\sphinxAtStartPar
Thread A acquires:    Counter = 1 {[}OK{]}
Thread A acquires:    Counter = 2 {[}OK{]}
Thread A acquires:    Counter = 3 {[}OK{]}
(other threads blocked)
Thread A releases:    Counter = 2 (still locked)
Thread A releases:    Counter = 1 (still locked)
Thread A releases:    Counter = 0 (NOW unlocked)
Thread B can now acquire
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{The Code Explained}
\label{\detokenize{cpu-concurrency/rlock_explained:the-code-explained}}
\sphinxAtStartPar
Looking at \sphinxcode{\sphinxupquote{/basics/03*rlock.py}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{class Box:}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self):}
\sphinxAtStartPar
self.lock = threading.RLock()  \# Reentrant lock
self.total\_items = 0

\sphinxlineitem{def execute(self, value):}\begin{description}
\sphinxlineitem{with self.lock:                \# Acquire lock}\begin{description}
\sphinxlineitem{self.total\_items += value  \# Modify shared data}
\sphinxAtStartPar
\# Release lock

\end{description}

\end{description}

\sphinxlineitem{def add(self):}\begin{description}
\sphinxlineitem{with self.lock:                \# Acquire lock (1st time)}\begin{description}
\sphinxlineitem{self.execute(1)            \# Calls execute…}
\sphinxAtStartPar
\# execute() tries to acquire same lock (2nd time)
\# RLock allows this! Regular Lock would deadlock

\end{description}

\end{description}

\sphinxlineitem{def remove(self):}\begin{description}
\sphinxlineitem{with self.lock:                \# Acquire lock}
\sphinxAtStartPar
self.execute(\sphinxhyphen{}1)           \# Same lock acquisition happens here

\end{description}

\end{description}

\end{description}


\subsubsection{Why RLock is Needed Here}
\label{\detokenize{cpu-concurrency/rlock_explained:why-rlock-is-needed-here}}
\sphinxAtStartPar
Without RLock, this would happen:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{With regular Lock \sphinxhyphen{} DEADLOCK!}
\label{\detokenize{cpu-concurrency/rlock_explained:with-regular-lock-deadlock}}
\sphinxAtStartPar
lock = threading.Lock()
\begin{description}
\sphinxlineitem{def execute(self, value):}\begin{description}
\sphinxlineitem{with lock:  \# Acquires lock}
\sphinxAtStartPar
self.total\_items += value

\end{description}

\sphinxlineitem{def add(self):}\begin{description}
\sphinxlineitem{with lock:  \# Acquires lock (1st time)}\begin{description}
\sphinxlineitem{self.execute(1)  \# Tries to acquire lock (2nd time)}
\sphinxAtStartPar
\# DEADLOCK! Same thread waiting for itself!

\end{description}

\end{description}

\end{description}

\sphinxAtStartPar
With RLock, it works fine:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{With RLock \sphinxhyphen{} NO DEADLOCK!}
\label{\detokenize{cpu-concurrency/rlock_explained:with-rlock-no-deadlock}}
\sphinxAtStartPar
rlock = threading.RLock()
\begin{description}
\sphinxlineitem{def execute(self, value):}\begin{description}
\sphinxlineitem{with rlock:  \# Acquires lock, counter = 1}
\sphinxAtStartPar
self.total\_items += value

\end{description}

\sphinxlineitem{def add(self):}\begin{description}
\sphinxlineitem{with rlock:  \# Acquires lock, counter = 1}\begin{description}
\sphinxlineitem{self.execute(1)  \# Acquires same lock, counter = 2}
\sphinxAtStartPar
\# No deadlock! Just increments counter
\# After execute() returns, counter = 1
\# After add() ends, counter = 0

\end{description}

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{When to Use RLock vs Regular Lock}
\label{\detokenize{cpu-concurrency/rlock_explained:when-to-use-rlock-vs-regular-lock}}

\subsubsection{Use Regular Lock When:}
\label{\detokenize{cpu-concurrency/rlock_explained:use-regular-lock-when}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Simple cases where you don’t have nested lock acquisitions

\item {} 
\sphinxAtStartPar
Different methods don’t call each other

\item {} 
\sphinxAtStartPar
Performance is critical (RLock is slightly slower)

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{class Counter:}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self):}
\sphinxAtStartPar
self.lock = threading.Lock()  \# Sufficient here
self.value = 0

\sphinxlineitem{def increment(self):}\begin{description}
\sphinxlineitem{with self.lock:}
\sphinxAtStartPar
self.value += 1  \# Simple, no nested calls

\end{description}

\end{description}

\end{description}


\subsubsection{Use RLock When:}
\label{\detokenize{cpu-concurrency/rlock_explained:use-rlock-when}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Methods call other methods that also need the lock

\item {} 
\sphinxAtStartPar
You have nested function calls

\item {} 
\sphinxAtStartPar
Complex class hierarchies

\item {} 
\sphinxAtStartPar
You’re unsure if nesting will happen

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{class Box:}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self):}
\sphinxAtStartPar
self.lock = threading.RLock()  \# Needed here!
self.items = 0

\sphinxlineitem{def add(self):}\begin{description}
\sphinxlineitem{with self.lock:}
\sphinxAtStartPar
self.execute(1)  \# Calls another method

\end{description}

\sphinxlineitem{def execute(self, value):}\begin{description}
\sphinxlineitem{with self.lock:      \# Same lock needed here too}
\sphinxAtStartPar
self.items += value

\end{description}

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Execution Flow of the Code}
\label{\detokenize{cpu-concurrency/rlock_explained:execution-flow-of-the-code}}\begin{description}
\sphinxlineitem{Thread 1 (adder):              Thread 2 (remover):}\begin{description}
\sphinxlineitem{add()}\begin{description}
\sphinxlineitem{with lock (count=1)}\begin{description}
\sphinxlineitem{execute(1)}\begin{description}
\sphinxlineitem{with lock (count=2)    (blocked \sphinxhyphen{} same thread can go)}
\sphinxAtStartPar
items += 1

\end{description}

\sphinxAtStartPar
(count=1)

\sphinxlineitem{(count=0)                remove()}\begin{description}
\sphinxlineitem{with lock (count=1)}\begin{description}
\sphinxlineitem{execute(\sphinxhyphen{}1)}\begin{description}
\sphinxlineitem{with lock (count=2)}
\sphinxAtStartPar
items \sphinxhyphen{}= 1

\end{description}

\sphinxAtStartPar
(count=1)

\end{description}

\sphinxAtStartPar
(count=0)

\end{description}

\end{description}

\end{description}

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{Point}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*} \PYG{n}{The} \PYG{n}{same} \PYG{n}{thread} \PYG{n}{can} \PYG{n}{acquire} \PYG{n}{the} \PYG{n}{same} \PYG{n}{RLock} \PYG{n}{multiple} \PYG{n}{times} \PYG{n}{without} \PYG{n}{blocking}\PYG{o}{.} \PYG{n}{With} \PYG{n}{a} \PYG{n}{regular} \PYG{n}{Lock}\PYG{p}{,} \PYG{n}{this} \PYG{n}{would} \PYG{n}{deadlock}\PYG{o}{.}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Reentrant Lock Counter Animation}
\label{\detokenize{cpu-concurrency/rlock_explained:reentrant-lock-counter-animation}}

\subsection{Time  Thread A              RLock Counter    Thread B Status}
\label{\detokenize{cpu-concurrency/rlock_explained:time-thread-a-rlock-counter-thread-b-status}}
\sphinxAtStartPar
1     acquire()            1                blocked (waiting)
2     acquire()            2                blocked
3     release()            1                blocked
4     release()            0                Can now acquire!
5     (out of lock)                        acquire() succeeds

\sphinxAtStartPar
With a regular Lock, Thread B would wait forever because Thread A keeps acquiring without releasing.

\sphinxAtStartPar
—


\subsection{Code Flow with Comments}
\label{\detokenize{cpu-concurrency/rlock_explained:code-flow-with-comments}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{class Box:}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self):}
\sphinxAtStartPar
self.lock = threading.RLock()  \# Counter starts at 0
self.total\_items = 0

\sphinxlineitem{def execute(self, value):}\begin{description}
\sphinxlineitem{with self.lock:                \# Counter: 0\sphinxhyphen{}\textgreater{}1}
\sphinxAtStartPar
self.total\_items += value

\end{description}

\sphinxAtStartPar
\# Counter: 1\sphinxhyphen{}\textgreater{}0 (lock released)

\sphinxlineitem{def add(self):}\begin{description}
\sphinxlineitem{with self.lock:                \# Counter: 0\sphinxhyphen{}\textgreater{}1}
\sphinxAtStartPar
self.execute(1)
\# Inside execute():
\# Counter: 1\sphinxhyphen{}\textgreater{}2 (REENTRANT \sphinxhyphen{} same thread!)
\# Modify items
\# Counter: 2\sphinxhyphen{}\textgreater{}1

\end{description}

\sphinxAtStartPar
\# Counter: 1\sphinxhyphen{}\textgreater{}0 (lock fully released)

\sphinxlineitem{def remove(self):}\begin{description}
\sphinxlineitem{with self.lock:                \# Counter: 0\sphinxhyphen{}\textgreater{}1}
\sphinxAtStartPar
self.execute(\sphinxhyphen{}1)
\# Inside execute():
\# Counter: 1\sphinxhyphen{}\textgreater{}2 (REENTRANT)
\# Modify items
\# Counter: 2\sphinxhyphen{}\textgreater{}1

\end{description}

\sphinxAtStartPar
\# Counter: 1\sphinxhyphen{}\textgreater{}0

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Visual Comparison}
\label{\detokenize{cpu-concurrency/rlock_explained:visual-comparison}}

\subsubsection{Regular Lock \sphinxhyphen{} Would Deadlock}
\label{\detokenize{cpu-concurrency/rlock_explained:regular-lock-would-deadlock}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Thread trying to re\sphinxhyphen{}acquire the same lock:
\begin{quote}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\end{quote}

\sphinxAtStartPar
+————{[}v{]}————\sphinxhyphen{}+
| Thread A tries to        |
| acquire again            |
| WAITING… (deadlock!)   | \textless{}\sphinxhyphen{} Blocked forever
| (waiting for itself)     |
+————————\sphinxhyphen{}+
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\subsubsection{RLock \sphinxhyphen{} No Deadlock}
\label{\detokenize{cpu-concurrency/rlock_explained:rlock-no-deadlock}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Thread re\sphinxhyphen{}acquiring the same lock:
\begin{quote}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\end{quote}

\sphinxAtStartPar
+————\sphinxhyphen{}{[}v{]}————\sphinxhyphen{}+
| Thread A acquires again   |
| Lock count = 2 {[}OK{]}          | \textless{}\sphinxhyphen{} Allowed!
| (same thread, no block)   |
+————\sphinxhyphen{}+————\sphinxhyphen{}+
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\end{quote}

\sphinxAtStartPar
+————\sphinxhyphen{}{[}v{]}————\sphinxhyphen{}+
| Thread A releases (2\sphinxhyphen{}\textgreater{}1)   |
| Lock still held {[}OK{]}         |
+————\sphinxhyphen{}+————\sphinxhyphen{}+
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\end{quote}

\sphinxAtStartPar
+————\sphinxhyphen{}{[}v{]}————\sphinxhyphen{}+
| Thread A releases (1\sphinxhyphen{}\textgreater{}0)   |
| Lock released {[}OK{]}           | \textless{}\sphinxhyphen{} Now others can acquire
+————————\textendash{}+
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Real\sphinxhyphen{}World Example}
\label{\detokenize{cpu-concurrency/rlock_explained:real-world-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{class Account:}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self, balance=0):}
\sphinxAtStartPar
self.lock = threading.RLock()
self.balance = balance

\sphinxlineitem{def withdraw(self, amount):}
\sphinxAtStartPar
“””Helper method \sphinxhyphen{} acquires lock”””
with self.lock:
\begin{quote}
\begin{description}
\sphinxlineitem{if self.balance \textgreater{}= amount:}
\sphinxAtStartPar
self.balance \sphinxhyphen{}= amount
return True

\end{description}

\sphinxAtStartPar
return False
\end{quote}

\sphinxlineitem{def transfer(self, other, amount):}
\sphinxAtStartPar
“””Main method \sphinxhyphen{} also needs lock”””
with self.lock:
\begin{quote}

\sphinxAtStartPar
\# Need to ensure consistent state while transferring
if self.withdraw(amount):  \# withdraw() also wants lock!
\begin{quote}
\begin{description}
\sphinxlineitem{with other.lock:}
\sphinxAtStartPar
other.balance += amount
return True

\end{description}
\end{quote}

\sphinxAtStartPar
return False
\end{quote}

\end{description}

\end{description}


\section{Without RLock:}
\label{\detokenize{cpu-concurrency/rlock_explained:without-rlock}}

\section{transfer() acquires self.lock}
\label{\detokenize{cpu-concurrency/rlock_explained:transfer-acquires-self-lock}}

\section{withdraw() tries to acquire self.lock again}
\label{\detokenize{cpu-concurrency/rlock_explained:withdraw-tries-to-acquire-self-lock-again}}

\section{DEADLOCK!}
\label{\detokenize{cpu-concurrency/rlock_explained:deadlock}}

\section{With RLock:}
\label{\detokenize{cpu-concurrency/rlock_explained:with-rlock}}

\section{transfer() acquires self.lock (count=1)}
\label{\detokenize{cpu-concurrency/rlock_explained:transfer-acquires-self-lock-count-1}}

\section{withdraw() acquires self.lock (count=2) \sphinxhyphen{} allowed!}
\label{\detokenize{cpu-concurrency/rlock_explained:withdraw-acquires-self-lock-count-2-allowed}}

\section{No deadlock!}
\label{\detokenize{cpu-concurrency/rlock_explained:no-deadlock}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Summary Table}
\label{\detokenize{cpu-concurrency/rlock_explained:summary-table}}
\begin{DUlineblock}{0em}
\item[] Aspect | Regular Lock | RLock |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}————\sphinxhyphen{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| \sphinxstylestrong{Same thread re\sphinxhyphen{}acquire} | {[}{[}FAIL{]}{]} Deadlock | {[}OK{]} Allowed |
| \sphinxstylestrong{Counter system} | No | Yes (0, 1, 2, …) |
| \sphinxstylestrong{Nested calls} | Dangerous | Safe |
| \sphinxstylestrong{Performance} | Faster | Slightly slower |
| \sphinxstylestrong{Use case} | Simple locking | Complex methods |

\sphinxAtStartPar
—


\subsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/rlock_explained:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{RLock = Reentrant Lock} \sphinxhyphen{} allows same thread to acquire multiple times

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Uses a counter} \sphinxhyphen{} increments on acquire, decrements on release

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Only fully released when counter = 0} \sphinxhyphen{} previous locks stay held

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Prevents deadlock} \sphinxhyphen{} same thread can safely call nested locked methods

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Slightly slower} \sphinxhyphen{} use regular Lock if you don’t need reentrancy

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{When in doubt, use RLock} \sphinxhyphen{} it’s safer for complex code with nested calls

\end{enumerate}

\sphinxAtStartPar
The code in \sphinxcode{\sphinxupquote{03*rlock.py}} uses RLock because:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{add()}} calls \sphinxcode{\sphinxupquote{execute()}} (both want the lock)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{remove()}} calls \sphinxcode{\sphinxupquote{execute()}} (both want the lock)
\sphinxhyphen{} Without RLock, these would deadlock
\sphinxhyphen{} With RLock, the same thread can safely reacquire the lock

\sphinxstepscope


\section{Semaphore Explained}
\label{\detokenize{cpu-concurrency/semaphore_explained:semaphore-explained}}\label{\detokenize{cpu-concurrency/semaphore_explained::doc}}

\subsection{What is a Semaphore?}
\label{\detokenize{cpu-concurrency/semaphore_explained:what-is-a-semaphore}}
\sphinxAtStartPar
A \sphinxstylestrong{semaphore} is a synchronization primitive that uses an \sphinxstylestrong{internal counter} to control access to a shared resource. It’s like having a certain number of “permits” or “tokens” that threads need to acquire.


\subsubsection{Key Concept}
\label{\detokenize{cpu-concurrency/semaphore_explained:key-concept}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Semaphore has a \sphinxstylestrong{counter} (starts at some number)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{acquire()}} \sphinxhyphen{} decrements the counter, if counter = 0, thread waits

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{release()}} \sphinxhyphen{} increments the counter, wakes up waiting threads

\end{itemize}

\sphinxAtStartPar
—


\subsection{Two Types of Semaphores}
\label{\detokenize{cpu-concurrency/semaphore_explained:two-types-of-semaphores}}

\subsubsection{1. Counting Semaphore (Counter \textgreater{} 1)}
\label{\detokenize{cpu-concurrency/semaphore_explained:counting-semaphore-counter-1}}
\sphinxAtStartPar
Controls access to a \sphinxstylestrong{pool of resources}. Example: 3 parking spots.

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
semaphore = threading.Semaphore(3)  \# 3 “permits”

\sphinxAtStartPar
semaphore.acquire()  \# Counter: 3 \sphinxhyphen{}\textgreater{} 2 (one car parks)
semaphore.acquire()  \# Counter: 2 \sphinxhyphen{}\textgreater{} 1 (another parks)
semaphore.acquire()  \# Counter: 1 \sphinxhyphen{}\textgreater{} 0 (another parks)
semaphore.acquire()  \# Counter: 0 \sphinxhyphen{} WAIT! (parking full, blocked)

\sphinxAtStartPar
semaphore.release()  \# Counter: 0 \sphinxhyphen{}\textgreater{} 1 (one car leaves, waiting car parks)


\subsubsection{2. Binary Semaphore (Counter = 0 or 1)}
\label{\detokenize{cpu-concurrency/semaphore_explained:binary-semaphore-counter-0-or-1}}
\sphinxAtStartPar
Acts like a \sphinxstylestrong{signal/notification}. Example: “data is ready”.

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
semaphore = threading.Semaphore(0)  \# Counter starts at 0

\sphinxAtStartPar
semaphore.acquire()  \# Counter: 0 \sphinxhyphen{} WAIT! (blocked, waiting for signal)
… thread blocks until…
==========================
semaphore.release()  \# Counter: 0 \sphinxhyphen{}\textgreater{} 1 (signal sent! waiting thread wakes up)
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{The Code Explained: Producer\sphinxhyphen{}Consumer Pattern}
\label{\detokenize{cpu-concurrency/semaphore_explained:the-code-explained-producer-consumer-pattern}}
\sphinxAtStartPar
Looking at \sphinxcode{\sphinxupquote{/basics/04*semaphore.py}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
semaphore = threading.Semaphore(0)  \# Binary semaphore, counter starts at 0
item = 0
\begin{description}
\sphinxlineitem{def consumer():}
\sphinxAtStartPar
logging.info(“Consumer waiting”)
semaphore.acquire()              \# BLOCKS HERE! Waiting for producer
logging.info(f”Consumer notify: item number \{item\}”)

\sphinxlineitem{def producer():}
\sphinxAtStartPar
global item
time.sleep(3)                    \# Simulate work
item = random.randint(0, 1000)   \# Produce data
logging.info(f”Producer notify: item number \{item\}”)
semaphore.release()              \# SIGNAL! Unblock the consumer

\end{description}


\subsubsection{Execution Timeline}
\label{\detokenize{cpu-concurrency/semaphore_explained:execution-timeline}}

\subsection{Time  Consumer                 Semaphore Counter    Producer}
\label{\detokenize{cpu-concurrency/semaphore_explained:time-consumer-semaphore-counter-producer}}
\sphinxAtStartPar
0     waiting…              0
\begin{description}
\sphinxlineitem{0     acquire()               BLOCKED!             (sleeping 3s)}
\sphinxAtStartPar
(blocked here)          0

\sphinxlineitem{3                             0                    wakes up}\begin{quote}

\sphinxAtStartPar
0                    item = 123
0                    release()
\end{quote}

\sphinxAtStartPar
UNBLOCKED!             1 \sphinxhyphen{}\textgreater{} 0                acquired from 0
prints item 123        0

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Semaphore Operations}
\label{\detokenize{cpu-concurrency/semaphore_explained:semaphore-operations}}

\subsubsection{\sphinxstyleliteralintitle{\sphinxupquote{acquire()}}}
\label{\detokenize{cpu-concurrency/semaphore_explained:acquire}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Decrements counter by 1}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{If counter becomes 0 or was 0}, thread \sphinxstylestrong{blocks/waits}

\item {} 
\sphinxAtStartPar
Thread resumes when another thread calls \sphinxcode{\sphinxupquote{release()}}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
semaphore = threading.Semaphore(2)

\sphinxAtStartPar
semaphore.acquire()  \# Counter: 2 \sphinxhyphen{}\textgreater{} 1 (continues)
semaphore.acquire()  \# Counter: 1 \sphinxhyphen{}\textgreater{} 0 (continues)
semaphore.acquire()  \# Counter: 0 \sphinxhyphen{} BLOCKS HERE (waits for release)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
``release()``
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
The woken thread can now \sphinxcode{\sphinxupquote{acquire()}}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
semaphore.acquire()  \# Counter: 0 \sphinxhyphen{} BLOCKED
… from another thread …
===========================
semaphore.release()  \# Counter: 0 \sphinxhyphen{}\textgreater{} 1 (wakes up blocked thread)
\begin{quote}

\sphinxAtStartPar
\# Blocked thread can now continue
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Visual Representation}
\label{\detokenize{cpu-concurrency/semaphore_explained:visual-representation}}

\subsubsection{Counting Semaphore (3 spots available)}
\label{\detokenize{cpu-concurrency/semaphore_explained:counting-semaphore-3-spots-available}}
\sphinxAtStartPar
Initial: Semaphore(3)
+———————————+
| Available Spots: 3              |
+———————————+

\sphinxAtStartPar
Thread A calls acquire():
+———————————+
| Available Spots: 2              |
| Thread A: ACQUIRED {[}OK{]}            |
+———————————+

\sphinxAtStartPar
Thread B calls acquire():
+———————————+
| Available Spots: 1              |
| Thread A: ACQUIRED              |
| Thread B: ACQUIRED {[}OK{]}            |
+———————————+

\sphinxAtStartPar
Thread C calls acquire():
+———————————+
| Available Spots: 0              |
| Thread A: ACQUIRED              |
| Thread B: ACQUIRED              |
| Thread C: ACQUIRED {[}OK{]}            |
+———————————+

\sphinxAtStartPar
Thread D calls acquire():
+———————————+
| Available Spots: 0              |
| Thread A: ACQUIRED              |
| Thread B: ACQUIRED              |
| Thread C: ACQUIRED              |
| Thread D: WAITING (blocked)     | \textless{}\sphinxhyphen{} Can’t proceed
+———————————+

\sphinxAtStartPar
Thread A calls release():
+———————————+
| Available Spots: 1              |
| Thread A: RELEASED              |
| Thread B: ACQUIRED              |
| Thread C: ACQUIRED              |
| Thread D: ACQUIRED (now!) {[}OK{]}     | \textless{}\sphinxhyphen{} Unblocked!
+———————————+
.. code\sphinxhyphen{}block:: text


\subsubsection{Binary Semaphore (Producer\sphinxhyphen{}Consumer)}
\label{\detokenize{cpu-concurrency/semaphore_explained:binary-semaphore-producer-consumer}}
\sphinxAtStartPar
Initial: Semaphore(0)
+———————\sphinxhyphen{}+
| Counter: 0           |
| Signal: NOT SET      |
+———————\sphinxhyphen{}+

\sphinxAtStartPar
Consumer calls acquire():
+———————\sphinxhyphen{}+
| Counter: 0           |
| Consumer: WAITING    | \textless{}\sphinxhyphen{} BLOCKED here
| Signal: NOT SET      |
+———————\sphinxhyphen{}+

\sphinxAtStartPar
Producer (after work) calls release():
+———————\sphinxhyphen{}+
| Counter: 1           |
| Consumer: PROCEED {[}OK{]}  | \textless{}\sphinxhyphen{} UNBLOCKED!
| Signal: SET          |
+———————\sphinxhyphen{}+
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Semaphore vs Lock}
\label{\detokenize{cpu-concurrency/semaphore_explained:semaphore-vs-lock}}
\begin{DUlineblock}{0em}
\item[] Aspect | Lock | Semaphore |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| \sphinxstylestrong{Purpose} | Mutual exclusion | Signaling / Resource pooling |
| \sphinxstylestrong{Counter} | 0 or 1 (locked/unlocked) | Any number |
| \sphinxstylestrong{Multiple resources} | 1 | Many |
| \sphinxstylestrong{Waiting threads} | Any thread can unlock | Only thread that acquired can release |
| \sphinxstylestrong{Use case} | Protect critical section | Signal events or manage pool |

\sphinxAtStartPar
—


\subsection{Real\sphinxhyphen{}World Examples}
\label{\detokenize{cpu-concurrency/semaphore_explained:real-world-examples}}

\subsubsection{Example 1: Swimming Pool with Limited Capacity}
\label{\detokenize{cpu-concurrency/semaphore_explained:example-1-swimming-pool-with-limited-capacity}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import threading
import time

\sphinxAtStartPar
pool\_capacity = threading.Semaphore(3)  \# Only 3 people can swim at once
\begin{description}
\sphinxlineitem{def swimmer(name):}
\sphinxAtStartPar
print(f”\{name\}: trying to enter pool…”)
pool\_capacity.acquire()              \# Wait if full

\sphinxAtStartPar
print(f”\{name\}: SWIMMING!”)
time.sleep(2)                        \# Swim for 2 seconds

\sphinxAtStartPar
pool\_capacity.release()              \# Leave, let someone else in
print(f”\{name\}: LEFT the pool”)

\sphinxlineitem{threads = {[}threading.Thread(target=swimmer, args=(f”Swimmer \{i\}”,))}
\sphinxAtStartPar
for i in range(5){]}

\sphinxlineitem{for t in threads:}
\sphinxAtStartPar
t.start()

\sphinxlineitem{for t in threads:}
\sphinxAtStartPar
t.join()

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Swimmer 0: trying to enter pool…
Swimmer 0: SWIMMING!
Swimmer 1: trying to enter pool…
Swimmer 1: SWIMMING!
Swimmer 2: trying to enter pool…
Swimmer 2: SWIMMING!
Swimmer 3: trying to enter pool…
Swimmer 3: WAITING (blocked \sphinxhyphen{} pool full)
Swimmer 4: trying to enter pool…
Swimmer 4: WAITING (blocked \sphinxhyphen{} pool full)

\sphinxAtStartPar
{[}After 2 seconds{]}
Swimmer 0: LEFT the pool
Swimmer 3: SWIMMING! (now can enter)
Swimmer 1: LEFT the pool
Swimmer 4: SWIMMING! (now can enter)
…
.. code\sphinxhyphen{}block:: text


\subsubsection{Example 2: Producer\sphinxhyphen{}Consumer (Like the Code)}
\label{\detokenize{cpu-concurrency/semaphore_explained:example-2-producer-consumer-like-the-code}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import threading
import time
import random

\sphinxAtStartPar
queue\_semaphore = threading.Semaphore(0)  \# 0 items initially
item\_produced = None
\begin{description}
\sphinxlineitem{def producer():}
\sphinxAtStartPar
global item\_produced
time.sleep(2)
item\_produced = random.randint(1, 100)
print(f”Producer: created item \{item\_produced\}”)
queue\_semaphore.release()  \# Signal: item ready!

\sphinxlineitem{def consumer():}
\sphinxAtStartPar
print(“Consumer: waiting for item…”)
queue\_semaphore.acquire()  \# Wait for item
print(f”Consumer: received item \{item\_produced\}”)

\end{description}

\sphinxAtStartPar
producer\_thread = threading.Thread(target=producer)
consumer\_thread = threading.Thread(target=consumer)

\sphinxAtStartPar
consumer\_thread.start()    \# Consumer waits first
producer\_thread.start()    \# Producer produces

\sphinxAtStartPar
consumer\_thread.join()
producer\_thread.join()

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Output}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Consumer: waiting for item…
{[}2 seconds pass{]}
Producer: created item 42
Consumer: received item 42
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{How the Code Works Step by Step}
\label{\detokenize{cpu-concurrency/semaphore_explained:how-the-code-works-step-by-step}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Initial state}
\label{\detokenize{cpu-concurrency/semaphore_explained:initial-state}}
\sphinxAtStartPar
semaphore = threading.Semaphore(0)  \# Counter = 0
item = 0


\section{Consumer thread starts}
\label{\detokenize{cpu-concurrency/semaphore_explained:consumer-thread-starts}}\begin{description}
\sphinxlineitem{def consumer():}
\sphinxAtStartPar
logging.info(“Consumer waiting”)
semaphore.acquire()              \# Counter is 0, BLOCKS HERE
\begin{quote}

\sphinxAtStartPar
\# Consumer thread is waiting
\end{quote}

\sphinxAtStartPar
logging.info(f”Consumer notify: item number \{item\}”)  \# Not reached yet

\end{description}


\section{Producer thread starts (10 times in the loop)}
\label{\detokenize{cpu-concurrency/semaphore_explained:producer-thread-starts-10-times-in-the-loop}}\begin{description}
\sphinxlineitem{def producer():}
\sphinxAtStartPar
global item
time.sleep(3)                    \# Simulate work for 3 seconds
item = random.randint(0, 1000)   \# Generate item
logging.info(f”Producer notify: item number \{item\}”)
semaphore.release()              \# Counter: 0 \sphinxhyphen{}\textgreater{} 1
\begin{quote}

\sphinxAtStartPar
\# This WAKES UP the consumer
\end{quote}

\end{description}


\section{Execution timeline:}
\label{\detokenize{cpu-concurrency/semaphore_explained:id1}}

\section{Time 0:   consumer() starts, calls acquire(), blocks (counter=0)}
\label{\detokenize{cpu-concurrency/semaphore_explained:time-0-consumer-starts-calls-acquire-blocks-counter-0}}

\section{Time 0:   producer() starts, sleeps for 3 seconds}
\label{\detokenize{cpu-concurrency/semaphore_explained:time-0-producer-starts-sleeps-for-3-seconds}}

\section{Time 3:   producer() wakes up, generates item}
\label{\detokenize{cpu-concurrency/semaphore_explained:time-3-producer-wakes-up-generates-item}}

\section{Time 3:   producer() calls release() \sphinxhyphen{} counter becomes 1}
\label{\detokenize{cpu-concurrency/semaphore_explained:time-3-producer-calls-release-counter-becomes-1}}

\section{Time 3:   consumer() wakes up from acquire(), can continue}
\label{\detokenize{cpu-concurrency/semaphore_explained:time-3-consumer-wakes-up-from-acquire-can-continue}}

\section{Time 3:   consumer() prints the item}
\label{\detokenize{cpu-concurrency/semaphore_explained:time-3-consumer-prints-the-item}}

\section{Time 3:   both threads finish (join completes)}
\label{\detokenize{cpu-concurrency/semaphore_explained:time-3-both-threads-finish-join-completes}}
\sphinxAtStartPar
\#
Loop repeats 10 times
=====================
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Semaphore States and Transitions}
\label{\detokenize{cpu-concurrency/semaphore_explained:semaphore-states-and-transitions}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Binary Semaphore (0 or 1) \sphinxhyphen{} Producer\sphinxhyphen{}Consumer

\sphinxAtStartPar
INITIAL STATE:
+————\sphinxhyphen{}+
| Counter: 0  |
| No signal   |
+——+——+
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\item[] Consumer acquire()
\item[] (counter = 0, so block)
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxtableatstartofbodyhook\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{2}}
\sphinxAtStartPar
Counter: 0
Consumer WAITING
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\item[] Producer release()
\item[] (counter 0 \sphinxhyphen{}\textgreater{} 1, wake consumer)
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Counting Semaphore States}
\label{\detokenize{cpu-concurrency/semaphore_explained:counting-semaphore-states}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Counting Semaphore(3) \sphinxhyphen{} Resource Pool

\sphinxAtStartPar
INITIAL:
+————\sphinxhyphen{}+
| Counter: 3  |
| 3 available |
+——+——+
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\item[] Thread A acquire()
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxtableatstartofbodyhook\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{2}}
\sphinxAtStartPar
Counter: 2
A acquired
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\item[] Thread B acquire()
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxtableatstartofbodyhook\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{2}}
\sphinxAtStartPar
Counter: 1
B acquired
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\item[] Thread C acquire()
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxtableatstartofbodyhook\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{2}}
\sphinxAtStartPar
Counter: 0
C acquired
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\item[] Thread D acquire() \sphinxhyphen{} BLOCKS!
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxtableatstartofbodyhook\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{2}}
\sphinxAtStartPar
Counter: 0
D WAITING
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\item[] Thread A release()
\item[] (counter 0 \sphinxhyphen{}\textgreater{} 1, wake D)
\end{DUlineblock}

\sphinxAtStartPar
{[}v{]}
\end{quote}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{T}
\sphinxtoprule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Counter: 0
D acquired!
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Common Use Cases}
\label{\detokenize{cpu-concurrency/semaphore_explained:common-use-cases}}

\subsubsection{1. \sphinxstylestrong{Limiting Concurrent Access}}
\label{\detokenize{cpu-concurrency/semaphore_explained:limiting-concurrent-access}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Only 5 threads can access database at once}
\label{\detokenize{cpu-concurrency/semaphore_explained:only-5-threads-can-access-database-at-once}}
\sphinxAtStartPar
db\_access = threading.Semaphore(5)
\begin{description}
\sphinxlineitem{def query\_database(query):}\begin{description}
\sphinxlineitem{with db\_access:  \# Use like a lock}
\sphinxAtStartPar
\# Access database
pass

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Sender signals receiver}
\label{\detokenize{cpu-concurrency/semaphore_explained:sender-signals-receiver}}
\sphinxAtStartPar
data\_ready = threading.Semaphore(0)
\begin{description}
\sphinxlineitem{def sender():}
\sphinxAtStartPar
prepare\_data()
data\_ready.release()  \# Signal: data ready!

\sphinxlineitem{def receiver():}
\sphinxAtStartPar
data\_ready.acquire()  \# Wait for signal
process\_data()

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Wait for all workers to finish}
\label{\detokenize{cpu-concurrency/semaphore_explained:wait-for-all-workers-to-finish}}
\sphinxAtStartPar
workers\_done = threading.Semaphore(0)
num\_workers = 5
\begin{description}
\sphinxlineitem{def worker():}
\sphinxAtStartPar
do\_work()
workers\_done.release()

\end{description}


\section{Main waits for all}
\label{\detokenize{cpu-concurrency/semaphore_explained:main-waits-for-all}}\begin{description}
\sphinxlineitem{for * in range(num\_workers):}
\sphinxAtStartPar
workers\_done.acquire()

\end{description}

\sphinxAtStartPar
print(“All workers done!”)
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Key Differences from Lock}
\label{\detokenize{cpu-concurrency/semaphore_explained:key-differences-from-lock}}

\subsubsection{Lock (\sphinxstyleliteralintitle{\sphinxupquote{threading.Lock}})}
\label{\detokenize{cpu-concurrency/semaphore_explained:lock-threading-lock}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
lock = threading.Lock()
\begin{description}
\sphinxlineitem{with lock:  \# Only ONE thread can be here at a time}
\sphinxAtStartPar
shared\_data += 1

\end{description}


\subsubsection{Semaphore (Counting)}
\label{\detokenize{cpu-concurrency/semaphore_explained:semaphore-counting}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
semaphore = threading.Semaphore(3)
\begin{description}
\sphinxlineitem{with semaphore:  \# Up to 3 threads can be here at a time}
\sphinxAtStartPar
access\_resource()

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\subsubsection{Semaphore (Binary \sphinxhyphen{} Used as Signal)}
\label{\detokenize{cpu-concurrency/semaphore_explained:semaphore-binary-used-as-signal}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
signal = threading.Semaphore(0)


\section{Thread A waits for signal}
\label{\detokenize{cpu-concurrency/semaphore_explained:thread-a-waits-for-signal}}
\sphinxAtStartPar
signal.acquire()
print(“I’ve been signaled!”)


\section{Thread B sends signal}
\label{\detokenize{cpu-concurrency/semaphore_explained:thread-b-sends-signal}}
\sphinxAtStartPar
signal.release()
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Summary Table}
\label{\detokenize{cpu-concurrency/semaphore_explained:summary-table}}
\begin{DUlineblock}{0em}
\item[] Operation | Effect | When |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| \sphinxcode{\sphinxupquote{acquire()}} | Counter \sphinxhyphen{} 1, blocks if 0 | When entering critical section |
| \sphinxcode{\sphinxupquote{release()}} | Counter + 1, wakes 1 thread | When leaving critical section |
| Multiple resources | {[}OK{]} Semaphore(n) | Manage pool |
| Signaling | {[}OK{]} Semaphore(0) | Event notification |

\sphinxAtStartPar
—


\subsection{Key Takeaways}
\label{\detokenize{cpu-concurrency/semaphore_explained:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Semaphore = Counter + Wait Queue}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{acquire()}} decrements, blocks if 0
\sphinxhyphen{} \sphinxcode{\sphinxupquote{release()}} increments, wakes waiting threads

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Binary Semaphore (0/1)} \sphinxhyphen{} Used for signaling
\sphinxhyphen{} Start at 0: consumer waits, producer signals
\sphinxhyphen{} Start at 1: resource available

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Counting Semaphore (\textgreater{}1)} \sphinxhyphen{} Used for resource pools
\sphinxhyphen{} Start at N: N threads can access resource
\sphinxhyphen{} Thread blocks if counter = 0

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Difference from Lock}
\sphinxhyphen{} Lock: binary (locked/unlocked)
\sphinxhyphen{} Semaphore: can have many resources

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{In the Code}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{Semaphore(0)}}: Consumer waits, producer signals
\sphinxhyphen{} \sphinxcode{\sphinxupquote{acquire()}}: Consumer blocks until data ready
\sphinxhyphen{} \sphinxcode{\sphinxupquote{release()}}: Producer unblocks consumer
\sphinxhyphen{} This is the \sphinxstylestrong{producer\sphinxhyphen{}consumer pattern}

\end{enumerate}

\sphinxAtStartPar
The code is a classic \sphinxstylestrong{producer\sphinxhyphen{}consumer synchronization} where the semaphore ensures the consumer doesn’t process data before the producer has created it.

\sphinxstepscope


\section{Why Different Approaches for CPU\sphinxhyphen{}bound vs I/O\sphinxhyphen{}bound Problems?}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:why-different-approaches-for-cpu-bound-vs-i-o-bound-problems}}\label{\detokenize{cpu-concurrency/patterns_problems_mapping::doc}}
\sphinxAtStartPar
A deep dive into the technical reasons behind choosing multiprocessing for CPU\sphinxhyphen{}bound tasks and threading/asyncio for I/O\sphinxhyphen{}bound tasks in Python.


\subsection{Table of Contents}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:table-of-contents}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/patterns_problems_mapping:the-fundamental-problem-the-gil}]{\sphinxsamedocref{The Fundamental Problem: The GIL}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/patterns_problems_mapping:how-cpu-and-io-operations-differ}]{\sphinxsamedocref{How CPU and I/O Operations Differ}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/patterns_problems_mapping:why-multiprocessing-for-cpu-bound}]{\sphinxsamedocref{Why Multiprocessing for CPU\sphinxhyphen{}bound}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/patterns_problems_mapping:why-threading-for-io-bound}]{\sphinxsamedocref{Why Threading for I/O\sphinxhyphen{}bound}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/patterns_problems_mapping:why-asyncio-for-io-bound}]{\sphinxsamedocref{Why Asyncio for I/O\sphinxhyphen{}bound}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/patterns_problems_mapping:deep-dive-what-happens-under-the-hood}]{\sphinxsamedocref{Deep Dive: What Happens Under the Hood}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/patterns_problems_mapping:performance-analysis}]{\sphinxsamedocref{Performance Analysis}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/patterns_problems_mapping:decision-tree}]{\sphinxsamedocref{Decision Tree}}}

\end{enumerate}

\sphinxAtStartPar
—


\subsection{The Fundamental Problem: The GIL}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:id1}}

\subsubsection{What is the GIL?}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:what-is-the-gil}}
\sphinxAtStartPar
The \sphinxstylestrong{Global Interpreter Lock (GIL)} is a mutex (mutual exclusion lock) in CPython that protects access to Python objects, preventing multiple threads from executing Python bytecode simultaneously.


\subsubsection{Why Does Python Have a GIL?}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:why-does-python-have-a-gil}}
\sphinxAtStartPar
Historical Context:
+———————————————+
| Python was designed in the late 1980s       |
| when single\sphinxhyphen{}core CPUs were the norm        |
|                                             |
| Design Decision:                            |
| * Simple memory management (ref counting)   |
| * Easy C extension integration              |
| * Thread\sphinxhyphen{}safe by default                    |
| * Trade\sphinxhyphen{}off: One GIL = Simple design       |
+———————————————+


\subsubsection{How the GIL Works}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:how-the-gil-works}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Conceptual representation of GIL behavior}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:conceptual-representation-of-gil-behavior}}\begin{description}
\sphinxlineitem{Thread 1: {[}Acquire GIL{]} \sphinxhyphen{}\textgreater{} Execute Python Code \sphinxhyphen{}\textgreater{} {[}Release GIL{]}}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
Thread 2:                    {[}Waiting…{]}  \sphinxhyphen{}\textgreater{} {[}Acquire GIL{]} \sphinxhyphen{}\textgreater{} Execute
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
\sphinxstylestrong{Key Point}: Only ONE thread can execute Python bytecode at a time, even on a multi\sphinxhyphen{}core CPU.

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{CPU\sphinxhyphen{}bound operation}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:cpu-bound-operation}}\begin{description}
\sphinxlineitem{def cpu\_intensive():}
\sphinxAtStartPar
total = 0
for i in range(10*000*000):
\begin{quote}

\sphinxAtStartPar
total += i
\end{quote}

\sphinxAtStartPar
return total

\end{description}


\section{Thread 1 acquires GIL \sphinxhyphen{}\textgreater{} executes \sphinxhyphen{}\textgreater{} releases after \textasciitilde{}100 bytecodes \sphinxhyphen{}\textgreater{} repeat}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:thread-1-acquires-gil-executes-releases-after-100-bytecodes-repeat}}

\section{Thread 2 waits \sphinxhyphen{}\textgreater{} acquires GIL \sphinxhyphen{}\textgreater{} executes \sphinxhyphen{}\textgreater{} releases \sphinxhyphen{}\textgreater{} repeat}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:thread-2-waits-acquires-gil-executes-releases-repeat}}

\section{Result: Threads take TURNS, no parallel execution}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:result-threads-take-turns-no-parallel-execution}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{I/O\sphinxhyphen{}bound operation}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:i-o-bound-operation}}\begin{description}
\sphinxlineitem{def io\_intensive():}
\sphinxAtStartPar
response = requests.get(’\sphinxurl{https://api.example.com/data}’)
return response.json()

\end{description}


\section{Thread 1 acquires GIL \sphinxhyphen{}\textgreater{} starts I/O \sphinxhyphen{}\textgreater{} RELEASES GIL during I/O wait}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:thread-1-acquires-gil-starts-i-o-releases-gil-during-i-o-wait}}

\section{Thread 2 can now acquire GIL and execute while Thread 1 waits}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:thread-2-can-now-acquire-gil-and-execute-while-thread-1-waits}}

\section{Result: Threads can work while others are waiting for I/O}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:result-threads-can-work-while-others-are-waiting-for-i-o}}
\begin{DUlineblock}{0em}
\item[] Operation Type | GIL Released During Operation? | Result |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——————————\sphinxhyphen{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| CPU\sphinxhyphen{}bound (pure Python) | {[}{[}FAIL{]}{]} No | Threads execute sequentially |
| I/O\sphinxhyphen{}bound (network, disk) | {[}{[}OK{]}{]} Yes | Threads can work concurrently |
| C extensions (NumPy, etc.) | {[}{[}OK{]}{]} Often yes | Can achieve parallelism |

\sphinxAtStartPar
—


\subsection{How CPU and I/O Operations Differ}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:id2}}

\subsubsection{CPU\sphinxhyphen{}bound Operations}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:cpu-bound-operations}}
\sphinxAtStartPar
\sphinxstylestrong{Definition}: Operations where execution time is determined by CPU processing speed.

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}:

\sphinxAtStartPar
CPU Usage:  \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\# (100\%)
I/O Wait:   (minimal or none)
Bottleneck: CPU cycles

\sphinxAtStartPar
Example Timeline:
0ms   \textendash{}{[}\textgreater{}{]} Processing \textendash{}{[}\textgreater{}{]} Processing \textendash{}{[}\textgreater{}{]} Processing \textendash{}{[}\textgreater{}{]} Done
\begin{quote}

\sphinxAtStartPar
(CPU busy)      (CPU busy)      (CPU busy)
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{What} \PYG{n}{happens}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
CPU fetches instructions from memory

\item {} 
\sphinxAtStartPar
CPU executes mathematical/logical operations

\item {} 
\sphinxAtStartPar
CPU writes results to memory/registers

\item {} 
\sphinxAtStartPar
Repeat continuously

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{No waiting} \sphinxhyphen{} CPU is constantly working.


\subsubsection{I/O\sphinxhyphen{}bound Operations}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:i-o-bound-operations}}
\sphinxAtStartPar
\sphinxstylestrong{Definition}: Operations where execution time is determined by waiting for input/output.

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}:

\sphinxAtStartPar
CPU Usage:  \#\_\_\_\_\_\_\_\#\_\_\_\_\_\_\_\# (sporadic, mostly idle)
I/O Wait:   \_\#\#\#\#\#\#\#\#\_\#\#\#\#\#\#\#\# (most of the time)
Bottleneck: Waiting for external resources

\sphinxAtStartPar
Example Timeline:
0ms   \textendash{}{[}\textgreater{}{]} Request \textendash{}{[}\textgreater{}{]} Waiting… \textendash{}{[}\textgreater{}{]} Waiting… \textendash{}{[}\textgreater{}{]} Response \textendash{}{[}\textgreater{}{]} Process
\begin{quote}
\begin{description}
\sphinxlineitem{(CPU)        (I/O device)   (I/O device)   (network)   (CPU)}
\sphinxAtStartPar
100mus           50ms           50ms         100ms      1ms

\end{description}
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{What} \PYG{n}{happens}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
CPU initiates I/O request (network/disk)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{CPU sits idle waiting} for response

\item {} 
\sphinxAtStartPar
I/O device/network does the work

\item {} 
\sphinxAtStartPar
Response arrives

\item {} 
\sphinxAtStartPar
CPU processes the response (brief)

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Lots of waiting} \sphinxhyphen{} CPU is idle most of the time.


\subsubsection{Real\sphinxhyphen{}World Analogy}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:real-world-analogy}}
\sphinxAtStartPar
\sphinxstylestrong{CPU\sphinxhyphen{}bound} (Computing factorial):
\begin{description}
\sphinxlineitem{You: Calculate 1000! in your head}
\sphinxAtStartPar
+\sphinxhyphen{}{[}\textgreater{}{]} You must think continuously
+\sphinxhyphen{}{[}\textgreater{}{]} Cannot do anything else while thinking
+\sphinxhyphen{}{[}\textgreater{}{]} Limited by your brain’s processing speed

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{I/O\sphinxhyphen{}bound} (Ordering pizza):
.. code\sphinxhyphen{}block:: text
\begin{description}
\sphinxlineitem{You: Call pizza place \sphinxhyphen{}\textgreater{} Wait 30 min \sphinxhyphen{}\textgreater{} Receive pizza}
\sphinxAtStartPar
+\sphinxhyphen{}{[}\textgreater{}{]} Phone call: 1 minute (active)
+\sphinxhyphen{}{[}\textgreater{}{]} Waiting: 29 minutes (idle \sphinxhyphen{} can do other things!)
+\sphinxhyphen{}{[}\textgreater{}{]} Receive: 1 minute (active)
+\sphinxhyphen{}{[}\textgreater{}{]} Limited by pizza shop’s speed, not yours

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Why Multiprocessing for CPU\sphinxhyphen{}bound}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:id3}}

\subsubsection{The Problem with Threading for CPU\sphinxhyphen{}bound}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:the-problem-with-threading-for-cpu-bound}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{CPU\sphinxhyphen{}intensive task with threading}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:cpu-intensive-task-with-threading}}
\sphinxAtStartPar
import threading
import time
\begin{description}
\sphinxlineitem{def cpu\_work():}
\sphinxAtStartPar
total = sum(i\_i for i in range(10*000*000))
return total

\end{description}


\section{Sequential}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:sequential}}
\sphinxAtStartPar
start = time.perf\_counter()
cpu\_work()
cpu\_work()
print(f”Sequential: \{time.perf\_counter() \sphinxhyphen{} start:.2f\}s”)
Output: Sequential: 2.50s
=========================


\section{Threading (SAME TIME OR WORSE!)}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:threading-same-time-or-worse}}
\sphinxAtStartPar
start = time.perf\_counter()
t1 = threading.Thread(target=cpu\_work)
t2 = threading.Thread(target=cpu\_work)
t1.start(); t2.start()
t1.join(); t2.join()
print(f”Threading: \{time.perf\_counter() \sphinxhyphen{} start:.2f\}s”)
Output: Threading: 2.55s (no improvement!)
==========================================

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why no speedup?**
\end{sphinxVerbatim}

\sphinxAtStartPar
With GIL (Threading):
Core 1: {[}Thread 1{]}{[}Thread 2{]}{[}Thread 1{]}{[}Thread 2{]}{[}Thread 1{]}{[}Thread 2{]}
Core 2: {[}idle…………………………………………….{]}
Core 3: {[}idle…………………………………………….{]}
Core 4: {[}idle…………………………………………….{]}
Time:   =============================================================================================================={[}\textgreater{}{]}

\sphinxAtStartPar
Result: Only using 1 core, taking turns due to GIL

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{CPU\sphinxhyphen{}intensive task with multiprocessing}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:cpu-intensive-task-with-multiprocessing}}
\sphinxAtStartPar
from concurrent.futures import ProcessPoolExecutor
import time
\begin{description}
\sphinxlineitem{def cpu\_work():}
\sphinxAtStartPar
return sum(i\_i for i in range(10*000*000))

\end{description}


\section{Multiprocessing}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:multiprocessing}}
\sphinxAtStartPar
start = time.perf\_counter()
with ProcessPoolExecutor(max\_workers=2) as executor:
\begin{quote}

\sphinxAtStartPar
futures = {[}executor.submit(cpu\_work) for * in range(2){]}
results = {[}f.result() for f in futures{]}
\end{quote}

\sphinxAtStartPar
print(f”Multiprocessing: \{time.perf\_counter() \sphinxhyphen{} start:.2f\}s”)
Output: Multiprocessing: 1.30s (nearly 2x speedup!)
===================================================

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Why} \PYG{n}{it} \PYG{n}{works}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Without GIL (Multiprocessing):
Process 1 on Core 1: {[}\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#{]} Complete
Process 2 on Core 2: {[}\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#{]} Complete
Process 3 on Core 3: {[}idle{]}
Process 4 on Core 4: {[}idle{]}
Time:                =================================================={[}\textgreater{}{]}

\sphinxAtStartPar
Result: Using 2 cores in TRUE parallel execution

\sphinxAtStartPar
Each process has:
\sphinxhyphen{} \sphinxstylestrong{Its own Python interpreter}
\sphinxhyphen{} \sphinxstylestrong{Its own GIL} (doesn’t interfere with other processes)
\sphinxhyphen{} \sphinxstylestrong{Its own memory space}
\sphinxhyphen{} \sphinxstylestrong{Its own process ID}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
|   Process 1      |  |   Process 2      |  |   Process 3      |
|                  |  |                  |  |                  |
|  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |
|  | GIL \PYGZsh{}1     |  |  |  | GIL \PYGZsh{}2     |  |  |  | GIL \PYGZsh{}3     |  |
|  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |
|  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |
|  | Interpreter|  |  |  | Interpreter|  |  |  | Interpreter|  |
|  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |
|  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |
|  |   Memory   |  |  |  |   Memory   |  |  |  |   Memory   |  |
|  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |  |  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  |
+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
CPU Core 1           CPU Core 2           CPU Core 3
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Advantages}:
\sphinxhyphen{} {[}{[}OK{]}{]} True parallelism \sphinxhyphen{} uses multiple CPU cores
\sphinxhyphen{} {[}{[}OK{]}{]} No GIL interference between processes
\sphinxhyphen{} {[}{[}OK{]}{]} Process isolation (crash in one doesn’t affect others)
\sphinxhyphen{} {[}{[}OK{]}{]} Can achieve near\sphinxhyphen{}linear speedup for CPU\sphinxhyphen{}bound tasks

\sphinxAtStartPar
\sphinxstylestrong{Disadvantages}:
\sphinxhyphen{} {[}{[}FAIL{]}{]} Higher memory usage (each process has full Python interpreter)
\sphinxhyphen{} {[}{[}FAIL{]}{]} Slower startup time (creating processes is expensive)
\sphinxhyphen{} {[}{[}FAIL{]}{]} Inter\sphinxhyphen{}process communication is complex and slow
\sphinxhyphen{} {[}{[}FAIL{]}{]} Cannot share memory directly (must pickle/unpickle data)

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Memory cost per process: \textasciitilde{}10\sphinxhyphen{}50 MB
Speedup for CPU\sphinxhyphen{}bound tasks: 2\sphinxhyphen{}8x (depending on cores)

\sphinxAtStartPar
Example:
Task: Process 1000 images (CPU\sphinxhyphen{}intensive)
{\color{red}\bfseries{}|}\sphinxhyphen{} Sequential: 100 seconds
{\color{red}\bfseries{}|}\sphinxhyphen{} Threading: 100 seconds (no improvement due to GIL)
+\sphinxhyphen{} Multiprocessing (4 cores): 27 seconds
\begin{quote}

\sphinxAtStartPar
+\sphinxhyphen{} Memory cost: 150 MB vs 50 MB
+\sphinxhyphen{} Worth it? YES! (3.7x speedup for 100MB extra)
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Why Threading for I/O\sphinxhyphen{}bound}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:id8}}

\subsubsection{The Problem: Wasted Time}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:the-problem-wasted-time}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Sequential I/O operations}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:sequential-i-o-operations}}
\sphinxAtStartPar
import requests
import time

\sphinxAtStartPar
urls = {[}’\sphinxurl{https://api1.com}’, ‘\sphinxurl{https://api2.com}’, ‘\sphinxurl{https://api3.com}’{]}

\sphinxAtStartPar
start = time.perf\_counter()
for url in urls:
\begin{quote}

\sphinxAtStartPar
response = requests.get(url)  \# Takes 2 seconds each
process(response)
\end{quote}

\sphinxAtStartPar
print(f”Sequential: \{time.perf\_counter() \sphinxhyphen{} start:.2f\}s”)
Output: Sequential: 6.00s (2s + 2s + 2s)
========================================

\sphinxAtStartPar
Timeline:
0s    2s    4s    6s
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}+\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}+\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
{\color{red}\bfseries{}|Wait1|Wait2|Wait3|}  \textless{}\sphinxhyphen{} CPU is IDLE during all this time!

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Threaded I/O operations}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:threaded-i-o-operations}}
\sphinxAtStartPar
import threading
import requests
import time
\begin{description}
\sphinxlineitem{def fetch(url):}
\sphinxAtStartPar
response = requests.get(url)
process(response)

\end{description}

\sphinxAtStartPar
urls = {[}’\sphinxurl{https://api1.com}’, ‘\sphinxurl{https://api2.com}’, ‘\sphinxurl{https://api3.com}’{]}

\sphinxAtStartPar
start = time.perf\_counter()
threads = {[}threading.Thread(target=fetch, args=(url,)) for url in urls{]}
for t in threads: t.start()
for t in threads: t.join()
print(f”Threading: \{time.perf\_counter() \sphinxhyphen{} start:.2f\}s”)
Output: Threading: 2.05s (all wait in parallel!)
================================================

\sphinxAtStartPar
Timeline:
0s    2s
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
{\color{red}\bfseries{}|Wait1|}  \textless{}\sphinxhyphen{} Thread 1
{\color{red}\bfseries{}|Wait2|}  \textless{}\sphinxhyphen{} Thread 2
{\color{red}\bfseries{}|Wait3|}  \textless{}\sphinxhyphen{} Thread 3 (all waiting simultaneously)

\sphinxAtStartPar
\sphinxstylestrong{The GIL is Released During I/O Operations!}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{What happens under the hood:}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:what-happens-under-the-hood}}\begin{description}
\sphinxlineitem{Thread 1: {[}Acquire GIL{]} \sphinxhyphen{}\textgreater{} Start network request \sphinxhyphen{}\textgreater{} {[}Release GIL{]} \sphinxhyphen{}\textgreater{} Wait…}
\sphinxAtStartPar
down

\sphinxlineitem{Thread 2:                    {[}Acquire GIL{]} \sphinxhyphen{}\textgreater{} Start disk read \sphinxhyphen{}\textgreater{} {[}Release GIL{]}}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
Thread 3:                                      {[}Acquire GIL{]} \sphinxhyphen{}\textgreater{} Start DB query

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Key Insight**: While Thread 1 waits for network I/O, Threads 2 and 3 can start their I/O operations. All three are waiting simultaneously!
\end{sphinxVerbatim}

\sphinxAtStartPar
When Python releases the GIL during I/O:
\begin{description}
\sphinxlineitem{Python Thread          Operating System           I/O Device}
\begin{DUlineblock}{0em}
\item[] |                        |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{} Read file \sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\textgreater{}|}                        |
| {[}Release GIL{]}          {\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{} Send request \sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\textgreater{}|}
|                        |                        |
| (Thread sleeps)        |                     (Working)
|                        |                        |
|                        {\color{red}\bfseries{}|\textless{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{} Data ready \sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
{\color{red}\bfseries{}|\textless{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{} Data ready \sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}                        |
| {[}Acquire GIL{]}          |                        |
{\color{red}\bfseries{}|}— Process data        |                        |

\end{description}

\sphinxAtStartPar
The OS handles I/O asynchronously while the thread waits, allowing other threads to work.

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Real example: Downloading 10 web pages}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:real-example-downloading-10-web-pages}}
\sphinxAtStartPar
Sequential (1 thread):
{\color{red}\bfseries{}|}\sphinxhyphen{} Page 1: 0.5s
{\color{red}\bfseries{}|}\sphinxhyphen{} Page 2: 0.5s
{\color{red}\bfseries{}|}\sphinxhyphen{} …
+\sphinxhyphen{} Page 10: 0.5s
Total: 5.0 seconds

\sphinxAtStartPar
Threading (10 threads):
{\color{red}\bfseries{}|}\sphinxhyphen{} All pages: 0.5s (in parallel)
+\sphinxhyphen{} Total: 0.5 seconds

\sphinxAtStartPar
Speedup: 10x! (near\sphinxhyphen{}perfect for I/O\sphinxhyphen{}bound)

\sphinxAtStartPar
Threading for I/O:
{\color{red}\bfseries{}|}\sphinxhyphen{} Memory: 50 MB (one process, 10 threads)
{\color{red}\bfseries{}|}\sphinxhyphen{} Startup: Instant (threads are lightweight)
{\color{red}\bfseries{}|}\sphinxhyphen{} Communication: Shared memory (easy)
+\sphinxhyphen{} Performance: 10x speedup

\sphinxAtStartPar
Multiprocessing for I/O:
{\color{red}\bfseries{}|}\sphinxhyphen{} Memory: 500 MB (10 processes x 50 MB each)
{\color{red}\bfseries{}|}\sphinxhyphen{} Startup: Slow (creating 10 processes)
{\color{red}\bfseries{}|}\sphinxhyphen{} Communication: IPC (complex, slow)
+\sphinxhyphen{} Performance: 10x speedup (same as threading!)

\sphinxAtStartPar
Verdict: Threading is MORE EFFICIENT for I/O
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
\sphinxstylestrong{Advantages}:
\sphinxhyphen{} {[}{[}OK{]}{]} Lightweight (minimal memory overhead)
\sphinxhyphen{} {[}{[}OK{]}{]} Fast to create/destroy
\sphinxhyphen{} {[}{[}OK{]}{]} Easy data sharing (shared memory)
\sphinxhyphen{} {[}{[}OK{]}{]} Perfect for I/O\sphinxhyphen{}bound tasks

\sphinxAtStartPar
\sphinxstylestrong{Disadvantages}:
\sphinxhyphen{} {[}{[}FAIL{]}{]} No speedup for CPU\sphinxhyphen{}bound tasks (GIL)
\sphinxhyphen{} {[}{[}FAIL{]}{]} Race conditions possible with shared state
\sphinxhyphen{} {[}{[}FAIL{]}{]} More complex debugging
\sphinxhyphen{} {[}{[}FAIL{]}{]} Limited by GIL for Python code execution

\sphinxAtStartPar
—


\subsection{Why Asyncio for I/O\sphinxhyphen{}bound}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:id31}}

\subsubsection{The Problem with Threading: Overhead}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:the-problem-with-threading-overhead}}
\sphinxAtStartPar
Creating 10,000 threads:
{\color{red}\bfseries{}|}\sphinxhyphen{} Memory: 10,000 threads x 8 MB stack = 80 GB! (impossible)
{\color{red}\bfseries{}|}\sphinxhyphen{} Context switching: OS must switch between 10,000 threads
{\color{red}\bfseries{}|}\sphinxhyphen{} Overhead: Significant CPU time spent on thread management
+\sphinxhyphen{} Result: System becomes unresponsive

\sphinxAtStartPar
Creating 10,000 asyncio tasks:
{\color{red}\bfseries{}|}\sphinxhyphen{} Memory: \textasciitilde{}10\sphinxhyphen{}100 MB total (tasks are lightweight)
{\color{red}\bfseries{}|}\sphinxhyphen{} Context switching: Controlled by Python (no OS involvement)
{\color{red}\bfseries{}|}\sphinxhyphen{} Overhead: Minimal
+\sphinxhyphen{} Result: Efficient and responsive


\subsubsection{Asyncio: Cooperative Multitasking}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:asyncio-cooperative-multitasking}}
\sphinxAtStartPar
\sphinxstylestrong{Threading} (Preemptive \sphinxhyphen{} OS decides when to switch):

\sphinxAtStartPar
OS: “Thread 1, you’ve used enough CPU, I’m switching to Thread 2”
Thread 1: “But I’m not done!”
OS: “Too bad, Thread 2’s turn now”
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
\sphinxstylestrong{Asyncio} (Cooperative \sphinxhyphen{} code decides when to yield):

\sphinxAtStartPar
Task 1: “I’m about to wait for network, let me yield control”
Event Loop: “Thanks! I’ll run Task 2 now”
Task 2: “I’m about to wait for disk, let me yield”
Event Loop: “Got it! I’ll check if Task 1’s network response arrived”


\subsubsection{How Asyncio Works}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:how-asyncio-works}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Asyncio example: 10,000 concurrent requests}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:asyncio-example-10-000-concurrent-requests}}
\sphinxAtStartPar
import asyncio
import aiohttp
\begin{description}
\sphinxlineitem{async def fetch(session, url):}\begin{description}
\sphinxlineitem{async with session.get(url) as response:}
\sphinxAtStartPar
return await response.text()

\end{description}

\sphinxlineitem{async def main():}
\sphinxAtStartPar
urls = {[}f’\sphinxurl{https://api.example.com/item}/\{i\}’ for i in range(10000){]}
async with aiohttp.ClientSession() as session:
\begin{quote}

\sphinxAtStartPar
tasks = {[}fetch(session, url) for url in urls{]}
results = await asyncio.gather({\color{red}\bfseries{}*}tasks)
\end{quote}

\end{description}

\sphinxAtStartPar
asyncio.run(main())
Can handle 10,000 requests efficiently!
=======================================

\sphinxAtStartPar
Event Loop (Single Thread):
+———————————————————+
|                                                         |
|  Ready Queue: {[}Task 1, Task 5, Task 12, …{]}          |
|                                                         |
|  Waiting for I/O: \{Task 2: socket 1,                   |
|                    Task 3: socket 2,                   |
|                    Task 4: socket 3, …\}              |
|                                                         |
|  Flow:                                                  |
|  1. Get next ready task                                |
|  2. Run until it awaits something                      |
|  3. Check which I/O operations completed               |
|  4. Move completed tasks to ready queue                |
|  5. Repeat                                             |
|                                                         |
+———————————————————+

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Threading approach}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:threading-approach}}
\sphinxAtStartPar
import threading
import requests
\begin{description}
\sphinxlineitem{def fetch\_url(url):}
\sphinxAtStartPar
response = requests.get(url)
return response.text

\end{description}

\sphinxAtStartPar
urls = {[}f’\sphinxurl{https://api.example.com}/\{i\}’ for i in range(1000){]}
threads = {[}threading.Thread(target=fetch\_url, args=(url,)) for url in urls{]}


\section{Problem: Creating 1000 threads!}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:problem-creating-1000-threads}}

\section{Memory: \textasciitilde{}8 GB (1000 x 8 MB stack per thread)}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:memory-8-gb-1000-x-8-mb-stack-per-thread}}

\section{OS overhead: Managing 1000 threads}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:os-overhead-managing-1000-threads}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Asyncio approach}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:asyncio-approach}}
\sphinxAtStartPar
import asyncio
import aiohttp
\begin{description}
\sphinxlineitem{async def fetch\_url(session, url):}\begin{description}
\sphinxlineitem{async with session.get(url) as response:}
\sphinxAtStartPar
return await response.text()

\end{description}

\sphinxlineitem{async def main():}
\sphinxAtStartPar
urls = {[}f’\sphinxurl{https://api.example.com}/\{i\}’ for i in range(1000){]}
async with aiohttp.ClientSession() as session:
\begin{quote}

\sphinxAtStartPar
tasks = {[}fetch\_url(session, url) for url in urls{]}
results = await asyncio.gather({\color{red}\bfseries{}*}tasks)
\end{quote}

\end{description}

\sphinxAtStartPar
asyncio.run(main())


\section{Solution: Single thread, 1000 lightweight tasks}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:solution-single-thread-1000-lightweight-tasks}}

\section{Memory: \textasciitilde{}50 MB total}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:memory-50-mb-total}}

\section{OS overhead: None (all managed by Python)}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:os-overhead-none-all-managed-by-python}}
\begin{DUlineblock}{0em}
\item[] Metric | Threading (1000 ops) | Asyncio (1000 ops) |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———————{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| Memory Usage | \textasciitilde{}8 GB | \textasciitilde{}50 MB |
| Context Switch | OS\sphinxhyphen{}level (slow) | Python\sphinxhyphen{}level (fast) |
| Scalability | \textasciitilde{}1000s | \textasciitilde{}100,000s |
| Startup Time | Slow (create threads) | Fast (create tasks) |
| CPU Overhead | High (OS scheduling) | Low (event loop) |

\sphinxAtStartPar
\sphinxstylestrong{Perfect for}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{p}{[}\PYG{n}{OK}\PYG{p}{]}\PYG{p}{]} \PYG{n}{Web} \PYG{n}{servers} \PYG{p}{(}\PYG{n}{handle} \PYG{n}{many} \PYG{n}{simultaneous} \PYG{n}{connections}\PYG{p}{)}
\PYG{p}{[}\PYG{p}{[}\PYG{n}{OK}\PYG{p}{]}\PYG{p}{]} \PYG{n}{Web} \PYG{n}{scraping} \PYG{p}{(}\PYG{n}{thousands} \PYG{n}{of} \PYG{n}{HTTP} \PYG{n}{requests}\PYG{p}{)}
\PYG{p}{[}\PYG{p}{[}\PYG{n}{OK}\PYG{p}{]}\PYG{p}{]} \PYG{n}{Database} \PYG{n}{queries} \PYG{p}{(}\PYG{n}{many} \PYG{n}{concurrent} \PYG{n}{queries}\PYG{p}{)}
\PYG{p}{[}\PYG{p}{[}\PYG{n}{OK}\PYG{p}{]}\PYG{p}{]} \PYG{n}{Microservices} \PYG{p}{(}\PYG{n}{coordinating} \PYG{n}{many} \PYG{n}{API} \PYG{n}{calls}\PYG{p}{)}
\PYG{p}{[}\PYG{p}{[}\PYG{n}{OK}\PYG{p}{]}\PYG{p}{]} \PYG{n}{Chat} \PYG{n}{applications} \PYG{p}{(}\PYG{n}{many} \PYG{n}{idle} \PYG{n}{connections}\PYG{p}{)}
\PYG{p}{[}\PYG{p}{[}\PYG{n}{OK}\PYG{p}{]}\PYG{p}{]} \PYG{n}{IoT} \PYG{n}{systems} \PYG{p}{(}\PYG{n}{many} \PYG{n}{devices} \PYG{n}{sending} \PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Not} \PYG{n}{ideal} \PYG{k}{for}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
{[}{[}FAIL{]}{]} CPU\sphinxhyphen{}intensive tasks (use multiprocessing)
{[}{[}FAIL{]}{]} Blocking libraries (must use async\sphinxhyphen{}compatible libraries)
{[}{[}FAIL{]}{]} Simple scripts with few I/O operations (threading is simpler)
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
\sphinxstylestrong{Advantages}:
\sphinxhyphen{} {[}{[}OK{]}{]} Extremely lightweight (handle 100,000+ concurrent operations)
\sphinxhyphen{} {[}{[}OK{]}{]} Low memory overhead
\sphinxhyphen{} {[}{[}OK{]}{]} Fast context switching (Python\sphinxhyphen{}level)
\sphinxhyphen{} {[}{[}OK{]}{]} Single\sphinxhyphen{}threaded (no race conditions)
\sphinxhyphen{} {[}{[}OK{]}{]} Explicit concurrency (clear control flow with \sphinxcode{\sphinxupquote{await}})

\sphinxAtStartPar
\sphinxstylestrong{Disadvantages}:
\sphinxhyphen{} {[}{[}FAIL{]}{]} Requires async\sphinxhyphen{}compatible libraries (can’t use standard \sphinxcode{\sphinxupquote{requests}}, etc.)
\sphinxhyphen{} {[}{[}FAIL{]}{]} Learning curve (async/await paradigm)
\sphinxhyphen{} {[}{[}FAIL{]}{]} Viral nature (once you go async, everything must be async)
\sphinxhyphen{} {[}{[}FAIL{]}{]} No speedup for CPU\sphinxhyphen{}bound tasks
\sphinxhyphen{} {[}{[}FAIL{]}{]} One blocking operation blocks everything

\sphinxAtStartPar
—


\subsection{Deep Dive: What Happens Under the Hood}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:id48}}

\subsubsection{CPU\sphinxhyphen{}bound with Threading: The GIL Dance}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:cpu-bound-with-threading-the-gil-dance}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Example: Two threads computing sum}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:example-two-threads-computing-sum}}
\sphinxAtStartPar
import threading
\begin{description}
\sphinxlineitem{def compute\_sum(n):}
\sphinxAtStartPar
total = 0
for i in range(n):
\begin{quote}

\sphinxAtStartPar
total += i
\end{quote}

\sphinxAtStartPar
return total

\end{description}

\sphinxAtStartPar
t1 = threading.Thread(target=compute\_sum, args=(10*000*000,))
t2 = threading.Thread(target=compute\_sum, args=(10*000*000,))

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{What} \PYG{n}{actually} \PYG{n}{happens}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Time \sphinxhyphen{}\textgreater{}
0ms    Thread 1: {[}Acquire GIL{]}
1ms              : Execute: total = 0
2ms              : Execute: total += 1
3ms              : Execute: total += 2
…
100ms            : {[}Release GIL{]} (every \textasciitilde{}100 bytecodes or 5ms)
100ms  Thread 2:                 {[}Acquire GIL{]}
101ms            :                 Execute: total = 0
102ms            :                 Execute: total += 1
…
200ms            :                 {[}Release GIL{]}
200ms  Thread 1: {[}Acquire GIL{]}
…
(continues alternating)

\sphinxAtStartPar
Result: Threads take turns executing Python bytecode
No parallelism for CPU work!
.. code\sphinxhyphen{}block:: text

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from concurrent.futures import ProcessPoolExecutor
\begin{description}
\sphinxlineitem{def compute\_sum(n):}
\sphinxAtStartPar
total = 0
for i in range(n):
\begin{quote}

\sphinxAtStartPar
total += i
\end{quote}

\sphinxAtStartPar
return total

\sphinxlineitem{with ProcessPoolExecutor(max\_workers=2) as executor:}
\sphinxAtStartPar
future1 = executor.submit(compute\_sum, 10*000*000)
future2 = executor.submit(compute\_sum, 10*000*000)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{What} \PYG{n}{actually} \PYG{n}{happens}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Time \sphinxhyphen{}\textgreater{}
0ms    Process 1 (Core 1): {[}Start{]} Create interpreter, load code
10ms                       : Execute: total = 0
11ms                       : Execute: total += 1
12ms                       : Execute: total += 2
…
1000ms                     : {[}Done{]} Return result via pipe

\sphinxAtStartPar
0ms    Process 2 (Core 2): {[}Start{]} Create interpreter, load code
10ms                       : Execute: total = 0
11ms                       : Execute: total += 1
12ms                       : Execute: total += 2
…
1000ms                     : {[}Done{]} Return result via pipe

\sphinxAtStartPar
Result: Both processes execute SIMULTANEOUSLY on different cores
True parallelism!
.. code\sphinxhyphen{}block:: text

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import threading
import requests
\begin{description}
\sphinxlineitem{def fetch\_url(url):}
\sphinxAtStartPar
response = requests.get(url)  \# I/O operation
return response.text

\end{description}

\sphinxAtStartPar
t1 = threading.Thread(target=fetch\_url, args=(’\sphinxurl{https://api1.com}’,))
t2 = threading.Thread(target=fetch\_url, args=(’\sphinxurl{https://api2.com}’,))

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{What} \PYG{n}{actually} \PYG{n}{happens}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Time \sphinxhyphen{}\textgreater{}
0ms    Thread 1: {[}Acquire GIL{]}
1ms             : Prepare HTTP request
2ms             : {[}Release GIL{]} \textless{}\sphinxhyphen{} Call to C library (requests)
2ms             : {[}OS: Send network packet{]}
3ms             : {[}OS: Waiting for response…{]}

\sphinxAtStartPar
2ms    Thread 2:                {[}Acquire GIL{]} \textless{}\sphinxhyphen{} Can run while T1 waits!
3ms             :                Prepare HTTP request
4ms             :                {[}Release GIL{]} \textless{}\sphinxhyphen{} Call to C library
4ms             :                {[}OS: Send network packet{]}
5ms             :                {[}OS: Waiting for response…{]}

\sphinxAtStartPar
200ms           : {[}OS: T1’s response arrives{]}
200ms  Thread 1: {[}Acquire GIL{]}
201ms           : Process response
202ms           : {[}Done{]}

\sphinxAtStartPar
210ms           : {[}OS: T2’s response arrives{]}
210ms  Thread 2: {[}Acquire GIL{]}
211ms           : Process response
212ms           : {[}Done{]}

\sphinxAtStartPar
Result: Both threads waited concurrently (overlapped I/O)
Total time: \textasciitilde{}210ms instead of 400ms sequential
.. code\sphinxhyphen{}block:: text

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import asyncio
import aiohttp
\begin{description}
\sphinxlineitem{async def fetch\_url(session, url):}\begin{description}
\sphinxlineitem{async with session.get(url) as response:}
\sphinxAtStartPar
return await response.text()

\end{description}

\sphinxlineitem{async def main():}\begin{description}
\sphinxlineitem{async with aiohttp.ClientSession() as session:}
\sphinxAtStartPar
task1 = asyncio.create\_task(fetch\_url(session, ‘\sphinxurl{https://api1.com}’))
task2 = asyncio.create\_task(fetch\_url(session, ‘\sphinxurl{https://api2.com}’))
results = await asyncio.gather(task1, task2)

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{What} \PYG{n}{actually} \PYG{n}{happens}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Time \sphinxhyphen{}\textgreater{} (Single Thread)
0ms    Event Loop: Create task1
1ms              : Create task2
2ms              : Run task1 until await
3ms    Task 1   : Send HTTP request
4ms              : await response.get() \textless{}\sphinxhyphen{} Yields control
4ms    Event Loop: task1 waiting for I/O, switch to task2
5ms    Task 2   : Send HTTP request
6ms              : await response.get() \textless{}\sphinxhyphen{} Yields control
6ms    Event Loop: Both tasks waiting, check I/O status
…
200ms  Event Loop: task1’s I/O completed
200ms  Task 1   : Process response
201ms            : Return result
201ms  Event Loop: task1 done, check task2
210ms  Event Loop: task2’s I/O completed
210ms  Task 2   : Process response
211ms            : Return result
212ms  Event Loop: Both tasks done, gather returns

\sphinxAtStartPar
Result: Single thread efficiently managing multiple I/O operations
No thread overhead, same concurrency benefit
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
—


\subsection{Performance Analysis}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:id49}}

\subsubsection{Benchmark: CPU\sphinxhyphen{}bound Task (Computing pi)}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:benchmark-cpu-bound-task-computing-pi}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def compute\_pi(iterations):}
\sphinxAtStartPar
“””CPU\sphinxhyphen{}intensive: Monte Carlo pi approximation”””
inside = 0
for * in range(iterations):
\begin{quote}

\sphinxAtStartPar
x, y = random.random(), random.random()
if x\_x + y\_y \textless{}= 1:
\begin{quote}

\sphinxAtStartPar
inside += 1
\end{quote}
\end{quote}

\sphinxAtStartPar
return 4 * inside / iterations

\end{description}

\sphinxAtStartPar
ITERATIONS = 10*000*000
TASKS = 4
.. code\sphinxhyphen{}block:: text

\sphinxAtStartPar
\sphinxstylestrong{Results on 4\sphinxhyphen{}core CPU}:

\begin{DUlineblock}{0em}
\item[] Approach | Time | Speedup | Memory |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———|
| Sequential | 10.0s | 1.0x | 50 MB |
| Threading (4 threads) | 10.2s | 0.98x {[}{[}FAIL{]}{]} | 55 MB |
| Asyncio (4 tasks) | 10.1s | 0.99x {[}{[}FAIL{]}{]} | 52 MB |
| Multiprocessing (4 proc) | 2.7s | 3.7x {[}{[}OK{]}{]} | 200 MB |

\sphinxAtStartPar
\sphinxstylestrong{Analysis}:
\sphinxhyphen{} Threading/Asyncio: No improvement (GIL limitation)
\sphinxhyphen{} Multiprocessing: Near\sphinxhyphen{}linear speedup (3.7x on 4 cores)
\sphinxhyphen{} Memory trade\sphinxhyphen{}off is worth it for 3.7x speedup


\subsubsection{Benchmark: I/O\sphinxhyphen{}bound Task (Web Requests)}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:benchmark-i-o-bound-task-web-requests}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def fetch\_page(session, url):}
\sphinxAtStartPar
“””I/O\sphinxhyphen{}intensive: Download web page”””
async with session.get(url) as response:
\begin{quote}

\sphinxAtStartPar
return await response.text()
\end{quote}

\end{description}

\sphinxAtStartPar
URLS = 100 (each takes \textasciitilde{}0.5s to fetch)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Results}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{DUlineblock}{0em}
\item[] Approach | Time | Speedup | Memory | Max Concurrent |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| Sequential | 50.0s | 1.0x | 50 MB | 1 |
| Threading (10 threads) | 5.2s | 9.6x {[}{[}OK{]}{]} | 130 MB | 10 |
| Threading (100 threads) | 1.8s | 27.8x {[}{[}OK{]}{]} | 850 MB | 100 |
| Asyncio (100 tasks) | 1.5s | 33.3x {[}{[}OK{]}{]} | 65 MB | 100 |
| Multiprocessing (4 proc) | 13.0s | 3.8x {[}{[}FAIL{]}{]} | 200 MB | 4 |

\sphinxAtStartPar
\sphinxstylestrong{Analysis}:
\sphinxhyphen{} Threading: Good speedup, but memory grows with threads
\sphinxhyphen{} Asyncio: Best speedup with lowest memory
\sphinxhyphen{} Multiprocessing: Poor choice (high overhead, limited concurrency)


\subsubsection{Benchmark: Mixed Workload}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:benchmark-mixed-workload}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{async def process\_data(session, url):}
\sphinxAtStartPar
“””Fetch data (I/O) then process (CPU)”””
\# I/O: Fetch data (2 seconds)
async with session.get(url) as response:
\begin{quote}

\sphinxAtStartPar
data = await response.json()
\end{quote}

\sphinxAtStartPar
\# CPU: Heavy processing (1 second)
result = complex\_computation(data)
return result

\end{description}

\sphinxAtStartPar
TASKS = 10

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Results}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{DUlineblock}{0em}
\item[] Approach | Time | Analysis |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| Sequential | 30.0s | (2s I/O + 1s CPU) x 10 |
| Threading | 15.5s | I/O concurrent, CPU sequential |
| Asyncio | 15.2s | I/O concurrent, CPU sequential |
| Asyncio + ProcessPool | 5.8s | I/O concurrent, CPU parallel {[}{[}OK{]}{]} |

\sphinxAtStartPar
\sphinxstylestrong{Best approach for mixed workload}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
import asyncio
from concurrent.futures import ProcessPoolExecutor
\begin{description}
\sphinxlineitem{async def process\_data(session, url, executor):}
\sphinxAtStartPar
\# I/O: Use asyncio
async with session.get(url) as response:
\begin{quote}

\sphinxAtStartPar
data = await response.json()
\end{quote}

\sphinxAtStartPar
\# CPU: Offload to process pool
loop = asyncio.get\_event*loop()
result = await loop.run\_in*executor(executor, complex\_computation, data)
return result

\sphinxlineitem{async def main():}\begin{description}
\sphinxlineitem{with ProcessPoolExecutor(max\_workers=4) as executor:}\begin{description}
\sphinxlineitem{async with aiohttp.ClientSession() as session:}
\sphinxAtStartPar
tasks = {[}process\_data(session, url, executor) for url in urls{]}
results = await asyncio.gather({\color{red}\bfseries{}*}tasks)

\end{description}

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Decision Tree}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:id52}}
\sphinxAtStartPar
Use this decision tree to choose the right approach:

\sphinxAtStartPar
Start: What type of operation?
|
{\color{red}\bfseries{}|}\sphinxhyphen{} CPU\sphinxhyphen{}bound (math, data processing, compression)
|  |
|  {\color{red}\bfseries{}|}\sphinxhyphen{} How many tasks?
|  |  {\color{red}\bfseries{}|}\sphinxhyphen{} Single task \sphinxhyphen{}\textgreater{} Sequential (simplest)
|  |  +\sphinxhyphen{} Multiple tasks \sphinxhyphen{}\textgreater{} Multiprocessing
|  |
|  +\sphinxhyphen{} How many CPU cores available?
|     {\color{red}\bfseries{}|}\sphinxhyphen{} 1 core \sphinxhyphen{}\textgreater{} Sequential (multiprocessing won’t help)
|     +\sphinxhyphen{} 2+ cores \sphinxhyphen{}\textgreater{} Multiprocessing (use max\_workers = CPU cores)
|
+\sphinxhyphen{} I/O\sphinxhyphen{}bound (network, disk, database)
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|}\sphinxhyphen{} How many concurrent operations?
|  |
|  {\color{red}\bfseries{}|}\sphinxhyphen{} Few (\textless{} 50)
|  |  {\color{red}\bfseries{}|}\sphinxhyphen{} Using blocking libraries? \sphinxhyphen{}\textgreater{} Threading
|  |  +\sphinxhyphen{} Can use async libraries? \sphinxhyphen{}\textgreater{} Asyncio (preferred)
|  |
|  {\color{red}\bfseries{}|}\sphinxhyphen{} Many (50\sphinxhyphen{}1000)
|  |  +\sphinxhyphen{} Asyncio (threading becomes expensive)
|  |
|  +\sphinxhyphen{} Very many (\textgreater{} 1000)
|     +\sphinxhyphen{} Asyncio (threading impossible)
|
+\sphinxhyphen{} Do you need simplicity or scalability?
\begin{quote}

\sphinxAtStartPar
{\color{red}\bfseries{}|}\sphinxhyphen{} Simplicity \sphinxhyphen{}\textgreater{} Threading (easier to understand)
+\sphinxhyphen{} Scalability \sphinxhyphen{}\textgreater{} Asyncio (better performance)
\end{quote}
\end{quote}


\subsubsection{Quick Reference Table}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:quick-reference-table}}
\begin{DUlineblock}{0em}
\item[] Scenario | Solution | Reason |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\sphinxhyphen{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| Image processing (1000 images) | Multiprocessing | CPU\sphinxhyphen{}bound, benefits from parallel cores |
| Web scraping (100 pages) | Asyncio | I/O\sphinxhyphen{}bound, many concurrent connections |
| REST API server | Asyncio | I/O\sphinxhyphen{}bound, handle many simultaneous requests |
| Video encoding | Multiprocessing | CPU\sphinxhyphen{}intensive, utilize all cores |
| Database queries (10 concurrent) | Threading | I/O\sphinxhyphen{}bound, simple implementation |
| Database queries (1000 concurrent) | Asyncio | I/O\sphinxhyphen{}bound, need high concurrency |
| File downloads (5 files) | Threading | I/O\sphinxhyphen{}bound, blocking library OK |
| WebSocket server (10000 clients) | Asyncio | I/O\sphinxhyphen{}bound, need extreme scalability |
| Scientific computation | Multiprocessing | CPU\sphinxhyphen{}intensive calculations |
| Real\sphinxhyphen{}time chat (1000 users) | Asyncio | I/O\sphinxhyphen{}bound, many idle connections |


\subsubsection{Code Templates}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:code-templates}}
\sphinxAtStartPar
\sphinxstylestrong{CPU\sphinxhyphen{}bound Template}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
from concurrent.futures import ProcessPoolExecutor
import os
\begin{description}
\sphinxlineitem{def cpu\_intensive*task(data):}
\sphinxAtStartPar
\# Your CPU\sphinxhyphen{}heavy computation here
result = complex\_calculation(data)
return result

\sphinxlineitem{def main():}
\sphinxAtStartPar
data\_items = {[}…{]}  \# Your data

\sphinxAtStartPar
\# Use number of CPU cores
max\_workers = os.cpu\_count()
\begin{description}
\sphinxlineitem{with ProcessPoolExecutor(max\_workers=max\_workers) as executor:}
\sphinxAtStartPar
results = executor.map(cpu\_intensive*task, data\_items)

\end{description}

\sphinxAtStartPar
return list(results)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{I/O\sphinxhyphen{}bound Template (Few operations, blocking library)}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
from concurrent.futures import ThreadPoolExecutor
import requests
\begin{description}
\sphinxlineitem{def io\_intensive*task(url):}
\sphinxAtStartPar
response = requests.get(url)
return process(response)

\sphinxlineitem{def main():}
\sphinxAtStartPar
urls = {[}…{]}  \# Your URLs
\begin{description}
\sphinxlineitem{with ThreadPoolExecutor(max\_workers=10) as executor:}
\sphinxAtStartPar
results = executor.map(io\_intensive*task, urls)

\end{description}

\sphinxAtStartPar
return list(results)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{I/O\sphinxhyphen{}bound Template (Many operations, async library)}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
import asyncio
import aiohttp
\begin{description}
\sphinxlineitem{async def io\_intensive*task(session, url):}\begin{description}
\sphinxlineitem{async with session.get(url) as response:}
\sphinxAtStartPar
data = await response.text()
return process(data)

\end{description}

\sphinxlineitem{async def main():}
\sphinxAtStartPar
urls = {[}…{]}  \# Your URLs
\begin{description}
\sphinxlineitem{async with aiohttp.ClientSession() as session:}
\sphinxAtStartPar
tasks = {[}io\_intensive*task(session, url) for url in urls{]}
results = await asyncio.gather({\color{red}\bfseries{}*}tasks)

\end{description}

\sphinxAtStartPar
return results

\sphinxlineitem{if \_\_name** == ‘\sphinxstylestrong{main}’:}
\sphinxAtStartPar
asyncio.run(main())

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Mixed} \PYG{n}{Workload} \PYG{n}{Template}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import asyncio
import aiohttp
from concurrent.futures import ProcessPoolExecutor
\begin{description}
\sphinxlineitem{def cpu\_intensive(data):}
\sphinxAtStartPar
\# CPU\sphinxhyphen{}heavy work here
return complex\_calculation(data)

\sphinxlineitem{async def mixed\_task(session, url, executor):}
\sphinxAtStartPar
\# I/O part (async)
async with session.get(url) as response:
\begin{quote}

\sphinxAtStartPar
data = await response.json()
\end{quote}

\sphinxAtStartPar
\# CPU part (process pool)
loop = asyncio.get\_event*loop()
result = await loop.run\_in*executor(executor, cpu\_intensive, data)

\sphinxAtStartPar
return result

\sphinxlineitem{async def main():}
\sphinxAtStartPar
urls = {[}…{]}
\begin{description}
\sphinxlineitem{with ProcessPoolExecutor(max\_workers=4) as executor:}\begin{description}
\sphinxlineitem{async with aiohttp.ClientSession() as session:}
\sphinxAtStartPar
tasks = {[}mixed\_task(session, url, executor) for url in urls{]}
results = await asyncio.gather({\color{red}\bfseries{}*}tasks)

\end{description}

\end{description}

\sphinxAtStartPar
return results

\sphinxlineitem{if \sphinxstylestrong{name} == ‘\sphinxstylestrong{main}’:}
\sphinxAtStartPar
asyncio.run(main())

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Summary: The Core Principles}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:summary-the-core-principles}}

\subsubsection{1. The GIL Controls Everything}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:the-gil-controls-everything}}
\sphinxAtStartPar
Python’s GIL:
{\color{red}\bfseries{}|}\sphinxhyphen{} Allows only ONE thread to execute Python bytecode at a time
{\color{red}\bfseries{}|}\sphinxhyphen{} Released during I/O operations (C library calls)
+\sphinxhyphen{} Not present in separate processes (each has own GIL)

\sphinxAtStartPar
Therefore:
{\color{red}\bfseries{}|}\sphinxhyphen{} CPU\sphinxhyphen{}bound + Threading = No parallelism (GIL bottleneck)
{\color{red}\bfseries{}|}\sphinxhyphen{} I/O\sphinxhyphen{}bound + Threading = Concurrency (GIL released during I/O)
+\sphinxhyphen{} CPU\sphinxhyphen{}bound + Multiprocessing = Parallelism (separate GILs)


\subsubsection{2. Resource Usage Matters}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:resource-usage-matters}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Operation Type  |  Resource Bottleneck  |  Solution  |  Why?
—————\sphinxhyphen{}+———————\sphinxhyphen{}+————+——————
CPU\sphinxhyphen{}bound       |  CPU cycles          |  Multi\sphinxhyphen{}    |  Bypass GIL,
\begin{quote}

\begin{DUlineblock}{0em}
\item[] (computation)       |  processing|  use all cores
\end{DUlineblock}
\end{quote}

\sphinxAtStartPar
—————\sphinxhyphen{}+———————\sphinxhyphen{}+————+——————
I/O\sphinxhyphen{}bound       |  Waiting for I/O     |  Asyncio/  |  GIL released,
(few ops)       |  (network, disk)     |  Threading |  work during wait
—————\sphinxhyphen{}+———————\sphinxhyphen{}+————+——————
I/O\sphinxhyphen{}bound       |  Waiting for I/O     |  Asyncio   |  Lightweight,
(many ops)      |  + scalability       |            |  handles 1000s
.. code\sphinxhyphen{}block:: text


\subsubsection{3. Trade\sphinxhyphen{}offs are Real}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:trade-offs-are-real}}
\sphinxAtStartPar
\sphinxstylestrong{Multiprocessing}:
\sphinxhyphen{} Pros: True parallelism, bypasses GIL
\sphinxhyphen{} Cons: Memory overhead, slow startup, complex IPC
\sphinxhyphen{} \sphinxstylestrong{Use when}: CPU\sphinxhyphen{}bound work benefits \textgreater{} memory cost

\sphinxAtStartPar
\sphinxstylestrong{Threading}:
\sphinxhyphen{} Pros: Lightweight, easy data sharing, fast startup
\sphinxhyphen{} Cons: No speedup for CPU work, race conditions possible
\sphinxhyphen{} \sphinxstylestrong{Use when}: I/O\sphinxhyphen{}bound with moderate concurrency

\sphinxAtStartPar
\sphinxstylestrong{Asyncio}:
\sphinxhyphen{} Pros: Extremely lightweight, handles 100,000+ operations
\sphinxhyphen{} Cons: Requires async libraries, learning curve
\sphinxhyphen{} \sphinxstylestrong{Use when}: I/O\sphinxhyphen{}bound with high concurrency


\subsubsection{4. Know Your Workload}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:know-your-workload}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Profile your code first!}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:profile-your-code-first}}
\sphinxAtStartPar
import time
\begin{description}
\sphinxlineitem{def profile\_task(task\_func):}
\sphinxAtStartPar
\# CPU time (actual processing)
cpu\_start = time.process\_time()
\# Wall time (including waiting)
wall\_start = time.perf\_counter()

\sphinxAtStartPar
result = task\_func()

\sphinxAtStartPar
cpu\_time = time.process\_time() \sphinxhyphen{} cpu\_start
wall\_time = time.perf\_counter() \sphinxhyphen{} wall\_start
\begin{description}
\sphinxlineitem{if cpu\_time / wall\_time \textgreater{} 0.8:}
\sphinxAtStartPar
print(“CPU\sphinxhyphen{}bound \sphinxhyphen{}\textgreater{} Use multiprocessing”)

\sphinxlineitem{else:}
\sphinxAtStartPar
print(“I/O\sphinxhyphen{}bound \sphinxhyphen{}\textgreater{} Use asyncio/threading”)

\end{description}

\sphinxAtStartPar
return result

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
—


\subsection{Final Thoughts}
\label{\detokenize{cpu-concurrency/patterns_problems_mapping:final-thoughts}}
\sphinxAtStartPar
The choice between multiprocessing, threading, and asyncio isn’t about which is “better” \sphinxhyphen{} it’s about matching the tool to the task:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multiprocessing}: Powerful but heavy. Use when you need true parallel CPU computation.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Threading}: Simple and effective for I/O. Use when blocking libraries are needed.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Asyncio}: Lightweight and scalable for I/O. Use when you need to handle many concurrent operations.

\end{itemize}

\sphinxAtStartPar
Understanding the GIL and how Python interacts with the OS is key to making the right choice. Always profile your code, measure the results, and choose based on your specific requirements.

\sphinxstepscope


\section{GPU Fundamentals}
\label{\detokenize{gpu-concepts/gpu-fundamentals:gpu-fundamentals}}\label{\detokenize{gpu-concepts/gpu-fundamentals::doc}}

\subsection{Understanding GPU Architecture}
\label{\detokenize{gpu-concepts/gpu-fundamentals:understanding-gpu-architecture}}
\sphinxAtStartPar
Modern GPUs are massively parallel processors designed to handle thousands of concurrent operations.
Unlike CPUs that optimize for sequential processing with a few powerful cores, GPUs have hundreds
or thousands of smaller cores optimized for parallel workloads.


\subsubsection{Key Differences: CPU vs GPU}
\label{\detokenize{gpu-concepts/gpu-fundamentals:key-differences-cpu-vs-gpu}}
\sphinxAtStartPar
\sphinxstylestrong{CPU Architecture}
\begin{itemize}
\item {} 
\sphinxAtStartPar
4\sphinxhyphen{}64 powerful cores

\item {} 
\sphinxAtStartPar
Large caches (MB per core)

\item {} 
\sphinxAtStartPar
Optimized for latency (single\sphinxhyphen{}thread performance)

\item {} 
\sphinxAtStartPar
Complex control logic and branch prediction

\item {} 
\sphinxAtStartPar
Best for: Sequential tasks, complex logic

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{GPU Architecture}
\begin{itemize}
\item {} 
\sphinxAtStartPar
1000s of simple cores

\item {} 
\sphinxAtStartPar
Small caches (KB per core)

\item {} 
\sphinxAtStartPar
Optimized for throughput (many threads)

\item {} 
\sphinxAtStartPar
Simple control logic

\item {} 
\sphinxAtStartPar
Best for: Parallel tasks, regular computation patterns

\end{itemize}


\subsection{GPU Hierarchy}
\label{\detokenize{gpu-concepts/gpu-fundamentals:gpu-hierarchy}}
\sphinxAtStartPar
Understanding the GPU hierarchy is essential for writing efficient kernels.


\subsubsection{Streaming Multiprocessors (SMs)}
\label{\detokenize{gpu-concepts/gpu-fundamentals:streaming-multiprocessors-sms}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Modern GPUs have 80\sphinxhyphen{}140 SMs (also called Compute Units on AMD)

\item {} 
\sphinxAtStartPar
Each SM contains:
\begin{itemize}
\item {} 
\sphinxAtStartPar
64\sphinxhyphen{}128 CUDA cores (FP32/INT32 units)

\item {} 
\sphinxAtStartPar
32\sphinxhyphen{}64 FP64 units

\item {} 
\sphinxAtStartPar
4\sphinxhyphen{}8 Tensor Cores (specialized matrix multiply units)

\item {} 
\sphinxAtStartPar
128\sphinxhyphen{}256 KB of shared memory (SRAM)

\item {} 
\sphinxAtStartPar
64K 32\sphinxhyphen{}bit registers

\item {} 
\sphinxAtStartPar
L1 cache

\end{itemize}

\item {} 
\sphinxAtStartPar
SMs execute independently and in parallel

\end{itemize}


\subsubsection{Thread Organization}
\label{\detokenize{gpu-concepts/gpu-fundamentals:thread-organization}}
\sphinxAtStartPar
\sphinxstylestrong{Threads}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Smallest unit of execution

\item {} 
\sphinxAtStartPar
Each thread has its own registers

\item {} 
\sphinxAtStartPar
Threads in a warp execute in lockstep (SIMD)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Warps (NVIDIA) / Wavefronts (AMD)}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Group of 32 threads (NVIDIA) or 64 threads (AMD)

\item {} 
\sphinxAtStartPar
Execute instructions in lockstep

\item {} 
\sphinxAtStartPar
Hardware scheduling unit

\item {} 
\sphinxAtStartPar
All threads in warp execute same instruction simultaneously

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Thread Blocks}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Group of up to 1024 threads

\item {} 
\sphinxAtStartPar
Threads in a block can:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Share data via shared memory

\item {} 
\sphinxAtStartPar
Synchronize with barriers

\item {} 
\sphinxAtStartPar
Cooperate on a task

\end{itemize}

\item {} 
\sphinxAtStartPar
Scheduled to single SM

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Grid}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Collection of thread blocks

\item {} 
\sphinxAtStartPar
Defines total parallel work

\item {} 
\sphinxAtStartPar
Blocks execute independently (can be scheduled to different SMs)

\end{itemize}


\subsection{SPMD Programming Model}
\label{\detokenize{gpu-concepts/gpu-fundamentals:spmd-programming-model}}
\sphinxAtStartPar
GPU programming uses the \sphinxstylestrong{Single Program, Multiple Data (SPMD)} model:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
You write code for \sphinxstylestrong{one thread/block}

\item {} 
\sphinxAtStartPar
GPU launches \sphinxstylestrong{thousands of copies} in parallel

\item {} 
\sphinxAtStartPar
Each copy processes \sphinxstylestrong{different data}

\end{enumerate}


\subsubsection{Example Visualization}
\label{\detokenize{gpu-concepts/gpu-fundamentals:example-visualization}}
\sphinxAtStartPar
For a vector of 1024 elements with block size 256:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Grid}\PYG{p}{:} \PYG{l+m+mi}{4} \PYG{n}{blocks}
\PYG{o}{|}\PYG{o}{\PYGZhy{}} \PYG{n}{Block} \PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{threads} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{255}   \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{process} \PYG{n}{elements} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{255}
\PYG{o}{|}\PYG{o}{\PYGZhy{}} \PYG{n}{Block} \PYG{l+m+mi}{1}\PYG{p}{:} \PYG{n}{threads} \PYG{l+m+mi}{256}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{511} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{process} \PYG{n}{elements} \PYG{l+m+mi}{256}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{511}
\PYG{o}{|}\PYG{o}{\PYGZhy{}} \PYG{n}{Block} \PYG{l+m+mi}{2}\PYG{p}{:} \PYG{n}{threads} \PYG{l+m+mi}{512}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{767} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{process} \PYG{n}{elements} \PYG{l+m+mi}{512}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{767}
\PYG{o}{+}\PYG{o}{\PYGZhy{}} \PYG{n}{Block} \PYG{l+m+mi}{3}\PYG{p}{:} \PYG{n}{threads} \PYG{l+m+mi}{768}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1023} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{process} \PYG{n}{elements} \PYG{l+m+mi}{768}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1023}
\end{sphinxVerbatim}

\sphinxAtStartPar
In Triton:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{n\PYGZus{}elements}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} This code runs for EACH block in parallel}
    \PYG{n}{pid} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Which block am I?}
    \PYG{n}{offsets} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Process my chunk of data...}
\end{sphinxVerbatim}


\subsection{Compute Capabilities}
\label{\detokenize{gpu-concepts/gpu-fundamentals:compute-capabilities}}
\sphinxAtStartPar
NVIDIA GPUs are categorized by compute capability (version number):
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{7.x}: Volta (V100) \sphinxhyphen{} First generation Tensor Cores

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{8.x}: Ampere (A100, RTX 30 series) \sphinxhyphen{} 2nd gen Tensor Cores, BF16 support

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{9.x}: Hopper (H100) \sphinxhyphen{} 4th gen Tensor Cores, FP8 support, Tensor Memory Accelerator

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{10.x}: Blackwell (B100) \sphinxhyphen{} 5th gen Tensor Cores, enhanced FP8

\end{itemize}

\sphinxAtStartPar
Higher compute capability = more features and better performance.


\subsection{Key Concepts Summary}
\label{\detokenize{gpu-concepts/gpu-fundamentals:key-concepts-summary}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{20}{100}\X{80}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Concept
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
SM
&
\sphinxAtStartPar
Independent processor with cores, memory, and schedulers
\\
\sphinxhline
\sphinxAtStartPar
Warp
&
\sphinxAtStartPar
32 threads executing in lockstep (SIMD)
\\
\sphinxhline
\sphinxAtStartPar
Thread Block
&
\sphinxAtStartPar
Group of threads that can cooperate (up to 1024 threads)
\\
\sphinxhline
\sphinxAtStartPar
Grid
&
\sphinxAtStartPar
Complete collection of thread blocks for a kernel
\\
\sphinxhline
\sphinxAtStartPar
SPMD
&
\sphinxAtStartPar
Single Program, Multiple Data \sphinxhyphen{} one code, many instances
\\
\sphinxhline
\sphinxAtStartPar
Occupancy
&
\sphinxAtStartPar
Ratio of active warps to maximum warps per SM
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsection{Next Steps}
\label{\detokenize{gpu-concepts/gpu-fundamentals:next-steps}}
\sphinxAtStartPar
Now that you understand GPU fundamentals, learn about:
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/memory-hierarchy::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Memory Hierarchy}}}} \sphinxhyphen{} Understanding fast vs slow memory

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/execution-model::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Execution Model}}}} \sphinxhyphen{} How GPUs schedule and execute work

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/performance-optimization::doc}]{\sphinxcrossref{\DUrole{doc}{Performance Optimization Strategies}}}} \sphinxhyphen{} Making your kernels fast

\end{itemize}

\sphinxAtStartPar
Then start with the tutorials:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{../tutorials/01\sphinxhyphen{}vector\sphinxhyphen{}add}}} \sphinxhyphen{} Your first GPU kernel

\end{itemize}

\sphinxstepscope


\section{GPU Memory Hierarchy}
\label{\detokenize{gpu-concepts/memory-hierarchy:gpu-memory-hierarchy}}\label{\detokenize{gpu-concepts/memory-hierarchy::doc}}
\sphinxAtStartPar
Understanding the GPU memory hierarchy is crucial for writing high\sphinxhyphen{}performance kernels.
Memory access patterns often determine whether your kernel is fast or slow.


\subsection{Overview}
\label{\detokenize{gpu-concepts/memory-hierarchy:overview}}
\sphinxAtStartPar
GPUs have a multi\sphinxhyphen{}level memory hierarchy, similar to CPUs but with different characteristics:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Registers (fastest, smallest)
    down \PYGZti{}1 cycle latency
L1 Cache / Shared Memory (SRAM)
    down \PYGZti{}10 cycles latency
L2 Cache
    down \PYGZti{}100 cycles latency
Global Memory / HBM (slowest, largest)
    down \PYGZti{}400 cycles latency
\end{sphinxVerbatim}


\subsubsection{The Performance Pyramid}
\label{\detokenize{gpu-concepts/memory-hierarchy:the-performance-pyramid}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{20}{100}\X{15}{100}\X{15}{100}\X{20}{100}\X{30}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Level
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Size
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Bandwidth
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Latency
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Scope
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Registers
&
\sphinxAtStartPar
256 KB/SM
&
\sphinxAtStartPar
\textasciitilde{}100 TB/s
&
\sphinxAtStartPar
1 cycle
&
\sphinxAtStartPar
Per thread
\\
\sphinxhline
\sphinxAtStartPar
Shared/L1
&
\sphinxAtStartPar
128\sphinxhyphen{}256 KB/SM
&
\sphinxAtStartPar
10\sphinxhyphen{}20 TB/s
&
\sphinxAtStartPar
\textasciitilde{}10 cycles
&
\sphinxAtStartPar
Per thread block
\\
\sphinxhline
\sphinxAtStartPar
L2 Cache
&
\sphinxAtStartPar
40\sphinxhyphen{}60 MB
&
\sphinxAtStartPar
\textasciitilde{}5 TB/s
&
\sphinxAtStartPar
\textasciitilde{}100 cycles
&
\sphinxAtStartPar
Whole GPU
\\
\sphinxhline
\sphinxAtStartPar
Global/HBM
&
\sphinxAtStartPar
40\sphinxhyphen{}80 GB
&
\sphinxAtStartPar
1\sphinxhyphen{}3 TB/s
&
\sphinxAtStartPar
\textasciitilde{}400 cycles
&
\sphinxAtStartPar
Whole GPU
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsection{Global Memory (HBM/DRAM)}
\label{\detokenize{gpu-concepts/memory-hierarchy:global-memory-hbm-dram}}
\sphinxAtStartPar
\sphinxstylestrong{Characteristics}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Largest memory (40\sphinxhyphen{}80 GB on modern GPUs)

\item {} 
\sphinxAtStartPar
Slowest access (\textasciitilde{}400 cycle latency)

\item {} 
\sphinxAtStartPar
Highest bandwidth (1\sphinxhyphen{}3 TB/s)

\item {} 
\sphinxAtStartPar
Off\sphinxhyphen{}chip (separate memory chips)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Best Practices}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Minimize accesses}: Each access costs \textasciitilde{}400 cycles

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Coalesce accesses}: Adjacent threads should access adjacent addresses

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Maximize bandwidth utilization}: Transfer large chunks, not individual elements

\end{enumerate}


\subsubsection{Memory Coalescing}
\label{\detokenize{gpu-concepts/memory-hierarchy:memory-coalescing}}
\sphinxAtStartPar
\sphinxstylestrong{Uncoalesced Access} (Slow):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Thread} \PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{Read} \PYG{n}{address} \PYG{l+m+mi}{0}
\PYG{n}{Thread} \PYG{l+m+mi}{1}\PYG{p}{:} \PYG{n}{Read} \PYG{n}{address} \PYG{l+m+mi}{1000}
\PYG{n}{Thread} \PYG{l+m+mi}{2}\PYG{p}{:} \PYG{n}{Read} \PYG{n}{address} \PYG{l+m+mi}{2000}
\PYG{n}{Thread} \PYG{l+m+mi}{3}\PYG{p}{:} \PYG{n}{Read} \PYG{n}{address} \PYG{l+m+mi}{3000}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{4} \PYG{n}{separate} \PYG{n}{memory} \PYG{n}{transactions}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Coalesced Access} (Fast):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Thread} \PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{Read} \PYG{n}{address} \PYG{l+m+mi}{0}
\PYG{n}{Thread} \PYG{l+m+mi}{1}\PYG{p}{:} \PYG{n}{Read} \PYG{n}{address} \PYG{l+m+mi}{4}
\PYG{n}{Thread} \PYG{l+m+mi}{2}\PYG{p}{:} \PYG{n}{Read} \PYG{n}{address} \PYG{l+m+mi}{8}
\PYG{n}{Thread} \PYG{l+m+mi}{3}\PYG{p}{:} \PYG{n}{Read} \PYG{n}{address} \PYG{l+m+mi}{12}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1} \PYG{n}{combined} \PYG{n}{memory} \PYG{n}{transaction} \PYG{p}{(}\PYG{l+m+mi}{128} \PYG{n+nb}{bytes}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
In Triton:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} This automatically coalesces!}
\PYG{n}{offsets} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Adjacent threads \PYGZhy{}\PYGZgt{} adjacent addresses}
\end{sphinxVerbatim}


\subsection{L2 Cache}
\label{\detokenize{gpu-concepts/memory-hierarchy:l2-cache}}
\sphinxAtStartPar
\sphinxstylestrong{Characteristics}
\begin{itemize}
\item {} 
\sphinxAtStartPar
40\sphinxhyphen{}60 MB on modern GPUs

\item {} 
\sphinxAtStartPar
Shared across all SMs

\item {} 
\sphinxAtStartPar
Caches global memory accesses

\item {} 
\sphinxAtStartPar
\textasciitilde{}100 cycle latency (4x faster than global memory)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Optimization Strategy}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reuse data across thread blocks

\item {} 
\sphinxAtStartPar
Process data in patterns that maximize L2 hit rate

\item {} 
\sphinxAtStartPar
Use “swizzling” techniques for matrix operations

\end{itemize}

\sphinxAtStartPar
Example in matrix multiplication:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Bad: Process blocks in row\sphinxhyphen{}major order

\item {} 
\sphinxAtStartPar
Good: Process blocks in grouped order (better L2 reuse)

\end{itemize}

\sphinxAtStartPar
See \DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{../tutorials/03\sphinxhyphen{}matrix\sphinxhyphen{}multiplication}}} for details.


\subsection{L1 Cache / Shared Memory (SRAM)}
\label{\detokenize{gpu-concepts/memory-hierarchy:l1-cache-shared-memory-sram}}
\sphinxAtStartPar
\sphinxstylestrong{Characteristics}
\begin{itemize}
\item {} 
\sphinxAtStartPar
128\sphinxhyphen{}256 KB per SM

\item {} 
\sphinxAtStartPar
Configurable split between L1 cache and shared memory

\item {} 
\sphinxAtStartPar
\textasciitilde{}10 cycle latency (40x faster than global memory!)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Explicitly managed} in CUDA, \sphinxstylestrong{automatic} in Triton

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Shared Memory}

\sphinxAtStartPar
Explicitly allocated memory shared by threads in a block:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CUDA (explicit)}
\PYG{n}{\PYGZus{}\PYGZus{}shared\PYGZus{}\PYGZus{}} \PYG{n+nb}{float} \PYG{n}{shared\PYGZus{}data}\PYG{p}{[}\PYG{l+m+mi}{256}\PYG{p}{]}\PYG{p}{;}

\PYG{c+c1}{\PYGZsh{} Triton (automatic when loading blocks)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Triton manages SRAM automatically}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key Uses}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Staging area}: Load from global \sphinxhyphen{}\textgreater{} SRAM \sphinxhyphen{}\textgreater{} process \sphinxhyphen{}\textgreater{} write back

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data reuse}: Multiple threads access same data from SRAM

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Communication}: Threads in block share results via SRAM

\end{enumerate}


\subsubsection{Example: Matrix Multiplication}
\label{\detokenize{gpu-concepts/memory-hierarchy:example-matrix-multiplication}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Load A block into SRAM (reused BLOCK\PYGZus{}N times)}
\PYG{n}{a} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{a\PYGZus{}ptrs}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Loaded to SRAM}

\PYG{c+c1}{\PYGZsh{} Load B block into SRAM (reused BLOCK\PYGZus{}M times)}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{b\PYGZus{}ptrs}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Loaded to SRAM}

\PYG{c+c1}{\PYGZsh{} Compute using SRAM data (very fast!)}
\PYG{n}{accumulator} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{,} \PYG{n}{accumulator}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Without SRAM, each element would be read from slow global memory every time it’s used!


\subsection{Registers}
\label{\detokenize{gpu-concepts/memory-hierarchy:registers}}
\sphinxAtStartPar
\sphinxstylestrong{Characteristics}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Fastest memory (1 cycle access)

\item {} 
\sphinxAtStartPar
256 KB per SM (divided among threads)

\item {} 
\sphinxAtStartPar
Private to each thread

\item {} 
\sphinxAtStartPar
Limited supply!

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Register Pressure}

\sphinxAtStartPar
Each thread needs registers for:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Local variables

\item {} 
\sphinxAtStartPar
Intermediate computations

\item {} 
\sphinxAtStartPar
Accumulation

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Too many registers} \sphinxhyphen{}\textgreater{} fewer threads per SM \sphinxhyphen{}\textgreater{} lower occupancy \sphinxhyphen{}\textgreater{} lower performance

\sphinxAtStartPar
Example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Each thread needs registers for:}
\PYG{n}{accumulator} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{BLOCK\PYGZus{}M}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}N}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{float32}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} BLOCK\PYGZus{}M=128, BLOCK\PYGZus{}N=128 \PYGZhy{}\PYGZgt{} 16K elements per thread!}
\PYG{c+c1}{\PYGZsh{} Each element needs 1 register \PYGZhy{}\PYGZgt{} 16K registers}
\PYG{c+c1}{\PYGZsh{} But only 255 registers available per thread!}

\PYG{c+c1}{\PYGZsh{} Solution: Divide work among multiple threads in block}
\end{sphinxVerbatim}


\subsection{Memory Access Patterns}
\label{\detokenize{gpu-concepts/memory-hierarchy:memory-access-patterns}}

\subsubsection{Pattern 1: Streaming (Bandwidth\sphinxhyphen{}Bound)}
\label{\detokenize{gpu-concepts/memory-hierarchy:pattern-1-streaming-bandwidth-bound}}
\sphinxAtStartPar
Read data once, process, write once:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{l+m+mi}{1}  \PYG{c+c1}{\PYGZsh{} Simple operation}
\PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{y\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Memory bandwidth is bottleneck

\item {} 
\sphinxAtStartPar
Examples: vector add, element\sphinxhyphen{}wise operations

\item {} 
\sphinxAtStartPar
Goal: Achieve high \% of peak bandwidth

\end{itemize}


\subsubsection{Pattern 2: Staged Computation (Compute\sphinxhyphen{}Bound)}
\label{\detokenize{gpu-concepts/memory-hierarchy:pattern-2-staged-computation-compute-bound}}
\sphinxAtStartPar
Load to SRAM, heavy computation, write result:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Load blocks to SRAM}
\PYG{n}{a} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{a\PYGZus{}ptrs}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} MxK block}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{b\PYGZus{}ptrs}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} KxN block}

\PYG{c+c1}{\PYGZsh{} Heavy computation on SRAM data}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{K}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}K}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{accumulator} \PYG{o}{+}\PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Many ops per load!}

\PYG{c+c1}{\PYGZsh{} Write result}
\PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{c\PYGZus{}ptrs}\PYG{p}{,} \PYG{n}{accumulator}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Computation is bottleneck (good!)

\item {} 
\sphinxAtStartPar
Data reused many times from SRAM

\item {} 
\sphinxAtStartPar
Examples: matrix multiplication, convolution

\item {} 
\sphinxAtStartPar
Goal: Maximize compute utilization

\end{itemize}


\subsubsection{Pattern 3: Reduction (Mixed)}
\label{\detokenize{gpu-concepts/memory-hierarchy:pattern-3-reduction-mixed}}
\sphinxAtStartPar
Load data, reduce to smaller output:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{row} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{input\PYGZus{}ptrs} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\PYG{n}{max\PYGZus{}val} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{row}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Many inputs \PYGZhy{}\PYGZgt{} one output}
\PYG{n}{sum\PYGZus{}val} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{row}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Many inputs \PYGZhy{}\PYGZgt{} one output}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Characteristics}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reads more data than writes

\item {} 
\sphinxAtStartPar
Benefits from keeping data in SRAM

\item {} 
\sphinxAtStartPar
Examples: softmax, layer norm

\item {} 
\sphinxAtStartPar
Goal: Minimize memory traffic, maximize SRAM use

\end{itemize}


\subsection{Optimizing for Memory Hierarchy}
\label{\detokenize{gpu-concepts/memory-hierarchy:optimizing-for-memory-hierarchy}}

\subsubsection{The Golden Rules}
\label{\detokenize{gpu-concepts/memory-hierarchy:the-golden-rules}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Minimize global memory accesses}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Load once, use many times

\item {} 
\sphinxAtStartPar
Fuse operations to avoid intermediate writes

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Maximize SRAM usage}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Keep data in SRAM as long as possible

\item {} 
\sphinxAtStartPar
Reuse data across threads

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ensure coalesced accesses}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Adjacent threads access adjacent addresses

\item {} 
\sphinxAtStartPar
Triton usually handles this automatically

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Consider register pressure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Don’t allocate too much per thread

\item {} 
\sphinxAtStartPar
Balance parallelism vs resources

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Optimize for L2 cache}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reuse data across blocks

\item {} 
\sphinxAtStartPar
Use swizzling for better locality

\end{itemize}

\end{enumerate}


\subsubsection{Example: Naive vs Optimized Softmax}
\label{\detokenize{gpu-concepts/memory-hierarchy:example-naive-vs-optimized-softmax}}
\sphinxAtStartPar
\sphinxstylestrong{Naive (Multiple passes through global memory)}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x\PYGZus{}max} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}              \PYG{c+c1}{\PYGZsh{} Pass 1: Read all}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{x\PYGZus{}max}                      \PYG{c+c1}{\PYGZsh{} Pass 2: Read all, write all}
\PYG{n}{numerator} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} Pass 3: Read all, write all}
\PYG{n}{denominator} \PYG{o}{=} \PYG{n}{numerator}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Pass 4: Read all}
\PYG{n}{output} \PYG{o}{=} \PYG{n}{numerator} \PYG{o}{/} \PYG{n}{denominator}   \PYG{c+c1}{\PYGZsh{} Pass 5: Read all, write all}

\PYG{c+c1}{\PYGZsh{} Total: 5 reads + 3 writes = 8 global memory passes!}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Optimized (Single pass with SRAM)}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{softmax\PYGZus{}kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Load row once into SRAM}
    \PYG{n}{row} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{input\PYGZus{}ptrs}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} All computation in SRAM}
    \PYG{n}{max\PYGZus{}val} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{row}\PYG{p}{)}
    \PYG{n}{numerator} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{row} \PYG{o}{\PYGZhy{}} \PYG{n}{max\PYGZus{}val}\PYG{p}{)}
    \PYG{n}{denominator} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{numerator}\PYG{p}{)}
    \PYG{n}{output} \PYG{o}{=} \PYG{n}{numerator} \PYG{o}{/} \PYG{n}{denominator}

    \PYG{c+c1}{\PYGZsh{} Write result once}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{output\PYGZus{}ptrs}\PYG{p}{,} \PYG{n}{output}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Total: 1 read + 1 write = 2 global memory passes!}
\PYG{c+c1}{\PYGZsh{} 4x reduction in memory traffic = 4x speedup!}
\end{sphinxVerbatim}


\subsection{Measuring Memory Performance}
\label{\detokenize{gpu-concepts/memory-hierarchy:measuring-memory-performance}}

\subsubsection{Key Metrics}
\label{\detokenize{gpu-concepts/memory-hierarchy:key-metrics}}
\sphinxAtStartPar
\sphinxstylestrong{Arithmetic Intensity}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{AI} \PYG{o}{=} \PYG{n}{FLOPs} \PYG{o}{/} \PYG{n}{Bytes} \PYG{n}{Transferred}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Low AI (\textless{}10): Memory\sphinxhyphen{}bound

\item {} 
\sphinxAtStartPar
High AI (\textgreater{}100): Compute\sphinxhyphen{}bound

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Memory Bandwidth Utilization}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Achieved} \PYG{n}{Bandwidth} \PYG{o}{/} \PYG{n}{Peak} \PYG{n}{Bandwidth}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Good performance: 60\sphinxhyphen{}80\% for memory\sphinxhyphen{}bound ops

\item {} 
\sphinxAtStartPar
Poor performance: \textless{}30\% suggests optimization opportunities

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Vector Add: 1 FLOP, 12 bytes (read x, read y, write z)
AI = 1/12 \PYGZti{} 0.08 FLOPs/byte
\PYGZhy{}\PYGZgt{} Heavily memory\PYGZhy{}bound!

Matrix Multiply: 2MNK FLOPs, 2(MK + KN + MN) bytes
For N=1024: AI = 512 FLOPs/byte
\PYGZhy{}\PYGZgt{} Compute\PYGZhy{}bound!
\end{sphinxVerbatim}


\subsubsection{Tools for Profiling}
\label{\detokenize{gpu-concepts/memory-hierarchy:tools-for-profiling}}
\sphinxAtStartPar
\sphinxstylestrong{NVIDIA}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nsys} \PYG{n}{profile} \PYG{n}{python} \PYG{n}{your\PYGZus{}script}\PYG{o}{.}\PYG{n}{py}  \PYG{c+c1}{\PYGZsh{} Timeline analysis}
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{set} \PYG{n}{full} \PYG{n}{python} \PYG{n}{your\PYGZus{}script}\PYG{o}{.}\PYG{n}{py}  \PYG{c+c1}{\PYGZsh{} Detailed metrics}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{AMD}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rocprof} \PYG{n}{python} \PYG{n}{your\PYGZus{}script}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

\sphinxAtStartPar
Key metrics to watch:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Memory throughput (GB/s)

\item {} 
\sphinxAtStartPar
L1/L2 cache hit rates

\item {} 
\sphinxAtStartPar
Occupancy

\item {} 
\sphinxAtStartPar
Register usage per thread

\end{itemize}


\subsection{Summary}
\label{\detokenize{gpu-concepts/memory-hierarchy:summary}}
\sphinxAtStartPar
The memory hierarchy is the key to GPU performance:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Registers}: Fastest, use for temporaries

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Shared Memory}: Fast, use for data reuse within block

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{L2 Cache}: Automatic, but can optimize access patterns

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Global Memory}: Slow, minimize accesses

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Core Strategy}: Load data from slow memory \sphinxhyphen{}\textgreater{} process in fast memory \sphinxhyphen{}\textgreater{} write result

\sphinxAtStartPar
Next: Learn about {\hyperref[\detokenize{gpu-concepts/execution-model::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Execution Model}}}} to understand how GPUs schedule work.

\sphinxstepscope


\section{GPU Execution Model}
\label{\detokenize{gpu-concepts/execution-model:gpu-execution-model}}\label{\detokenize{gpu-concepts/execution-model::doc}}
\sphinxAtStartPar
Understanding how GPUs schedule and execute work is essential for writing efficient kernels.


\subsection{Thread Execution}
\label{\detokenize{gpu-concepts/execution-model:thread-execution}}

\subsubsection{Warps and SIMD Execution}
\label{\detokenize{gpu-concepts/execution-model:warps-and-simd-execution}}
\sphinxAtStartPar
GPUs execute threads in groups called \sphinxstylestrong{warps} (NVIDIA) or \sphinxstylestrong{wavefronts} (AMD):
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Warp size}: 32 threads (NVIDIA), 64 threads (AMD)

\item {} 
\sphinxAtStartPar
All threads in a warp execute the \sphinxstylestrong{same instruction} simultaneously

\item {} 
\sphinxAtStartPar
This is \sphinxstylestrong{SIMD}: Single Instruction, Multiple Data

\end{itemize}

\sphinxAtStartPar
Example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} All 32 threads in warp execute simultaneously:}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} 32 loads in parallel}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{x} \PYG{o}{*} \PYG{l+m+mi}{2}                    \PYG{c+c1}{\PYGZsh{} 32 multiplies in parallel}
\PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} 32 stores in parallel}
\end{sphinxVerbatim}


\subsubsection{Thread Divergence}
\label{\detokenize{gpu-concepts/execution-model:thread-divergence}}
\sphinxAtStartPar
\sphinxstylestrong{Problem}: What if threads in a warp take different paths?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{n}{condition}\PYG{p}{:}
    \PYG{n}{path\PYGZus{}a}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Some threads take this}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{path\PYGZus{}b}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Other threads take this}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Answer}: Both paths execute! (if any thread needs each path)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Threads that don’t need a path are masked out

\item {} 
\sphinxAtStartPar
Wastes compute cycles

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Solution}: Use predication instead of branching:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Bad (branching)}
\PYG{k}{if} \PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{:}
    \PYG{n}{result} \PYG{o}{=} \PYG{n}{x} \PYG{o}{*} \PYG{l+m+mi}{2}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{result} \PYG{o}{=} \PYG{l+m+mi}{0}

\PYG{c+c1}{\PYGZsh{} Good (predication)}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{x} \PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} No branching!}
\end{sphinxVerbatim}


\subsection{Occupancy}
\label{\detokenize{gpu-concepts/execution-model:occupancy}}

\subsubsection{What is Occupancy?}
\label{\detokenize{gpu-concepts/execution-model:what-is-occupancy}}
\sphinxAtStartPar
\sphinxstylestrong{Occupancy} = (Active warps per SM) / (Maximum warps per SM)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Higher occupancy = more threads = better latency hiding

\item {} 
\sphinxAtStartPar
But not always! Quality \textgreater{} quantity

\end{itemize}


\subsubsection{Factors Limiting Occupancy}
\label{\detokenize{gpu-concepts/execution-model:factors-limiting-occupancy}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Registers per thread}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Each SM has \textasciitilde{}65K registers

\item {} 
\sphinxAtStartPar
More registers per thread = fewer threads per SM

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Shared memory per block}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Each SM has 128\sphinxhyphen{}256 KB shared memory

\item {} 
\sphinxAtStartPar
More shared memory per block = fewer blocks per SM

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Thread block size}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Maximum 1024 threads per block

\item {} 
\sphinxAtStartPar
Too small = poor utilization

\item {} 
\sphinxAtStartPar
Too large = resource constraints

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hardware limits}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Max 16\sphinxhyphen{}32 blocks per SM

\item {} 
\sphinxAtStartPar
Max 1024\sphinxhyphen{}2048 threads per SM

\end{itemize}

\end{enumerate}


\subsubsection{Example Calculation}
\label{\detokenize{gpu-concepts/execution-model:example-calculation}}
\sphinxAtStartPar
For A100 GPU:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Max 64 warps per SM (2048 threads)

\item {} 
\sphinxAtStartPar
65536 registers per SM

\end{itemize}

\sphinxAtStartPar
If your kernel uses 128 registers per thread:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Max} \PYG{n}{threads} \PYG{o}{=} \PYG{l+m+mi}{65536} \PYG{o}{/} \PYG{l+m+mi}{128} \PYG{o}{=} \PYG{l+m+mi}{512} \PYG{n}{threads} \PYG{n}{per} \PYG{n}{SM}
\PYG{n}{Max} \PYG{n}{warps} \PYG{o}{=} \PYG{l+m+mi}{512} \PYG{o}{/} \PYG{l+m+mi}{32} \PYG{o}{=} \PYG{l+m+mi}{16} \PYG{n}{warps} \PYG{n}{per} \PYG{n}{SM}
\PYG{n}{Occupancy} \PYG{o}{=} \PYG{l+m+mi}{16} \PYG{o}{/} \PYG{l+m+mi}{64} \PYG{o}{=} \PYG{l+m+mi}{25}\PYG{o}{\PYGZpc{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Ouch!} Register pressure killed occupancy.

\sphinxAtStartPar
Solutions:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reduce local variables

\item {} 
\sphinxAtStartPar
Use smaller BLOCK\_SIZE

\item {} 
\sphinxAtStartPar
Let compiler optimize

\end{itemize}


\subsection{Latency Hiding}
\label{\detokenize{gpu-concepts/execution-model:latency-hiding}}

\subsubsection{Why Occupancy Matters}
\label{\detokenize{gpu-concepts/execution-model:why-occupancy-matters}}
\sphinxAtStartPar
Memory access takes \textasciitilde{}400 cycles. If only one warp is active:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Warp 0: Load data ... (wait 400 cycles) ... continue
\PYGZhy{}\PYGZgt{} GPU idle for 400 cycles!
\end{sphinxVerbatim}

\sphinxAtStartPar
With 16 active warps:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Warp 0:  Load ... (switch to warp 1)
Warp 1:  Load ... (switch to warp 2)
...
Warp 15: Load ... (switch to warp 0)
Warp 0:  Data ready! Continue...
\PYGZhy{}\PYGZgt{} No idle time!
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Latency Hiding}: While one warp waits, others compute.


\subsubsection{The Occupancy Sweet Spot}
\label{\detokenize{gpu-concepts/execution-model:the-occupancy-sweet-spot}}
\sphinxAtStartPar
\sphinxstylestrong{Not always better!}
\begin{itemize}
\item {} 
\sphinxAtStartPar
25\% occupancy: Often sufficient for memory\sphinxhyphen{}bound ops

\item {} 
\sphinxAtStartPar
50\% occupancy: Good balance

\item {} 
\sphinxAtStartPar
100\% occupancy: Not always achievable or necessary

\end{itemize}

\sphinxAtStartPar
For compute\sphinxhyphen{}bound operations, higher occupancy helps more.


\subsection{Kernel Launch Configuration}
\label{\detokenize{gpu-concepts/execution-model:kernel-launch-configuration}}

\subsubsection{Grid and Block Dimensions}
\label{\detokenize{gpu-concepts/execution-model:grid-and-block-dimensions}}
\sphinxAtStartPar
In Triton:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grid} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{meta}\PYG{p}{:} \PYG{p}{(}
    \PYG{n}{triton}\PYG{o}{.}\PYG{n}{cdiv}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,} \PYG{n}{meta}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}SIZE\PYGZus{}M}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Grid dimension 0}
    \PYG{n}{triton}\PYG{o}{.}\PYG{n}{cdiv}\PYG{p}{(}\PYG{n}{N}\PYG{p}{,} \PYG{n}{meta}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}SIZE\PYGZus{}N}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Grid dimension 1}
\PYG{p}{)}
\PYG{n}{kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Each grid dimension creates more parallel work.


\subsubsection{Choosing Block Size}
\label{\detokenize{gpu-concepts/execution-model:choosing-block-size}}
\sphinxAtStartPar
\sphinxstylestrong{Trade\sphinxhyphen{}offs}:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Aspect
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Small Blocks (64\sphinxhyphen{}128)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Large Blocks (256\sphinxhyphen{}1024)
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Occupancy
&
\sphinxAtStartPar
Higher (more blocks fit)
&
\sphinxAtStartPar
Lower (fewer blocks fit)
\\
\sphinxhline
\sphinxAtStartPar
Data reuse
&
\sphinxAtStartPar
Less
&
\sphinxAtStartPar
More
\\
\sphinxhline
\sphinxAtStartPar
Launch overhead
&
\sphinxAtStartPar
More
&
\sphinxAtStartPar
Less
\\
\sphinxhline
\sphinxAtStartPar
Flexibility
&
\sphinxAtStartPar
Better load balancing
&
\sphinxAtStartPar
Can waste resources
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Rules of thumb}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Multiples of warp size (32 for NVIDIA)

\item {} 
\sphinxAtStartPar
Powers of 2 usually work well

\item {} 
\sphinxAtStartPar
128\sphinxhyphen{}256 is often a good starting point

\item {} 
\sphinxAtStartPar
Use auto\sphinxhyphen{}tuning to find optimum

\end{itemize}


\subsection{Synchronization}
\label{\detokenize{gpu-concepts/execution-model:synchronization}}

\subsubsection{Within a Block}
\label{\detokenize{gpu-concepts/execution-model:within-a-block}}
\sphinxAtStartPar
Threads in a block can synchronize:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CUDA}
\PYG{n}{\PYGZus{}\PYGZus{}syncthreads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

\PYG{c+c1}{\PYGZsh{} Triton (automatic at certain operations)}
\PYG{c+c1}{\PYGZsh{} Barrier inserted automatically when needed}
\end{sphinxVerbatim}

\sphinxAtStartPar
Use cases:
\begin{itemize}
\item {} 
\sphinxAtStartPar
All threads load data \sphinxhyphen{}\textgreater{} sync \sphinxhyphen{}\textgreater{} all threads use data

\item {} 
\sphinxAtStartPar
Producer\sphinxhyphen{}consumer patterns within block

\end{itemize}


\subsubsection{Between Blocks}
\label{\detokenize{gpu-concepts/execution-model:between-blocks}}
\sphinxAtStartPar
\sphinxstylestrong{Cannot synchronize between blocks directly!}

\sphinxAtStartPar
Blocks must be independent. Why?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Blocks can execute in any order

\item {} 
\sphinxAtStartPar
Some blocks might not start until others finish

\item {} 
\sphinxAtStartPar
Different SMs, no shared synchronization

\end{itemize}

\sphinxAtStartPar
If you need cross\sphinxhyphen{}block coordination:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Launch multiple kernels

\item {} 
\sphinxAtStartPar
Use atomic operations (careful!)

\item {} 
\sphinxAtStartPar
Redesign algorithm

\end{itemize}


\subsection{Warp\sphinxhyphen{}Level Operations}
\label{\detokenize{gpu-concepts/execution-model:warp-level-operations}}

\subsubsection{Warp Shuffles}
\label{\detokenize{gpu-concepts/execution-model:warp-shuffles}}
\sphinxAtStartPar
Share data between threads in a warp \sphinxstylestrong{without memory}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Each thread has a value}
\PYG{c+c1}{\PYGZsh{} Thread 0 wants value from thread 5}
\PYG{n}{value} \PYG{o}{=} \PYG{n}{warp\PYGZus{}shuffle}\PYG{p}{(}\PYG{n}{my\PYGZus{}value}\PYG{p}{,} \PYG{n}{source\PYGZus{}lane}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Extremely fast (1 cycle) vs shared memory (\textasciitilde{}10 cycles).

\sphinxAtStartPar
Triton uses these automatically in reductions!


\subsubsection{Warp\sphinxhyphen{}Level Reductions}
\label{\detokenize{gpu-concepts/execution-model:warp-level-reductions}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Sum across all threads in warp}
\PYG{n}{warp\PYGZus{}sum} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{value}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Uses warp shuffles}

\PYG{c+c1}{\PYGZsh{} Max across warp}
\PYG{n}{warp\PYGZus{}max} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{value}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Much faster than using shared memory for small reductions.


\subsection{Software Pipelining}
\label{\detokenize{gpu-concepts/execution-model:software-pipelining}}

\subsubsection{Overlap Compute and Memory}
\label{\detokenize{gpu-concepts/execution-model:overlap-compute-and-memory}}
\sphinxAtStartPar
\sphinxstylestrong{Without pipelining}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{data} \PYG{o}{=} \PYG{n}{load\PYGZus{}from\PYGZus{}memory}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Wait...}
    \PYG{n}{result} \PYG{o}{=} \PYG{n}{compute}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} Compute}
    \PYG{n}{store}\PYG{p}{(}\PYG{n}{result}\PYG{p}{)}              \PYG{c+c1}{\PYGZsh{} Store}

\PYG{c+c1}{\PYGZsh{} Timeline: Load | Compute | Store | Load | Compute | Store | ...}
\PYG{c+c1}{\PYGZsh{}            \PYGZca{}\PYGZca{}\PYGZca{}\PYGZca{}\PYGZca{}\PYGZca{} Idle compute while loading!}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{With pipelining} (num\_stages=3):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Stage 0: Load data[0]}
\PYG{c+c1}{\PYGZsh{} Stage 1: Load data[1], Compute on data[0]}
\PYG{c+c1}{\PYGZsh{} Stage 2: Load data[2], Compute on data[1], Store result[0]}
\PYG{c+c1}{\PYGZsh{} Stage 3: Load data[3], Compute on data[2], Store result[1]}
\PYG{c+c1}{\PYGZsh{} ...}

\PYG{c+c1}{\PYGZsh{} Timeline: Multiple operations overlap!}
\PYG{c+c1}{\PYGZsh{} Compute runs while memory loads happen}
\end{sphinxVerbatim}

\sphinxAtStartPar
In Triton:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{num\PYGZus{}stages}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} Enable software pipelining}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{K}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}K}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{a} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{a\PYGZus{}ptrs}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Automatically pipelined!}
        \PYG{n}{b} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{b\PYGZus{}ptrs}\PYG{p}{)}
        \PYG{n}{acc} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{,} \PYG{n}{acc}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Cost: Need more registers/SRAM to hold multiple stages.


\subsection{Persistent Kernels}
\label{\detokenize{gpu-concepts/execution-model:persistent-kernels}}

\subsubsection{Traditional Approach}
\label{\detokenize{gpu-concepts/execution-model:traditional-approach}}
\sphinxAtStartPar
Launch one block per task:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 1000 tasks \PYGZhy{}\PYGZgt{} launch 1000 blocks}
\PYG{n}{kernel}\PYG{p}{[}\PYG{l+m+mi}{1000}\PYG{p}{]}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Problem}: Launch overhead, poor load balancing for variable\sphinxhyphen{}size tasks.


\subsubsection{Persistent Approach}
\label{\detokenize{gpu-concepts/execution-model:persistent-approach}}
\sphinxAtStartPar
Launch fewer blocks, each processes multiple tasks:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{persistent\PYGZus{}kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{task\PYGZus{}id} \PYG{o+ow}{in} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{range}\PYG{p}{(}\PYG{n}{start}\PYG{p}{,} \PYG{n}{end}\PYG{p}{,} \PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} Process task\PYGZus{}id}
        \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}

\PYG{c+c1}{\PYGZsh{} Only 100 blocks, each handles 10 tasks}
\PYG{n}{kernel}\PYG{p}{[}\PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Amortize launch overhead

\item {} 
\sphinxAtStartPar
Better load balancing

\item {} 
\sphinxAtStartPar
Better L2 cache utilization

\end{itemize}

\sphinxAtStartPar
See \DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{../tutorials/02\sphinxhyphen{}fused\sphinxhyphen{}softmax}}} for example.


\subsection{Performance Considerations}
\label{\detokenize{gpu-concepts/execution-model:performance-considerations}}

\subsubsection{Key Factors}
\label{\detokenize{gpu-concepts/execution-model:key-factors}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Occupancy}: Enough warps to hide latency?

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory coalescing}: Adjacent threads \sphinxhyphen{}\textgreater{} adjacent addresses?

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Thread divergence}: Minimize branching

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Register pressure}: Don’t use too many per thread

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Shared memory usage}: Maximize reuse, don’t overflow

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Instruction mix}: Balance memory and compute

\end{enumerate}


\subsubsection{Profiling Tools}
\label{\detokenize{gpu-concepts/execution-model:profiling-tools}}
\sphinxAtStartPar
\sphinxstylestrong{NVIDIA}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Occupancy}
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{metrics} \PYG{n}{sm\PYGZus{}\PYGZus{}warps\PYGZus{}active}\PYG{o}{.}\PYG{n}{avg}\PYG{o}{.}\PYG{n}{pct\PYGZus{}of\PYGZus{}peak\PYGZus{}sustained\PYGZus{}active}

\PYG{c+c1}{\PYGZsh{} Memory throughput}
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{metrics} \PYG{n}{dram\PYGZus{}\PYGZus{}throughput}\PYG{o}{.}\PYG{n}{avg}\PYG{o}{.}\PYG{n}{pct\PYGZus{}of\PYGZus{}peak\PYGZus{}sustained\PYGZus{}elapsed}

\PYG{c+c1}{\PYGZsh{} L1/L2 hit rates}
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{metrics} \PYG{n}{l1tex\PYGZus{}\PYGZus{}t\PYGZus{}sectors\PYGZus{}pipe\PYGZus{}lsu\PYGZus{}mem\PYGZus{}global\PYGZus{}op\PYGZus{}ld}\PYG{o}{.}\PYG{n}{sum}\PYG{o}{.}\PYG{n}{pct\PYGZus{}of\PYGZus{}peak\PYGZus{}sustained\PYGZus{}elapsed}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{AMD}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rocprof} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{stats} \PYG{n}{python} \PYG{n}{your\PYGZus{}script}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}


\subsection{Summary}
\label{\detokenize{gpu-concepts/execution-model:summary}}
\sphinxAtStartPar
Key concepts:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Warps}: 32 threads execute in lockstep (SIMD)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Occupancy}: More active warps = better latency hiding

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Thread divergence}: Avoid branches, use predication

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Synchronization}: Within block (yes), between blocks (no)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Pipelining}: Overlap memory and compute

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Persistent kernels}: Amortize overhead, better load balancing

\end{itemize}

\sphinxAtStartPar
Next: Learn {\hyperref[\detokenize{gpu-concepts/performance-optimization::doc}]{\sphinxcrossref{\DUrole{doc}{Performance Optimization Strategies}}}} strategies.

\sphinxstepscope


\section{Performance Optimization Strategies}
\label{\detokenize{gpu-concepts/performance-optimization:performance-optimization-strategies}}\label{\detokenize{gpu-concepts/performance-optimization::doc}}
\sphinxAtStartPar
A practical guide to making your GPU kernels fast.


\subsection{The Optimization Process}
\label{\detokenize{gpu-concepts/performance-optimization:the-optimization-process}}

\subsubsection{Step 1: Profile First}
\label{\detokenize{gpu-concepts/performance-optimization:step-1-profile-first}}
\sphinxAtStartPar
\sphinxstylestrong{Never optimize blindly!}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Measure baseline performance

\item {} 
\sphinxAtStartPar
Identify bottleneck (memory vs compute)

\item {} 
\sphinxAtStartPar
Target the bottleneck

\item {} 
\sphinxAtStartPar
Measure improvement

\item {} 
\sphinxAtStartPar
Repeat

\end{enumerate}

\sphinxAtStartPar
Tools:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} NVIDIA}
\PYG{n}{nsys} \PYG{n}{profile} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{stats}\PYG{o}{=}\PYG{n}{true} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{set} \PYG{n}{full} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}

\PYG{c+c1}{\PYGZsh{} AMD}
\PYG{n}{rocprof} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{stats} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}


\subsubsection{Step 2: Identify Bottleneck}
\label{\detokenize{gpu-concepts/performance-optimization:step-2-identify-bottleneck}}
\sphinxAtStartPar
\sphinxstylestrong{Memory\sphinxhyphen{}Bound}: Low arithmetic intensity
\begin{itemize}
\item {} 
\sphinxAtStartPar
Achieved bandwidth \textgreater{} 60\% of peak \sphinxhyphen{}\textgreater{} Good!

\item {} 
\sphinxAtStartPar
Achieved bandwidth \textless{} 30\% \sphinxhyphen{}\textgreater{} Optimization needed

\item {} 
\sphinxAtStartPar
Focus on: Reducing memory traffic, coalescing

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Compute\sphinxhyphen{}Bound}: High arithmetic intensity
\begin{itemize}
\item {} 
\sphinxAtStartPar
Compute utilization \textgreater{} 80\% \sphinxhyphen{}\textgreater{} Good!

\item {} 
\sphinxAtStartPar
Compute utilization \textless{} 50\% \sphinxhyphen{}\textgreater{} Optimization needed

\item {} 
\sphinxAtStartPar
Focus on: Using Tensor Cores, increasing parallelism

\end{itemize}


\subsection{Memory Optimization}
\label{\detokenize{gpu-concepts/performance-optimization:memory-optimization}}

\subsubsection{Strategy 1: Kernel Fusion}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-1-kernel-fusion}}
\sphinxAtStartPar
Combine multiple operations to reduce memory traffic.

\sphinxAtStartPar
\sphinxstylestrong{Before} (3 separate kernels):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Kernel 1: Read X, compute Y = exp(X), write Y}
\PYG{c+c1}{\PYGZsh{} Kernel 2: Read Y, compute Z = Y / sum(Y), write Z}
\PYG{c+c1}{\PYGZsh{} Kernel 3: Read Z, compute W = Z * scale, write W}
\PYG{c+c1}{\PYGZsh{} Total: 3 reads + 3 writes = 6 memory operations}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{After} (1 fused kernel):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{fused\PYGZus{}kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} Read once}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{y} \PYG{o}{/} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}
    \PYG{n}{w} \PYG{o}{=} \PYG{n}{z} \PYG{o}{*} \PYG{n}{scale}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{w}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} Write once}
\PYG{c+c1}{\PYGZsh{} Total: 1 read + 1 write = 2 memory operations}
\PYG{c+c1}{\PYGZsh{} 3x reduction in memory traffic!}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Examples in this guide}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{../tutorials/02\sphinxhyphen{}fused\sphinxhyphen{}softmax}}}

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{../tutorials/06\sphinxhyphen{}fused\sphinxhyphen{}attention}}}

\end{itemize}


\subsubsection{Strategy 2: Tiling}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-2-tiling}}
\sphinxAtStartPar
Load data into fast SRAM, reuse multiple times.

\sphinxAtStartPar
\sphinxstylestrong{Example}: Matrix multiplication:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Load A block to SRAM (reused for many B blocks)}
\PYG{c+c1}{\PYGZsh{} Load B block to SRAM (reused for many A blocks)}
\PYG{c+c1}{\PYGZsh{} Compute in SRAM (fast!)}
\end{sphinxVerbatim}

\sphinxAtStartPar
See \DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{../tutorials/03\sphinxhyphen{}matrix\sphinxhyphen{}multiplication}}} for details.


\subsubsection{Strategy 3: Vectorized Loads}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-3-vectorized-loads}}
\sphinxAtStartPar
Load multiple elements per instruction:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Bad: Load 1 element at a time (slow)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{load}\PYG{p}{(}\PYG{n}{ptr} \PYG{o}{+} \PYG{n}{i}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Good: Load BLOCK\PYGZus{}SIZE elements at once}
\PYG{n}{offsets} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Vectorized!}
\end{sphinxVerbatim}

\sphinxAtStartPar
Triton does this automatically for you.


\subsubsection{Strategy 4: Memory Coalescing}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-4-memory-coalescing}}
\sphinxAtStartPar
Ensure adjacent threads access adjacent memory.

\sphinxAtStartPar
\sphinxstylestrong{Coalesced} (fast):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Thread i accesses address base + i}
\PYG{n}{offset} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE} \PYG{o}{+} \PYG{n}{thread\PYGZus{}id}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{load}\PYG{p}{(}\PYG{n}{ptr} \PYG{o}{+} \PYG{n}{offset}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Uncoalesced} (slow):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Random access pattern}
\PYG{n}{offset} \PYG{o}{=} \PYG{n}{random\PYGZus{}indices}\PYG{p}{[}\PYG{n}{thread\PYGZus{}id}\PYG{p}{]}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{load}\PYG{p}{(}\PYG{n}{ptr} \PYG{o}{+} \PYG{n}{offset}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Triton’s built\sphinxhyphen{}in patterns are usually coalesced.


\subsection{Compute Optimization}
\label{\detokenize{gpu-concepts/performance-optimization:compute-optimization}}

\subsubsection{Strategy 1: Use Tensor Cores}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-1-use-tensor-cores}}
\sphinxAtStartPar
For matrix operations, Tensor Cores provide 10\sphinxhyphen{}100x speedup!

\sphinxAtStartPar
\sphinxstylestrong{Automatically used by}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{accumulator} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{,} \PYG{n}{accumulator}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Requirements}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
FP16, BF16, TF32, FP8, or INT8 inputs

\item {} 
\sphinxAtStartPar
Block sizes multiple of 16

\item {} 
\sphinxAtStartPar
Contiguous memory layout

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Performance}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Regular cores: \textasciitilde{}30 TFLOPS

\item {} 
\sphinxAtStartPar
With Tensor Cores: \textasciitilde{}300 TFLOPS

\end{itemize}


\subsubsection{Strategy 2: Increase Arithmetic Intensity}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-2-increase-arithmetic-intensity}}
\sphinxAtStartPar
Do more work per byte loaded:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Low AI: Load data, do simple operation}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{l+m+mi}{1}  \PYG{c+c1}{\PYGZsh{} AI = 1 FLOP / 8 bytes = 0.125}

\PYG{c+c1}{\PYGZsh{} High AI: Load data, do many operations}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{a} \PYG{o}{*} \PYG{n}{x}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{3} \PYG{o}{+} \PYG{n}{b} \PYG{o}{*} \PYG{n}{x}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{c} \PYG{o}{*} \PYG{n}{x} \PYG{o}{+} \PYG{n}{d}
\PYG{c+c1}{\PYGZsh{} AI = 7 FLOPs / 8 bytes = 0.875}
\end{sphinxVerbatim}

\sphinxAtStartPar
Tiling naturally increases AI by reusing data.


\subsubsection{Strategy 3: Minimize Thread Divergence}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-3-minimize-thread-divergence}}
\sphinxAtStartPar
\sphinxstylestrong{Use predication}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Bad: Branching}
\PYG{k}{if} \PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{n}{threshold}\PYG{p}{:}
    \PYG{n}{result} \PYG{o}{=} \PYG{n}{expensive\PYGZus{}computation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{result} \PYG{o}{=} \PYG{l+m+mi}{0}

\PYG{c+c1}{\PYGZsh{} Good: Predication}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{n}{threshold}\PYG{p}{,} \PYG{n}{expensive\PYGZus{}computation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Note}: For expensive computations, short\sphinxhyphen{}circuit evaluation helps.


\subsubsection{Strategy 4: Optimize Loop Structure}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-4-optimize-loop-structure}}
\sphinxAtStartPar
\sphinxstylestrong{Unroll loops when possible}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Compiler can unroll for better instruction\PYGZhy{}level parallelism}
\PYG{c+c1}{\PYGZsh{}pragma unroll}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{K}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}K}\PYG{p}{)}\PYG{p}{:}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

\sphinxAtStartPar
Triton auto\sphinxhyphen{}unrolls when beneficial.


\subsection{Occupancy Optimization}
\label{\detokenize{gpu-concepts/performance-optimization:occupancy-optimization}}

\subsubsection{Strategy 1: Reduce Register Usage}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-1-reduce-register-usage}}
\sphinxAtStartPar
\sphinxstylestrong{Monitor}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{metrics} \PYG{n}{sm\PYGZus{}\PYGZus{}sass\PYGZus{}inst\PYGZus{}executed\PYGZus{}per\PYGZus{}thread}\PYG{o}{.}\PYG{n}{avg}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Reduce by}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Smaller local arrays

\item {} 
\sphinxAtStartPar
Recompute instead of store

\item {} 
\sphinxAtStartPar
Use shared memory for large temporaries

\end{itemize}


\subsubsection{Strategy 2: Tune Shared Memory}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-2-tune-shared-memory}}
\sphinxAtStartPar
\sphinxstylestrong{Too much}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Only 2 blocks fit per SM}
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{shared} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1024}\PYG{p}{,} \PYG{l+m+mi}{1024}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Too big!}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Balanced}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{shared} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{]}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} BLOCK\PYGZus{}SIZE=128 \PYGZhy{}\PYGZgt{} 64KB, allows 4 blocks per SM}
\end{sphinxVerbatim}


\subsubsection{Strategy 3: Adjust Block Size}
\label{\detokenize{gpu-concepts/performance-optimization:strategy-3-adjust-block-size}}
\sphinxAtStartPar
\sphinxstylestrong{Trade\sphinxhyphen{}offs}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Smaller blocks: Higher occupancy, less data reuse

\item {} 
\sphinxAtStartPar
Larger blocks: Lower occupancy, more data reuse

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Find sweet spot with auto\sphinxhyphen{}tuning!}


\subsection{Auto\sphinxhyphen{}Tuning}
\label{\detokenize{gpu-concepts/performance-optimization:auto-tuning}}

\subsubsection{Why Auto\sphinxhyphen{}Tune?}
\label{\detokenize{gpu-concepts/performance-optimization:why-auto-tune}}
\sphinxAtStartPar
Optimal configuration varies with:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Problem size (M, N, K)

\item {} 
\sphinxAtStartPar
Hardware (A100 vs H100)

\item {} 
\sphinxAtStartPar
Data type (FP16 vs FP8)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Solution}: Try multiple configs, pick the fastest.


\subsubsection{Triton Auto\sphinxhyphen{}Tuning}
\label{\detokenize{gpu-concepts/performance-optimization:triton-auto-tuning}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{autotune}\PYG{p}{(}
    \PYG{n}{configs}\PYG{o}{=}\PYG{p}{[}
        \PYG{n}{triton}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}M}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}N}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}stages}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{num\PYGZus{}warps}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{triton}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}M}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{128}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}N}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}stages}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{num\PYGZus{}warps}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{triton}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}M}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}N}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{128}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}stages}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{num\PYGZus{}warps}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{,}
        \PYG{c+c1}{\PYGZsh{} ... more configs}
    \PYG{p}{]}\PYG{p}{,}
    \PYG{n}{key}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{M}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{N}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{K}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Cache best config for these values}
\PYG{p}{)}
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Process}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
First call: Triton benchmarks all configs

\item {} 
\sphinxAtStartPar
Caches best config for (M, N, K)

\item {} 
\sphinxAtStartPar
Subsequent calls: Uses cached best config

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Config Parameters}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{BLOCK\_M, BLOCK\_N, BLOCK\_K}}: Tile sizes

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{num\_stages}}: Software pipelining depth (2\sphinxhyphen{}5)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{num\_warps}}: Warps per block (2, 4, 8, 16)

\end{itemize}


\subsection{Advanced Techniques}
\label{\detokenize{gpu-concepts/performance-optimization:advanced-techniques}}

\subsubsection{Warp Specialization}
\label{\detokenize{gpu-concepts/performance-optimization:warp-specialization}}
\sphinxAtStartPar
Different warps do different tasks:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Warp 0\PYGZhy{}3: Load data from memory}
\PYG{c+c1}{\PYGZsh{} Warp 4\PYGZhy{}7: Compute on data}
\end{sphinxVerbatim}

\sphinxAtStartPar
Benefits:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Overlap memory and compute

\item {} 
\sphinxAtStartPar
Better resource utilization

\end{itemize}

\sphinxAtStartPar
Available on Hopper/Blackwell GPUs.


\subsubsection{Persistent Kernels}
\label{\detokenize{gpu-concepts/performance-optimization:persistent-kernels}}
\sphinxAtStartPar
Each block processes multiple work items:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{item} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{start}\PYG{p}{,} \PYG{n}{end}\PYG{p}{,} \PYG{n}{stride}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{process}\PYG{p}{(}\PYG{n}{item}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Benefits:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Amortize launch overhead

\item {} 
\sphinxAtStartPar
Better L2 cache utilization

\item {} 
\sphinxAtStartPar
Flexible load balancing

\end{itemize}


\subsubsection{Recomputation}
\label{\detokenize{gpu-concepts/performance-optimization:recomputation}}
\sphinxAtStartPar
Trade compute for memory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Standard: Store intermediate}
\PYG{n}{intermediate} \PYG{o}{=} \PYG{n}{expensive\PYGZus{}compute}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{save}\PYG{p}{(}\PYG{n}{intermediate}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Memory cost}
\PYG{n}{later\PYGZus{}use}\PYG{p}{(}\PYG{n}{intermediate}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Optimized: Recompute}
\PYG{n}{later\PYGZus{}use}\PYG{p}{(}\PYG{n}{expensive\PYGZus{}compute}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Compute cost, no memory}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{When useful}: Memory\sphinxhyphen{}bound kernels with spare compute.

\sphinxAtStartPar
\sphinxstylestrong{Example}: Flash Attention recomputes attention scores in backward pass.


\subsection{Common Patterns}
\label{\detokenize{gpu-concepts/performance-optimization:common-patterns}}

\subsubsection{Pattern: Reduction}
\label{\detokenize{gpu-concepts/performance-optimization:pattern-reduction}}
\sphinxAtStartPar
Sum/max/min across elements:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{reduce\PYGZus{}kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Load data}
    \PYG{n}{data} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Reduce}
    \PYG{n}{result} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Efficient reduction}

    \PYG{c+c1}{\PYGZsh{} Store}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{result}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key}: Triton uses warp shuffles and shared memory efficiently.


\subsubsection{Pattern: Element\sphinxhyphen{}wise}
\label{\detokenize{gpu-concepts/performance-optimization:pattern-element-wise}}
\sphinxAtStartPar
Independent operation on each element:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{elementwise\PYGZus{}kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{offsets} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Any function}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{y\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Optimization}: Fuse multiple element\sphinxhyphen{}wise ops.


\subsubsection{Pattern: Matrix Multiply}
\label{\detokenize{gpu-concepts/performance-optimization:pattern-matrix-multiply}}
\sphinxAtStartPar
Blocked algorithm with tiling:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{K}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}K}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{a} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{a\PYGZus{}ptrs}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Load A block}
    \PYG{n}{b} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{b\PYGZus{}ptrs}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Load B block}
    \PYG{n}{acc} \PYG{o}{+}\PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Accumulate}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key}: Data reuse in SRAM, Tensor Core usage.


\subsection{Debugging Performance Issues}
\label{\detokenize{gpu-concepts/performance-optimization:debugging-performance-issues}}

\subsubsection{Issue: Low Bandwidth}
\label{\detokenize{gpu-concepts/performance-optimization:issue-low-bandwidth}}
\sphinxAtStartPar
\sphinxstylestrong{Symptoms}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Achieved bandwidth \textless{}\textless{} peak bandwidth

\item {} 
\sphinxAtStartPar
Memory\sphinxhyphen{}bound operation

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Check}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Coalescing: Use \sphinxcode{\sphinxupquote{ncu \sphinxhyphen{}\sphinxhyphen{}metrics l1tex\_\_t\_sectors\_pipe\_lsu\_mem\_global\_op\_ld.sum}}

\item {} 
\sphinxAtStartPar
L2 hit rate: Might be hitting cache

\item {} 
\sphinxAtStartPar
Occupancy: Too low?

\end{enumerate}


\subsubsection{Issue: Low Compute Utilization}
\label{\detokenize{gpu-concepts/performance-optimization:issue-low-compute-utilization}}
\sphinxAtStartPar
\sphinxstylestrong{Symptoms}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Low \% of peak FLOPs

\item {} 
\sphinxAtStartPar
Compute\sphinxhyphen{}bound operation

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Check}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Tensor Core usage: Are they being used?

\item {} 
\sphinxAtStartPar
Thread divergence: Branching killing performance?

\item {} 
\sphinxAtStartPar
Occupancy: Need more warps?

\end{enumerate}


\subsubsection{Issue: Lower Than PyTorch}
\label{\detokenize{gpu-concepts/performance-optimization:issue-lower-than-pytorch}}
\sphinxAtStartPar
\sphinxstylestrong{Possible causes}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Not using Tensor Cores (PyTorch does)

\item {} 
\sphinxAtStartPar
Sub\sphinxhyphen{}optimal config (need auto\sphinxhyphen{}tuning)

\item {} 
\sphinxAtStartPar
Missing optimizations (fusion, tiling)

\item {} 
\sphinxAtStartPar
Data layout issues (non\sphinxhyphen{}contiguous)

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Debug}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Profile both with \sphinxcode{\sphinxupquote{ncu}}

\item {} 
\sphinxAtStartPar
Compare metrics (bandwidth, compute, occupancy)

\item {} 
\sphinxAtStartPar
Check if PyTorch uses vendor lib (cuBLAS, cuDNN)

\end{itemize}


\subsection{Performance Checklist}
\label{\detokenize{gpu-concepts/performance-optimization:performance-checklist}}

\subsubsection{Before You Optimize}
\label{\detokenize{gpu-concepts/performance-optimization:before-you-optimize}}
\sphinxAtStartPar
{[} {]} Profile to identify bottleneck
{[} {]} Measure baseline performance
{[} {]} Set performance target (realistic!)


\subsubsection{Memory Optimizations}
\label{\detokenize{gpu-concepts/performance-optimization:memory-optimizations}}
\sphinxAtStartPar
{[} {]} Fuse operations to reduce memory traffic
{[} {]} Use tiling to maximize SRAM reuse
{[} {]} Ensure coalesced memory accesses
{[} {]} Minimize global memory accesses


\subsubsection{Compute Optimizations}
\label{\detokenize{gpu-concepts/performance-optimization:compute-optimizations}}
\sphinxAtStartPar
{[} {]} Use Tensor Cores for matmul
{[} {]} Minimize thread divergence
{[} {]} Maximize arithmetic intensity
{[} {]} Use appropriate data types (FP16, BF16)


\subsubsection{Occupancy Optimization}
\label{\detokenize{gpu-concepts/performance-optimization:id1}}
\sphinxAtStartPar
{[} {]} Check register usage
{[} {]} Check shared memory usage
{[} {]} Tune block size
{[} {]} Measure actual occupancy


\subsubsection{Advanced}
\label{\detokenize{gpu-concepts/performance-optimization:advanced}}
\sphinxAtStartPar
{[} {]} Auto\sphinxhyphen{}tune configurations
{[} {]} Use software pipelining
{[} {]} Consider persistent kernels
{[} {]} Profile with vendor tools


\subsection{Summary}
\label{\detokenize{gpu-concepts/performance-optimization:summary}}
\sphinxAtStartPar
\sphinxstylestrong{Optimization hierarchy}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Algorithm}: Choose efficient algorithm

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory}: Reduce traffic, maximize reuse

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compute}: Use specialized hardware (Tensor Cores)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Parallelism}: Balance resources for good occupancy

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tuning}: Auto\sphinxhyphen{}tune for specific hardware and problem sizes

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Remember}: Profile \sphinxhyphen{}\textgreater{} Optimize \sphinxhyphen{}\textgreater{} Measure \sphinxhyphen{}\textgreater{} Repeat


\subsection{Next Steps}
\label{\detokenize{gpu-concepts/performance-optimization:next-steps}}
\sphinxAtStartPar
Apply these concepts in practice:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{../tutorials/02\sphinxhyphen{}fused\sphinxhyphen{}softmax}}} \sphinxhyphen{} Memory optimization through fusion

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{../tutorials/03\sphinxhyphen{}matrix\sphinxhyphen{}multiplication}}} \sphinxhyphen{} Compute optimization with Tensor Cores

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{../tutorials/06\sphinxhyphen{}fused\sphinxhyphen{}attention}}} \sphinxhyphen{} Advanced optimization combining all techniques

\end{itemize}

\sphinxstepscope


\section{Vector Addition in Triton}
\label{\detokenize{gpu-tutorials/01-vector-add:vector-addition-in-triton}}\label{\detokenize{gpu-tutorials/01-vector-add::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/01-vector-add:overview}}
\sphinxAtStartPar
Vector addition is the fundamental “Hello World” of GPU programming. This tutorial demonstrates how to write a simple element\sphinxhyphen{}wise vector addition kernel in Triton and introduces core GPU programming concepts.


\subsection{What You’ll Learn}
\label{\detokenize{gpu-tutorials/01-vector-add:what-you-ll-learn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The basic programming model of GPU parallel computing

\item {} 
\sphinxAtStartPar
How Triton kernels are structured using \sphinxcode{\sphinxupquote{@triton.jit}}

\item {} 
\sphinxAtStartPar
The SPMD (Single Program, Multiple Data) execution model

\item {} 
\sphinxAtStartPar
Memory access patterns and coalescing

\item {} 
\sphinxAtStartPar
How to benchmark GPU kernels

\end{itemize}


\subsection{GPU/CUDA Concepts}
\label{\detokenize{gpu-tutorials/01-vector-add:gpu-cuda-concepts}}

\subsubsection{SPMD (Single Program, Multiple Data)}
\label{\detokenize{gpu-tutorials/01-vector-add:spmd-single-program-multiple-data}}
\sphinxAtStartPar
Unlike traditional CPU programming where you write a loop to process all elements, GPU programming uses the SPMD model:
\sphinxhyphen{} You write code that processes \sphinxstylestrong{one block} of data
\sphinxhyphen{} The GPU launches \sphinxstylestrong{thousands of copies} of this program in parallel
\sphinxhyphen{} Each copy (called a “program” in Triton, or “thread block” in CUDA) processes a different portion of the data


\subsubsection{Program ID and Block Processing}
\label{\detokenize{gpu-tutorials/01-vector-add:program-id-and-block-processing}}
\sphinxAtStartPar
In the kernel:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pid} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Get this program\PYGZsq{}s unique ID}
\PYG{n}{block\PYGZus{}start} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE}
\PYG{n}{offsets} \PYG{o}{=} \PYG{n}{block\PYGZus{}start} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you have a vector of 256 elements and \sphinxcode{\sphinxupquote{BLOCK\_SIZE=64}}:
\sphinxhyphen{} Program 0 processes elements {[}0:64{]}
\sphinxhyphen{} Program 1 processes elements {[}64:128{]}
\sphinxhyphen{} Program 2 processes elements {[}128:192{]}
\sphinxhyphen{} Program 3 processes elements {[}192:256{]}


\subsubsection{Grid Size}
\label{\detokenize{gpu-tutorials/01-vector-add:grid-size}}
\sphinxAtStartPar
The “grid” determines how many program instances run in parallel:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grid} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{meta}\PYG{p}{:} \PYG{p}{(}\PYG{n}{triton}\PYG{o}{.}\PYG{n}{cdiv}\PYG{p}{(}\PYG{n}{n\PYGZus{}elements}\PYG{p}{,} \PYG{n}{meta}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}SIZE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
This calculates: \sphinxcode{\sphinxupquote{num\_programs = ceil(n\_elements / BLOCK\_SIZE)}}


\subsubsection{Memory Hierarchy}
\label{\detokenize{gpu-tutorials/01-vector-add:memory-hierarchy}}
\sphinxAtStartPar
GPUs have a multi\sphinxhyphen{}level memory hierarchy:
1. \sphinxstylestrong{Global Memory (DRAM)} \sphinxhyphen{} Large but slow (hundreds of GB/s bandwidth)
2. \sphinxstylestrong{L2 Cache} \sphinxhyphen{} Medium size, faster (TBs/s)
3. \sphinxstylestrong{L1 Cache/Shared Memory (SRAM)} \sphinxhyphen{} Small but very fast (10+ TB/s)
4. \sphinxstylestrong{Registers} \sphinxhyphen{} Fastest, per\sphinxhyphen{}thread storage


\subsubsection{Memory Coalescing}
\label{\detokenize{gpu-tutorials/01-vector-add:memory-coalescing}}
\sphinxAtStartPar
When loading data from DRAM:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Triton automatically coalesces these loads. In CUDA terms, this means:
\sphinxhyphen{} Adjacent threads load adjacent memory addresses
\sphinxhyphen{} The memory controller combines multiple loads into fewer transactions
\sphinxhyphen{} This is crucial for achieving high bandwidth


\subsubsection{Masking for Boundary Conditions}
\label{\detokenize{gpu-tutorials/01-vector-add:masking-for-boundary-conditions}}
\sphinxAtStartPar
What if your vector size isn’t a perfect multiple of BLOCK\_SIZE?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mask} \PYG{o}{=} \PYG{n}{offsets} \PYG{o}{\PYGZlt{}} \PYG{n}{n\PYGZus{}elements}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
The mask ensures out\sphinxhyphen{}of\sphinxhyphen{}bounds elements aren’t loaded

\item {} 
\sphinxAtStartPar
This prevents memory access violations

\item {} 
\sphinxAtStartPar
In CUDA, you’d typically use conditional statements for this

\end{itemize}


\subsection{How the Kernel Works}
\label{\detokenize{gpu-tutorials/01-vector-add:how-the-kernel-works}}

\subsubsection{Step\sphinxhyphen{}by\sphinxhyphen{}Step Execution}
\label{\detokenize{gpu-tutorials/01-vector-add:step-by-step-execution}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Get Program ID}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pid} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Each parallel program instance gets a unique ID (0, 1, 2, …).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Calculate Block Offsets}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{block\PYGZus{}start} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE}
\PYG{n}{offsets} \PYG{o}{=} \PYG{n}{block\PYGZus{}start} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Determines which elements this program will process.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Create Boundary Mask}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mask} \PYG{o}{=} \PYG{n}{offsets} \PYG{o}{\PYGZlt{}} \PYG{n}{n\PYGZus{}elements}
\end{sphinxVerbatim}

\sphinxAtStartPar
Handles cases where input size isn’t a multiple of BLOCK\_SIZE.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Load Data from DRAM to Registers}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{y\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Brings data from slow global memory to fast registers.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compute Result}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{output} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}
\end{sphinxVerbatim}

\sphinxAtStartPar
Performs the actual addition in registers (extremely fast).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Store Result Back to DRAM}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{output\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{output}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Writes the result back to global memory.

\end{enumerate}


\subsection{Performance Characteristics}
\label{\detokenize{gpu-tutorials/01-vector-add:performance-characteristics}}

\subsubsection{Bandwidth\sphinxhyphen{}Bound Operation}
\label{\detokenize{gpu-tutorials/01-vector-add:bandwidth-bound-operation}}
\sphinxAtStartPar
Vector addition is \sphinxstylestrong{memory\sphinxhyphen{}bound}, not \sphinxstylestrong{compute\sphinxhyphen{}bound}:
\sphinxhyphen{} \sphinxstylestrong{Compute}: Just one addition per element (trivial)
\sphinxhyphen{} \sphinxstylestrong{Memory}: Must read 2 values and write 1 value (3x data movement)


\subsubsection{Arithmetic Intensity}
\label{\detokenize{gpu-tutorials/01-vector-add:arithmetic-intensity}}
\sphinxAtStartPar
Arithmetic Intensity = FLOPs / Bytes Transferred
\sphinxhyphen{} For vector add: 1 FLOP / 12 bytes (assuming float32) = 0.083 FLOPs/byte
\sphinxhyphen{} Very low! Most time is spent waiting for memory.


\subsubsection{Theoretical Performance}
\label{\detokenize{gpu-tutorials/01-vector-add:theoretical-performance}}
\sphinxAtStartPar
For a GPU with 1 TB/s memory bandwidth:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Max} \PYG{n}{GB}\PYG{o}{/}\PYG{n}{s} \PYG{o}{=} \PYG{n}{Memory} \PYG{n}{Bandwidth} \PYG{o}{=} \PYG{l+m+mi}{1000} \PYG{n}{GB}\PYG{o}{/}\PYG{n}{s}
\PYG{n}{For} \PYG{n}{vector} \PYG{n}{add}\PYG{p}{:} \PYG{n}{Need} \PYG{n}{to} \PYG{n}{move} \PYG{l+m+mi}{3} \PYG{o}{*} \PYG{l+m+mi}{4} \PYG{n+nb}{bytes} \PYG{n}{per} \PYG{n}{element} \PYG{o}{=} \PYG{l+m+mi}{12} \PYG{n+nb}{bytes}
\PYG{n}{Max} \PYG{n}{elements}\PYG{o}{/}\PYG{n}{s} \PYG{o}{=} \PYG{l+m+mi}{1000} \PYG{n}{GB}\PYG{o}{/}\PYG{n}{s} \PYG{o}{/} \PYG{l+m+mi}{12} \PYG{n+nb}{bytes} \PYG{o}{\PYGZti{}} \PYG{l+m+mi}{83} \PYG{n}{billion} \PYG{n}{elements}\PYG{o}{/}\PYG{n}{s}
\end{sphinxVerbatim}


\subsection{Triton\sphinxhyphen{}Specific Features}
\label{\detokenize{gpu-tutorials/01-vector-add:triton-specific-features}}

\subsubsection{\sphinxstyleliteralintitle{\sphinxupquote{@triton.jit}} Decorator}
\label{\detokenize{gpu-tutorials/01-vector-add:triton-jit-decorator}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{add\PYGZus{}kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Marks the function as a Triton kernel

\item {} 
\sphinxAtStartPar
Triton JIT\sphinxhyphen{}compiles this to optimized GPU code

\item {} 
\sphinxAtStartPar
Automatically handles type inference and optimization

\end{itemize}


\subsubsection{\sphinxstyleliteralintitle{\sphinxupquote{constexpr}} for Compile\sphinxhyphen{}Time Constants}
\label{\detokenize{gpu-tutorials/01-vector-add:constexpr-for-compile-time-constants}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Tells Triton this value is known at compile time

\item {} 
\sphinxAtStartPar
Allows better optimizations (loop unrolling, etc.)

\item {} 
\sphinxAtStartPar
In CUDA, this is like template parameters

\end{itemize}


\subsubsection{Launch Grid Syntax}
\label{\detokenize{gpu-tutorials/01-vector-add:launch-grid-syntax}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{add\PYGZus{}kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{output}\PYG{p}{,} \PYG{n}{n\PYGZus{}elements}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{o}{=}\PYG{l+m+mi}{1024}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{{[}grid{]}}} specifies how many program instances to launch

\item {} 
\sphinxAtStartPar
Triton automatically converts torch tensors to pointers

\item {} 
\sphinxAtStartPar
Meta\sphinxhyphen{}parameters (like BLOCK\_SIZE) are passed as keywords

\end{itemize}


\subsection{Benchmarking Insights}
\label{\detokenize{gpu-tutorials/01-vector-add:benchmarking-insights}}
\sphinxAtStartPar
The benchmark code measures \sphinxstylestrong{GB/s (Gigabytes per second)}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{gbps} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{ms}\PYG{p}{:} \PYG{l+m+mi}{3} \PYG{o}{*} \PYG{n}{x}\PYG{o}{.}\PYG{n}{numel}\PYG{p}{(}\PYG{p}{)} \PYG{o}{*} \PYG{n}{x}\PYG{o}{.}\PYG{n}{element\PYGZus{}size}\PYG{p}{(}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{1e\PYGZhy{}9} \PYG{o}{/} \PYG{p}{(}\PYG{n}{ms} \PYG{o}{*} \PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Why multiply by 3?
\sphinxhyphen{} Read x: \sphinxcode{\sphinxupquote{n\_elements * 4 bytes}}
\sphinxhyphen{} Read y: \sphinxcode{\sphinxupquote{n\_elements * 4 bytes}}
\sphinxhyphen{} Write output: \sphinxcode{\sphinxupquote{n\_elements * 4 bytes}}
\sphinxhyphen{} Total: \sphinxcode{\sphinxupquote{3 * n\_elements * 4 bytes}}


\subsubsection{Expected Results}
\label{\detokenize{gpu-tutorials/01-vector-add:expected-results}}
\sphinxAtStartPar
For modern GPUs:
\sphinxhyphen{} \sphinxstylestrong{Peak memory bandwidth}: 500\sphinxhyphen{}2000 GB/s
\sphinxhyphen{} \sphinxstylestrong{Typical achieved bandwidth}: 60\sphinxhyphen{}80\% of peak
\sphinxhyphen{} Both PyTorch and Triton should achieve similar performance (both are memory\sphinxhyphen{}bound)


\subsection{Common Patterns You’ll See}
\label{\detokenize{gpu-tutorials/01-vector-add:common-patterns-you-ll-see}}

\subsubsection{1. Pointer Arithmetic}
\label{\detokenize{gpu-tutorials/01-vector-add:pointer-arithmetic}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}
\end{sphinxVerbatim}

\sphinxAtStartPar
In Triton, pointers are just memory addresses, and you can add offsets to them.


\subsubsection{2. Vectorized Operations}
\label{\detokenize{gpu-tutorials/01-vector-add:vectorized-operations}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{output} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}
\end{sphinxVerbatim}

\sphinxAtStartPar
This single line actually adds BLOCK\_SIZE elements in parallel!


\subsubsection{3. Masked Memory Operations}
\label{\detokenize{gpu-tutorials/01-vector-add:masked-memory-operations}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{ptr}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{ptr}\PYG{p}{,} \PYG{n}{value}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Essential for handling non\sphinxhyphen{}uniform sizes safely.


\subsection{Key Takeaways}
\label{\detokenize{gpu-tutorials/01-vector-add:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GPU programming uses SPMD}: Write code for one block, GPU runs many copies in parallel

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Block size matters}: Too small = overhead, too large = wasted resources

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory is the bottleneck}: For simple operations like vector add, memory bandwidth limits performance

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Masking is essential}: Handle boundary conditions properly to avoid crashes

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Triton abstracts complexity}: You don’t need to write raw CUDA to get good performance

\end{enumerate}


\subsection{Comparison to CUDA}
\label{\detokenize{gpu-tutorials/01-vector-add:comparison-to-cuda}}
\sphinxAtStartPar
If you were to write this in CUDA, you’d need:
\sphinxhyphen{} Explicit block/thread indexing (\sphinxcode{\sphinxupquote{blockIdx.x}}, \sphinxcode{\sphinxupquote{threadIdx.x}})
\sphinxhyphen{} Manual memory management (cudaMalloc, cudaMemcpy)
\sphinxhyphen{} Kernel launch syntax (\sphinxcode{\sphinxupquote{kernel\textless{}\textless{}\textless{}grid, block\textgreater{}\textgreater{}\textgreater{}}})
\sphinxhyphen{} Error checking boilerplate

\sphinxAtStartPar
Triton handles most of this automatically while still achieving similar performance!


\subsection{Next Steps}
\label{\detokenize{gpu-tutorials/01-vector-add:next-steps}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Try different BLOCK\_SIZE values (powers of 2 work best)

\item {} 
\sphinxAtStartPar
Experiment with different input sizes

\item {} 
\sphinxAtStartPar
Look at the generated assembly with \sphinxcode{\sphinxupquote{triton.tools.disasm()}}

\item {} 
\sphinxAtStartPar
Profile with \sphinxcode{\sphinxupquote{nvprof}} or \sphinxcode{\sphinxupquote{nsys}} to see memory bandwidth utilization

\end{itemize}

\sphinxstepscope


\section{Fused Softmax in Triton}
\label{\detokenize{gpu-tutorials/02-fused-softmax:fused-softmax-in-triton}}\label{\detokenize{gpu-tutorials/02-fused-softmax::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/02-fused-softmax:overview}}
\sphinxAtStartPar
This tutorial demonstrates \sphinxstylestrong{kernel fusion}, a critical GPU optimization technique. By fusing the softmax operation into a single kernel, we can achieve \textasciitilde{}4x speedup over naive implementations by reducing memory traffic.


\subsection{What You’ll Learn}
\label{\detokenize{gpu-tutorials/02-fused-softmax:what-you-ll-learn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The concept and benefits of \sphinxstylestrong{kernel fusion}

\item {} 
\sphinxAtStartPar
How to perform \sphinxstylestrong{reduction operations} on GPU

\item {} 
\sphinxAtStartPar
Why \sphinxstylestrong{bandwidth\sphinxhyphen{}bound operations} benefit from fusion

\item {} 
\sphinxAtStartPar
The difference between \sphinxstylestrong{SRAM} and \sphinxstylestrong{DRAM}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Numerical stability} techniques for softmax

\end{itemize}


\subsection{The Problem with Naive Softmax}
\label{\detokenize{gpu-tutorials/02-fused-softmax:the-problem-with-naive-softmax}}

\subsubsection{Standard Softmax Formula}
\label{\detokenize{gpu-tutorials/02-fused-softmax:standard-softmax-formula}}
\sphinxAtStartPar
softmax(x\_i) = exp(x\_i) / SIGMA exp(x\_j)

\sphinxAtStartPar
For numerical stability, we subtract the max:

\sphinxAtStartPar
softmax(x\_i) = exp(x\_i \sphinxhyphen{} max(x)) / SIGMA exp(x\_i \sphinxhyphen{} max(x))


\subsubsection{Naive PyTorch Implementation}
\label{\detokenize{gpu-tutorials/02-fused-softmax:naive-pytorch-implementation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
x\_max = x.max(dim=1){[}0{]}           \# Read MN, write M
z = x \sphinxhyphen{} x\_max{[}:, None{]}             \# Read MN+M, write MN
numerator = torch.exp(z)           \# Read MN, write MN
denominator = numerator.sum(dim=1) \# Read MN, write M
ret = numerator / denominator      \# Read MN+M, write MN

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Total} \PYG{n}{Memory} \PYG{n}{Traffic}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reads}: 5MN + 2M elements

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Writes}: 3MN + 2M elements

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Total}: 8MN + 4M elements

\end{itemize}

\sphinxAtStartPar
For a matrix of size 4096x1024 (float32):
\sphinxhyphen{} Total data movement: \textasciitilde{}134 MB
\sphinxhyphen{} But the actual result is only: \textasciitilde{}16 MB

\sphinxAtStartPar
\sphinxstylestrong{Problem}: We’re moving 8x more data than necessary!


\subsection{GPU Memory Hierarchy}
\label{\detokenize{gpu-tutorials/02-fused-softmax:gpu-memory-hierarchy}}

\subsubsection{DRAM (Global Memory)}
\label{\detokenize{gpu-tutorials/02-fused-softmax:dram-global-memory}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Size}: 8\sphinxhyphen{}80 GB

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Bandwidth}: 500\sphinxhyphen{}2000 GB/s

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Latency}: 200\sphinxhyphen{}400 cycles

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Location}: Off\sphinxhyphen{}chip (separate memory chips)

\end{itemize}


\subsubsection{SRAM (Shared Memory / L1 Cache)}
\label{\detokenize{gpu-tutorials/02-fused-softmax:sram-shared-memory-l1-cache}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Size}: 48\sphinxhyphen{}256 KB per SM

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Bandwidth}: 10\sphinxhyphen{}20 TB/s (10\sphinxhyphen{}20x faster!)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Latency}: 1\sphinxhyphen{}10 cycles

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Location}: On\sphinxhyphen{}chip (on the GPU die)

\end{itemize}


\subsubsection{The Key Insight}
\label{\detokenize{gpu-tutorials/02-fused-softmax:the-key-insight}}
\sphinxAtStartPar
If a row fits in SRAM, we can:
1. Load the row once from DRAM \sphinxhyphen{}\textgreater{} SRAM
2. Do all computations in SRAM
3. Write result once back to DRAM

\sphinxAtStartPar
\sphinxstylestrong{Memory Traffic}: Read MN, write MN (2MN total) vs. 8MN+4M!

\sphinxAtStartPar
This is what \sphinxstylestrong{kernel fusion} achieves.


\subsection{How the Fused Kernel Works}
\label{\detokenize{gpu-tutorials/02-fused-softmax:how-the-fused-kernel-works}}

\subsubsection{Block\sphinxhyphen{}Level Processing}
\label{\detokenize{gpu-tutorials/02-fused-softmax:block-level-processing}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
row\_idx = tl.program\_id(0)

\sphinxAtStartPar
Each program processes one or more complete rows:
\sphinxhyphen{} \sphinxstylestrong{Why rows?} Softmax is row\sphinxhyphen{}wise operation (normalize each row independently)
\sphinxhyphen{} \sphinxstylestrong{One row per program}: Simple, good for small row sizes
\sphinxhyphen{} \sphinxstylestrong{Multiple rows per program}: Better for tiny rows (reduces launch overhead)


\subsubsection{Step 1: Load Row Into SRAM}
\label{\detokenize{gpu-tutorials/02-fused-softmax:step-1-load-row-into-sram}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
col\_offsets = tl.arange(0, BLOCK\_SIZE)
input\_ptrs = row\_start*ptr + col\_offsets
mask = col\_offsets \textless{} n\_cols
row = tl.load(input\_ptrs, mask=mask, other=\sphinxhyphen{}float(‘inf’))

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Important} \PYG{n}{Details}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{BLOCK\_SIZE}} must be power of 2 (GPU requirement)

\item {} 
\sphinxAtStartPar
If \sphinxcode{\sphinxupquote{n\_cols \textless{} BLOCK\_SIZE}}, we pad with \sphinxcode{\sphinxupquote{\sphinxhyphen{}inf}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\sphinxhyphen{}inf}} is safe: \sphinxcode{\sphinxupquote{exp(\sphinxhyphen{}inf) = 0}}, doesn’t affect sum

\end{itemize}


\subsubsection{Step 2: Compute Max (Reduction)}
\label{\detokenize{gpu-tutorials/02-fused-softmax:step-2-compute-max-reduction}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
row\_minus*max = row \sphinxhyphen{} tl.max(row, axis=0)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Reduction} \PYG{n}{Operation}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Combine} \PYG{n}{many} \PYG{n}{values} \PYG{n}{into} \PYG{n}{one}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Triton does this efficiently using \sphinxstylestrong{warp shuffles} and \sphinxstylestrong{shared memory}

\item {} 
\sphinxAtStartPar
In CUDA, you’d manually write a tree reduction

\item {} 
\sphinxAtStartPar
All happens in SRAM (very fast!)

\end{itemize}


\subsubsection{Step 3: Exponentiation}
\label{\detokenize{gpu-tutorials/02-fused-softmax:step-3-exponentiation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
numerator = tl.exp(row\_minus*max)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{GPU} \PYG{n}{Math} \PYG{n}{Functions}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Uses hardware\sphinxhyphen{}accelerated exp (CUDA \sphinxcode{\sphinxupquote{\_\_expf}})

\item {} 
\sphinxAtStartPar
Fast but approximate (\textasciitilde{}1 ULP error)

\item {} 
\sphinxAtStartPar
Good enough for neural networks!

\end{itemize}


\subsubsection{Step 4: Compute Sum (Another Reduction)}
\label{\detokenize{gpu-tutorials/02-fused-softmax:step-4-compute-sum-another-reduction}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
denominator = tl.sum(numerator, axis=0)

\sphinxAtStartPar
Again, efficient reduction in SRAM.


\subsubsection{Step 5: Normalize and Write Back}
\label{\detokenize{gpu-tutorials/02-fused-softmax:step-5-normalize-and-write-back}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
softmax\_output = numerator / denominator
tl.store(output\_ptrs, softmax\_output, mask=mask)

\sphinxAtStartPar
Only one write to DRAM for the entire row!


\subsection{Power\sphinxhyphen{}of\sphinxhyphen{}Two Block Sizes}
\label{\detokenize{gpu-tutorials/02-fused-softmax:power-of-two-block-sizes}}

\subsubsection{Why Must BLOCK\_SIZE Be Power of 2?}
\label{\detokenize{gpu-tutorials/02-fused-softmax:why-must-block-size-be-power-of-2}}
\sphinxAtStartPar
GPU hardware is optimized for power\sphinxhyphen{}of\sphinxhyphen{}2 sizes:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory Alignment}:
\sphinxhyphen{} Memory accesses are most efficient at 128\sphinxhyphen{}byte boundaries
\sphinxhyphen{} Powers of 2 naturally align to these boundaries

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Warp Operations}:
\sphinxhyphen{} NVIDIA warps are 32 threads
\sphinxhyphen{} Efficient operations require BLOCK\_SIZE \% 32 == 0
\sphinxhyphen{} Powers of 2 guarantee this

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reduction Trees}:
\sphinxhyphen{} Reduction algorithms are most efficient with power\sphinxhyphen{}of\sphinxhyphen{}2 sizes
\sphinxhyphen{} Enables balanced tree structure

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
BLOCK\_SIZE = triton.next\_power*of\_2(n\_cols)

\sphinxAtStartPar
For example:
\sphinxhyphen{} n\_cols = 1000 \sphinxhyphen{}\textgreater{} BLOCK\_SIZE = 1024
\sphinxhyphen{} n\_cols = 513 \sphinxhyphen{}\textgreater{} BLOCK\_SIZE = 1024
\sphinxhyphen{} n\_cols = 512 \sphinxhyphen{}\textgreater{} BLOCK\_SIZE = 512


\subsection{Occupancy and Performance Tuning}
\label{\detokenize{gpu-tutorials/02-fused-softmax:occupancy-and-performance-tuning}}

\subsubsection{Number of Warps}
\label{\detokenize{gpu-tutorials/02-fused-softmax:number-of-warps}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
num\_warps = 8

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Warps}\PYG{o}{*}\PYG{o}{*} \PYG{p}{(}\PYG{n}{NVIDIA}\PYG{p}{)} \PYG{o+ow}{or} \PYG{o}{*}\PYG{o}{*}\PYG{n}{Wavefronts}\PYG{o}{*}\PYG{o}{*} \PYG{p}{(}\PYG{n}{AMD}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Group of 32 threads (NVIDIA) or 64 threads (AMD)

\item {} 
\sphinxAtStartPar
Execute instructions in lockstep (SIMD)

\item {} 
\sphinxAtStartPar
GPU schedules at warp granularity

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Why 8 warps?}
\sphinxhyphen{} Total threads = 8 x 32 = 256 threads per block
\sphinxhyphen{} Enough to hide memory latency
\sphinxhyphen{} Not so many that we run out of registers


\subsubsection{Number of Pipeline Stages}
\label{\detokenize{gpu-tutorials/02-fused-softmax:number-of-pipeline-stages}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
num\_stages = 4 if SIZE\_SMEM \textgreater{} 200000 else 2

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Software} \PYG{n}{Pipelining}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Overlap} \PYG{n}{memory} \PYG{n}{loads} \PYG{k}{with} \PYG{n}{computation}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Stage 1: Load block A while computing nothing

\item {} 
\sphinxAtStartPar
Stage 2: Load block B while computing on block A

\item {} 
\sphinxAtStartPar
Stage 3: Load block C while computing on block B

\item {} 
\sphinxAtStartPar
…

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{More stages} = better hiding of memory latency, but requires more SRAM


\subsubsection{Computing Occupancy}
\label{\detokenize{gpu-tutorials/02-fused-softmax:computing-occupancy}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
occupancy = NUM\_REGS // (n\_regs * WARP\_SIZE * num\_warps)
occupancy = min(occupancy, SIZE\_SMEM // size\_smem)
num\_programs = NUM\_SM * occupancy

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Occupancy}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{How} \PYG{n}{many} \PYG{n}{thread} \PYG{n}{blocks} \PYG{n}{can} \PYG{n}{run} \PYG{n}{simultaneously} \PYG{n}{on} \PYG{n}{an} \PYG{n}{SM}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Limiting Factors}:
1. \sphinxstylestrong{Registers}: Each thread needs registers; limited supply per SM
2. \sphinxstylestrong{Shared Memory}: Each block uses SRAM; limited per SM
3. \sphinxstylestrong{Hardware limits}: Max threads/blocks per SM

\sphinxAtStartPar
\sphinxstylestrong{Example Calculation} (A100 GPU):
\sphinxhyphen{} NUM\_SM = 108 (streaming multiprocessors)
\sphinxhyphen{} NUM\_REGS = 65536 registers per SM
\sphinxhyphen{} SIZE\_SMEM = 164 KB per SM
\sphinxhyphen{} num\_warps = 8, WARP\_SIZE = 32
\sphinxhyphen{} Suppose kernel uses 64 registers/thread

\sphinxAtStartPar
Register occupancy:

\sphinxAtStartPar
occupancy = 65536 / (64 * 32 * 8) = 65536 / 16384 \textasciitilde{} 4 blocks per SM

\sphinxAtStartPar
Total concurrent blocks:

\sphinxAtStartPar
num\_programs = 108 * 4 = 432 blocks

\sphinxAtStartPar
If you have 4096 rows, many waves needed:

\sphinxAtStartPar
waves = ceil(4096 / 432) \textasciitilde{} 10 waves


\subsection{Persistent Kernels}
\label{\detokenize{gpu-tutorials/02-fused-softmax:persistent-kernels}}

\subsubsection{The Pattern}
\label{\detokenize{gpu-tutorials/02-fused-softmax:the-pattern}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{for row\_idx in tl.range(row\_start, n\_rows, row\_step, num\_stages=num\_stages):}
\sphinxAtStartPar
\# process row\_idx

\end{description}

\sphinxAtStartPar
Instead of launching one kernel per row:
\sphinxhyphen{} Launch \sphinxcode{\sphinxupquote{num\_programs}} kernels total
\sphinxhyphen{} Each kernel processes multiple rows in a loop
\sphinxhyphen{} Called \sphinxstylestrong{persistent kernels} or \sphinxstylestrong{persistent threads}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
1. Amortize kernel launch overhead
2. Better L2 cache utilization
3. More flexible load balancing

\sphinxAtStartPar
\sphinxstylestrong{When to use}:
\sphinxhyphen{} Many small tasks (like many small rows)
\sphinxhyphen{} When kernel launch overhead is significant


\subsection{Numerical Stability}
\label{\detokenize{gpu-tutorials/02-fused-softmax:numerical-stability}}

\subsubsection{Why Subtract Max?}
\label{\detokenize{gpu-tutorials/02-fused-softmax:why-subtract-max}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
row\_minus*max = row \sphinxhyphen{} tl.max(row, axis=0)

\sphinxAtStartPar
Without this, for large values:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
exp(1000) = overflow! (inf in float32)
softmax({[}1000, 1001, 1002{]}) \sphinxhyphen{}\textgreater{} {[}nan, nan, nan{]}

\sphinxAtStartPar
With max subtraction:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
x = {[}1000, 1001, 1002{]}
max\_x = 1002
x \sphinxhyphen{} max\_x = {[}\sphinxhyphen{}2, \sphinxhyphen{}1, 0{]}
exp({[}\sphinxhyphen{}2, \sphinxhyphen{}1, 0{]}) = {[}0.135, 0.368, 1.0{]}  \# no overflow!
softmax \textasciitilde{} {[}0.09, 0.24, 0.67{]}  \# correct!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Mathematical} \PYG{n}{correctness}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
softmax(x + c) = softmax(x) for any constant c

\sphinxAtStartPar
So subtracting max doesn’t change the result, just prevents overflow.


\subsubsection{Padding with \sphinxhyphen{}inf}
\label{\detokenize{gpu-tutorials/02-fused-softmax:padding-with-inf}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
row = tl.load(input\_ptrs, mask=mask, other=\sphinxhyphen{}float(‘inf’))

\sphinxAtStartPar
For out\sphinxhyphen{}of\sphinxhyphen{}bounds elements:
\sphinxhyphen{} Setting to \sphinxhyphen{}inf means \sphinxcode{\sphinxupquote{exp(\sphinxhyphen{}inf) = 0}}
\sphinxhyphen{} These don’t contribute to the sum
\sphinxhyphen{} Result is mathematically correct for the actual row length


\subsection{Memory Bandwidth Analysis}
\label{\detokenize{gpu-tutorials/02-fused-softmax:memory-bandwidth-analysis}}

\subsubsection{Theoretical Speedup}
\label{\detokenize{gpu-tutorials/02-fused-softmax:theoretical-speedup}}
\sphinxAtStartPar
Naive: 8MN + 4M bytes
Fused: 2MN bytes

\sphinxAtStartPar
For large N: speedup \textasciitilde{} (8MN) / (2MN) = \sphinxstylestrong{4x}


\subsubsection{Actual Performance}
\label{\detokenize{gpu-tutorials/02-fused-softmax:actual-performance}}
\sphinxAtStartPar
From benchmark results, you might see:
\sphinxhyphen{} Naive softmax: \textasciitilde{}50 GB/s
\sphinxhyphen{} Torch softmax: \textasciitilde{}150 GB/s (has some fusion)
\sphinxhyphen{} Triton fused: \textasciitilde{}200 GB/s

\sphinxAtStartPar
\sphinxstylestrong{Why not 4x over naive?}
\sphinxhyphen{} Reductions have some overhead
\sphinxhyphen{} Computation (exp) takes time too
\sphinxhyphen{} Memory bandwidth isn’t the only factor

\sphinxAtStartPar
\sphinxstylestrong{Why faster than Torch?}
\sphinxhyphen{} PyTorch softmax handles general ND tensors
\sphinxhyphen{} Triton kernel specialized for row\sphinxhyphen{}wise 2D case
\sphinxhyphen{} Less overhead, better optimization for this specific case


\subsection{Key CUDA/Triton Concepts}
\label{\detokenize{gpu-tutorials/02-fused-softmax:key-cuda-triton-concepts}}

\subsubsection{Reduction Operations}
\label{\detokenize{gpu-tutorials/02-fused-softmax:reduction-operations}}
\sphinxAtStartPar
\sphinxstylestrong{Challenge}: Combine N values into 1 across many threads

\sphinxAtStartPar
\sphinxstylestrong{Triton approach}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
tl.max(row, axis=0)  \# Finds max across row
tl.sum(row, axis=0)  \# Sums across row

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{CUDA} \PYG{n}{approach}\PYG{o}{*}\PYG{o}{*} \PYG{p}{(}\PYG{n}{what} \PYG{n}{Triton} \PYG{n}{generates}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Each thread computes partial result

\item {} 
\sphinxAtStartPar
Use warp shuffle instructions for intra\sphinxhyphen{}warp reduction

\item {} 
\sphinxAtStartPar
Use shared memory for inter\sphinxhyphen{}warp reduction

\item {} 
\sphinxAtStartPar
Final result in thread 0

\end{enumerate}


\subsubsection{Warp Shuffles}
\label{\detokenize{gpu-tutorials/02-fused-softmax:warp-shuffles}}
\sphinxAtStartPar
Hardware instructions to share data between threads in a warp:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{**shfl\_down*sync()}}: Shuffle data from higher\sphinxhyphen{}indexed thread
\sphinxhyphen{} Much faster than shared memory
\sphinxhyphen{} Triton uses these automatically


\subsubsection{Block vs Thread}
\label{\detokenize{gpu-tutorials/02-fused-softmax:block-vs-thread}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Triton program} = \sphinxstylestrong{CUDA thread block}

\item {} 
\sphinxAtStartPar
Triton abstracts away individual threads

\item {} 
\sphinxAtStartPar
You think in terms of vectors (BLOCK\_SIZE elements)

\item {} 
\sphinxAtStartPar
Compiler generates thread code automatically

\end{itemize}


\subsection{Common Pitfalls}
\label{\detokenize{gpu-tutorials/02-fused-softmax:common-pitfalls}}

\subsubsection{1. Row Too Large for SRAM}
\label{\detokenize{gpu-tutorials/02-fused-softmax:row-too-large-for-sram}}
\sphinxAtStartPar
If \sphinxcode{\sphinxupquote{BLOCK\_SIZE * sizeof(float32) \textgreater{} SRAM\_SIZE}}:
\sphinxhyphen{} Kernel won’t fit in SRAM
\sphinxhyphen{} Will spill to DRAM (slow!)
\sphinxhyphen{} Solution: Process row in chunks (not shown in this tutorial)


\subsubsection{2. Numerical Precision}
\label{\detokenize{gpu-tutorials/02-fused-softmax:numerical-precision}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
numerator = tl.exp(row\_minus*max)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZhy{} Using ``float16`` can cause underflow
\PYGZhy{} Always use ``float32`` for intermediate computations
\PYGZhy{} Cast to ``float16`` only for final output if needed
\end{sphinxVerbatim}


\subsubsection{3. Masking Errors}
\label{\detokenize{gpu-tutorials/02-fused-softmax:masking-errors}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
mask = col\_offsets \textless{} n\_cols

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZhy{} Forgetting mask \PYGZhy{}\PYGZgt{} out\PYGZhy{}of\PYGZhy{}bounds access \PYGZhy{}\PYGZgt{} crash
\PYGZhy{} Wrong mask value \PYGZhy{}\PYGZgt{} incorrect results
\PYGZhy{} ``\PYGZhy{}inf`` is usually the right padding value for softmax
\end{sphinxVerbatim}


\subsection{Performance Tips}
\label{\detokenize{gpu-tutorials/02-fused-softmax:performance-tips}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ensure rows fit in SRAM}: Check \sphinxcode{\sphinxupquote{BLOCK\_SIZE * 4 \textless{} SRAM\_SIZE}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use float32 for accumulation}: Better numerical accuracy

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tune num\_warps}: Try 4, 8, or 16 depending on row size

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Adjust num\_stages}: More stages hide latency but need more SRAM

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Profile occupancy}: Use \sphinxcode{\sphinxupquote{nsys}} or \sphinxcode{\sphinxupquote{ncu}} to see if you’re register/SRAM limited

\end{enumerate}


\subsection{Comparison to Other Approaches}
\label{\detokenize{gpu-tutorials/02-fused-softmax:comparison-to-other-approaches}}

\subsubsection{JIT Fusion (torch.jit.script)}
\label{\detokenize{gpu-tutorials/02-fused-softmax:jit-fusion-torch-jit-script}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Attempts to fuse operations automatically

\item {} 
\sphinxAtStartPar
Often doesn’t work for complex patterns

\item {} 
\sphinxAtStartPar
Limited control over optimization

\end{itemize}


\subsubsection{Manual CUDA}
\label{\detokenize{gpu-tutorials/02-fused-softmax:manual-cuda}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Full control, best possible performance

\item {} 
\sphinxAtStartPar
Very complex to write and maintain

\item {} 
\sphinxAtStartPar
Triton achieves 95\%+ of CUDA performance with 1/3 the code

\end{itemize}


\subsubsection{CuDNN/CuBLAS}
\label{\detokenize{gpu-tutorials/02-fused-softmax:cudnn-cublas}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Vendor libraries, highly optimized

\item {} 
\sphinxAtStartPar
Not customizable, no fusion with custom ops

\item {} 
\sphinxAtStartPar
Triton lets you fuse softmax with other operations

\end{itemize}


\subsection{Extensions}
\label{\detokenize{gpu-tutorials/02-fused-softmax:extensions}}
\sphinxAtStartPar
Ideas to explore:
1. \sphinxstylestrong{Fused operations}: Softmax + scaling, softmax + masking
2. \sphinxstylestrong{Backward pass}: Implement grad\_softmax
3. \sphinxstylestrong{Dimension flexibility}: Handle column\sphinxhyphen{}wise softmax
4. \sphinxstylestrong{Multi\sphinxhyphen{}row processing}: Process multiple rows per program for tiny rows
5. \sphinxstylestrong{Flash Attention}: Use this softmax as building block


\subsection{Key Takeaways}
\label{\detokenize{gpu-tutorials/02-fused-softmax:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Kernel fusion reduces memory traffic}: Fewer DRAM accesses = faster

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{SRAM is 10\sphinxhyphen{}20x faster than DRAM}: Keep data on\sphinxhyphen{}chip when possible

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reductions are fundamental}: Many operations need them (sum, max, etc.)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Power\sphinxhyphen{}of\sphinxhyphen{}2 sizes}: Required for efficient GPU operations

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Numerical stability matters}: Always subtract max in softmax

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Triton simplifies GPU programming}: Achieves near\sphinxhyphen{}CUDA performance with much simpler code

\end{enumerate}

\sphinxAtStartPar
This pattern of fusion applies to many operations: layer norm, RMSNorm, GELU, and more!

\sphinxstepscope


\section{Matrix Multiplication in Triton}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:matrix-multiplication-in-triton}}\label{\detokenize{gpu-tutorials/03-matrix-multiplication::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:overview}}
\sphinxAtStartPar
Matrix multiplication (GEMM: General Matrix Multiply) is the cornerstone of deep learning. This tutorial shows how to write a high\sphinxhyphen{}performance matmul kernel that rivals cuBLAS/rocBLAS. You’ll learn advanced GPU optimization techniques including \sphinxstylestrong{tiling}, \sphinxstylestrong{cache optimization}, and \sphinxstylestrong{auto\sphinxhyphen{}tuning}.


\subsection{What You’ll Learn}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:what-you-ll-learn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Block\sphinxhyphen{}level tiling} for matrix multiplication

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}dimensional pointer arithmetic}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{L2 cache optimization} through program reordering

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Auto\sphinxhyphen{}tuning} for performance optimization

\item {} 
\sphinxAtStartPar
Why matmul is \sphinxstylestrong{compute\sphinxhyphen{}bound} (not memory\sphinxhyphen{}bound)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tensor Cores} and specialized hardware

\end{itemize}


\subsection{The Matrix Multiplication Problem}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:the-matrix-multiplication-problem}}

\subsubsection{Basic Algorithm}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:basic-algorithm}}
\sphinxAtStartPar
Compute C = A x B where:
\sphinxhyphen{} A has shape (M, K)
\sphinxhyphen{} B has shape (K, N)
\sphinxhyphen{} C has shape (M, N)

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{for i in range(M):}\begin{description}
\sphinxlineitem{for j in range(N):}
\sphinxAtStartPar
C{[}i, j{]} = sum(A{[}i, k{]} * B{[}k, j{]} for k in range(K))

\end{description}

\end{description}

\sphinxAtStartPar
Time complexity: \sphinxstylestrong{O(M x N x K)}
For 1024x1024 matrices: \textasciitilde{}1 billion operations!


\subsubsection{Why It’s Hard to Optimize}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:why-it-s-hard-to-optimize}}
\sphinxAtStartPar
\sphinxstylestrong{Naive implementation problems}:
1. Poor memory locality
2. Inefficient use of cache
3. Not utilizing GPU parallelism
4. Missing out on specialized hardware (Tensor Cores)

\sphinxAtStartPar
\sphinxstylestrong{Our goal}: Achieve 10+ TFLOPS (Tera\sphinxhyphen{}FLOPs, trillions of operations per second)!


\subsection{GPU Matrix Multiplication Strategy}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:gpu-matrix-multiplication-strategy}}

\subsubsection{Blocked Algorithm}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:blocked-algorithm}}
\sphinxAtStartPar
Instead of computing one element at a time, we compute \sphinxstylestrong{blocks} of C:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Pseudocode for our kernel}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:pseudocode-for-our-kernel}}\begin{description}
\sphinxlineitem{for m in range(0, M, BLOCK\_M):           \# Parallel on GPU}\begin{description}
\sphinxlineitem{for n in range(0, N, BLOCK\_N):       \# Parallel on GPU}
\sphinxAtStartPar
accumulator = zeros(BLOCK\_M, BLOCK\_N)
for k in range(0, K, BLOCK\_K):   \# Sequential within each block
\begin{quote}

\sphinxAtStartPar
A\_block = load A{[}m:m+BLOCK\_M, k:k+BLOCK\_K{]}
B\_block = load B{[}k:k+BLOCK\_K, n:n+BLOCK\_N{]}
accumulator += dot(A\_block, B\_block)
\end{quote}

\sphinxAtStartPar
store accumulator to C{[}m:m+BLOCK\_M, n:n+BLOCK\_N{]}

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{insight}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{The} \PYG{n}{outer} \PYG{n}{two} \PYG{n}{loops} \PYG{n}{are} \PYG{n}{parallelized} \PYG{n}{across} \PYG{n}{GPU} \PYG{n}{programs}\PYG{p}{,} \PYG{k}{while} \PYG{n}{the} \PYG{n}{K} \PYG{n}{loop} \PYG{o+ow}{is} \PYG{n}{sequential} \PYG{n}{within} \PYG{n}{each} \PYG{n}{program}\PYG{o}{.}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Memory hierarchy}:
\begin{description}
\sphinxlineitem{Registers (fastest, \textasciitilde{}1 cycle)}
\sphinxAtStartPar
down

\sphinxlineitem{L1/Shared Memory (\textasciitilde{}10 cycles, 192 KB)}
\sphinxAtStartPar
down

\sphinxlineitem{L2 Cache (\textasciitilde{}100 cycles, 40 MB)}
\sphinxAtStartPar
down

\end{description}

\sphinxAtStartPar
HBM/Global Memory (slowest, \textasciitilde{}400 cycles, 40\sphinxhyphen{}80 GB)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{With} \PYG{n}{tiling}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Load A\_block into shared memory (reused BLOCK\_N times)

\item {} 
\sphinxAtStartPar
Load B\_block into shared memory (reused BLOCK\_M times)

\item {} 
\sphinxAtStartPar
Accumulate into registers (reused K/BLOCK\_K times)

\item {} 
\sphinxAtStartPar
Write result once to global memory

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Data reuse} = fewer slow memory accesses!


\subsection{Pointer Arithmetic in 2D}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:pointer-arithmetic-in-2d}}

\subsubsection{Understanding Strides}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:understanding-strides}}
\sphinxAtStartPar
For a row\sphinxhyphen{}major matrix A{[}M, K{]}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
A{[}i, j{]} = {\color{red}\bfseries{}*}(A\_ptr + i\_stride*M + j\_stride*K)

\sphinxAtStartPar
Where:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{stride\_M}} = number of elements to next row = K
\sphinxhyphen{} \sphinxcode{\sphinxupquote{stride\_K}} = number of elements to next column = 1

\sphinxAtStartPar
Example for A{[}4, 3{]}:

\sphinxAtStartPar
Elements in memory: {[}a00, a01, a02, a10, a11, a12, a20, a21, a22, a30, a31, a32{]}
A{[}2, 1{]} = A\_ptr + 2*3 + 1*1 = A\_ptr{[}7{]} = a21 {[}OK{]}


\subsubsection{Block Pointers}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:block-pointers}}
\sphinxAtStartPar
To get pointers to a block A{[}m:m+BLOCK\_M, k:k+BLOCK\_K{]}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
offs\_am = (pid\_m * BLOCK\_M + tl.arange(0, BLOCK\_M)) \% M
offs\_k = tl.arange(0, BLOCK\_K)
a\_ptrs = a\_ptr + offs\_am{[}:, None{]}*stride\_am + offs\_k{[}None, :{]}*stride\_ak

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Broadcasting}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{offs\_am{[}:, None{]}}} has shape (BLOCK\_M, 1)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{offs\_k{[}None, :{]}}} has shape (1, BLOCK\_K)

\item {} 
\sphinxAtStartPar
Result \sphinxcode{\sphinxupquote{a\_ptrs}} has shape (BLOCK\_M, BLOCK\_K)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example} (BLOCK\_M=2, BLOCK\_K=2, pid\_m=0):
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
offs\_am = {[}0, 1{]}        \# shape (2,)
offs\_k = {[}0, 1{]}          \# shape (2,)
\begin{description}
\sphinxlineitem{offs\_am{[}:, None{]} = {[}{[}0{]},   \# shape (2, 1)}
\sphinxAtStartPar
{[}1{]}{]}

\end{description}

\sphinxAtStartPar
offs\_k{[}None, :{]} = {[}{[}0, 1{]}{]}  \# shape (1, 2)


\section{Broadcasting multiplication:}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:broadcasting-multiplication}}
\sphinxAtStartPar
offs\_am{[}:, None{]} * stride\_am + offs\_k{[}None, :{]} * stride\_ak
= {[}{[}0*stride\_am + 0*stride\_ak, 0*stride\_am + 1*stride\_ak{]},
\begin{quote}

\sphinxAtStartPar
{[}1*stride\_am + 0*stride\_ak, 1*stride\_am + 1*stride\_ak{]}{]}
\end{quote}

\sphinxAtStartPar
This gives pointers to a 2x2 block!

\sphinxAtStartPar
To move to next K block:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
a\_ptrs += BLOCK\_K * stride\_ak
b\_ptrs += BLOCK\_K * stride\_bk

\sphinxAtStartPar
This shifts all pointers in the block by BLOCK\_K positions in the K dimension.


\subsection{L2 Cache Optimization}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:l2-cache-optimization}}

\subsubsection{The Problem with Row\sphinxhyphen{}Major Ordering}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:the-problem-with-row-major-ordering}}
\sphinxAtStartPar
If we process blocks in simple row\sphinxhyphen{}major order:

\sphinxAtStartPar
Program 0 \sphinxhyphen{}\textgreater{} Block C{[}0, 0{]}  (needs A{[}0, :{]} and B{[}:, 0{]})
Program 1 \sphinxhyphen{}\textgreater{} Block C{[}0, 1{]}  (needs A{[}0, :{]} and B{[}:, 1{]})
…
Program 9 \sphinxhyphen{}\textgreater{} Block C{[}1, 0{]}  (needs A{[}1, :{]} and B{[}:, 0{]})

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Issue**: By the time we compute C[1, 0], B[:, 0] might be evicted from L2 cache!
\end{sphinxVerbatim}


\subsubsection{Grouped Ordering (Swizzling)}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:grouped-ordering-swizzling}}
\sphinxAtStartPar
Instead, process blocks in groups:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
num\_pid*in\_group = GROUP\_SIZE*M * num\_pid*n
group\_id = pid // num\_pid*in\_group
first\_pid*m = group\_id * GROUP\_SIZE*M
group\_size*m = min(num\_pid*m \sphinxhyphen{} first\_pid*m, GROUP\_SIZE*M)
pid\_m = first\_pid*m + ((pid \% num\_pid*in\_group) \% group\_size*m)
pid\_n = (pid \% num\_pid*in\_group) // group\_size*m

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Visual} \PYG{n}{Example}\PYG{o}{*}\PYG{o}{*} \PYG{p}{(}\PYG{n}{GROUP\PYGZus{}SIZE}\PYG{o}{*}\PYG{n}{M}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{n}{x9} \PYG{n}{blocks}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
Row\sphinxhyphen{}major order:

\sphinxAtStartPar
0\sphinxhyphen{}\textgreater{} 1\sphinxhyphen{}\textgreater{} 2\sphinxhyphen{}\textgreater{} 3\sphinxhyphen{}\textgreater{} 4\sphinxhyphen{}\textgreater{} 5\sphinxhyphen{}\textgreater{} 6\sphinxhyphen{}\textgreater{} 7\sphinxhyphen{}\textgreater{} 8\sphinxhyphen{}\textgreater{}
9\sphinxhyphen{}\textgreater{} 10\sphinxhyphen{}\textgreater{} …

\sphinxAtStartPar
Needs to load: 90 unique blocks into SRAM

\sphinxAtStartPar
Grouped order:

\sphinxAtStartPar
0down 3down 6down 1down 4down 7down 2down 5down 8down
9down 12down 15down 10down 13down 16down 11down 14down 17down
…

\sphinxAtStartPar
Needs to load: 54 unique blocks into SRAM

\sphinxAtStartPar
\sphinxstylestrong{Savings}: 40\% fewer loads from HBM! This can improve performance by 10\sphinxhyphen{}20\%.


\subsection{Auto\sphinxhyphen{}Tuning}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:auto-tuning}}

\subsubsection{The Configuration Space}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:the-configuration-space}}
\sphinxAtStartPar
Many parameters affect performance:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{BLOCK\_SIZE*M}}: 32, 64, 128, 256
\sphinxhyphen{} \sphinxcode{\sphinxupquote{BLOCK\_SIZE*N}}: 32, 64, 128, 256
\sphinxhyphen{} \sphinxcode{\sphinxupquote{BLOCK\_SIZE*K}}: 16, 32, 64, 128
\sphinxhyphen{} \sphinxcode{\sphinxupquote{GROUP\_SIZE*M}}: 4, 8, 16
\sphinxhyphen{} \sphinxcode{\sphinxupquote{num\_warps}}: 2, 4, 8, 16
\sphinxhyphen{} \sphinxcode{\sphinxupquote{num\_stages}}: 2, 3, 4, 5

\sphinxAtStartPar
Total combinations: \textasciitilde{}1000+

\sphinxAtStartPar
\sphinxstylestrong{Problem}: Optimal configuration depends on:
\sphinxhyphen{} Matrix sizes (M, N, K)
\sphinxhyphen{} GPU architecture (compute capability, SRAM size, etc.)
\sphinxhyphen{} Data types (fp16, fp32, int8, etc.)


\subsubsection{Triton’s Auto\sphinxhyphen{}Tuner}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:triton-s-auto-tuner}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{@triton.autotune(}
\sphinxAtStartPar
configs=get\_autotune*config(),
key={[}‘M’, ‘N’, ‘K’{]},

\end{description}

\sphinxAtStartPar
)
@triton.jit
def matmul\_kernel(…):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{How} \PYG{n}{it} \PYG{n}{works}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Define list of candidate configurations

\item {} 
\sphinxAtStartPar
Specify key parameters (M, N, K)

\item {} 
\sphinxAtStartPar
At runtime, Triton:
\sphinxhyphen{} Tries each configuration
\sphinxhyphen{} Measures performance
\sphinxhyphen{} Caches best config for this (M, N, K)
\sphinxhyphen{} Uses cached result for future calls

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Example config}:
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{triton.Config(}
\sphinxAtStartPar
\{‘BLOCK\_SIZE*M’: 128, ‘BLOCK\_SIZE*N’: 256, ‘BLOCK\_SIZE*K’: 64, ‘GROUP\_SIZE*M’: 8\},
num\_stages=3,
num\_warps=8

\end{description}


\paragraph{)}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:id3}}

\subsubsection{Good Configurations}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:good-configurations}}
\sphinxAtStartPar
\sphinxstylestrong{For NVIDIA (CUDA)}:
\sphinxhyphen{} Large blocks (128x256) for large matrices
\sphinxhyphen{} More stages (3\sphinxhyphen{}4) to hide latency
\sphinxhyphen{} 8 warps for good occupancy

\sphinxAtStartPar
\sphinxstylestrong{For AMD (HIP)}:
\sphinxhyphen{} Medium blocks (64x64, 128x128)
\sphinxhyphen{} Fewer stages (2) due to different architecture
\sphinxhyphen{} Different thread group sizes

\sphinxAtStartPar
\sphinxstylestrong{FP8 (8\sphinxhyphen{}bit floating point)}:
\sphinxhyphen{} Even larger blocks (256x256)
\sphinxhyphen{} Can fit more data in SRAM
\sphinxhyphen{} Tensor Cores process 4x more data per cycle


\subsection{The Kernel Implementation}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:the-kernel-implementation}}

\subsubsection{Step 1: Compute Program IDs}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:step-1-compute-program-ids}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
pid = tl.program\_id(axis=0)
num\_pid*m = tl.cdiv(M, BLOCK\_SIZE*M)
num\_pid*n = tl.cdiv(N, BLOCK\_SIZE*N)

\sphinxAtStartPar
Map linear program ID to 2D (pid\_m, pid\_n) using grouped ordering.


\subsubsection{Step 2: Initialize Pointers}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:step-2-initialize-pointers}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
offs\_am = (pid\_m * BLOCK\_SIZE*M + tl.arange(0, BLOCK\_SIZE*M)) \% M
offs\_bn = (pid\_n * BLOCK\_SIZE*N + tl.arange(0, BLOCK\_SIZE*N)) \% N
offs\_k = tl.arange(0, BLOCK\_SIZE*K)

\sphinxAtStartPar
a\_ptrs = a\_ptr + (offs\_am{[}:, None{]}*stride\_am + offs\_k{[}None, :{]}*stride\_ak)
b\_ptrs = b\_ptr + (offs\_k{[}:, None{]}*stride\_bk + offs\_bn{[}None, :{]}*stride\_bn)

\sphinxAtStartPar
Create pointers for first blocks of A and B.

\sphinxAtStartPar
\sphinxstylestrong{Note the modulo}:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{offs\_am \% M}} handles M not being multiple of BLOCK\_SIZE*M
\sphinxhyphen{} Wraps around, so we load valid (though repeated) data
\sphinxhyphen{} Doesn’t matter because we mask the output store


\subsubsection{Step 3: Accumulation Loop}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:step-3-accumulation-loop}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
accumulator = tl.zeros((BLOCK\_SIZE*M, BLOCK\_SIZE*N), dtype=tl.float32)
\begin{description}
\sphinxlineitem{for k in range(0, tl.cdiv(K, BLOCK\_SIZE*K)):}
\sphinxAtStartPar
a = tl.load(a\_ptrs, mask=offs\_k{[}None, :{]} \textless{} K \sphinxhyphen{} k\_BLOCK*SIZE\_K, other=0.0)
b = tl.load(b\_ptrs, mask=offs\_k{[}:, None{]} \textless{} K \sphinxhyphen{} k\_BLOCK*SIZE\_K, other=0.0)

\sphinxAtStartPar
accumulator = tl.dot(a, b, accumulator)

\sphinxAtStartPar
a\_ptrs += BLOCK\_SIZE*K * stride\_ak
b\_ptrs += BLOCK\_SIZE*K * stride\_bk

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{points}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Accumulate in \sphinxcode{\sphinxupquote{float32}} for numerical accuracy

\item {} 
\sphinxAtStartPar
Mask loads when K not multiple of BLOCK\_SIZE*K

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.dot()}} uses hardware accelerators (Tensor Cores on NVIDIA)

\end{itemize}


\subsubsection{Step 4: Apply Activation (Optional)}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:step-4-apply-activation-optional}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{if ACTIVATION == “leaky\_relu”:}
\sphinxAtStartPar
accumulator = leaky\_relu(accumulator)

\end{description}

\sphinxAtStartPar
c = accumulator.to(tl.float16)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Kernel fusion**: Apply activation while data is in registers (fast)!
\end{sphinxVerbatim}


\subsubsection{Step 5: Store Result}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:step-5-store-result}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
offs\_cm = pid\_m * BLOCK\_SIZE*M + tl.arange(0, BLOCK\_SIZE*M)
offs\_cn = pid\_n * BLOCK\_SIZE*N + tl.arange(0, BLOCK\_SIZE*N)
c\_ptrs = c\_ptr + stride\_cm*offs\_cm{[}:, None{]} + stride\_cn*offs\_cn{[}None, :{]}
c\_mask = (offs\_cm{[}:, None{]} \textless{} M) \& (offs\_cn{[}None, :{]} \textless{} N)
tl.store(c\_ptrs, c, mask=c\_mask)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Masking}\PYG{o}{*}\PYG{o}{*} \PYG{n}{ensures} \PYG{n}{we} \PYG{n}{don}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t write out of bounds.}
\end{sphinxVerbatim}


\subsection{Tensor Cores}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:tensor-cores}}

\subsubsection{What Are Tensor Cores?}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:what-are-tensor-cores}}
\sphinxAtStartPar
Special hardware units on modern GPUs for matrix multiplication:
\sphinxhyphen{} \sphinxstylestrong{NVIDIA}: Tensor Cores (Volta, Turing, Ampere, Hopper)
\sphinxhyphen{} \sphinxstylestrong{AMD}: Matrix Cores (CDNA2, CDNA3)

\sphinxAtStartPar
\sphinxstylestrong{Performance}:
\sphinxhyphen{} Regular CUDA cores: 1 FP16 multiply\sphinxhyphen{}add per cycle per core
\sphinxhyphen{} Tensor Cores: 64\sphinxhyphen{}256 FP16 multiply\sphinxhyphen{}adds per cycle per core
\sphinxhyphen{} \sphinxstylestrong{Speedup}: 10\sphinxhyphen{}100x for matmul!


\subsubsection{How Tensor Cores Work}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:how-tensor-cores-work}}
\sphinxAtStartPar
Operate on small matrices (e.g., 16x16):

\sphinxAtStartPar
D = A x B + C

\sphinxAtStartPar
Where A, B, C, D are 16x16 matrices.

\sphinxAtStartPar
\sphinxstylestrong{Single instruction}, massive computation!


\subsubsection{Triton and Tensor Cores}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:triton-and-tensor-cores}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
accumulator = tl.dot(a, b, accumulator)

\sphinxAtStartPar
When you use \sphinxcode{\sphinxupquote{tl.dot()}} with appropriate types and sizes, Triton automatically:
1. Detects Tensor Core availability
2. Arranges data in Tensor Core format
3. Emits Tensor Core instructions (e.g., \sphinxcode{\sphinxupquote{mma.sync}})

\sphinxAtStartPar
\sphinxstylestrong{Requirements for Tensor Cores}:
\sphinxhyphen{} FP16, BF16, TF32, FP8, or INT8 inputs
\sphinxhyphen{} Block sizes that are multiples of Tensor Core dimensions (usually 16)


\subsection{Performance Analysis}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:performance-analysis}}

\subsubsection{Arithmetic Intensity}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:arithmetic-intensity}}
\sphinxAtStartPar
For matmul C = A x B:
\sphinxhyphen{} \sphinxstylestrong{FLOPs}: 2MNK (each output element: K multiplies + K adds)
\sphinxhyphen{} \sphinxstylestrong{Memory}: 2(MK + KN + MN) bytes (read A, B, write C)

\sphinxAtStartPar
Arithmetic Intensity = 2MNK / (2(MK + KN + MN))

\sphinxAtStartPar
For square matrices (M=N=K):

\sphinxAtStartPar
AI = 2N\textasciicircum{}3 / (4N\textasciicircum{}2) = N/2

\sphinxAtStartPar
For N=1024: AI = 512 FLOPs/byte

\sphinxAtStartPar
\sphinxstylestrong{This is very high!} Matmul is \sphinxstylestrong{compute\sphinxhyphen{}bound}, not memory\sphinxhyphen{}bound.


\subsubsection{Roofline Model}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:roofline-model}}
\sphinxAtStartPar
Performance is limited by:

\sphinxAtStartPar
min(Peak\_Compute, Peak\_Memory*BW * AI)

\sphinxAtStartPar
For A100:
\sphinxhyphen{} Peak FP16 Tensor Core: 312 TFLOPS
\sphinxhyphen{} Peak Memory BW: 2 TB/s
\sphinxhyphen{} For N=1024: 2 TB/s * 512 = 1024 TFLOPS

\sphinxAtStartPar
\sphinxstylestrong{Bottleneck}: Compute (312 TFLOPS), not memory!

\sphinxAtStartPar
For small matrices (N=64): AI = 32
\sphinxhyphen{} Memory bound: 2 TB/s * 32 = 64 TFLOPS
\sphinxhyphen{} Can achieve much less than peak compute


\subsubsection{Expected Performance}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:expected-performance}}
\sphinxAtStartPar
\sphinxstylestrong{Theoretical Peak} (A100, FP16):
\sphinxhyphen{} 312 TFLOPS with Tensor Cores

\sphinxAtStartPar
\sphinxstylestrong{Achievable}:
\sphinxhyphen{} cuBLAS: \textasciitilde{}280\sphinxhyphen{}300 TFLOPS (90\sphinxhyphen{}95\% peak)
\sphinxhyphen{} Triton (optimized): \textasciitilde{}250\sphinxhyphen{}280 TFLOPS (80\sphinxhyphen{}90\% peak)
\sphinxhyphen{} Naive implementation: \textasciitilde{}10\sphinxhyphen{}50 TFLOPS (\textless{}20\% peak)

\sphinxAtStartPar
\sphinxstylestrong{Why not 100\%?}
\sphinxhyphen{} Launch overhead
\sphinxhyphen{} Imperfect tiling (boundaries)
\sphinxhyphen{} L2 cache misses
\sphinxhyphen{} Pipeline stalls


\subsection{Advanced Optimizations}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:advanced-optimizations}}

\subsubsection{Software Pipelining}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:software-pipelining}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
num\_stages = 3

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Idea}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Overlap} \PYG{n}{memory} \PYG{n}{loads} \PYG{k}{with} \PYG{n}{computation}
\end{sphinxVerbatim}

\sphinxAtStartPar
Without pipelining:

\sphinxAtStartPar
Load A, B \sphinxhyphen{}\textgreater{} Wait \sphinxhyphen{}\textgreater{} Compute \sphinxhyphen{}\textgreater{} Load A, B \sphinxhyphen{}\textgreater{} Wait \sphinxhyphen{}\textgreater{} Compute

\sphinxAtStartPar
With 3\sphinxhyphen{}stage pipelining:

\sphinxAtStartPar
Stage 0: Load A\_0, B\_0
Stage 1: Load A\_1, B\_1 | Compute with A\_0, B\_0
Stage 2: Load A\_2, B\_2 | Compute with A\_1, B\_1
Stage 3: Load A\_3, B\_3 | Compute with A\_2, B\_2
…

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Benefit**: Computation and memory loads happen simultaneously!
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Cost}: Need more registers and SRAM to hold multiple stages.


\subsubsection{Loop Unrolling}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:loop-unrolling}}
\sphinxAtStartPar
Triton automatically unrolls the K loop when possible:
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{for k in range(0, tl.cdiv(K, BLOCK\_SIZE*K)):}
\sphinxAtStartPar
accumulator = tl.dot(a, b, accumulator)

\end{description}

\sphinxAtStartPar
Becomes (if K/BLOCK\_K = 4):
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
accumulator = tl.dot(a0, b0, accumulator)
accumulator = tl.dot(a1, b1, accumulator)
accumulator = tl.dot(a2, b2, accumulator)
accumulator = tl.dot(a3, b3, accumulator)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Benefits}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Eliminates loop overhead

\item {} 
\sphinxAtStartPar
Better instruction\sphinxhyphen{}level parallelism

\item {} 
\sphinxAtStartPar
Easier for compiler to optimize

\end{itemize}


\subsubsection{Register Pressure}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:register-pressure}}
\sphinxAtStartPar
Each thread needs registers for:
\sphinxhyphen{} A block elements
\sphinxhyphen{} B block elements
\sphinxhyphen{} Accumulator elements
\sphinxhyphen{} Temporary variables

\sphinxAtStartPar
\sphinxstylestrong{Example} (BLOCK\_M=128, BLOCK\_N=128, BLOCK\_K=32, 8 warps):
\sphinxhyphen{} Threads per block: 8 * 32 = 256
\sphinxhyphen{} Elements per thread: (128*128) / 256 = 64 for accumulator
\sphinxhyphen{} Registers needed: \textasciitilde{}100\sphinxhyphen{}150 per thread

\sphinxAtStartPar
\sphinxstylestrong{Limited supply}: 65536 registers per SM on A100

\sphinxAtStartPar
If too many registers \sphinxhyphen{}\textgreater{} fewer blocks per SM \sphinxhyphen{}\textgreater{} lower occupancy \sphinxhyphen{}\textgreater{} lower performance.

\sphinxAtStartPar
\sphinxstylestrong{Balance}: Larger blocks = more reuse but more register pressure.


\subsection{Common Pitfalls}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:common-pitfalls}}

\subsubsection{1. Non\sphinxhyphen{}Contiguous Tensors}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:non-contiguous-tensors}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
assert a.is\_contiguous(), “Matrix A must be contiguous”

\sphinxAtStartPar
Non\sphinxhyphen{}contiguous tensors have unexpected strides \sphinxhyphen{}\textgreater{} wrong pointer arithmetic \sphinxhyphen{}\textgreater{} incorrect results.

\sphinxAtStartPar
\sphinxstylestrong{Solution}: Call \sphinxcode{\sphinxupquote{.contiguous()}} or handle arbitrary strides.


\subsubsection{2. Wrong Stride Calculation}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:wrong-stride-calculation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
a.stride(0), a.stride(1)  \# Correct

\sphinxAtStartPar
Don’t hardcode strides! Transposed matrices have different strides.


\subsubsection{3. Boundary Conditions}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:boundary-conditions}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
offs\_am = (…) \% M  \# Modulo for safety
c\_mask = (offs\_cm{[}:, None{]} \textless{} M) \& (offs\_cn{[}None, :{]} \textless{} N)  \# Mask stores

\sphinxAtStartPar
Forgetting these \sphinxhyphen{}\textgreater{} out\sphinxhyphen{}of\sphinxhyphen{}bounds accesses \sphinxhyphen{}\textgreater{} crashes or wrong results.


\subsubsection{4. Numerical Precision}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:numerical-precision}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
accumulator = tl.zeros((BLOCK\_SIZE*M, BLOCK\_SIZE*N), dtype=tl.float32)

\sphinxAtStartPar
Using FP16 for accumulation \sphinxhyphen{}\textgreater{} loss of precision \sphinxhyphen{}\textgreater{} degraded accuracy.

\sphinxAtStartPar
\sphinxstylestrong{Best practice}: Accumulate in FP32, cast to FP16 for storage.


\subsection{Benchmarking Tips}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:benchmarking-tips}}

\subsubsection{1. Warm\sphinxhyphen{}up}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:warm-up}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
triton.testing.do\_bench(fn)  \# Automatically does warm\sphinxhyphen{}up

\sphinxAtStartPar
First few kernel launches are slow (compilation, cache loading).


\subsubsection{2. Measure TFLOPS}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:measure-tflops}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
perf = lambda ms: 2 * M * N * K * 1e\sphinxhyphen{}12 / (ms * 1e\sphinxhyphen{}3)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why 2MNK?**
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Each of MN output elements: K multiply\sphinxhyphen{}adds

\item {} 
\sphinxAtStartPar
Multiply\sphinxhyphen{}add = 2 FLOPs (1 multiply + 1 add)

\item {} 
\sphinxAtStartPar
Total: 2MNK FLOPs

\end{itemize}


\subsubsection{3. Compare Against cuBLAS/rocBLAS}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:compare-against-cublas-rocblas}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
torch.matmul(a, b)  \# Uses cuBLAS on NVIDIA, rocBLAS on AMD

\sphinxAtStartPar
These are \sphinxstylestrong{highly optimized} by vendors. Matching them is a huge achievement!


\subsubsection{4. Test Different Sizes}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:test-different-sizes}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
x\_vals={[}128 * i for i in range(2, 33){]}  \# 256, 384, …, 4096

\sphinxAtStartPar
Performance varies with size:
\sphinxhyphen{} Small: Memory\sphinxhyphen{}bound, lower TFLOPS
\sphinxhyphen{} Medium: Transition zone
\sphinxhyphen{} Large: Compute\sphinxhyphen{}bound, approaching peak


\subsection{Key Takeaways}
\label{\detokenize{gpu-tutorials/03-matrix-multiplication:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tiling/Blocking is essential}: Reuse data in fast SRAM

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Pointer arithmetic}: Understanding strides is crucial for multi\sphinxhyphen{}dimensional arrays

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{L2 cache matters}: Grouped ordering can give 10\sphinxhyphen{}20\% speedup

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Auto\sphinxhyphen{}tuning is powerful}: Optimal configs vary with size and hardware

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tensor Cores are game\sphinxhyphen{}changers}: 10\sphinxhyphen{}100x speedup for matmul

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Matmul is compute\sphinxhyphen{}bound}: Unlike vector add (memory\sphinxhyphen{}bound)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Accumulate in FP32}: Maintain numerical accuracy

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Triton simplifies complex optimizations}: Achieves near\sphinxhyphen{}cuBLAS performance with readable code

\end{enumerate}

\sphinxAtStartPar
Matrix multiplication is the foundation of deep learning. Mastering these concepts will help you understand and optimize transformers, CNNs, and other neural architectures!

\sphinxstepscope


\section{Low\sphinxhyphen{}Memory Dropout in Triton}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:low-memory-dropout-in-triton}}\label{\detokenize{gpu-tutorials/04-low-memory-dropout::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:overview}}
\sphinxAtStartPar
This tutorial demonstrates a memory\sphinxhyphen{}efficient dropout implementation using \sphinxstylestrong{pseudo\sphinxhyphen{}random number generation (PRNG)} on GPU. Instead of storing a mask tensor, we use a single seed to reproduce random numbers on\sphinxhyphen{}the\sphinxhyphen{}fly, dramatically reducing memory footprint.


\subsection{What You’ll Learn}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:what-you-ll-learn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Why naive dropout is memory\sphinxhyphen{}inefficient

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Parallel random number generation} in Triton

\item {} 
\sphinxAtStartPar
The \sphinxstylestrong{Philox algorithm} for PRNG

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deterministic reproducibility} with seeds

\item {} 
\sphinxAtStartPar
Memory vs computation trade\sphinxhyphen{}offs

\end{itemize}


\subsection{What is Dropout?}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:what-is-dropout}}

\subsubsection{Purpose}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:purpose}}
\sphinxAtStartPar
\sphinxstylestrong{Dropout} is a regularization technique for neural networks:
\sphinxhyphen{} During training: Randomly set fraction \sphinxcode{\sphinxupquote{p}} of neurons to zero
\sphinxhyphen{} During evaluation: Use all neurons, scale by \sphinxcode{\sphinxupquote{(1\sphinxhyphen{}p)}}

\sphinxAtStartPar
\sphinxstylestrong{Why it helps}:
\sphinxhyphen{} Prevents overfitting
\sphinxhyphen{} Reduces co\sphinxhyphen{}adaptation of neurons
\sphinxhyphen{} Acts like ensemble of sub\sphinxhyphen{}networks


\subsubsection{Mathematical Definition}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:mathematical-definition}}
\sphinxAtStartPar
Forward pass (training):
\begin{description}
\sphinxlineitem{output{[}i{]} = \{}
\sphinxAtStartPar
x{[}i{]} / (1\sphinxhyphen{}p)   with probability (1\sphinxhyphen{}p)
0               with probability p

\end{description}


\paragraph{\}}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:id1}}
\sphinxAtStartPar
Forward pass (inference):

\sphinxAtStartPar
output{[}i{]} = x{[}i{]}  \# No dropout, no scaling needed


\subsubsection{Scaling Factor}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:scaling-factor}}
\sphinxAtStartPar
\sphinxstylestrong{Why divide by (1\sphinxhyphen{}p)?}

\sphinxAtStartPar
Without scaling:

\sphinxAtStartPar
E{[}output{]} = (1\sphinxhyphen{}p) * E{[}x{]}  \# Expected value decreases!

\sphinxAtStartPar
With scaling:
\begin{description}
\sphinxlineitem{E{[}output{]} = E{[}x{[}i{]} / (1\sphinxhyphen{}p) * keep{]}}
\sphinxAtStartPar
= E{[}x{[}i{]}{]} / (1\sphinxhyphen{}p) * (1\sphinxhyphen{}p)
= E{[}x{[}i{]}{]}  \# Maintains expectation!

\end{description}

\sphinxAtStartPar
This is called \sphinxstylestrong{inverted dropout} (most common variant).


\subsection{Naive Implementation Problems}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:naive-implementation-problems}}

\subsubsection{Standard PyTorch Dropout}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:standard-pytorch-dropout}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def dropout(x, p):}
\sphinxAtStartPar
keep\_mask = (torch.rand\_like(x) \textgreater{} p).to(torch.float32)
return x * keep\_mask / (1 \sphinxhyphen{} p), keep\_mask

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Memory} \PYG{n}{Requirements}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input \sphinxcode{\sphinxupquote{x}}: N bytes

\item {} 
\sphinxAtStartPar
Output: N bytes

\item {} 
\sphinxAtStartPar
Mask tensor: N bytes (or N/8 if packed as bits)

\item {} 
\sphinxAtStartPar
Total: \sphinxstylestrong{2\sphinxhyphen{}3x input size}

\end{itemize}

\sphinxAtStartPar
For a large tensor (e.g., 1GB):
\sphinxhyphen{} Need 2\sphinxhyphen{}3 GB total
\sphinxhyphen{} Mask must be stored for backward pass


\subsubsection{The Backward Pass Problem}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:the-backward-pass-problem}}
\sphinxAtStartPar
During training, we need the same dropout mask for:
1. Forward pass: Apply dropout
2. Backward pass: Gradient flows only through kept neurons

\sphinxAtStartPar
\sphinxstylestrong{Traditional approach}: Store the mask
.. code\sphinxhyphen{}block:: python


\section{Forward}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:forward}}
\sphinxAtStartPar
output, mask = dropout\_forward(x, p)


\section{Backward (later)}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:backward-later}}
\sphinxAtStartPar
dx = dropout\_backward(grad\_output, mask)  \# Need same mask!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Storage} \PYG{n}{cost}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{For} \PYG{n}{transformer} \PYG{k}{with} \PYG{l+m+mi}{1}\PYG{n}{B} \PYG{n}{parameters}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Each activation: \textasciitilde{}1\sphinxhyphen{}10 GB

\item {} 
\sphinxAtStartPar
Dozens of dropout layers

\item {} 
\sphinxAtStartPar
Total: 10\sphinxhyphen{}100 GB just for dropout masks!

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{With gradient checkpointing} (recompute activations to save memory):
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{with torch.no\_grad():}
\sphinxAtStartPar
output = dropout(x, p)  \# Different random numbers!

\end{description}


\section{Backward pass uses different mask \sphinxhyphen{}\textgreater{} wrong gradients!}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:backward-pass-uses-different-mask-wrong-gradients}}
\sphinxAtStartPar
Need \sphinxcode{\sphinxupquote{preserve\_rng*state=True}} \sphinxhyphen{}\textgreater{} more complexity and overhead.


\subsection{Seeded Dropout Solution}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:seeded-dropout-solution}}

\subsubsection{Key Insight}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:key-insight}}
\sphinxAtStartPar
Instead of storing the mask:
1. Generate random numbers from a \sphinxstylestrong{seed}
2. Store only the seed (4 bytes!)
3. Regenerate same random numbers when needed

\sphinxAtStartPar
\sphinxstylestrong{Memory savings}:
\sphinxhyphen{} Traditional: N bytes for mask
\sphinxhyphen{} Seeded: 4 bytes for seed
\sphinxhyphen{} \sphinxstylestrong{Savings}: N/4 bytes (\textasciitilde{}250 MB for 1 GB tensor!)


\subsubsection{The Triton Implementation}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:the-triton-implementation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def {\color{red}\bfseries{}*}seeded\_dropout(x\_ptr, output\_ptr, n\_elements, p, seed, BLOCK\_SIZE: tl.constexpr):
\begin{quote}

\sphinxAtStartPar
pid = tl.program\_id(axis=0)
block\_start = pid * BLOCK\_SIZE
offsets = block\_start + tl.arange(0, BLOCK\_SIZE)

\sphinxAtStartPar
mask = offsets \textless{} n\_elements
x = tl.load(x\_ptr + offsets, mask=mask)

\sphinxAtStartPar
\# Generate random numbers from seed!
random = tl.rand(seed, offsets)
x\_keep = random \textgreater{} p

\sphinxAtStartPar
output = tl.where(x\_keep, x / (1 \sphinxhyphen{} p), 0.0)
tl.store(output\_ptr + offsets, output, mask=mask)
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{line}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
random = tl.rand(seed, offsets)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZhy{} ``seed``: Determines the random sequence
\PYGZhy{} ``offsets``: Unique ID for each element (position in tensor)
\PYGZhy{} Same ``seed`` + same ``offsets`` \PYGZhy{}\PYGZgt{} **same random numbers**!
\end{sphinxVerbatim}


\subsection{Parallel Random Number Generation}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:parallel-random-number-generation}}

\subsubsection{The Challenge}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:the-challenge}}
\sphinxAtStartPar
On a CPU (sequential):
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
rng = Random(seed)
for i in range(n):
\begin{quote}

\sphinxAtStartPar
x{[}i{]} = rng.next()  \# Each depends on previous state
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Problem} \PYG{n}{on} \PYG{n}{GPU}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Can}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t parallelize this! Each thread would need previous thread}\PYG{l+s+s1}{\PYGZsq{}}\PYG{n}{s} \PYG{n}{state}\PYG{o}{.}
\end{sphinxVerbatim}


\subsubsection{The Solution: Counter\sphinxhyphen{}Based PRNG}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:the-solution-counter-based-prng}}
\sphinxAtStartPar
Instead of maintaining state, use a \sphinxstylestrong{function}:

\sphinxAtStartPar
random\_value = hash(seed, counter)

\sphinxAtStartPar
Where \sphinxcode{\sphinxupquote{hash}} is a deterministic, pseudo\sphinxhyphen{}random function.

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
1. \sphinxstylestrong{Parallel\sphinxhyphen{}friendly}: Each thread computes independently
2. \sphinxstylestrong{Reproducible}: Same inputs \sphinxhyphen{}\textgreater{} same outputs
3. \sphinxstylestrong{No state}: Just compute when needed


\subsubsection{The Philox Algorithm}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:the-philox-algorithm}}
\sphinxAtStartPar
Triton uses \sphinxstylestrong{Philox} (Parallel Random Number Generator):

\sphinxAtStartPar
Philox(seed, counter) \sphinxhyphen{}\textgreater{} pseudo\sphinxhyphen{}random uint64

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Properties}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fast}: A few multiply\sphinxhyphen{}add operations

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High quality}: Passes statistical randomness tests

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Parallel}: No inter\sphinxhyphen{}thread communication

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deterministic}: Same seed + counter \sphinxhyphen{}\textgreater{} same result

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{How it works} (simplified):
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{def philox(seed, counter):}
\sphinxAtStartPar
key = seed
ctr = counter
\begin{description}
\sphinxlineitem{for round in range(10):  \# Multiple rounds for mixing}
\sphinxAtStartPar
ctr = mix(ctr, key)   \# Bijective mixing function
key = bumpKey(key)    \# Update key

\end{description}

\sphinxAtStartPar
return ctr

\end{description}

\sphinxAtStartPar
Each round applies multiplication, addition, and bit shifts to thoroughly mix the bits.


\subsubsection{Using Philox in Triton}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:using-philox-in-triton}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
random = tl.rand(seed, offsets)

\sphinxAtStartPar
Under the hood:
1. \sphinxcode{\sphinxupquote{offsets}} are element indices (counters)
2. Applies Philox: \sphinxcode{\sphinxupquote{hash(seed, offsets{[}i{]})}} for each i
3. Converts uint64 \sphinxhyphen{}\textgreater{} float32 in {[}0, 1)
4. Returns vector of random numbers

\sphinxAtStartPar
\sphinxstylestrong{Example}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
seed = 42
offsets = {[}0, 1, 2, 3{]}

\sphinxAtStartPar
random = tl.rand(seed, offsets)
Generates: {[}0.37, 0.95, 0.22, 0.68{]}  (example values)
=====================================================


\section{Call again with same seed and offsets:}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:call-again-with-same-seed-and-offsets}}
\sphinxAtStartPar
random = tl.rand(seed, offsets)
Generates: {[}0.37, 0.95, 0.22, 0.68{]}  (same values!)
===================================================


\section{Different seed:}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:different-seed}}
\sphinxAtStartPar
random = tl.rand(seed=99, offsets)
Generates: {[}0.81, 0.12, 0.44, 0.67{]}  (different values)
=======================================================


\subsection{Memory and Performance Trade\sphinxhyphen{}offs}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:memory-and-performance-trade-offs}}

\subsubsection{Memory Comparison}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:memory-comparison}}
\sphinxAtStartPar
For a tensor with N elements:

\begin{DUlineblock}{0em}
\item[] Method | Forward Memory | Backward Memory | Total |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}—————{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——\sphinxhyphen{}|
| Naive PyTorch | N + N mask | N + N mask | 4N bytes |
| Triton Seeded | N | 4 bytes (seed) | N + 4 bytes |

\sphinxAtStartPar
\sphinxstylestrong{Example} (1B parameters, fp32):
\sphinxhyphen{} Naive: 4 GB + 4 GB = 8 GB
\sphinxhyphen{} Seeded: 4 GB + 4 bytes \textasciitilde{} 4 GB
\sphinxhyphen{} \sphinxstylestrong{Savings: 4 GB} (50\%)


\subsubsection{Computational Cost}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:computational-cost}}
\sphinxAtStartPar
\sphinxstylestrong{Naive dropout}:
\sphinxhyphen{} Forward: Load mask, multiply, divide
\sphinxhyphen{} Backward: Load mask, multiply

\sphinxAtStartPar
\sphinxstylestrong{Seeded dropout}:
\sphinxhyphen{} Forward: Generate random numbers (Philox), compare, multiply, divide
\sphinxhyphen{} Backward: Re\sphinxhyphen{}generate random numbers, multiply

\sphinxAtStartPar
\sphinxstylestrong{Philox cost}: \textasciitilde{}10\sphinxhyphen{}20 instructions per random number
\sphinxhyphen{} Still very fast (nanoseconds per number)
\sphinxhyphen{} Typically negligible compared to other operations

\sphinxAtStartPar
\sphinxstylestrong{Net result}: Slightly more compute, but huge memory savings.


\subsubsection{When to Use Seeded Dropout}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:when-to-use-seeded-dropout}}
\sphinxAtStartPar
\sphinxstylestrong{Use seeded dropout when}:
\sphinxhyphen{} Memory is limited
\sphinxhyphen{} Tensors are large
\sphinxhyphen{} Using gradient checkpointing
\sphinxhyphen{} Training very deep networks

\sphinxAtStartPar
\sphinxstylestrong{Use traditional dropout when}:
\sphinxhyphen{} Memory is abundant
\sphinxhyphen{} Tensors are small
\sphinxhyphen{} Maximum performance is critical

\sphinxAtStartPar
In practice, \sphinxstylestrong{seeded dropout is almost always better} for modern deep learning.


\subsection{Reproducibility and Determinism}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:reproducibility-and-determinism}}

\subsubsection{Ensuring Same Random Numbers}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:ensuring-same-random-numbers}}
\sphinxAtStartPar
For correct training, we need:
.. code\sphinxhyphen{}block:: python


\section{Forward pass}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:forward-pass}}
\sphinxAtStartPar
output = dropout(x, p, seed=123)


\section{Backward pass (later)}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:backward-pass-later}}

\section{Must use SAME seed to get SAME mask}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:must-use-same-seed-to-get-same-mask}}
\sphinxAtStartPar
grad = dropout\_backward(grad\_output, x, p, seed=123)

\sphinxAtStartPar
\sphinxstylestrong{Per\sphinxhyphen{}layer seeds}:
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{class Dropout(nn.Module):}\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}**}init**(self, p):}
\sphinxAtStartPar
self.p = p
self.seed = random.randint(0, 2**31)

\sphinxlineitem{def forward(self, x):}\begin{description}
\sphinxlineitem{if self.training:}
\sphinxAtStartPar
return seeded\_dropout(x, self.p, self.seed)

\end{description}

\sphinxAtStartPar
return x

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Global} \PYG{n}{seed} \PYG{k}{with} \PYG{n}{offsets}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
global\_seed = 42
layer\_id = 5
seed = global\_seed + layer\_id

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
x = torch.randn(10000, device=’cuda’)

\sphinxAtStartPar
out1 = seeded\_dropout(x, p=0.5, seed=123)
out2 = seeded\_dropout(x, p=0.5, seed=123)
assert torch.equal(out1, out2)  \# Exactly the same!

\sphinxAtStartPar
out3 = seeded\_dropout(x, p=0.5, seed=456)
assert not torch.equal(out1, out3)  \# Different seed \sphinxhyphen{}\textgreater{} different output


\subsection{Implementation Details}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:implementation-details}}

\subsubsection{The \sphinxstyleliteralintitle{\sphinxupquote{tl.where}} Function}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:the-tl-where-function}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
output = tl.where(x\_keep, x / (1 \sphinxhyphen{} p), 0.0)

\sphinxAtStartPar
Equivalent to:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
output{[}i{]} = x{[}i{]} / (1\sphinxhyphen{}p) if x\_keep{[}i{]} else 0.0

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why not use branching?**
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{if x\_keep{[}i{]}:}
\sphinxAtStartPar
output{[}i{]} = x{[}i{]} / (1\sphinxhyphen{}p)

\sphinxlineitem{else:}
\sphinxAtStartPar
output{[}i{]} = 0.0

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**GPU thread divergence**: Within a warp, all threads execute both branches if any thread takes each branch!
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Wastes compute} on the branch not taken

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.where}} compiles to a \sphinxstylestrong{select instruction} (no branching)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Much faster} on GPU

\end{itemize}


\subsubsection{Masking for Boundary Conditions}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:masking-for-boundary-conditions}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
mask = offsets \textless{} n\_elements
x = tl.load(x\_ptr + offsets, mask=mask)
tl.store(output\_ptr + offsets, output, mask=mask)

\sphinxAtStartPar
As always, handle non\sphinxhyphen{}multiple\sphinxhyphen{}of\sphinxhyphen{}BLOCK\_SIZE tensors safely.


\subsubsection{Random Number Distribution}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:random-number-distribution}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
random = tl.rand(seed, offsets)  \# Returns float32 in {[}0, 1)
x\_keep = random \textgreater{} p               \# Bernoulli with probability (1\sphinxhyphen{}p)

\sphinxAtStartPar
If p=0.3 (drop 30\%):
\sphinxhyphen{} \sphinxcode{\sphinxupquote{random \textgreater{} 0.3}} is True 70\% of the time
\sphinxhyphen{} So 70\% of elements are kept {[}OK{]}


\subsection{Advanced Considerations}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:advanced-considerations}}

\subsubsection{Different Random Distributions}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:different-random-distributions}}
\sphinxAtStartPar
Triton provides:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{tl.rand()}}: Uniform in {[}0, 1)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{tl.randn()}}: Standard normal (mean=0, std=1)
\sphinxhyphen{} Custom: Apply transformations

\sphinxAtStartPar
\sphinxstylestrong{Example \sphinxhyphen{} Normal dropout}:
.. code\sphinxhyphen{}block:: python


\section{Generate normal distribution}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:generate-normal-distribution}}
\sphinxAtStartPar
random\_normal = tl.randn(seed, offsets)
Apply threshold
===============
x\_keep = tl.abs(random\_normal) \textless{} threshold

\sphinxAtStartPar
\sphinxstylestrong{Philox properties}:
\sphinxhyphen{} Period: 2\textasciicircum{}128 (practically infinite)
\sphinxhyphen{} Equidistribution: Passes stringent statistical tests
\sphinxhyphen{} Independence: Consecutive values are uncorrelated

\sphinxAtStartPar
\sphinxstylestrong{Good enough for ML?} Yes!
\sphinxhyphen{} Better than many software PRNGs
\sphinxhyphen{} Fast enough for real\sphinxhyphen{}time generation
\sphinxhyphen{} Used in production systems (JAX, PyTorch)

\sphinxAtStartPar
\sphinxstylestrong{No race conditions} because:
\sphinxhyphen{} Each thread computes its own random numbers
\sphinxhyphen{} No shared state between threads
\sphinxhyphen{} Purely functional: \sphinxcode{\sphinxupquote{output = f(seed, offset)}}

\sphinxAtStartPar
\sphinxstylestrong{Contrast with CPU}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
global\_rng = Random(seed)
random\_val = global\_rng.next()  \# Race condition if parallel!


\subsection{Performance Benchmarks}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:performance-benchmarks}}

\subsubsection{Expected Performance}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:expected-performance}}
\sphinxAtStartPar
For a 10M element tensor (40 MB):

\begin{DUlineblock}{0em}
\item[] Method | Time (ms) | Memory (MB) | Bandwidth (GB/s) |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——————|
| Naive | 0.05 | 80 (2x tensor) | 1600 |
| Seeded | 0.08 | 40 (1x tensor) | 1000 |

\sphinxAtStartPar
\sphinxstylestrong{Trade\sphinxhyphen{}off}:
\sphinxhyphen{} 60\% more time (still \textless{} 0.1 ms, negligible)
\sphinxhyphen{} 50\% less memory (40 MB saved, significant)

\sphinxAtStartPar
For large models, the memory savings enable:
\sphinxhyphen{} Larger batch sizes
\sphinxhyphen{} Deeper networks
\sphinxhyphen{} Faster training (less memory swapping)


\subsubsection{Bottleneck Analysis}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:bottleneck-analysis}}
\sphinxAtStartPar
\sphinxstylestrong{Naive dropout}:
\sphinxhyphen{} \sphinxstylestrong{Memory\sphinxhyphen{}bound}: Reads mask from DRAM
\sphinxhyphen{} Limited by memory bandwidth (\textasciitilde{}2 TB/s)

\sphinxAtStartPar
\sphinxstylestrong{Seeded dropout}:
\sphinxhyphen{} \sphinxstylestrong{Compute\sphinxhyphen{}bound}: Philox computation
\sphinxhyphen{} Compute throughput much higher than memory bandwidth
\sphinxhyphen{} Still fast enough!


\subsection{Practical Usage}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:practical-usage}}

\subsubsection{Integration with PyTorch}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:integration-with-pytorch}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{class SeededDropout(torch.autograd.Function):}
\sphinxAtStartPar
@staticmethod
def forward(ctx, x, p, seed):
\begin{quote}

\sphinxAtStartPar
output = seeded\_dropout*triton(x, p, seed)
ctx.save\_for*backward(x)
ctx.p = p
ctx.seed = seed
return output
\end{quote}

\sphinxAtStartPar
@staticmethod
def backward(ctx, grad\_output):
\begin{quote}

\sphinxAtStartPar
x, = ctx.saved\_tensors
\# Reuse same seed to get same mask!
dropout\_mask = generate\_mask*triton(x.shape, ctx.p, ctx.seed)
grad\_x = grad\_output * dropout\_mask / (1 \sphinxhyphen{} ctx.p)
return grad\_x, None, None  \# No gradients for p and seed
\end{quote}

\end{description}


\subsubsection{Seed Generation Strategies}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:seed-generation-strategies}}
\sphinxAtStartPar
\sphinxstylestrong{Random seed per forward pass}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
seed = random.randint(0, 2**31)
output = dropout(x, p, seed)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Deterministic} \PYG{k}{for} \PYG{n}{debugging}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
torch.manual\_seed(42)
seed = 42
output = dropout(x, p, seed)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Distributed} \PYG{n}{training}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Ensure different seeds on different GPUs}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:ensure-different-seeds-on-different-gpus}}
\sphinxAtStartPar
rank = dist.get\_rank()
seed = base\_seed + rank


\subsection{Key Takeaways}
\label{\detokenize{gpu-tutorials/04-low-memory-dropout:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Seeded dropout trades computation for memory}: Tiny compute cost, huge memory savings

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Philox enables parallel PRNG}: No state, deterministic, high quality

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reproducibility is crucial}: Same seed \sphinxhyphen{}\textgreater{} same mask \sphinxhyphen{}\textgreater{} correct gradients

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory efficiency matters}: Enables larger models and batches

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Counter\sphinxhyphen{}based PRNGs are perfect for GPUs}: Parallel\sphinxhyphen{}friendly, no communication

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Small overhead, big benefit}: \textasciitilde{}50\% memory reduction with negligible slowdown

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Production\sphinxhyphen{}ready}: Used in modern frameworks (JAX, others)

\end{enumerate}

\sphinxAtStartPar
This pattern applies beyond dropout: any operation needing random numbers on GPU can benefit from seeded, counter\sphinxhyphen{}based generation!

\sphinxstepscope


\section{Layer Normalization in Triton}
\label{\detokenize{gpu-tutorials/05-layer-norm:layer-normalization-in-triton}}\label{\detokenize{gpu-tutorials/05-layer-norm::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/05-layer-norm:overview}}
\sphinxAtStartPar
Layer normalization is a critical component of Transformers and modern neural networks. This tutorial shows how to implement both \sphinxstylestrong{forward} and \sphinxstylestrong{backward} passes with advanced techniques like \sphinxstylestrong{parallel reduction} and \sphinxstylestrong{gradient accumulation}.


\subsection{What You’ll Learn}
\label{\detokenize{gpu-tutorials/05-layer-norm:what-you-ll-learn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The mathematics of Layer Normalization

\item {} 
\sphinxAtStartPar
Implementing \sphinxstylestrong{forward and backward passes}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Parallel reduction} strategies for mean and variance

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Atomic operations} and locks for thread synchronization

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Two\sphinxhyphen{}stage gradient computation} for efficiency

\item {} 
\sphinxAtStartPar
Why Layer Norm is important for Transformers

\end{itemize}


\subsection{What is Layer Normalization?}
\label{\detokenize{gpu-tutorials/05-layer-norm:what-is-layer-normalization}}

\subsubsection{The Formula}
\label{\detokenize{gpu-tutorials/05-layer-norm:the-formula}}
\sphinxAtStartPar
Given input vector x of length N:

\sphinxAtStartPar
y = (x \sphinxhyphen{} E{[}x{]}) / sqrt(Var{[}x{]} + eps) * w + b

\sphinxAtStartPar
Where:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{E{[}x{]}}}: Mean of x
\sphinxhyphen{} \sphinxcode{\sphinxupquote{Var{[}x{]}}}: Variance of x
\sphinxhyphen{} \sphinxcode{\sphinxupquote{eps}}: Small constant for numerical stability (e.g., 1e\sphinxhyphen{}5)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{w}}: Learnable weight (scaling)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{b}}: Learnable bias (shifting)


\subsubsection{Step\sphinxhyphen{}by\sphinxhyphen{}Step Math}
\label{\detokenize{gpu-tutorials/05-layer-norm:step-by-step-math}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compute mean}:

\sphinxAtStartPar
mu = (1/N) * SIGMA x{[}i{]}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compute variance}:

\sphinxAtStartPar
sigma\textasciicircum{}2 = (1/N) * SIGMA (x{[}i{]} \sphinxhyphen{} mu)\textasciicircum{}2

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Normalize}:

\sphinxAtStartPar
x\_hat{[}i{]} = (x{[}i{]} \sphinxhyphen{} mu) / sqrt(sigma\textasciicircum{}2 + eps)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scale and shift}:

\sphinxAtStartPar
y{[}i{]} = x\_hat{[}i{]} * w{[}i{]} + b{[}i{]}

\end{enumerate}


\subsubsection{Why Layer Normalization?}
\label{\detokenize{gpu-tutorials/05-layer-norm:why-layer-normalization}}
\sphinxAtStartPar
\sphinxstylestrong{Batch Normalization problems}:
\sphinxhyphen{} Requires large batch sizes (statistics unstable for small batches)
\sphinxhyphen{} Different behavior between training and inference
\sphinxhyphen{} Doesn’t work well for RNNs/Transformers

\sphinxAtStartPar
\sphinxstylestrong{Layer Normalization benefits}:
\sphinxhyphen{} Works with batch size = 1
\sphinxhyphen{} Same behavior for training and inference
\sphinxhyphen{} Normalizes across features (not batch)
\sphinxhyphen{} Essential for Transformers (BERT, GPT, etc.)


\subsubsection{Batch Norm vs Layer Norm}
\label{\detokenize{gpu-tutorials/05-layer-norm:batch-norm-vs-layer-norm}}
\sphinxAtStartPar
For input shape (Batch, Features):

\sphinxAtStartPar
\sphinxstylestrong{Batch Norm}: Normalize across batch dimension
\begin{description}
\sphinxlineitem{For each feature j:}
\sphinxAtStartPar
mu{[}j{]} = mean(x{[}:, j{]})  \# Mean across batch
sigma\textasciicircum{}2{[}j{]} = var(x{[}:, j{]})  \# Variance across batch

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Layer} \PYG{n}{Norm}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Normalize} \PYG{n}{across} \PYG{n}{feature} \PYG{n}{dimension}
\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{For each sample i:}
\sphinxAtStartPar
mu{[}i{]} = mean(x{[}i, :{]})  \# Mean across features
sigma\textasciicircum{}2{[}i{]} = var(x{[}i, :{]})  \# Variance across features

\end{description}


\subsection{The Forward Pass}
\label{\detokenize{gpu-tutorials/05-layer-norm:the-forward-pass}}

\subsubsection{Kernel Structure}
\label{\detokenize{gpu-tutorials/05-layer-norm:kernel-structure}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def {\color{red}\bfseries{}*}layer\_norm*fwd\_fused(X, Y, W, B, Mean, Rstd, stride, N, eps, BLOCK\_SIZE: tl.constexpr):
\begin{quote}

\sphinxAtStartPar
row = tl.program\_id(0)  \# Each program handles one row
Y += row * stride
X += row * stride
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{design}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{One} \PYG{n}{program} \PYG{n}{per} \PYG{n}{row}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Row = one sample in the batch

\item {} 
\sphinxAtStartPar
Each program independently normalizes its row

\item {} 
\sphinxAtStartPar
Perfectly parallel across batch dimension

\end{itemize}


\subsubsection{Computing the Mean}
\label{\detokenize{gpu-tutorials/05-layer-norm:computing-the-mean}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
mean = 0
{\color{red}\bfseries{}*}mean = tl.zeros({[}BLOCK\_SIZE{]}, dtype=tl.float32)
for off in range(0, N, BLOCK\_SIZE):
\begin{quote}

\sphinxAtStartPar
cols = off + tl.arange(0, BLOCK\_SIZE)
a = tl.load(X + cols, mask=cols \textless{} N, other=0.).to(tl.float32)
{\color{red}\bfseries{}*}mean += a
\end{quote}

\sphinxAtStartPar
mean = tl.sum({\color{red}\bfseries{}*}mean, axis=0) / N

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why a loop?** If N \PYGZgt{} BLOCK\PYGZus{}SIZE, can\PYGZsq{}t load entire row at once.
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Process}:
1. Initialize accumulator \sphinxcode{\sphinxupquote{*mean}}
2. Load chunks of size BLOCK\_SIZE
3. Add to accumulator
4. After loop: reduce accumulator to single value
5. Divide by N to get mean

\sphinxAtStartPar
\sphinxstylestrong{Example} (N=1000, BLOCK\_SIZE=256):

\sphinxAtStartPar
Iteration 0: Load x{[}0:256{]}, add to {\color{red}\bfseries{}*}mean
Iteration 1: Load x{[}256:512{]}, add to {\color{red}\bfseries{}*}mean
Iteration 2: Load x{[}512:768{]}, add to {\color{red}\bfseries{}*}mean
Iteration 3: Load x{[}768:1000{]}, add to {\color{red}\bfseries{}*}mean (only 232 elements, rest masked)
Final: sum({\color{red}\bfseries{}*}mean) / 1000


\subsubsection{Computing the Variance}
\label{\detokenize{gpu-tutorials/05-layer-norm:computing-the-variance}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
{\color{red}\bfseries{}*}var = tl.zeros({[}BLOCK\_SIZE{]}, dtype=tl.float32)
for off in range(0, N, BLOCK\_SIZE):
\begin{quote}

\sphinxAtStartPar
cols = off + tl.arange(0, BLOCK\_SIZE)
x = tl.load(X + cols, mask=cols \textless{} N, other=0.).to(tl.float32)
x = tl.where(cols \textless{} N, x \sphinxhyphen{} mean, 0.)
{\color{red}\bfseries{}*}var += x * x
\end{quote}

\sphinxAtStartPar
var = tl.sum({\color{red}\bfseries{}*}var, axis=0) / N

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Two}\PYG{o}{\PYGZhy{}}\PYG{k}{pass} \PYG{n}{algorithm}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
First pass: Compute mean

\item {} 
\sphinxAtStartPar
Second pass: Compute variance using the mean

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Why two passes?} Numerically more stable than one\sphinxhyphen{}pass algorithms.

\sphinxAtStartPar
\sphinxstylestrong{Note}: \sphinxcode{\sphinxupquote{tl.where(cols \textless{} N, x \sphinxhyphen{} mean, 0.)}} ensures masked elements don’t contribute.


\subsubsection{Normalization and Transformation}
\label{\detokenize{gpu-tutorials/05-layer-norm:normalization-and-transformation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
rstd = 1 / tl.sqrt(var + eps)
tl.store(Mean + row, mean)
tl.store(Rstd + row, rstd)
\begin{description}
\sphinxlineitem{for off in range(0, N, BLOCK\_SIZE):}
\sphinxAtStartPar
cols = off + tl.arange(0, BLOCK\_SIZE)
mask = cols \textless{} N
w = tl.load(W + cols, mask=mask)
b = tl.load(B + cols, mask=mask)
x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)
x\_hat = (x \sphinxhyphen{} mean) * rstd
y = x\_hat * w + b
tl.store(Y + cols, y, mask=mask)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why store rstd instead of std?**
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Avoid division in backward pass

\item {} 
\sphinxAtStartPar
One division now vs many in backward

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Third pass}: Apply normalization and affine transformation
\sphinxhyphen{} Load weights and biases
\sphinxhyphen{} Normalize: \sphinxcode{\sphinxupquote{(x \sphinxhyphen{} mean) * rstd}}
\sphinxhyphen{} Transform: \sphinxcode{\sphinxupquote{x\_hat * w + b}}
\sphinxhyphen{} Store result


\subsection{The Backward Pass}
\label{\detokenize{gpu-tutorials/05-layer-norm:the-backward-pass}}

\subsubsection{Gradient Mathematics}
\label{\detokenize{gpu-tutorials/05-layer-norm:gradient-mathematics}}
\sphinxAtStartPar
Given upstream gradient d/dL/d/dy, compute:
1. \sphinxstylestrong{d/dL/d/dx} (gradient w.r.t. input)
2. \sphinxstylestrong{d/dL/d/dw} (gradient w.r.t. weights)
3. \sphinxstylestrong{d/dL/d/db} (gradient w.r.t. biases)


\subsubsection{Gradient for Biases}
\label{\detokenize{gpu-tutorials/05-layer-norm:gradient-for-biases}}
\sphinxAtStartPar
Simplest:

\sphinxAtStartPar
d/dL/d/db = d/dL/d/dy

\sphinxAtStartPar
Just pass through the gradient!


\subsubsection{Gradient for Weights}
\label{\detokenize{gpu-tutorials/05-layer-norm:gradient-for-weights}}
\sphinxAtStartPar
d/dL/d/dw = d/dL/d/dy (o) x\_hat

\sphinxAtStartPar
Where \sphinxcode{\sphinxupquote{(o)}} is element\sphinxhyphen{}wise multiplication.

\sphinxAtStartPar
\sphinxstylestrong{But}: Same w and b are used for all samples in batch!

\sphinxAtStartPar
d/dL/d/dw = SIGMA (d/dL/d/dy{[}i{]} (o) x\_hat{[}i{]}) for all i in batch

\sphinxAtStartPar
Must \sphinxstylestrong{sum gradients across batch dimension}.


\subsubsection{Gradient for Input (Complex!)}
\label{\detokenize{gpu-tutorials/05-layer-norm:gradient-for-input-complex}}
\sphinxAtStartPar
d/dL/d/dx = (1/sigma) * (d/dL/d/dy (o) w \sphinxhyphen{} c\_1 (o) x\_hat \sphinxhyphen{} c\_2)

\sphinxAtStartPar
Where:

\sphinxAtStartPar
c\_1 = (1/N) * (x\_hat * (d/dL/d/dy (o) w))  \# Scalar dot product
c\_2 = (1/N) * (d/dL/d/dy * w)            \# Scalar dot product
sigma = sqrt(Var{[}x{]} + eps)                 \# Standard deviation

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Intuition}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
First term: Direct gradient through normalization

\item {} 
\sphinxAtStartPar
c\_1 term: Gradient from variance

\item {} 
\sphinxAtStartPar
c\_2 term: Gradient from mean

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Derivation} (brief sketch):
1. d/dy/d/dx\_hat = w (affine transform gradient)
2. d/dx\_hat/d/dx involves d/d(x\sphinxhyphen{}mu)/d/dx and d/dsigma/d/dx
3. d/dmu/d/dx = 1/N (each x affects mean)
4. d/dsigma\textasciicircum{}2/d/dx involves all x values
5. Chain rule gives the formula above


\subsection{Parallel Reduction Strategy}
\label{\detokenize{gpu-tutorials/05-layer-norm:parallel-reduction-strategy}}

\subsubsection{The Challenge}
\label{\detokenize{gpu-tutorials/05-layer-norm:the-challenge}}
\sphinxAtStartPar
For weight gradients:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
dw = SIGMA (dy{[}i{]} (o) x\_hat{[}i{]}) for i in batch

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Problem**: Multiple threads updating same dw simultaneously \PYGZhy{}\PYGZgt{} **race condition**!
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Naive solution}: Atomic adds
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
atomicAdd(\&dw{[}j{]}, dy{[}i, j{]} * x\_hat{[}i, j{]})

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Problem} \PYG{k}{with} \PYG{n}{atomics}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Very slow on GPUs

\item {} 
\sphinxAtStartPar
Serialize all updates

\item {} 
\sphinxAtStartPar
Can be 10\sphinxhyphen{}100x slower

\end{itemize}


\subsubsection{Two\sphinxhyphen{}Stage Reduction}
\label{\detokenize{gpu-tutorials/05-layer-norm:two-stage-reduction}}
\sphinxAtStartPar
\sphinxstylestrong{Stage 1}: Partial sums

\sphinxAtStartPar
Divide batch into groups
Each group accumulates into separate buffer
Use locks to prevent conflicts within group

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Stage} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Final} \PYG{n}{reduction}
\end{sphinxVerbatim}

\sphinxAtStartPar
Sum all group buffers to get final dw and db


\subsubsection{Group Assignment}
\label{\detokenize{gpu-tutorials/05-layer-norm:group-assignment}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
GROUP\_SIZE*M = 64  \# Rows per group
row = tl.program\_id(0)
group\_id = row // GROUP\_SIZE*M

\sphinxAtStartPar
Example (M=256 rows, GROUP\_SIZE*M=64):

\sphinxAtStartPar
Rows 0\sphinxhyphen{}63   \sphinxhyphen{}\textgreater{} Group 0 \sphinxhyphen{}\textgreater{} Buffer 0
Rows 64\sphinxhyphen{}127 \sphinxhyphen{}\textgreater{} Group 1 \sphinxhyphen{}\textgreater{} Buffer 1
Rows 128\sphinxhyphen{}191 \sphinxhyphen{}\textgreater{} Group 2 \sphinxhyphen{}\textgreater{} Buffer 2
Rows 192\sphinxhyphen{}255 \sphinxhyphen{}\textgreater{} Group 3 \sphinxhyphen{}\textgreater{} Buffer 3

\sphinxAtStartPar
Total buffers needed: ceil(256 / 64) = 4


\subsubsection{Using Locks}
\label{\detokenize{gpu-tutorials/05-layer-norm:using-locks}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Lock = tl.zeros({[}num\_groups{]}, dtype=tl.int32)


\section{In kernel:}
\label{\detokenize{gpu-tutorials/05-layer-norm:in-kernel}}
\sphinxAtStartPar
group\_id = row // GROUP\_SIZE*M


\section{Acquire lock}
\label{\detokenize{gpu-tutorials/05-layer-norm:acquire-lock}}\begin{description}
\sphinxlineitem{while tl.atomic\_cas(Lock + group\_id, 0, 1) == 1:}
\sphinxAtStartPar
pass  \# Spin until lock acquired

\end{description}


\section{Critical section: Update DW and DB}
\label{\detokenize{gpu-tutorials/05-layer-norm:critical-section-update-dw-and-db}}
\sphinxAtStartPar
dw\_ptrs = DW + group\_id * N + cols
db\_ptrs = DB + group\_id * N + cols
current\_dw = tl.load(dw\_ptrs, mask=mask)
current\_db = tl.load(db\_ptrs, mask=mask)
tl.store(dw\_ptrs, current\_dw + dw, mask=mask)
tl.store(db\_ptrs, current\_db + db, mask=mask)


\section{Release lock}
\label{\detokenize{gpu-tutorials/05-layer-norm:release-lock}}
\sphinxAtStartPar
tl.atomic\_xchg(Lock + group\_id, 0)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Atomic Compare\PYGZhy{}And\PYGZhy{}Swap** (``atomic\PYGZus{}cas``):
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
old\_value = atomic\_cas(ptr, compare, new)
If {\color{red}\bfseries{}*}ptr == compare: {\color{red}\bfseries{}*}ptr = new
==============================
Return old value of {\color{red}\bfseries{}*}ptr
========================

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Spin} \PYG{n}{lock} \PYG{n}{pattern}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{while atomic\_cas(Lock, 0, 1) == 1:  \# Try to set lock from 0 to 1}
\sphinxAtStartPar
pass  \# If already 1, keep trying

\end{description}


\section{Lock acquired (we set it to 1)}
\label{\detokenize{gpu-tutorials/05-layer-norm:lock-acquired-we-set-it-to-1}}

\section{Do work…}
\label{\detokenize{gpu-tutorials/05-layer-norm:do-work}}
\sphinxAtStartPar
atomic\_xchg(Lock, 0)  \# Release lock

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}*}layer\_norm*bwd\_dwdb(DW, DB, FINAL\_DW, FINAL\_DB, M, N, GROUP\_SIZE*M):}
\sphinxAtStartPar
num\_groups = triton.cdiv(M, GROUP\_SIZE*M)
\begin{description}
\sphinxlineitem{for group in range(num\_groups):}\begin{description}
\sphinxlineitem{for col in range(N):}
\sphinxAtStartPar
FINAL\_DW{[}col{]} += DW{[}group, col{]}
FINAL\_DB{[}col{]} += DB{[}group, col{]}

\end{description}

\end{description}

\end{description}

\sphinxAtStartPar
Simple sum across all group buffers.

\sphinxAtStartPar
\sphinxstylestrong{Why not do this in GPU kernel?}
\sphinxhyphen{} Could, but would need another level of synchronization
\sphinxhyphen{} Simple CPU loop is fine (N is usually small)
\sphinxhyphen{} In practice, often done with a separate GPU kernel


\subsection{Triton Implementation Details}
\label{\detokenize{gpu-tutorials/05-layer-norm:triton-implementation-details}}

\subsubsection{Memory Layout}
\label{\detokenize{gpu-tutorials/05-layer-norm:memory-layout}}
\sphinxAtStartPar
X: (batch\_size, N)  Row\sphinxhyphen{}major
Y: (batch\_size, N)  Row\sphinxhyphen{}major
W: (N,)             1D
B: (N,)             1D
Mean: (batch\_size,) 1D
Rstd: (batch\_size,) 1D (reciprocal standard deviation)


\subsubsection{Stride Usage}
\label{\detokenize{gpu-tutorials/05-layer-norm:stride-usage}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
Y += row * stride
X += row * stride

\begin{sphinxVerbatim}[commandchars=\\\{\}]
``stride`` is typically N (for row\PYGZhy{}major layout).
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Flexibility}: Can handle different layouts by passing appropriate strides.


\subsubsection{Why Use float32 for Accumulation?}
\label{\detokenize{gpu-tutorials/05-layer-norm:why-use-float32-for-accumulation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
a = tl.load(X + cols, mask=cols \textless{} N, other=0.).to(tl.float32)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Precision} \PYG{n}{issues} \PYG{k}{with} \PYG{n}{fp16}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
Sum of 1000 fp16 numbers: Accumulation errors
Variance computation: Very sensitive to precision

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Best} \PYG{n}{practice}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Load as fp16/bf16 (save memory bandwidth)

\item {} 
\sphinxAtStartPar
Accumulate in fp32 (maintain precision)

\item {} 
\sphinxAtStartPar
Store result as fp16/bf16 if needed

\end{itemize}


\subsection{Performance Characteristics}
\label{\detokenize{gpu-tutorials/05-layer-norm:performance-characteristics}}

\subsubsection{Computational Complexity}
\label{\detokenize{gpu-tutorials/05-layer-norm:computational-complexity}}
\sphinxAtStartPar
Per sample (row):
\sphinxhyphen{} Mean: N additions + 1 division = O(N)
\sphinxhyphen{} Variance: N multiplications + N additions + 1 division = O(N)
\sphinxhyphen{} Normalization: 2N operations = O(N)
\sphinxhyphen{} Affine: 2N operations = O(N)

\sphinxAtStartPar
Total: O(N) per sample, O(MN) for batch of M samples.


\subsubsection{Memory Bandwidth}
\label{\detokenize{gpu-tutorials/05-layer-norm:memory-bandwidth}}
\sphinxAtStartPar
Reads:
\sphinxhyphen{} X: MN (3 passes, but amortized)
\sphinxhyphen{} W: N
\sphinxhyphen{} B: N

\sphinxAtStartPar
Writes:
\sphinxhyphen{} Y: MN
\sphinxhyphen{} Mean: M
\sphinxhyphen{} Rstd: M

\sphinxAtStartPar
Total: \textasciitilde{}2MN + 2N + 2M \textasciitilde{} 2MN for large N.

\sphinxAtStartPar
\sphinxstylestrong{Bandwidth\sphinxhyphen{}bound}: Similar to softmax, most time spent on memory.


\subsubsection{Optimization Opportunities}
\label{\detokenize{gpu-tutorials/05-layer-norm:optimization-opportunities}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fuse with other ops}: Layer norm + residual, layer norm + dropout

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Larger BLOCK\_SIZE}: Better memory efficiency (fewer iterations)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Adjust GROUP\_SIZE*M}: Balance parallelism vs lock contention

\end{enumerate}


\subsection{Common Pitfalls}
\label{\detokenize{gpu-tutorials/05-layer-norm:common-pitfalls}}

\subsubsection{1. Numerical Stability}
\label{\detokenize{gpu-tutorials/05-layer-norm:numerical-stability}}
\sphinxAtStartPar
\sphinxstylestrong{Problem}: \sphinxcode{\sphinxupquote{sqrt(var)}} when var \textasciitilde{} 0
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
rstd = 1 / tl.sqrt(var + eps)

\sphinxAtStartPar
Always add \sphinxcode{\sphinxupquote{eps}} (e.g., 1e\sphinxhyphen{}5) before square root.


\subsubsection{2. Dimension Confusion}
\label{\detokenize{gpu-tutorials/05-layer-norm:dimension-confusion}}
\sphinxAtStartPar
Layer norm normalizes \sphinxstylestrong{across features} (last dimension):
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
x = torch.randn(32, 128)  \# (batch, features)
Normalize each of 32 samples across 128 features
================================================

\sphinxAtStartPar
Not across batch (that’s batch norm).


\subsubsection{3. Gradient Accumulation Race Conditions}
\label{\detokenize{gpu-tutorials/05-layer-norm:gradient-accumulation-race-conditions}}
\sphinxAtStartPar
\sphinxstylestrong{Wrong}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
dw{[}j{]} += local\_dw  \# Multiple threads writing simultaneously!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Right}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Use locks or separate buffers}
\label{\detokenize{gpu-tutorials/05-layer-norm:use-locks-or-separate-buffers}}\begin{description}
\sphinxlineitem{with lock:}
\sphinxAtStartPar
dw{[}j{]} += local\_dw

\end{description}

\sphinxAtStartPar
Weights are shared across batch:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
dw = sum(dy{[}i{]} * x\_hat{[}i{]} for i in batch)  \# Must sum!

\sphinxAtStartPar
Not:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
dw = dy{[}0{]} * x\_hat{[}0{]}  \# Wrong! Only uses first sample


\subsection{Advanced Concepts}
\label{\detokenize{gpu-tutorials/05-layer-norm:advanced-concepts}}

\subsubsection{RMSNorm (Simpler Variant)}
\label{\detokenize{gpu-tutorials/05-layer-norm:rmsnorm-simpler-variant}}
\sphinxAtStartPar
Layer norm without mean subtraction:

\sphinxAtStartPar
y = x / sqrt(mean(x\textasciicircum{}2) + eps) * w

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Benefits}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Faster (no mean computation)

\item {} 
\sphinxAtStartPar
Used in LLaMA, GPT\sphinxhyphen{}NeoX, etc.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Triton implementation}: Remove mean computation, simplify backward pass.


\subsubsection{GroupNorm}
\label{\detokenize{gpu-tutorials/05-layer-norm:groupnorm}}
\sphinxAtStartPar
Normalize within groups of features:

\sphinxAtStartPar
Divide N features into G groups
Normalize within each group independently

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Use} \PYG{n}{case}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{When} \PYG{n}{batch} \PYG{n}{size} \PYG{o+ow}{is} \PYG{n}{very} \PYG{n}{small} \PYG{p}{(}\PYG{n}{e}\PYG{o}{.}\PYG{n}{g}\PYG{o}{.}\PYG{p}{,} \PYG{n}{batch}\PYG{o}{=}\PYG{l+m+mi}{1} \PYG{k}{for} \PYG{n}{high}\PYG{o}{\PYGZhy{}}\PYG{n}{res} \PYG{n}{images}\PYG{p}{)}\PYG{o}{.}
\end{sphinxVerbatim}


\subsubsection{FP8 and Mixed Precision}
\label{\detokenize{gpu-tutorials/05-layer-norm:fp8-and-mixed-precision}}
\sphinxAtStartPar
\sphinxstylestrong{Challenge}: Layer norm requires high precision
\sphinxhyphen{} Mean/variance computation sensitive to rounding
\sphinxhyphen{} Use fp32 for accumulators
\sphinxhyphen{} Can use fp8/fp16 for input/output


\subsection{Comparison to PyTorch}
\label{\detokenize{gpu-tutorials/05-layer-norm:comparison-to-pytorch}}

\subsubsection{PyTorch Implementation}
\label{\detokenize{gpu-tutorials/05-layer-norm:pytorch-implementation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
torch.nn.LayerNorm(normalized\_shape, eps=1e\sphinxhyphen{}5)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Under} \PYG{n}{the} \PYG{n}{hood}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Uses vendor libraries (cuDNN on NVIDIA)

\item {} 
\sphinxAtStartPar
Highly optimized, multiple kernel launches

\item {} 
\sphinxAtStartPar
Not easily customizable

\end{itemize}


\subsubsection{Triton Advantages}
\label{\detokenize{gpu-tutorials/05-layer-norm:triton-advantages}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fusion}: Can fuse layer norm with other ops

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Customization}: Easy to modify for variants (RMSNorm, etc.)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Transparency}: See exactly what’s happening

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Portability}: Works on different GPU vendors

\end{enumerate}


\subsubsection{Performance}
\label{\detokenize{gpu-tutorials/05-layer-norm:performance}}
\sphinxAtStartPar
\sphinxstylestrong{Typically}:
\sphinxhyphen{} PyTorch (cuDNN): 100\%
\sphinxhyphen{} Triton (optimized): 90\sphinxhyphen{}100\%
\sphinxhyphen{} Naive implementation: 30\sphinxhyphen{}50\%

\sphinxAtStartPar
Triton achieves near\sphinxhyphen{}native performance with much more flexibility!


\subsection{Key Takeaways}
\label{\detokenize{gpu-tutorials/05-layer-norm:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Layer norm is essential for Transformers}: Enables training deep networks

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Two\sphinxhyphen{}pass algorithm}: First mean, then variance (numerically stable)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Parallel reduction}: Need efficient strategies for sum/mean/variance

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Locks prevent race conditions}: But add overhead (use groups to amortize)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Two\sphinxhyphen{}stage gradient accumulation}: Reduces lock contention

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Precision matters}: Use fp32 for accumulation, fp16 for storage

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Forward and backward together}: Backward pass is often more complex

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fusion opportunities}: Layer norm rarely stands alone in practice

\end{enumerate}

\sphinxAtStartPar
This pattern of forward + backward + gradient accumulation applies to many normalization layers: Batch Norm, Group Norm, RMS Norm, and more!

\sphinxstepscope


\section{Fused Attention (Flash Attention) in Triton}
\label{\detokenize{gpu-tutorials/06-fused-attention:fused-attention-flash-attention-in-triton}}\label{\detokenize{gpu-tutorials/06-fused-attention::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/06-fused-attention:overview}}
\sphinxAtStartPar
This implements \sphinxstylestrong{Flash Attention v2}, a revolutionary algorithm for computing attention in Transformers. It reduces memory usage from O(N\textasciicircum{}2) to O(N) and achieves 2\sphinxhyphen{}4x speedup by using \sphinxstylestrong{tiling}, \sphinxstylestrong{online softmax}, and \sphinxstylestrong{recomputation} strategies. This is one of the most advanced GPU kernels you’ll encounter.


\subsection{What You’ll Learn}
\label{\detokenize{gpu-tutorials/06-fused-attention:what-you-ll-learn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The \sphinxstylestrong{quadratic memory problem} in standard attention

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Flash Attention algorithm} and its innovations

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Online softmax}: Computing softmax without storing QK\textasciicircum{}T matrix

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Causal masking} for autoregressive models

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tensor descriptors} for advanced memory access

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Shared memory management} for large K/V blocks

\item {} 
\sphinxAtStartPar
Why attention is the bottleneck in long\sphinxhyphen{}sequence Transformers

\end{itemize}


\subsection{Background: The Attention Mechanism}
\label{\detokenize{gpu-tutorials/06-fused-attention:background-the-attention-mechanism}}

\subsubsection{Standard Attention Formula}
\label{\detokenize{gpu-tutorials/06-fused-attention:standard-attention-formula}}
\sphinxAtStartPar
Attention(Q, K, V) = softmax(QK\textasciicircum{}T / sqrtd) V

\sphinxAtStartPar
Where:
\sphinxhyphen{} Q (Query): (batch, heads, seq\_len, head\_dim)
\sphinxhyphen{} K (Key): (batch, heads, seq\_len, head\_dim)
\sphinxhyphen{} V (Value): (batch, heads, seq\_len, head\_dim)
\sphinxhyphen{} Output: (batch, heads, seq\_len, head\_dim)


\subsubsection{Step\sphinxhyphen{}by\sphinxhyphen{}Step Computation}
\label{\detokenize{gpu-tutorials/06-fused-attention:step-by-step-computation}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compute similarity scores}:

\sphinxAtStartPar
S = QK\textasciicircum{}T / sqrtd\_k
S shape: (seq\_len, seq\_len)  \# NxN matrix!

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Apply softmax}:

\sphinxAtStartPar
P = softmax(S)
P shape: (seq\_len, seq\_len)  \# Still NxN!

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Weighted sum of values}:

\sphinxAtStartPar
O = PV
O shape: (seq\_len, head\_dim)

\end{enumerate}


\subsubsection{The Memory Problem}
\label{\detokenize{gpu-tutorials/06-fused-attention:the-memory-problem}}
\sphinxAtStartPar
For sequence length N=2048, head\_dim=64, fp16:

\sphinxAtStartPar
\sphinxstylestrong{Intermediate matrices}:
\sphinxhyphen{} S (scores): 2048 x 2048 x 2 bytes = 8 MB
\sphinxhyphen{} P (attention weights): 2048 x 2048 x 2 bytes = 8 MB
\sphinxhyphen{} Total per head: 16 MB

\sphinxAtStartPar
\sphinxstylestrong{For a Transformer}:
\sphinxhyphen{} Batch size: 32
\sphinxhyphen{} Heads: 16
\sphinxhyphen{} Total: 32 x 16 x 16 MB = \sphinxstylestrong{8 GB} just for attention matrices!

\sphinxAtStartPar
\sphinxstylestrong{For N=16384} (long sequences):
\sphinxhyphen{} Per head: 16384\textasciicircum{}2 x 2 bytes = 512 MB
\sphinxhyphen{} Total: 32 x 16 x 512 MB = \sphinxstylestrong{256 GB} {[}wow{]}

\sphinxAtStartPar
\sphinxstylestrong{Clearly unsustainable!}


\subsection{Flash Attention: The Key Insights}
\label{\detokenize{gpu-tutorials/06-fused-attention:flash-attention-the-key-insights}}

\subsubsection{Insight 1: We Don’t Need to Store S and P}
\label{\detokenize{gpu-tutorials/06-fused-attention:insight-1-we-don-t-need-to-store-s-and-p}}
\sphinxAtStartPar
\sphinxstylestrong{Standard approach}:
1. Compute full S = QK\textasciicircum{}T
2. Store S in HBM (slow)
3. Compute full P = softmax(S)
4. Store P in HBM
5. Compute O = PV

\sphinxAtStartPar
\sphinxstylestrong{Flash Attention}:
1. Compute and process S and P in \sphinxstylestrong{blocks}
2. Keep blocks in SRAM (fast)
3. Never materialize full S or P in HBM
4. Only store final output O

\sphinxAtStartPar
\sphinxstylestrong{Memory}: O(N\textasciicircum{}2) \sphinxhyphen{}\textgreater{} O(N) {[}celebrate{]}


\subsubsection{Insight 2: Online Softmax}
\label{\detokenize{gpu-tutorials/06-fused-attention:insight-2-online-softmax}}
\sphinxAtStartPar
\sphinxstylestrong{Challenge}: How to compute softmax without storing full matrix?

\sphinxAtStartPar
\sphinxstylestrong{Standard softmax} requires two passes:
.. code\sphinxhyphen{}block:: python


\section{Pass 1: Find max}
\label{\detokenize{gpu-tutorials/06-fused-attention:pass-1-find-max}}
\sphinxAtStartPar
max\_score = max(S)


\section{Pass 2: Compute softmax}
\label{\detokenize{gpu-tutorials/06-fused-attention:pass-2-compute-softmax}}
\sphinxAtStartPar
P = exp(S \sphinxhyphen{} max\_score) / sum(exp(S \sphinxhyphen{} max\_score))

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Online algorithm**: Update running statistics as we process blocks!
\end{sphinxVerbatim}

\sphinxAtStartPar
Process attention in blocks:
\begin{description}
\sphinxlineitem{For each block of Q (BLOCK\_M rows):}\begin{description}
\sphinxlineitem{For each block of K,V (BLOCK\_N columns):}
\sphinxAtStartPar
Compute block of S
Update running softmax statistics
Accumulate into output

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Benefit**: Each block fits in SRAM (fast)!
\end{sphinxVerbatim}


\subsection{The Online Softmax Algorithm}
\label{\detokenize{gpu-tutorials/06-fused-attention:the-online-softmax-algorithm}}

\subsubsection{Standard Softmax}
\label{\detokenize{gpu-tutorials/06-fused-attention:standard-softmax}}
\sphinxAtStartPar
For a row S\_i = {[}s\_1, s\_2, …, s\_n{]}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
m = max(s\_1, s\_2, …, s\_n)
numerator = {[}exp(s\_1\sphinxhyphen{}m), exp(s\_2\sphinxhyphen{}m), …, exp(s\_n\sphinxhyphen{}m){]}
l = sum(numerator)
P\_i = numerator / l


\subsubsection{Block\sphinxhyphen{}wise Computation}
\label{\detokenize{gpu-tutorials/06-fused-attention:block-wise-computation}}
\sphinxAtStartPar
Suppose we process in two blocks: {[}s\_1, s\_2{]} and {[}s\_3, s\_4{]}:

\sphinxAtStartPar
\sphinxstylestrong{After block 1}:

\sphinxAtStartPar
m\_old = max(s\_1, s\_2)
l\_old = exp(s\_1 \sphinxhyphen{} m\_old) + exp(s\_2 \sphinxhyphen{} m\_old)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{After} \PYG{n}{block} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
m\_new = max(m\_old, max(s\_3, s\_4))

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Problem**: How to update l\PYGZus{}old when m changes?
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Solution}: Correction factor!

\sphinxAtStartPar
alpha = exp(m\_old \sphinxhyphen{} m\_new)
l\_new = alpha * l\_old + exp(s\_3 \sphinxhyphen{} m\_new) + exp(s\_4 \sphinxhyphen{} m\_new)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why alpha?**
\end{sphinxVerbatim}

\sphinxAtStartPar
Old contribution: exp(s\_1 \sphinxhyphen{} m\_old) + exp(s\_2 \sphinxhyphen{} m\_old)
With new max:      exp(s\_1 \sphinxhyphen{} m\_new) + exp(s\_2 \sphinxhyphen{} m\_new)
\begin{quote}

\sphinxAtStartPar
= exp(s\_1 \sphinxhyphen{} m\_old \sphinxhyphen{} (m\_new \sphinxhyphen{} m\_old)) + exp(s\_2 \sphinxhyphen{} m\_old \sphinxhyphen{} (m\_new \sphinxhyphen{} m\_old))
= {[}exp(s\_1 \sphinxhyphen{} m\_old) + exp(s\_2 \sphinxhyphen{} m\_old){]} * exp(m\_old \sphinxhyphen{} m\_new)
= l\_old * alpha
\end{quote}


\subsubsection{Updating the Output}
\label{\detokenize{gpu-tutorials/06-fused-attention:updating-the-output}}
\sphinxAtStartPar
Similarly, the accumulated output needs correction:

\sphinxAtStartPar
output\_old = (1/l\_old) * {[}exp(s\_1\sphinxhyphen{}m\_old)*v\_1 + exp(s\_2\sphinxhyphen{}m\_old)*v\_2{]}

\sphinxAtStartPar
When we update max:

\sphinxAtStartPar
output\_new = alpha * output\_old + (1/l\_new) * {[}exp(s\_3\sphinxhyphen{}m\_new)*v\_3 + exp(s\_4\sphinxhyphen{}m\_new)*v\_4{]}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**This is the heart of Flash Attention!**
\end{sphinxVerbatim}


\subsection{Triton Implementation Details}
\label{\detokenize{gpu-tutorials/06-fused-attention:triton-implementation-details}}

\subsubsection{The Inner Loop}
\label{\detokenize{gpu-tutorials/06-fused-attention:the-inner-loop}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}*}attn\_fwd*inner(acc, l\_i, m\_i, q, desc\_k, desc\_v, …):}\begin{description}
\sphinxlineitem{for start\_n in range(lo, hi, BLOCK\_N):}
\sphinxAtStartPar
\# Load K block
k = desc\_k.load({[}offsetk\_y, 0{]}).T

\sphinxAtStartPar
\# Compute QK\textasciicircum{}T for this block
qk = tl.dot(q, k)

\sphinxAtStartPar
\# Apply scaling and mask (if causal)
qk = qk * qk\_scale
if STAGE == 2:  \# Causal
\begin{quote}

\sphinxAtStartPar
mask = offs\_m{[}:, None{]} \textgreater{}= (start\_n + offs\_n{[}None, :{]})
qk = qk + tl.where(mask, 0, \sphinxhyphen{}1.0e6)
\end{quote}

\sphinxAtStartPar
\# Update max
m\_ij = tl.maximum(m\_i, tl.max(qk, 1))

\sphinxAtStartPar
\# Compute softmax probabilities for this block
p = tl.exp(qk \sphinxhyphen{} m\_ij{[}:, None{]})

\sphinxAtStartPar
\# Correction factor
alpha = tl.exp(m\_i \sphinxhyphen{} m\_ij)

\sphinxAtStartPar
\# Update sum
l\_ij = tl.sum(p, 1)

\sphinxAtStartPar
\# Update output with correction
acc = acc * alpha{[}:, None{]}

\sphinxAtStartPar
\# Load V block
v = desc\_v.load({[}offsetv\_y, 0{]})

\sphinxAtStartPar
\# Accumulate
acc = tl.dot(p, v, acc)

\sphinxAtStartPar
\# Update running statistics
l\_i = l\_i * alpha + l\_ij
m\_i = m\_ij

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{variables}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{m\_i}}: Current max for each row

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{l\_i}}: Current sum of exponentials for each row

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{acc}}: Current weighted sum output

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{alpha}}: Correction factor when max changes

\end{itemize}


\subsubsection{Causal Masking}
\label{\detokenize{gpu-tutorials/06-fused-attention:causal-masking}}
\sphinxAtStartPar
For autoregressive models (GPT), prevent attending to future tokens:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
mask = offs\_m{[}:, None{]} \textgreater{}= (start\_n + offs\_n{[}None, :{]})
qk = qk + tl.where(mask, 0, \sphinxhyphen{}1.0e6)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Visual} \PYG{n}{example}\PYG{o}{*}\PYG{o}{*} \PYG{p}{(}\PYG{n}{BLOCK\PYGZus{}M}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}N}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
Query rows: {[}0, 1, 2, 3{]}
Key columns: {[}0, 1, 2, 3{]}

\sphinxAtStartPar
Mask (1 = allowed, 0 = masked):
{[}{[}1, 0, 0, 0{]},   \# Query 0 can only attend to Key 0
\begin{quote}

\sphinxAtStartPar
{[}1, 1, 0, 0{]},   \# Query 1 can attend to Keys 0\sphinxhyphen{}1
{[}1, 1, 1, 0{]},   \# Query 2 can attend to Keys 0\sphinxhyphen{}2
{[}1, 1, 1, 1{]}{]}   \# Query 3 can attend to Keys 0\sphinxhyphen{}3
\end{quote}

\sphinxAtStartPar
Setting masked positions to \sphinxcode{\sphinxupquote{\sphinxhyphen{}1e6}} \sphinxhyphen{}\textgreater{} \sphinxcode{\sphinxupquote{exp(\sphinxhyphen{}1e6) \textasciitilde{} 0}} \sphinxhyphen{}\textgreater{} effectively ignored.


\subsubsection{Stages for Causal Attention}
\label{\detokenize{gpu-tutorials/06-fused-attention:stages-for-causal-attention}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{if STAGE == 1:}
\sphinxAtStartPar
lo, hi = 0, start\_m * BLOCK\_M  \# Before diagonal

\sphinxlineitem{elif STAGE == 2:}
\sphinxAtStartPar
lo, hi = start\_m * BLOCK\_M, (start\_m + 1) * BLOCK\_M  \# On diagonal (causal)

\sphinxlineitem{else:}
\sphinxAtStartPar
lo, hi = 0, N\_CTX  \# Full attention (non\sphinxhyphen{}causal)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{For} \PYG{n}{causal} \PYG{n}{attention}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Stage 1}: Process all blocks before the diagonal (full blocks, no masking needed)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Stage 2}: Process diagonal block (needs causal masking)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Optimization}: Stage 1 can skip masking checks, runs faster!


\subsection{Memory Optimizations}
\label{\detokenize{gpu-tutorials/06-fused-attention:memory-optimizations}}

\subsubsection{Tensor Descriptors}
\label{\detokenize{gpu-tutorials/06-fused-attention:tensor-descriptors}}
\sphinxAtStartPar
Modern GPUs (Hopper, Blackwell) support \sphinxstylestrong{tensor descriptors}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{desc\_q = TensorDescriptor(q, shape={[}y\_dim, HEAD\_DIM{]},}
\sphinxAtStartPar
strides={[}HEAD\_DIM, 1{]},
block\_shape={[}BLOCK\_M, HEAD\_DIM{]})

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Benefits}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Hardware\sphinxhyphen{}assisted bounds checking

\item {} 
\sphinxAtStartPar
Automatic address calculation

\item {} 
\sphinxAtStartPar
Faster than manual pointer arithmetic

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Triton abstracts this}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
q = desc\_q.load({[}offset\_y, 0{]})

\sphinxAtStartPar
Looks simple, but uses advanced hardware features!


\subsubsection{Warp Specialization}
\label{\detokenize{gpu-tutorials/06-fused-attention:warp-specialization}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
warp\_specialize = True

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Concept}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Different} \PYG{n}{warps} \PYG{o+ow}{in} \PYG{n}{a} \PYG{n}{block} \PYG{n}{do} \PYG{n}{different} \PYG{n}{tasks}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Some warps: Load data from DRAM

\item {} 
\sphinxAtStartPar
Other warps: Compute on data

\item {} 
\sphinxAtStartPar
Overlap communication and computation!

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Hopper/Blackwell specific}: These GPUs have dedicated units for loads, so specialization helps.


\subsubsection{Reduced Shared Memory}
\label{\detokenize{gpu-tutorials/06-fused-attention:reduced-shared-memory}}
\sphinxAtStartPar
By using online softmax:
\sphinxhyphen{} Don’t store full QK\textasciicircum{}T matrix
\sphinxhyphen{} Only need current blocks in SRAM
\sphinxhyphen{} Typical usage: \textasciitilde{}100 KB vs MBs for naive implementation


\subsubsection{FP8 Support}
\label{\detokenize{gpu-tutorials/06-fused-attention:fp8-support}}
\sphinxAtStartPar
For Blackwell GPUs with FP8 Tensor Cores:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{if dtype == tl.float8e5:}
\sphinxAtStartPar
v = desc\_v.load({[}0, offsetv\_y{]}).T  \# Transposed layout for FP8

\sphinxlineitem{else:}
\sphinxAtStartPar
v = desc\_v.load({[}offsetv\_y, 0{]})

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{FP8} \PYG{n}{benefits}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
2x memory bandwidth (8 bits vs 16 bits)

\item {} 
\sphinxAtStartPar
2x compute throughput (Tensor Cores)

\item {} 
\sphinxAtStartPar
Trade\sphinxhyphen{}off: Slight accuracy loss (acceptable for ML)

\end{itemize}


\subsection{Backward Pass}
\label{\detokenize{gpu-tutorials/06-fused-attention:backward-pass}}
\sphinxAtStartPar
The backward pass is even more complex!


\subsubsection{Preprocess Step}
\label{\detokenize{gpu-tutorials/06-fused-attention:preprocess-step}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}*}attn\_bwd*preprocess(O, DO, Delta, …):}
\sphinxAtStartPar
\# Compute delta = sum(O * DO) for each row
o = tl.load(O + …)
do = tl.load(DO + …)
delta = tl.sum(o * do, axis=1)
tl.store(Delta + …, delta)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Delta}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Row}\PYG{o}{\PYGZhy{}}\PYG{n}{wise} \PYG{n}{dot} \PYG{n}{product} \PYG{n}{of} \PYG{n}{output} \PYG{o+ow}{and} \PYG{n}{output} \PYG{n}{gradient}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Used in both dK/dV and dQ computation

\item {} 
\sphinxAtStartPar
Precompute once, use multiple times

\end{itemize}


\subsubsection{Computing dK and dV}
\label{\detokenize{gpu-tutorials/06-fused-attention:computing-dk-and-dv}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}*}attn\_bwd*dkdv(dk, dv, Q, k, v, DO, M, D, …):}\begin{description}
\sphinxlineitem{for each block of Q:}
\sphinxAtStartPar
\# Recompute attention weights
qk = tl.dot(k, Q)
p = tl.exp(qk \sphinxhyphen{} M)  \# M was saved from forward pass

\sphinxAtStartPar
\# Compute dV
dv += tl.dot(p, DO)

\sphinxAtStartPar
\# Compute dP (gradient of attention weights)
dp = tl.dot(v, DO.T)

\sphinxAtStartPar
\# Compute dS (before softmax)
ds = p * (dp \sphinxhyphen{} D)  \# D is delta from preprocess

\sphinxAtStartPar
\# Compute dK
dk += tl.dot(ds, Q.T)

\end{description}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{insight}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Recompute} \PYG{n}{attention} \PYG{n}{weights} \PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{Q}\PYG{p}{,} \PYG{n}{K} \PYG{p}{(}\PYG{n}{which} \PYG{n}{we} \PYG{n}{have}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Don’t need to store P from forward pass

\item {} 
\sphinxAtStartPar
Classic memory\sphinxhyphen{}compute trade\sphinxhyphen{}off

\end{itemize}


\subsubsection{Computing dQ}
\label{\detokenize{gpu-tutorials/06-fused-attention:computing-dq}}
\sphinxAtStartPar
Similar structure, but processes different blocks:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def {\color{red}\bfseries{}*}attn\_bwd*dq(dq, q, K, V, DO, M, D, …):}\begin{description}
\sphinxlineitem{for each block of K,V:}
\sphinxAtStartPar
\# Recompute attention
qk = tl.dot(q, K)
p = tl.exp(qk \sphinxhyphen{} M)

\sphinxAtStartPar
\# Compute dP
dp = tl.dot(DO, V.T)

\sphinxAtStartPar
\# Compute dS
ds = p * (dp \sphinxhyphen{} D)

\sphinxAtStartPar
\# Compute dQ
dq += tl.dot(ds, K)

\end{description}

\end{description}


\subsubsection{Why Recomputation?}
\label{\detokenize{gpu-tutorials/06-fused-attention:why-recomputation}}
\sphinxAtStartPar
\sphinxstylestrong{Traditional approach}:
\sphinxhyphen{} Forward: Compute and store P (N\textasciicircum{}2 memory)
\sphinxhyphen{} Backward: Load P, compute gradients

\sphinxAtStartPar
\sphinxstylestrong{Flash Attention}:
\sphinxhyphen{} Forward: Don’t store P (O(N) memory)
\sphinxhyphen{} Backward: Recompute P from Q, K (extra compute, but much less memory)

\sphinxAtStartPar
\sphinxstylestrong{Trade\sphinxhyphen{}off}:
\sphinxhyphen{} 2x compute (recompute in backward)
\sphinxhyphen{} But enables 100x longer sequences!


\subsection{Performance Characteristics}
\label{\detokenize{gpu-tutorials/06-fused-attention:performance-characteristics}}

\subsubsection{Memory Complexity}
\label{\detokenize{gpu-tutorials/06-fused-attention:memory-complexity}}
\sphinxAtStartPar
\sphinxstylestrong{Standard attention}:
\sphinxhyphen{} O(N\textasciicircum{}2) for S and P matrices
\sphinxhyphen{} Becomes prohibitive for long sequences

\sphinxAtStartPar
\sphinxstylestrong{Flash Attention}:
\sphinxhyphen{} O(N) for Q, K, V, O
\sphinxhyphen{} No intermediate N\textasciicircum{}2 matrices
\sphinxhyphen{} Enables sequences of 16K, 32K, even 100K+ tokens


\subsubsection{Compute Complexity}
\label{\detokenize{gpu-tutorials/06-fused-attention:compute-complexity}}
\sphinxAtStartPar
Still O(N\textasciicircum{}2) FLOPs (can’t change the math), but:
\sphinxhyphen{} Better memory access patterns
\sphinxhyphen{} Higher arithmetic intensity
\sphinxhyphen{} Better cache utilization

\sphinxAtStartPar
\sphinxstylestrong{Result}: 2\sphinxhyphen{}4x faster despite same FLOPs!


\subsubsection{Arithmetic Intensity}
\label{\detokenize{gpu-tutorials/06-fused-attention:arithmetic-intensity}}
\sphinxAtStartPar
With tiling (BLOCK\_M=128, BLOCK\_N=64):
\sphinxhyphen{} Load Q block: 128 x 64 elements
\sphinxhyphen{} Load K block: 64 x 64 elements
\sphinxhyphen{} Load V block: 64 x 64 elements
\sphinxhyphen{} Compute QK: 128 x 64 x 64 = 512K FLOPs
\sphinxhyphen{} Compute PV: 128 x 64 x 64 = 512K FLOPs
\sphinxhyphen{} Total: \textasciitilde{}1M FLOPs per \textasciitilde{}20K bytes loaded

\sphinxAtStartPar
Arithmetic Intensity: 1M / 20K \textasciitilde{} 50 FLOPs/byte

\sphinxAtStartPar
\sphinxstylestrong{Compare to naive} (no tiling):
\sphinxhyphen{} Each element of S: 1 load Q row, 1 load K row
\sphinxhyphen{} Each element of P: 1 load/store
\sphinxhyphen{} AI \textasciitilde{} 2 FLOPs/byte

\sphinxAtStartPar
Flash Attention is \sphinxstylestrong{25x more compute\sphinxhyphen{}efficient} in memory access!


\subsection{Auto\sphinxhyphen{}Tuning Configurations}
\label{\detokenize{gpu-tutorials/06-fused-attention:auto-tuning-configurations}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{configs = {[}}
\sphinxAtStartPar
triton.Config(\{‘BLOCK\_M’: BM, ‘BLOCK\_N’: BN\}, num\_stages=s, num\_warps=w)
for BM in {[}64, 128{]}
for BN in {[}32, 64, 128{]}
for s in {[}2, 3, 4{]}
for w in {[}4, 8{]}

\end{description}

\sphinxAtStartPar
\sphinxstylestrong{Block size trade\sphinxhyphen{}offs}:
\sphinxhyphen{} \sphinxstylestrong{Larger blocks}: More reuse, but more SRAM usage
\sphinxhyphen{} \sphinxstylestrong{Smaller blocks}: Less SRAM, but more overhead

\sphinxAtStartPar
\sphinxstylestrong{Typical good configs}:
\sphinxhyphen{} BLOCK\_M=128, BLOCK\_N=64: Balanced
\sphinxhyphen{} BLOCK\_M=64, BLOCK\_N=32: Smaller SRAM GPUs
\sphinxhyphen{} More stages: Better for Hopper/Blackwell with more SRAM


\subsection{Common Pitfalls}
\label{\detokenize{gpu-tutorials/06-fused-attention:common-pitfalls}}

\subsubsection{1. SRAM Overflow}
\label{\detokenize{gpu-tutorials/06-fused-attention:sram-overflow}}
\sphinxAtStartPar
Error: out of resource: shared memory

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Cause}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Blocks} \PYG{n}{too} \PYG{n}{large} \PYG{k}{for} \PYG{n}{available} \PYG{n}{SRAM}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Solution}:
\sphinxhyphen{} Reduce BLOCK\_M or BLOCK\_N
\sphinxhyphen{} Reduce num\_stages
\sphinxhyphen{} Adjust auto\sphinxhyphen{}tune configs


\subsubsection{2. Numerical Instability}
\label{\detokenize{gpu-tutorials/06-fused-attention:numerical-instability}}
\sphinxAtStartPar
\sphinxstylestrong{Problem}: Exp overflow without proper max subtraction

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Wrong}
\label{\detokenize{gpu-tutorials/06-fused-attention:wrong}}
\sphinxAtStartPar
p = tl.exp(qk)


\section{Right}
\label{\detokenize{gpu-tutorials/06-fused-attention:right}}
\sphinxAtStartPar
m\_ij = tl.max(qk, 1)
p = tl.exp(qk \sphinxhyphen{} m\_ij{[}:, None{]})

\sphinxAtStartPar
\sphinxstylestrong{Common mistake}: Using \sphinxcode{\sphinxupquote{\textgreater{}}} instead of \sphinxcode{\sphinxupquote{\textgreater{}=}}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Wrong for causal (token can’t attend to itself!)}
\label{\detokenize{gpu-tutorials/06-fused-attention:wrong-for-causal-token-can-t-attend-to-itself}}
\sphinxAtStartPar
mask = offs\_m{[}:, None{]} \textgreater{} (start\_n + offs\_n{[}None, :{]})


\section{Right}
\label{\detokenize{gpu-tutorials/06-fused-attention:id9}}
\sphinxAtStartPar
mask = offs\_m{[}:, None{]} \textgreater{}= (start\_n + offs\_n{[}None, :{]})

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Must update both m\_i and l\_i!}
\label{\detokenize{gpu-tutorials/06-fused-attention:must-update-both-m-i-and-l-i}}
\sphinxAtStartPar
l\_i = l\_i * alpha + l\_ij
m\_i = m\_ij


\section{And correct accumulator}
\label{\detokenize{gpu-tutorials/06-fused-attention:and-correct-accumulator}}
\sphinxAtStartPar
acc = acc * alpha{[}:, None{]}

\sphinxAtStartPar
Forgetting any of these \sphinxhyphen{}\textgreater{} wrong results.


\subsection{Extensions and Variants}
\label{\detokenize{gpu-tutorials/06-fused-attention:extensions-and-variants}}

\subsubsection{Multi\sphinxhyphen{}Query Attention (MQA)}
\label{\detokenize{gpu-tutorials/06-fused-attention:multi-query-attention-mqa}}
\sphinxAtStartPar
Use same K, V for all heads:
\sphinxhyphen{} K, V shape: (batch, 1, seq\_len, head\_dim)
\sphinxhyphen{} Q shape: (batch, num\_heads, seq\_len, head\_dim)
\sphinxhyphen{} Memory savings: num\_headsx less for K, V


\subsubsection{Grouped Query Attention (GQA)}
\label{\detokenize{gpu-tutorials/06-fused-attention:grouped-query-attention-gqa}}
\sphinxAtStartPar
Groups of heads share K, V:
\sphinxhyphen{} Middle ground between MQA and full attention
\sphinxhyphen{} Used in Llama 2, Mistral


\subsubsection{Sliding Window Attention}
\label{\detokenize{gpu-tutorials/06-fused-attention:sliding-window-attention}}
\sphinxAtStartPar
Only attend to nearby tokens:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
mask = abs(offs\_m{[}:, None{]} \sphinxhyphen{} offs\_n{[}None, :{]}) \textless{}= window\_size

\sphinxAtStartPar
Reduces complexity to O(N x window\_size).


\subsubsection{Flash Attention 3}
\label{\detokenize{gpu-tutorials/06-fused-attention:flash-attention-3}}
\sphinxAtStartPar
Latest version (Hopper H100+):
\sphinxhyphen{} Warp\sphinxhyphen{}specialization optimizations
\sphinxhyphen{} Even better overlap of compute and memory
\sphinxhyphen{} FP8 support with dynamic scaling
\sphinxhyphen{} 1.5\sphinxhyphen{}2x faster than Flash Attention 2


\subsection{Comparison to Standard Attention}
\label{\detokenize{gpu-tutorials/06-fused-attention:comparison-to-standard-attention}}
\begin{DUlineblock}{0em}
\item[] Aspect | Standard | Flash Attention |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\sphinxhyphen{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| Memory | O(N\textasciicircum{}2) | O(N) |
| Speed (2K seq) | 1x | 2\sphinxhyphen{}3x faster |
| Speed (16K seq) | OOM | 10x+ faster |
| Max sequence | \textasciitilde{}2048 | 100K+ |
| Implementation | Simple | Complex |


\subsection{Key Takeaways}
\label{\detokenize{gpu-tutorials/06-fused-attention:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Flash Attention solves the O(N\textasciicircum{}2) memory problem}: Enables long sequences

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Online softmax is the key innovation}: Compute softmax without full materialization

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tiling + correction factors}: Update statistics as we process blocks

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Recomputation in backward pass}: Trade compute for memory

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{SRAM management is critical}: Blocks must fit in fast memory

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Causal masking}: Essential for autoregressive models (GPT, etc.)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hardware features matter}: Tensor descriptors, warp specialization, FP8

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{This enables modern LLMs}: GPT\sphinxhyphen{}4, Llama, etc. all use variants of Flash Attention

\end{enumerate}

\sphinxAtStartPar
Flash Attention is arguably the most important algorithmic innovation for Transformers in recent years. It enabled the jump from \textasciitilde{}2K to 100K+ token context windows!

\sphinxstepscope


\section{Using External Functions (libdevice) in Triton}
\label{\detokenize{gpu-tutorials/07-extern-functions:using-external-functions-libdevice-in-triton}}\label{\detokenize{gpu-tutorials/07-extern-functions::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/07-extern-functions:overview}}
\sphinxAtStartPar
This tutorial shows how to call external library functions from Triton kernels. Specifically, we’ll use CUDA’s \sphinxcode{\sphinxupquote{libdevice}} (or AMD’s device libraries) to access optimized mathematical functions that aren’t available in standard Triton.


\subsection{What You’ll Learn}
\label{\detokenize{gpu-tutorials/07-extern-functions:what-you-ll-learn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
How to use \sphinxstylestrong{external library functions} in Triton

\item {} 
\sphinxAtStartPar
What \sphinxstylestrong{libdevice} is and why it’s useful

\item {} 
\sphinxAtStartPar
How to specify \sphinxstylestrong{library paths} for compilation

\item {} 
\sphinxAtStartPar
The difference between Triton intrinsics and external functions

\item {} 
\sphinxAtStartPar
When to use external functions vs Triton built\sphinxhyphen{}ins

\end{itemize}


\subsection{What is libdevice?}
\label{\detokenize{gpu-tutorials/07-extern-functions:what-is-libdevice}}

\subsubsection{NVIDIA’s libdevice}
\label{\detokenize{gpu-tutorials/07-extern-functions:nvidia-s-libdevice}}
\sphinxAtStartPar
\sphinxstylestrong{libdevice} is a library of device\sphinxhyphen{}side mathematical functions for CUDA:
\sphinxhyphen{} Provided by NVIDIA as part of CUDA toolkit
\sphinxhyphen{} Highly optimized for GPU architectures
\sphinxhyphen{} Covers special functions not in standard CUDA

\sphinxAtStartPar
\sphinxstylestrong{File format}: \sphinxcode{\sphinxupquote{.bc}} (LLVM bitcode)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{libdevice.10.bc}}: Version for Compute Capability 10+ (Hopper, Blackwell)
\sphinxhyphen{} Contains pre\sphinxhyphen{}compiled device functions


\subsubsection{AMD’s Device Libraries}
\label{\detokenize{gpu-tutorials/07-extern-functions:amd-s-device-libraries}}
\sphinxAtStartPar
AMD provides similar libraries for ROCm:
\sphinxhyphen{} \sphinxstylestrong{ocml.bc}: OpenCL math library (sin, cos, exp, etc.)
\sphinxhyphen{} \sphinxstylestrong{ockl.bc}: OpenCL kernel library (atomics, barriers, etc.)


\subsubsection{What Functions Are Available?}
\label{\detokenize{gpu-tutorials/07-extern-functions:what-functions-are-available}}
\sphinxAtStartPar
\sphinxstylestrong{Examples from libdevice}:
.. code\sphinxhyphen{}block:: c

\sphinxAtStartPar
double \_\_nv*asin(double)      // Arc sine
float  \_\_nv*asinf(float)       // Arc sine (single precision)
double \_\_nv*j0(double)         // Bessel function J0
float  \_\_nv*erfcinvf(float)    // Inverse complementary error function
double \_\_nv*tgamma(double)     // Gamma function

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Full list**: See `CUDA libdevice Users Guide \PYGZlt{}https://docs.nvidia.com/cuda/libdevice\PYGZhy{}users\PYGZhy{}guide/\PYGZgt{}`\PYGZus{}
\end{sphinxVerbatim}


\subsection{Why Use External Functions?}
\label{\detokenize{gpu-tutorials/07-extern-functions:why-use-external-functions}}

\subsubsection{Triton Built\sphinxhyphen{}in Math}
\label{\detokenize{gpu-tutorials/07-extern-functions:triton-built-in-math}}
\sphinxAtStartPar
Triton provides common functions:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
tl.exp(x)    \# Exponential
tl.log(x)    \# Natural logarithm
tl.sin(x)    \# Sine
tl.cos(x)    \# Cosine
tl.sqrt(x)   \# Square root
tl.abs(x)    \# Absolute value

\sphinxAtStartPar
These are fast and convenient!


\subsubsection{When You Need More}
\label{\detokenize{gpu-tutorials/07-extern-functions:when-you-need-more}}
\sphinxAtStartPar
For specialized functions:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
asin(x)      \# Arc sine \sphinxhyphen{} NOT in Triton intrinsics!
bessel\_j0(x) \# Bessel function \sphinxhyphen{} NOT available!
erf(x)       \# Error function \sphinxhyphen{} need libdevice
gamma(x)     \# Gamma function \sphinxhyphen{} need libdevice

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Options}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Implement yourself (complex, error\sphinxhyphen{}prone)

\item {} 
\sphinxAtStartPar
Use libdevice (optimized, tested, easy)

\end{enumerate}


\subsection{Using libdevice in Triton}
\label{\detokenize{gpu-tutorials/07-extern-functions:using-libdevice-in-triton}}

\subsubsection{The Simple Way (Default Path)}
\label{\detokenize{gpu-tutorials/07-extern-functions:the-simple-way-default-path}}
\sphinxAtStartPar
Triton automatically finds libdevice:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from triton.language.extra import libdevice

\sphinxAtStartPar
@triton.jit
def asin\_kernel(x\_ptr, y\_ptr, n\_elements, BLOCK\_SIZE: tl.constexpr):
\begin{quote}

\sphinxAtStartPar
pid = tl.program\_id(axis=0)
offsets = pid * BLOCK\_SIZE + tl.arange(0, BLOCK\_SIZE)
mask = offsets \textless{} n\_elements

\sphinxAtStartPar
x = tl.load(x\_ptr + offsets, mask=mask)
y = libdevice.asin(x)  \# Call libdevice function!
tl.store(y\_ptr + offsets, y, mask=mask)
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Triton} \PYG{n}{handles}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Finding libdevice.bc in the installation

\item {} 
\sphinxAtStartPar
Linking it during compilation

\item {} 
\sphinxAtStartPar
Type matching (float vs double)

\end{itemize}


\subsubsection{The Custom Path Way}
\label{\detokenize{gpu-tutorials/07-extern-functions:the-custom-path-way}}
\sphinxAtStartPar
Specify library path explicitly:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import triton
from pathlib import Path


\section{Find libdevice in Triton installation}
\label{\detokenize{gpu-tutorials/07-extern-functions:find-libdevice-in-triton-installation}}
\sphinxAtStartPar
triton\_dir = Path(triton.**file**).parent
libdir = triton\_dir / ‘backends/nvidia/lib’
extern\_libs = \{‘libdevice’: str(libdir / ‘libdevice.10.bc’)\}


\section{Pass to kernel}
\label{\detokenize{gpu-tutorials/07-extern-functions:pass-to-kernel}}
\sphinxAtStartPar
asin\_kernel\textasciigrave{}grid \textless{}x, y, n\_elements, BLOCK\_SIZE=1024, extern\_libs=extern\_libs\textgreater{}\textasciigrave{}\_

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{When} \PYG{n}{to} \PYG{n}{use} \PYG{n}{custom} \PYG{n}{paths}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Testing different library versions

\item {} 
\sphinxAtStartPar
Using custom\sphinxhyphen{}built libraries

\item {} 
\sphinxAtStartPar
Debugging linking issues

\end{itemize}


\subsection{How It Works Under the Hood}
\label{\detokenize{gpu-tutorials/07-extern-functions:how-it-works-under-the-hood}}

\subsubsection{Compilation Process}
\label{\detokenize{gpu-tutorials/07-extern-functions:compilation-process}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Triton \sphinxhyphen{}\textgreater{} LLVM IR}: Your kernel is compiled to LLVM intermediate representation

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Link external libraries}: LLVM linker merges your code with libdevice.bc

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Optimize}: LLVM optimizes the combined code

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Generate PTX} (NVIDIA) or \sphinxstylestrong{AMDGCN} (AMD): Backend\sphinxhyphen{}specific assembly

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{JIT to machine code}: Final GPU binary

\end{enumerate}


\subsubsection{Type Dispatch}
\label{\detokenize{gpu-tutorials/07-extern-functions:type-dispatch}}
\sphinxAtStartPar
Triton’s \sphinxcode{\sphinxupquote{libdevice.asin}} is a wrapper:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def asin(x):}\begin{description}
\sphinxlineitem{if x.dtype == tl.float32:}
\sphinxAtStartPar
return \_\_nv*asinf(x)  \# Single precision

\sphinxlineitem{elif x.dtype == tl.float64:}
\sphinxAtStartPar
return \_\_nv*asin(x)   \# Double precision

\end{description}

\sphinxAtStartPar
\# … etc for other types

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Benefit**: You write ``libdevice.asin(x)``, Triton picks the right implementation!
\end{sphinxVerbatim}


\subsubsection{Calling Convention}
\label{\detokenize{gpu-tutorials/07-extern-functions:calling-convention}}
\sphinxAtStartPar
Libdevice functions use standard LLVM calling conventions:
\sphinxhyphen{} Arguments in registers or stack
\sphinxhyphen{} Return value in register
\sphinxhyphen{} Triton generates correct call sequences automatically


\subsection{Available libdevice Functions in Triton}
\label{\detokenize{gpu-tutorials/07-extern-functions:available-libdevice-functions-in-triton}}

\subsubsection{Triton’s libdevice Wrapper}
\label{\detokenize{gpu-tutorials/07-extern-functions:triton-s-libdevice-wrapper}}
\sphinxAtStartPar
Located in \sphinxcode{\sphinxupquote{triton/language/extra/libdevice.py}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from triton.language.extra import libdevice


\section{Trigonometric}
\label{\detokenize{gpu-tutorials/07-extern-functions:trigonometric}}
\sphinxAtStartPar
libdevice.sin(x)
libdevice.cos(x)
libdevice.tan(x)
libdevice.asin(x)   \# Arc sine (our example!)
libdevice.acos(x)   \# Arc cosine
libdevice.atan(x)   \# Arc tangent


\section{Hyperbolic}
\label{\detokenize{gpu-tutorials/07-extern-functions:hyperbolic}}
\sphinxAtStartPar
libdevice.sinh(x)
libdevice.cosh(x)
libdevice.tanh(x)


\section{Exponential/Logarithmic}
\label{\detokenize{gpu-tutorials/07-extern-functions:exponential-logarithmic}}
\sphinxAtStartPar
libdevice.exp(x)
libdevice.log(x)
libdevice.pow(x, y)


\section{Special functions}
\label{\detokenize{gpu-tutorials/07-extern-functions:special-functions}}
\sphinxAtStartPar
libdevice.erf(x)     \# Error function
libdevice.erfc(x)    \# Complementary error function
libdevice.j0(x)      \# Bessel function J0
libdevice.j1(x)      \# Bessel function J1
libdevice.y0(x)      \# Bessel function Y0


\section{And many more!}
\label{\detokenize{gpu-tutorials/07-extern-functions:and-many-more}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Tip**: Look at ``triton/language/extra/libdevice.py`` in the Triton source for full list.
\end{sphinxVerbatim}


\subsection{Example: Arc Sine (asin)}
\label{\detokenize{gpu-tutorials/07-extern-functions:example-arc-sine-asin}}

\subsubsection{The Math}
\label{\detokenize{gpu-tutorials/07-extern-functions:the-math}}
\sphinxAtStartPar
Arc sine is the inverse of sine:

\sphinxAtStartPar
If sin(y) = x, then asin(x) = y
Domain: {[}\sphinxhyphen{}1, 1{]}
Range: {[}\sphinxhyphen{}pi/2, pi/2{]}


\subsubsection{Why Use libdevice?}
\label{\detokenize{gpu-tutorials/07-extern-functions:why-use-libdevice}}
\sphinxAtStartPar
\sphinxstylestrong{Implementing asin yourself}:
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{def asin\_approx(x):}
\sphinxAtStartPar
\# Use Taylor series or other approximation
\# Need to handle edge cases (x near +/\sphinxhyphen{}1)
\# Accuracy vs performance trade\sphinxhyphen{}off
\# Lots of potential bugs!

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Using} \PYG{n}{libdevice}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
y = libdevice.asin(x)  \# Done! Optimized, accurate, tested


\subsubsection{Full Example}
\label{\detokenize{gpu-tutorials/07-extern-functions:full-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import torch
import triton
import triton.language as tl
from triton.language.extra import libdevice

\sphinxAtStartPar
@triton.jit
def asin\_kernel(x\_ptr, y\_ptr, n\_elements, BLOCK\_SIZE: tl.constexpr):
\begin{quote}

\sphinxAtStartPar
pid = tl.program\_id(axis=0)
block\_start = pid * BLOCK\_SIZE
offsets = block\_start + tl.arange(0, BLOCK\_SIZE)
mask = offsets \textless{} n\_elements

\sphinxAtStartPar
\# Load input
x = tl.load(x\_ptr + offsets, mask=mask)

\sphinxAtStartPar
\# Apply arc sine using libdevice
y = libdevice.asin(x)

\sphinxAtStartPar
\# Store result
tl.store(y\_ptr + offsets, y, mask=mask)
\end{quote}
\begin{description}
\sphinxlineitem{def asin\_triton(x):}
\sphinxAtStartPar
output = torch.empty\_like(x)
n\_elements = x.numel()
grid = lambda meta: (triton.cdiv(n\_elements, meta{[}‘BLOCK\_SIZE’{]}),)
asin\_kernel\textasciigrave{}grid \textless{}x, output, n\_elements, BLOCK\_SIZE=1024\textgreater{}\textasciigrave{}\_
return output

\end{description}


\section{Test}
\label{\detokenize{gpu-tutorials/07-extern-functions:test}}
\sphinxAtStartPar
x = torch.rand(100000, device=’cuda’)  \# Random values in {[}0, 1{]}
y\_torch = torch.asin(x)
y\_triton = asin\_triton(x)


\section{Should match!}
\label{\detokenize{gpu-tutorials/07-extern-functions:should-match}}
\sphinxAtStartPar
max\_diff = torch.max(torch.abs(y\_torch \sphinxhyphen{} y\_triton))
print(f’Max difference: \{max\_diff\}’)  \# Should be \textasciitilde{}1e\sphinxhyphen{}7 (float precision)


\subsection{Performance Considerations}
\label{\detokenize{gpu-tutorials/07-extern-functions:performance-considerations}}

\subsubsection{Libdevice Performance}
\label{\detokenize{gpu-tutorials/07-extern-functions:libdevice-performance}}
\sphinxAtStartPar
\sphinxstylestrong{Generally very good}:
\sphinxhyphen{} Hand\sphinxhyphen{}optimized by NVIDIA/AMD
\sphinxhyphen{} Uses GPU\sphinxhyphen{}specific instructions
\sphinxhyphen{} Often as fast as or faster than DIY implementations

\sphinxAtStartPar
\sphinxstylestrong{Example timings} (1M elements):
\sphinxhyphen{} \sphinxcode{\sphinxupquote{libdevice.asin}}: \textasciitilde{}0.05 ms
\sphinxhyphen{} Custom approximation: \textasciitilde{}0.03\sphinxhyphen{}0.10 ms (depends on accuracy)
\sphinxhyphen{} Benefit: Guaranteed correctness + good performance


\subsubsection{When to Use vs Triton Intrinsics}
\label{\detokenize{gpu-tutorials/07-extern-functions:when-to-use-vs-triton-intrinsics}}
\sphinxAtStartPar
\sphinxstylestrong{Use Triton intrinsics when available}:
.. code\sphinxhyphen{}block:: python

\sphinxAtStartPar
tl.exp(x)   \# Prefer this over libdevice.exp
tl.sin(x)   \# Prefer this over libdevice.sin

\sphinxAtStartPar
Triton intrinsics may have special optimizations.

\sphinxAtStartPar
\sphinxstylestrong{Use libdevice for}:
\sphinxhyphen{} Functions not in Triton (asin, bessel, erf, etc.)
\sphinxhyphen{} When you need specific precision guarantees
\sphinxhyphen{} Complex mathematical operations


\subsection{Linking Multiple Libraries (AMD Example)}
\label{\detokenize{gpu-tutorials/07-extern-functions:linking-multiple-libraries-amd-example}}
\sphinxAtStartPar
For AMD GPUs, need both ocml and ockl:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from pathlib import Path
import triton
\begin{description}
\sphinxlineitem{if is\_hip():}
\sphinxAtStartPar
triton\_dir = Path(triton.**file**).parent
libdir = triton\_dir / ‘backends/amd/lib’
\begin{description}
\sphinxlineitem{extern\_libs = \{}
\sphinxAtStartPar
‘ocml’: str(libdir / ‘ocml.bc’),
‘ockl’: str(libdir / ‘ockl.bc’)

\end{description}

\sphinxAtStartPar
\}

\sphinxAtStartPar
my\_kernel\textasciigrave{}grid \textless{}…, extern\_libs=extern\_libs\textgreater{}\textasciigrave{}\_

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why multiple libraries?**
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Different functions in different libraries

\item {} 
\sphinxAtStartPar
OCML: Math functions

\item {} 
\sphinxAtStartPar
OCKL: Kernel utilities (barriers, atomics, etc.)

\end{itemize}


\subsection{Debugging Linking Issues}
\label{\detokenize{gpu-tutorials/07-extern-functions:debugging-linking-issues}}

\subsubsection{Common Errors}
\label{\detokenize{gpu-tutorials/07-extern-functions:common-errors}}
\sphinxAtStartPar
\sphinxstylestrong{1. Library Not Found}

\sphinxAtStartPar
FileNotFoundError: {[}Errno 2{]} No such file or directory: ‘/path/to/libdevice.10.bc’

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Solution}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Check} \PYG{n}{library} \PYG{n}{path}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
libdir = Path(triton.\_\_file**).parent / ‘backends/nvidia/lib’
print(libdir.exists())
print(list(libdir.glob(’{\color{red}\bfseries{}*}.bc’)))

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mf}{2.} \PYG{n}{Undefined} \PYG{n}{Symbol}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
Error: undefined reference to ‘{\color{red}\bfseries{}**}nv\_asin’

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Solution**: Make sure library is in ``extern\PYGZus{}libs`` dict
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{3. Type Mismatch}

\sphinxAtStartPar
Error: cannot convert ‘float’ to ‘double’

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Solution}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:} \PYG{n}{Ensure} \PYG{n+nb}{input} \PYG{n}{types} \PYG{n}{match} \PYG{n}{function} \PYG{n}{expectations}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
x = x.to(tl.float32)  \# Explicitly cast if needed


\subsubsection{Verifying Libraries}
\label{\detokenize{gpu-tutorials/07-extern-functions:verifying-libraries}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from pathlib import Path
import triton

\sphinxAtStartPar
triton\_dir = Path(triton.\_\_file**).parent


\section{NVIDIA}
\label{\detokenize{gpu-tutorials/07-extern-functions:nvidia}}
\sphinxAtStartPar
nvidia\_lib = triton\_dir / ‘backends/nvidia/lib/libdevice.10.bc’
print(f’NVIDIA libdevice: \{nvidia\_lib.exists()\}’)


\section{AMD}
\label{\detokenize{gpu-tutorials/07-extern-functions:amd}}
\sphinxAtStartPar
amd\_lib = triton\_dir / ‘backends/amd/lib/ocml.bc’
print(f’AMD ocml: \{amd\_lib.exists()\}’)


\subsection{Advanced Usage}
\label{\detokenize{gpu-tutorials/07-extern-functions:advanced-usage}}

\subsubsection{Calling Custom External Functions}
\label{\detokenize{gpu-tutorials/07-extern-functions:calling-custom-external-functions}}
\sphinxAtStartPar
You can link your own LLVM bitcode:

\sphinxAtStartPar
\sphinxstylestrong{1. Write CUDA device function}:
.. code\sphinxhyphen{}block:: cuda

\sphinxAtStartPar
// my\_func.cu
extern “C” \_\_device** float my\_special*func(float x) \{
\begin{quote}

\sphinxAtStartPar
return x * x + 2.0f * x + 1.0f;
\end{quote}


\paragraph{\}}
\label{\detokenize{gpu-tutorials/07-extern-functions:id5}}
\sphinxAtStartPar
\sphinxstylestrong{2. Compile to bitcode}:
.. code\sphinxhyphen{}block:: bash

\sphinxAtStartPar
clang++ \textendash{}cuda\sphinxhyphen{}device\sphinxhyphen{}only \sphinxhyphen{}emit\sphinxhyphen{}llvm \sphinxhyphen{}c my\_func.cu \sphinxhyphen{}o my\_func.bc

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mf}{3.} \PYG{n}{Link} \PYG{o+ow}{in} \PYG{n}{Triton}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
extern\_libs = \{‘my\_lib’: ‘/path/to/my\_func.bc’\}
my\_kernel\textasciigrave{}grid \textless{}…, extern\_libs=extern\_libs\textgreater{}\textasciigrave{}\_

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mf}{4.} \PYG{n}{Declare} \PYG{o+ow}{in} \PYG{n}{Triton}\PYG{o}{*}\PYG{o}{*}\PYG{p}{:}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def my\_kernel(…):
\begin{quote}

\sphinxAtStartPar
\# Declare external function
my\_special*func = tl.extern\_func(‘my\_special*func’, tl.float32, {[}tl.float32{]})

\sphinxAtStartPar
\# Use it
y = my\_special*func(x)
\end{quote}


\subsubsection{Mixing Multiple External Libraries}
\label{\detokenize{gpu-tutorials/07-extern-functions:mixing-multiple-external-libraries}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{extern\_libs = \{}
\sphinxAtStartPar
‘libdevice’: ‘/path/to/libdevice.10.bc’,
‘my\_lib’: ‘/path/to/my\_func.bc’,
‘another\_lib’: ‘/path/to/another.bc’

\end{description}

\sphinxAtStartPar
\}

\sphinxAtStartPar
my\_kernel\textasciigrave{}grid \textless{}…, extern\_libs=extern\_libs\textgreater{}\textasciigrave{}\_

\sphinxAtStartPar
All libraries are linked together during compilation.


\subsection{Portability Considerations}
\label{\detokenize{gpu-tutorials/07-extern-functions:portability-considerations}}

\subsubsection{NVIDIA vs AMD}
\label{\detokenize{gpu-tutorials/07-extern-functions:nvidia-vs-amd}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def get\_extern*libs():}
\sphinxAtStartPar
triton\_dir = Path(triton.**file**).parent
\begin{description}
\sphinxlineitem{if is\_cuda():}\begin{description}
\sphinxlineitem{return \{}
\sphinxAtStartPar
‘libdevice’: str(triton\_dir / ‘backends/nvidia/lib/libdevice.10.bc’)

\end{description}

\sphinxAtStartPar
\}

\sphinxlineitem{elif is\_hip():}
\sphinxAtStartPar
libdir = triton\_dir / ‘backends/amd/lib’
return \{
\begin{quote}

\sphinxAtStartPar
‘ocml’: str(libdir / ‘ocml.bc’),
‘ockl’: str(libdir / ‘ockl.bc’)
\end{quote}

\sphinxAtStartPar
\}

\sphinxlineitem{else:}
\sphinxAtStartPar
return \{\}

\end{description}

\end{description}

\sphinxAtStartPar
extern\_libs = get\_extern*libs()
kernel\textasciigrave{}grid \textless{}…, extern\_libs=extern\_libs\textgreater{}\textasciigrave{}\_


\subsubsection{Function Name Differences}
\label{\detokenize{gpu-tutorials/07-extern-functions:function-name-differences}}
\sphinxAtStartPar
Some functions have different names:
\sphinxhyphen{} NVIDIA: \sphinxcode{\sphinxupquote{\_\_nv*sinf}}
\sphinxhyphen{} AMD: \sphinxcode{\sphinxupquote{\_\_ocml*sin\_f32}}

\sphinxAtStartPar
\sphinxstylestrong{Triton’s libdevice wrapper handles this automatically!}


\subsection{Comparison to CUDA}
\label{\detokenize{gpu-tutorials/07-extern-functions:comparison-to-cuda}}

\subsubsection{In CUDA}
\label{\detokenize{gpu-tutorials/07-extern-functions:in-cuda}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
\#include \textless{}math\_functions.h\textgreater{}
\begin{description}
\sphinxlineitem{\_\_global** void asin\_kernel(float* x, float* y, int n) \{}
\sphinxAtStartPar
int i = blockIdx.x * blockDim.x + threadIdx.x;
if (i \textless{} n) \{
\begin{quote}

\sphinxAtStartPar
y{[}i{]} = asinf(x{[}i{]});  // Automatically linked
\end{quote}

\sphinxAtStartPar
\}

\end{description}


\paragraph{\}}
\label{\detokenize{gpu-tutorials/07-extern-functions:id6}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Include header

\item {} 
\sphinxAtStartPar
Use function

\item {} 
\sphinxAtStartPar
nvcc handles linking

\end{itemize}


\subsubsection{In Triton}
\label{\detokenize{gpu-tutorials/07-extern-functions:in-triton}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from triton.language.extra import libdevice

\sphinxAtStartPar
@triton.jit
def asin\_kernel(x\_ptr, y\_ptr, n\_elements, BLOCK\_SIZE: tl.constexpr):
\begin{quote}

\sphinxAtStartPar
\# …
y = libdevice.asin(x)  \# Import wrapper, use function
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Very similar experience!**
\end{sphinxVerbatim}


\subsection{Key Takeaways}
\label{\detokenize{gpu-tutorials/07-extern-functions:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{libdevice provides optimized math functions}: Beyond Triton’s built\sphinxhyphen{}ins

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Easy to use}: \sphinxcode{\sphinxupquote{from triton.language.extra import libdevice}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Automatic type dispatch}: Triton picks float vs double version

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Custom paths available}: For advanced use cases

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Good performance}: NVIDIA/AMD optimized implementations

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Portability}: Triton handles differences between backends

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Extensible}: Can link your own LLVM bitcode

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Essential for complex math}: Bessel functions, error functions, etc.

\end{enumerate}

\sphinxAtStartPar
External functions let you leverage decades of numerical optimization work while writing simple Triton code!

\sphinxstepscope


\section{Tutorial 08: Grouped GEMM}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:tutorial-08-grouped-gemm}}\label{\detokenize{gpu-tutorials/08-grouped-gemm::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:overview}}
\sphinxAtStartPar
\sphinxstylestrong{Grouped GEMM} (General Matrix Multiply) allows you to compute multiple independent matrix multiplications in a single kernel launch. This is essential for applications like:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Mixture of Experts (MoE) models} \sphinxhyphen{} Different experts process different tokens

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Batched inference} \sphinxhyphen{} Multiple requests with different shapes

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sparse computations} \sphinxhyphen{} Non\sphinxhyphen{}uniform workload distribution

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}task learning} \sphinxhyphen{} Different tasks with different matrix sizes

\end{itemize}

\sphinxAtStartPar
Instead of launching separate kernels for each GEMM, grouped GEMM uses a \sphinxstylestrong{fixed number of CTAs (Cooperative Thread Arrays)} that statically schedule work across all problems.


\subsection{Key Concepts}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:key-concepts}}

\subsubsection{Static On\sphinxhyphen{}Device Scheduling}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:static-on-device-scheduling}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
NUM\_SM: tl.constexpr  \# Fixed number of streaming multiprocessors

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}} \PYG{n}{Launch} \PYG{n}{a} \PYG{n}{fixed} \PYG{n}{number} \PYG{n}{of} \PYG{n}{CTAs} \PYG{p}{(}\PYG{n}{typically} \PYG{n}{equal} \PYG{n}{to} \PYG{n}{GPU} \PYG{n}{SM} \PYG{n}{count}\PYG{p}{)}
\PYG{o}{\PYGZhy{}} \PYG{n}{Each} \PYG{n}{CTA} \PYG{n}{iterates} \PYG{n}{through} \PYG{n}{tiles} \PYG{n}{across} \PYG{o}{*}\PYG{o}{*}\PYG{n+nb}{all}\PYG{o}{*}\PYG{o}{*} \PYG{n}{GEMM} \PYG{n}{problems}
\PYG{o}{\PYGZhy{}} \PYG{n}{Scheduling} \PYG{o+ow}{is} \PYG{n}{done} \PYG{o}{*}\PYG{o}{*}\PYG{n}{on}\PYG{o}{\PYGZhy{}}\PYG{n}{device}\PYG{o}{*}\PYG{o}{*} \PYG{n}{at} \PYG{n}{runtime}
\PYG{o}{\PYGZhy{}} \PYG{n}{More} \PYG{n}{efficient} \PYG{n}{than} \PYG{n}{launching} \PYG{n}{multiple} \PYG{n}{separate} \PYG{n}{kernels}
\end{sphinxVerbatim}


\subsubsection{Group Problem Representation}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:group-problem-representation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
group\_a*ptrs  \# Device tensor of pointers to A matrices
group\_b*ptrs  \# Device tensor of pointers to B matrices
group\_c*ptrs  \# Device tensor of pointers to C matrices
group\_gemm*sizes  \# Shape {[}group\_size, 3{]} storing {[}M, N, K{]} for each GEMM
g\_lds  \# Leading dimensions {[}lda, ldb, ldc{]} for each GEMM

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why device tensors?**
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Kernels can’t directly access Python lists

\item {} 
\sphinxAtStartPar
All metadata must be in GPU memory

\item {} 
\sphinxAtStartPar
Allows dynamic problem lookup during execution

\end{itemize}


\subsection{Code Walkthrough}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:code-walkthrough}}

\subsubsection{1. Grouped GEMM Kernel (Basic Version)}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:grouped-gemm-kernel-basic-version}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def grouped\_matmul*kernel(
\begin{quote}

\sphinxAtStartPar
group\_a*ptrs, group\_b*ptrs, group\_c*ptrs,
group\_gemm*sizes, g\_lds, group\_size,
NUM\_SM: tl.constexpr,
BLOCK\_SIZE*M: tl.constexpr,
BLOCK\_SIZE*N: tl.constexpr,
BLOCK\_SIZE*K: tl.constexpr,
\end{quote}
\begin{description}
\sphinxlineitem{):}
\sphinxAtStartPar
tile\_idx = tl.program\_id(0)
last\_problem*end = 0

\sphinxAtStartPar
\# Iterate through all problems
for g in range(group\_size):
\begin{quote}

\sphinxAtStartPar
\# Load problem dimensions
gm = tl.load(group\_gemm*sizes + g * 3)      \# M dimension
gn = tl.load(group\_gemm*sizes + g * 3 + 1)  \# N dimension
gk = tl.load(group\_gemm*sizes + g * 3 + 2)  \# K dimension

\sphinxAtStartPar
num\_m*tiles = tl.cdiv(gm, BLOCK\_SIZE*M)
num\_n*tiles = tl.cdiv(gn, BLOCK\_SIZE*N)
num\_tiles = num\_m*tiles * num\_n*tiles

\sphinxAtStartPar
\# Process tiles belonging to current problem
while (tile\_idx \textgreater{}= last\_problem*end and
\begin{quote}
\begin{quote}

\sphinxAtStartPar
tile\_idx \textless{} last\_problem*end + num\_tiles):
\end{quote}

\sphinxAtStartPar
\# Get matrix pointers for this problem
a\_ptr = tl.load(group\_a*ptrs + g).to(tl.pointer\_type(tl.float16))
b\_ptr = tl.load(group\_b*ptrs + g).to(tl.pointer\_type(tl.float16))
c\_ptr = tl.load(group\_c*ptrs + g).to(tl.pointer\_type(tl.float16))

\sphinxAtStartPar
\# Figure out tile coordinates within this problem
tile\_idx*in\_gemm = tile\_idx \sphinxhyphen{} last\_problem*end
tile\_m*idx = tile\_idx*in\_gemm // num\_n*tiles
tile\_n*idx = tile\_idx*in\_gemm \% num\_n*tiles

\sphinxAtStartPar
\# Standard matmul computation for this tile
\# … (compute accumulator) …

\sphinxAtStartPar
\# Jump to next tile assigned to this CTA
tile\_idx += NUM\_SM
\end{quote}

\sphinxAtStartPar
\# Move to next problem
last\_problem*end = last\_problem*end + num\_tiles
\end{quote}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Scheduling} \PYG{n}{Logic}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Each CTA starts with \sphinxcode{\sphinxupquote{tile\_idx = program\_id(0)}} (0, 1, 2, …, NUM\_SM\sphinxhyphen{}1)

\item {} 
\sphinxAtStartPar
Process tiles at indices: \sphinxcode{\sphinxupquote{tile\_idx}}, \sphinxcode{\sphinxupquote{tile\_idx + NUM\_SM}}, \sphinxcode{\sphinxupquote{tile\_idx + 2*NUM\_SM}}, …

\item {} 
\sphinxAtStartPar
This distributes work evenly across all SMs

\end{enumerate}


\subsubsection{2. TMA (Tensor Memory Accelerator) Version}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:tma-tensor-memory-accelerator-version}}
\sphinxAtStartPar
For GPUs with compute capability \textgreater{}= 9.0 (Hopper and beyond):

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def grouped\_matmul*tma\_kernel(…):
\begin{quote}

\sphinxAtStartPar
\# Create TMA descriptors for each problem
a\_desc = tl.make\_tensor*descriptor(
\begin{quote}

\sphinxAtStartPar
a\_ptr,
shape={[}gm, gk{]},
strides={[}lda, 1{]},
block\_shape={[}BLOCK\_SIZE*M, BLOCK\_SIZE*K{]},
\end{quote}

\sphinxAtStartPar
)

\sphinxAtStartPar
\# Load using TMA
a = a\_desc.load({[}offs\_am, offs\_k{]})
b = b\_desc.load({[}offs\_bn, offs\_k{]})
accumulator = tl.dot(a, b.T, accumulator)
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{TMA} \PYG{n}{Benefits}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Hardware\sphinxhyphen{}accelerated memory transfers

\item {} 
\sphinxAtStartPar
Better memory coalescing

\item {} 
\sphinxAtStartPar
Reduced register pressure

\item {} 
\sphinxAtStartPar
Automatic boundary handling

\end{itemize}


\subsubsection{3. Host Function Setup}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:host-function-setup}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def group\_gemm*fn(group\_A, group\_B):}
\sphinxAtStartPar
group\_size = len(group\_A)

\sphinxAtStartPar
\# Collect matrix metadata
A\_addrs = {[}{]}
B\_addrs = {[}{]}
C\_addrs = {[}{]}
g\_sizes = {[}{]}
g\_lds = {[}{]}
\begin{description}
\sphinxlineitem{for i in range(group\_size):}
\sphinxAtStartPar
A = group\_A{[}i{]}
B = group\_B{[}i{]}
M, K = A.shape
K, N = B.shape
C = torch.empty((M, N), device=device, dtype=A.dtype)

\sphinxAtStartPar
\# Store pointers as Python integers
A\_addrs.append(A.data\_ptr())
B\_addrs.append(B.data\_ptr())
C\_addrs.append(C.data\_ptr())

\sphinxAtStartPar
\# Store dimensions and strides
g\_sizes += {[}M, N, K{]}
g\_lds += {[}A.stride(0), B.stride(0), C.stride(0){]}

\end{description}

\sphinxAtStartPar
\# Convert to device tensors
d\_a*ptrs = torch.tensor(A\_addrs, device=device)
d\_b*ptrs = torch.tensor(B\_addrs, device=device)
d\_c*ptrs = torch.tensor(C\_addrs, device=device)
d\_g*sizes = torch.tensor(g\_sizes, dtype=torch.int32, device=device)
d\_g*lds = torch.tensor(g\_lds, dtype=torch.int32, device=device)

\sphinxAtStartPar
\# Fixed grid size
grid = lambda META: (META{[}‘NUM\_SM’{]}, )
grouped\_matmul*kernel\textasciigrave{}grid \textless{}
\begin{quote}

\sphinxAtStartPar
d\_a*ptrs, d\_b*ptrs, d\_c*ptrs,
d\_g*sizes, d\_g*lds, group\_size,
\end{quote}

\sphinxAtStartPar
\textgreater{}\textasciigrave{}\_

\sphinxAtStartPar
return group\_C

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{Points}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Grid size is \sphinxstylestrong{fixed} (NUM\_SM CTAs), not dependent on problem size

\item {} 
\sphinxAtStartPar
All metadata lives in GPU memory

\item {} 
\sphinxAtStartPar
Python list of output matrices returned

\end{itemize}


\subsection{Auto\sphinxhyphen{}tuning Configurations}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:auto-tuning-configurations}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{@triton.autotune(}\begin{description}
\sphinxlineitem{configs={[}}\begin{description}
\sphinxlineitem{triton.Config(\{‘BLOCK\_SIZE*M’: 128, ‘BLOCK\_SIZE*N’: 128,}
\sphinxAtStartPar
‘BLOCK\_SIZE*K’: 32, ‘NUM\_SM’: 84\}),

\sphinxlineitem{triton.Config(\{‘BLOCK\_SIZE*M’: 64, ‘BLOCK\_SIZE*N’: 64,}
\sphinxAtStartPar
‘BLOCK\_SIZE*K’: 32, ‘NUM\_SM’: 128\}),

\sphinxlineitem{triton.Config(\{‘BLOCK\_SIZE*M’: 128, ‘BLOCK\_SIZE*N’: 128,}
\sphinxAtStartPar
‘BLOCK\_SIZE*K’: 64, ‘NUM\_SM’: num\_sms()\}),

\end{description}

\end{description}

\sphinxAtStartPar
{]},
key={[}‘group\_size’{]},

\end{description}

\sphinxAtStartPar
\sphinxstylestrong{Configuration Choices:}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{NUM\_SM}}: Number of CTAs to launch (84, 128, or actual SM count)
\sphinxhyphen{} Block sizes: Smaller blocks = better load balancing, larger = better throughput
\sphinxhyphen{} Auto\sphinxhyphen{}tuning finds optimal configuration for your GPU


\subsection{Memory Layout Considerations}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:memory-layout-considerations}}

\subsubsection{Pointer Indirection}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:pointer-indirection}}\begin{description}
\sphinxlineitem{group\_a*ptrs (on GPU) \sphinxhyphen{}\textgreater{} {[}ptr0, ptr1, ptr2, …{]} \sphinxhyphen{}\textgreater{} A matrices (on GPU)}\begin{quote}

\sphinxAtStartPar
down     down     down
\end{quote}

\sphinxAtStartPar
A0    A1    A2

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}} \PYG{n}{Two}\PYG{o}{\PYGZhy{}}\PYG{n}{level} \PYG{n}{indirection}\PYG{p}{:} \PYG{n}{array} \PYG{n}{of} \PYG{n}{pointers}\PYG{p}{,} \PYG{n}{then} \PYG{n}{actual} \PYG{n}{data}
\PYG{o}{\PYGZhy{}} \PYG{n}{Overhead} \PYG{o+ow}{is} \PYG{n}{minimal} \PYG{k}{for} \PYG{n}{large} \PYG{n}{matrices}
\PYG{o}{\PYGZhy{}} \PYG{n}{Allows} \PYG{n}{arbitrary} \PYG{n}{matrix} \PYG{n}{shapes} \PYG{o+ow}{and} \PYG{n}{layouts}
\end{sphinxVerbatim}


\subsubsection{Leading Dimension (Stride)}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:leading-dimension-stride}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{For row\sphinxhyphen{}major matrix A{[}M, K{]}:}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:for-row-major-matrix-a-m-k}}
\sphinxAtStartPar
lda = A.stride(0)  \# Usually equals K for contiguous tensors


\section{Element at (i, j) is at: base\_ptr + i * lda + j}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:element-at-i-j-is-at-base-ptr-i-lda-j}}

\subsection{Performance Characteristics}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:performance-characteristics}}

\subsubsection{When Grouped GEMM Wins}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:when-grouped-gemm-wins}}
\sphinxAtStartPar
{[}{[}OK{]}{]} \sphinxstylestrong{Good cases:}
\sphinxhyphen{} Multiple small\sphinxhyphen{}to\sphinxhyphen{}medium GEMMs (hundreds to thousands of elements)
\sphinxhyphen{} Variable problem sizes (not batched)
\sphinxhyphen{} GPU utilization is low with separate launches
\sphinxhyphen{} Total compute justifies kernel overhead


\subsubsection{When to Use Separate Kernels}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:when-to-use-separate-kernels}}
\sphinxAtStartPar
{[}{[}FAIL{]}{]} \sphinxstylestrong{Bad cases:}
\sphinxhyphen{} Very few problems (\textless{} 10) with large matrices
\sphinxhyphen{} All matrices are the same size (use batched GEMM instead)
\sphinxhyphen{} Problems are too small (\textless{} 64x64)


\subsubsection{Benchmarking Results}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:benchmarking-results}}
\sphinxAtStartPar
From the script:
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{@triton.testing.perf\_report(}
\sphinxAtStartPar
x\_names={[}‘N’{]},
x\_vals={[}2**i for i in range(7, 11){]},  \# 128 to 1024
line\_vals={[}‘cublas’, ‘triton’, ‘triton\sphinxhyphen{}tma’{]},

\end{description}


\paragraph{)}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:id1}}
\sphinxAtStartPar
\sphinxstylestrong{Typical results:}
\sphinxhyphen{} Grouped GEMM competitive with cuBLAS for moderate sizes
\sphinxhyphen{} TMA version faster on Hopper (compute capability 9.0+)
\sphinxhyphen{} Overhead becomes negligible for N \textgreater{}= 256


\subsection{GPU Architecture Insights}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:gpu-architecture-insights}}

\subsubsection{Why Fixed Number of CTAs?}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:why-fixed-number-of-ctas}}
\sphinxAtStartPar
Traditional approach: Launch one CTA per tile
\sphinxhyphen{} Problem 1: 10 tiles \sphinxhyphen{}\textgreater{} 10 CTAs
\sphinxhyphen{} Problem 2: 20 tiles \sphinxhyphen{}\textgreater{} 20 CTAs
\sphinxhyphen{} Total: 30 kernel launches or 30 CTAs

\sphinxAtStartPar
Grouped GEMM: Launch NUM\_SM CTAs total
\sphinxhyphen{} 84 CTAs process all 30 tiles
\sphinxhyphen{} Single kernel launch
\sphinxhyphen{} Better SM utilization


\subsubsection{Work Distribution}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:work-distribution}}
\sphinxAtStartPar
SM 0: Tiles 0, 84, 168, …
SM 1: Tiles 1, 85, 169, …
SM 2: Tiles 2, 86, 170, …
…
SM 83: Tiles 83, 167, …

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}} \PYG{n}{Round}\PYG{o}{\PYGZhy{}}\PYG{n}{robin} \PYG{n}{distribution}
\PYG{o}{\PYGZhy{}} \PYG{n}{Automatic} \PYG{n}{load} \PYG{n}{balancing}
\PYG{o}{\PYGZhy{}} \PYG{n}{Handles} \PYG{n}{variable} \PYG{n}{problem} \PYG{n}{sizes} \PYG{n}{gracefully}
\end{sphinxVerbatim}


\subsection{Advanced: TMA Descriptor Creation}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:advanced-tma-descriptor-creation}}
\sphinxAtStartPar
For TMA version:
.. code\sphinxhyphen{}block:: python
\begin{description}
\sphinxlineitem{a\_desc = tl.make\_tensor*descriptor(}
\sphinxAtStartPar
a\_ptr,                    \# Base pointer
shape={[}gm, gk{]},          \# Logical tensor shape
strides={[}lda, 1{]},        \# Row\sphinxhyphen{}major stride
block\_shape={[}BM, BK{]},    \# Block to load

\end{description}

\sphinxAtStartPar
)


\section{Load a block starting at (offs\_am, offs\_k)}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:load-a-block-starting-at-offs-am-offs-k}}
\sphinxAtStartPar
a = a\_desc.load({[}offs\_am, offs\_k{]})

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Benefits} \PYG{n}{over} \PYG{n}{manual} \PYG{n}{loads}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Hardware manages bounds checking

\item {} 
\sphinxAtStartPar
Better memory coalescing

\item {} 
\sphinxAtStartPar
Reduced register usage

\item {} 
\sphinxAtStartPar
Simplified kernel code

\end{itemize}


\subsection{Common Pitfalls}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:common-pitfalls}}

\subsubsection{1. Forgetting Contiguity}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:forgetting-contiguity}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Bad: B might not be contiguous after transpose}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:bad-b-might-not-be-contiguous-after-transpose}}
\sphinxAtStartPar
B = B.T
grouped\_matmul*kernel\textasciigrave{}grid \textless{}…\textgreater{}\textasciigrave{}\_


\section{Good: Ensure contiguity}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:good-ensure-contiguity}}
\sphinxAtStartPar
B\_T = B.T.contiguous()

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Wrong: Assuming contiguous}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:wrong-assuming-contiguous}}
\sphinxAtStartPar
lda = K


\section{Correct: Use actual stride}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:correct-use-actual-stride}}
\sphinxAtStartPar
lda = A.stride(0)

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{FP8 requires compute capability \textgreater{}= 9.0}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:fp8-requires-compute-capability-9-0}}\begin{description}
\sphinxlineitem{if dtype == torch.float8\_e4m3fn:}
\sphinxAtStartPar
assert torch.cuda.get\_device*capability(){[}0{]} \textgreater{}= 9

\end{description}


\subsection{Practical Example}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:practical-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Define different problem sizes}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:define-different-problem-sizes}}
\sphinxAtStartPar
group\_m = {[}1024, 512, 256, 128{]}
group\_n = {[}1024, 512, 256, 128{]}
group\_k = {[}1024, 512, 256, 128{]}


\section{Create random matrices}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:create-random-matrices}}
\sphinxAtStartPar
group\_A = {[}{]}
group\_B = {[}{]}
for i in range(len(group\_m)):
\begin{quote}

\sphinxAtStartPar
M, N, K = group\_m{[}i{]}, group\_n{[}i{]}, group\_k{[}i{]}
A = torch.rand((M, K), device=’cuda’, dtype=torch.float16)
B = torch.rand((K, N), device=’cuda’, dtype=torch.float16)
group\_A.append(A)
group\_B.append(B)
\end{quote}


\section{Compute all GEMMs in one kernel launch}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:compute-all-gemms-in-one-kernel-launch}}
\sphinxAtStartPar
tri\_out = group\_gemm*fn(group\_A, group\_B)


\section{Verify against PyTorch}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:verify-against-pytorch}}
\sphinxAtStartPar
ref\_out = {[}torch.matmul(a, b) for a, b in zip(group\_A, group\_B){]}
for i in range(len(group\_m)):
\begin{quote}

\sphinxAtStartPar
assert torch.allclose(ref\_out{[}i{]}, tri\_out{[}i{]}, atol=1e\sphinxhyphen{}2)
\end{quote}


\subsection{Summary}
\label{\detokenize{gpu-tutorials/08-grouped-gemm:summary}}
\sphinxAtStartPar
\sphinxstylestrong{Grouped GEMM} is a powerful technique for computing multiple independent matrix multiplications efficiently:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Static scheduling} on device avoids multiple kernel launches

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fixed number of CTAs} improves SM utilization

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{TMA support} for Hopper+ GPUs provides additional speedup

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Auto\sphinxhyphen{}tuning} finds optimal block sizes

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Competitive with cuBLAS} for moderate problem sizes

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Use cases:}
\sphinxhyphen{} Mixture of Experts models
\sphinxhyphen{} Variable\sphinxhyphen{}size batched inference
\sphinxhyphen{} Sparse neural networks
\sphinxhyphen{} Multi\sphinxhyphen{}task learning

\sphinxAtStartPar
\sphinxstylestrong{Performance tips:}
\sphinxhyphen{} Use TMA version on Hopper+ GPUs
\sphinxhyphen{} Auto\sphinxhyphen{}tune for your specific problem sizes
\sphinxhyphen{} Ensure matrices are contiguous
\sphinxhyphen{} Profile to verify GPU utilization

\sphinxAtStartPar
This technique is essential for modern ML workloads where you need to process many different\sphinxhyphen{}sized operations efficiently!

\sphinxstepscope


\section{Tutorial 09: Persistent Matmul}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:tutorial-09-persistent-matmul}}\label{\detokenize{gpu-tutorials/09-persistent-matmul::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:overview}}
\sphinxAtStartPar
\sphinxstylestrong{Persistent kernels} are an advanced GPU programming technique where a fixed number of thread blocks stay resident on the GPU and process multiple tiles of work. This tutorial demonstrates several matmul implementations:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Naive matmul} \sphinxhyphen{} Standard one\sphinxhyphen{}tile\sphinxhyphen{}per\sphinxhyphen{}CTA approach

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Persistent matmul} \sphinxhyphen{} Fixed CTAs process multiple tiles

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{TMA matmul} \sphinxhyphen{} Using Tensor Memory Accelerator (Hopper+)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{TMA persistent} \sphinxhyphen{} Combining both techniques

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Warp specialization} \sphinxhyphen{} Different warps do different work (Blackwell+)

\end{itemize}

\sphinxAtStartPar
This tutorial also uses \sphinxstylestrong{Triton Proton profiler} for detailed performance analysis.


\subsection{Key Concepts}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:key-concepts}}

\subsubsection{Persistent Kernel Pattern}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:persistent-kernel-pattern}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Traditional kernel: One CTA per tile}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:traditional-kernel-one-cta-per-tile}}\begin{description}
\sphinxlineitem{for tile in tiles:}
\sphinxAtStartPar
launch\_kernel\textasciigrave{}1*CTA\_per*tile \textless{}tile\textgreater{}\textasciigrave{}\_

\end{description}


\section{Persistent kernel: NUM\_SMS CTAs process all tiles}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:persistent-kernel-num-sms-ctas-process-all-tiles}}
\sphinxAtStartPar
launch\_kernel\textasciigrave{}NUM\_SMS \textless{}all\_tiles\textgreater{}\textasciigrave{}\_
for tile in my\_assigned*tiles:
\begin{quote}

\sphinxAtStartPar
process(tile)
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Benefits}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reduced kernel launch overhead

\item {} 
\sphinxAtStartPar
Better SM utilization

\item {} 
\sphinxAtStartPar
Amortizes setup costs across multiple tiles

\item {} 
\sphinxAtStartPar
Enables more sophisticated scheduling

\end{itemize}

\sphinxAtStartPar
On Blackwell (compute capability 10.0+), different warps can be assigned different roles:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{for ki in tl.range(k\_tiles, warp\_specialize=True):}
\sphinxAtStartPar
\# Hardware scheduler can assign:
\# \sphinxhyphen{} Some warps to memory loads (producer)
\# \sphinxhyphen{} Other warps to compute (consumer)
a = a\_desc.load({[}offs\_am, offs\_k{]})
b = b\_desc.load({[}offs\_bn, offs\_k{]})
accumulator = tl.dot(a, b.T, accumulator)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Why} \PYG{n}{it} \PYG{n}{matters}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Overlaps memory and compute more effectively

\item {} 
\sphinxAtStartPar
Improves pipeline utilization

\item {} 
\sphinxAtStartPar
Hardware\sphinxhyphen{}managed producer\sphinxhyphen{}consumer pattern

\end{itemize}


\subsection{Code Walkthrough}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:code-walkthrough}}

\subsubsection{1. Naive Matmul (Baseline)}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:naive-matmul-baseline}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def matmul\_kernel(a\_ptr, b\_ptr, c\_ptr,
\begin{quote}
\begin{quote}

\sphinxAtStartPar
M, N, K,
stride\_am, stride\_ak,
stride\_bk, stride\_bn,
stride\_cm, stride\_cn,
BLOCK\_SIZE*M: tl.constexpr,
BLOCK\_SIZE*N: tl.constexpr,
BLOCK\_SIZE*K: tl.constexpr,
GROUP\_SIZE*M: tl.constexpr):
\end{quote}

\sphinxAtStartPar
pid = tl.program\_id(axis=0)

\sphinxAtStartPar
\# Swizzling for better L2 cache hit rate
num\_pid*m = tl.cdiv(M, BLOCK\_SIZE*M)
num\_pid*n = tl.cdiv(N, BLOCK\_SIZE*N)
num\_pid*in\_group = GROUP\_SIZE*M * num\_pid*n
group\_id = pid // num\_pid*in\_group
first\_pid*m = group\_id * GROUP\_SIZE*M
group\_size*m = min(num\_pid*m \sphinxhyphen{} first\_pid*m, GROUP\_SIZE*M)
pid\_m = first\_pid*m + (pid \% group\_size*m)
pid\_n = (pid \% num\_pid*in\_group) // group\_size*m

\sphinxAtStartPar
\# Standard blocked matmul
accumulator = tl.zeros((BLOCK\_SIZE*M, BLOCK\_SIZE*N), dtype=tl.float32)
for k in range(0, tl.cdiv(K, BLOCK\_SIZE*K)):
\begin{quote}

\sphinxAtStartPar
a = tl.load(a\_ptrs, mask=…)
b = tl.load(b\_ptrs, mask=…)
accumulator = tl.dot(a, b, accumulator)
a\_ptrs += BLOCK\_SIZE*K * stride\_ak
b\_ptrs += BLOCK\_SIZE*K * stride\_bk
\end{quote}

\sphinxAtStartPar
c = accumulator.to(tl.float16)
tl.store(c\_ptrs, c, mask=c\_mask)
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Grid} \PYG{n}{launch}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{grid = lambda META: (}
\sphinxAtStartPar
triton.cdiv(M, META{[}“BLOCK\_SIZE*M”{]}) *
triton.cdiv(N, META{[}“BLOCK\_SIZE*N”{]}),

\end{description}


\paragraph{)}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:id1}}\begin{itemize}
\item {} 
\sphinxAtStartPar
One CTA per output tile

\item {} 
\sphinxAtStartPar
Simple but has kernel launch overhead

\end{itemize}


\subsubsection{2. Persistent Matmul}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:persistent-matmul}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def matmul\_kernel*persistent(a\_ptr, b\_ptr, c\_ptr, …,
\begin{quote}
\begin{quote}

\sphinxAtStartPar
NUM\_SMS: tl.constexpr):
\end{quote}

\sphinxAtStartPar
start\_pid = tl.program\_id(axis=0)
num\_pid*m = tl.cdiv(M, BLOCK\_SIZE*M)
num\_pid*n = tl.cdiv(N, BLOCK\_SIZE*N)
num\_tiles = num\_pid*m * num\_pid*n

\sphinxAtStartPar
\# Loop over tiles assigned to this CTA
for tile\_id in tl.range(start\_pid, num\_tiles, NUM\_SMS, flatten=True):
\begin{quote}

\sphinxAtStartPar
\# Compute which tile this is
pid\_m, pid\_n = {\color{red}\bfseries{}*}compute\_pid(tile\_id, …)

\sphinxAtStartPar
\# Load and compute for this tile
accumulator = tl.zeros((BLOCK\_SIZE*M, BLOCK\_SIZE*N), dtype=tl.float32)
for ki in range(k\_tiles):
\begin{quote}

\sphinxAtStartPar
a = tl.load(…)
b = tl.load(…)
accumulator = tl.dot(a, b, accumulator)
\end{quote}

\sphinxAtStartPar
\# Store result
c = accumulator.to(output\_dtype)
tl.store(c\_ptrs, c, mask=c\_mask)
\end{quote}
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Grid} \PYG{n}{launch}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
NUM\_SMS = torch.cuda.get\_device*properties(“cuda”).multi\_processor*count
grid = lambda META: (
\begin{quote}

\sphinxAtStartPar
min(NUM\_SMS, num\_tiles),  \# Don’t launch more CTAs than tiles
\end{quote}


\paragraph{)}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:id4}}
\sphinxAtStartPar
\sphinxstylestrong{Key differences:}
\sphinxhyphen{} Fixed number of CTAs (NUM\_SMS)
\sphinxhyphen{} Each CTA processes \sphinxcode{\sphinxupquote{ceil(num\_tiles / NUM\_SMS)}} tiles
\sphinxhyphen{} Single kernel launch for entire matmul


\subsubsection{3. TMA Matmul}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:tma-matmul}}
\sphinxAtStartPar
For Hopper (compute capability 9.0+):

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def matmul\_kernel*tma(a\_desc, b\_desc, c\_desc,
\begin{quote}
\begin{quote}

\sphinxAtStartPar
M, N, K,
BLOCK\_SIZE*M: tl.constexpr,
BLOCK\_SIZE*N: tl.constexpr,
BLOCK\_SIZE*K: tl.constexpr,
FP8\_OUTPUT: tl.constexpr,
WARP\_SPECIALIZE: tl.constexpr):
\end{quote}

\sphinxAtStartPar
dtype = tl.float8e4nv if FP8\_OUTPUT else tl.float16
pid = tl.program\_id(axis=0)
\# … compute pid\_m, pid\_n …

\sphinxAtStartPar
k\_tiles = tl.cdiv(K, BLOCK\_SIZE*K)
accumulator = tl.zeros((BLOCK\_SIZE*M, BLOCK\_SIZE*N), dtype=tl.float32)

\sphinxAtStartPar
\# Warp specialization for better overlap
for k in tl.range(k\_tiles, warp\_specialize=WARP\_SPECIALIZE):
\begin{quote}

\sphinxAtStartPar
offs\_k = k * BLOCK\_SIZE*K
\# TMA loads
a = a\_desc.load({[}offs\_am, offs\_k{]})
b = b\_desc.load({[}offs\_bn, offs\_k{]})
accumulator = tl.dot(a, b.T, accumulator)
\end{quote}

\sphinxAtStartPar
c = accumulator.to(dtype)
c\_desc.store({[}offs\_cm, offs\_cn{]}, c)
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{TMA} \PYG{n}{Descriptor} \PYG{n}{creation}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from triton.tools.tensor\_descriptor import TensorDescriptor

\sphinxAtStartPar
a\_desc = TensorDescriptor.from\_tensor(a, {[}BLOCK\_M, BLOCK\_K{]})
b\_desc = TensorDescriptor.from\_tensor(b, {[}BLOCK\_N, BLOCK\_K{]})
c\_desc = TensorDescriptor.from\_tensor(c, {[}BLOCK\_M, BLOCK\_N{]})


\subsubsection{4. TMA Persistent with Epilogue Subtiling}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:tma-persistent-with-epilogue-subtiling}}
\sphinxAtStartPar
Advanced optimization for memory\sphinxhyphen{}bound kernels:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def matmul\_kernel*tma\_persistent(…,
\begin{quote}
\begin{quote}

\sphinxAtStartPar
EPILOGUE\_SUBTILE: tl.constexpr):
\end{quote}

\sphinxAtStartPar
\# … main computation …
\begin{description}
\sphinxlineitem{if EPILOGUE\_SUBTILE:}
\sphinxAtStartPar
\# Split BLOCK\_M x BLOCK\_N into 2 BLOCK\_M x (BLOCK\_N//2) chunks
acc = tl.reshape(accumulator, (BLOCK\_M, 2, BLOCK\_N // 2))
acc = tl.permute(acc, (0, 2, 1))
acc0, acc1 = tl.split(acc)

\sphinxAtStartPar
\# Store in two parts
c0 = acc0.to(dtype)
c\_desc.store({[}offs\_cm, offs\_cn{]}, c0)

\sphinxAtStartPar
c1 = acc1.to(dtype)
c\_desc.store({[}offs\_cm, offs\_cn + BLOCK\_N // 2{]}, c1)

\sphinxlineitem{else:}
\sphinxAtStartPar
accumulator = accumulator.to(dtype)
c\_desc.store({[}offs\_cm, offs\_cn{]}, accumulator)

\end{description}
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why epilogue subtiling?**
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reduces shared memory usage in epilogue

\item {} 
\sphinxAtStartPar
Frees SRAM for more pipeline stages

\item {} 
\sphinxAtStartPar
Improves register utilization

\item {} 
\sphinxAtStartPar
Can increase overall throughput

\end{itemize}


\subsection{Proton Profiler Integration}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:proton-profiler-integration}}
\sphinxAtStartPar
This tutorial demonstrates using Triton’s built\sphinxhyphen{}in profiler:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
import triton.profiler as proton


\section{Start profiling}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:start-profiling}}
\sphinxAtStartPar
proton.start(“matmul”, hook=”triton”)
proton.deactivate()  \# Don’t profile initialization


\section{Run benchmarks}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:run-benchmarks}}\begin{description}
\sphinxlineitem{for K in range(K\_min, K\_max, K\_step):}
\sphinxAtStartPar
proton.activate(0)
for * in range(reps):
\begin{quote}

\sphinxAtStartPar
matmul(a, b)
\end{quote}

\sphinxAtStartPar
proton.deactivate(0)

\end{description}


\section{Finalize and show results}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:finalize-and-show-results}}
\sphinxAtStartPar
proton.finalize()
show\_profile(“fp16”, “matmul”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Metrics} \PYG{n}{collected}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Time per kernel (ms)

\item {} 
\sphinxAtStartPar
TFLOPS (teraflops per second)

\item {} 
\sphinxAtStartPar
Memory bandwidth utilization

\item {} 
\sphinxAtStartPar
Kernel launch counts

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def show\_profile(precision, profile\_name):}
\sphinxAtStartPar
import triton.profiler.viewer as proton\_viewer

\sphinxAtStartPar
metric\_names = {[}“time/ms”{]}
if precision == ‘fp8’:
\begin{quote}

\sphinxAtStartPar
metric\_names = {[}“tflop8/s”{]} + metric\_names
\end{quote}
\begin{description}
\sphinxlineitem{elif precision == ‘fp16’:}
\sphinxAtStartPar
metric\_names = {[}“tflop16/s”{]} + metric\_names

\end{description}

\sphinxAtStartPar
file\_name = f”\{profile\_name\}.hatchet”
tree, metrics = proton\_viewer.parse(metric\_names, file\_name)
proton\_viewer.print\_tree(tree, metrics)

\end{description}

\sphinxAtStartPar
Output shows hierarchical breakdown:

\sphinxAtStartPar
matmul
{\color{red}\bfseries{}|}\sphinxhyphen{} naive {[}M=8192, N=8192, K=512{]}      1.23 ms, 220 tflop16/s
{\color{red}\bfseries{}|}\sphinxhyphen{} persistent {[}M=8192, N=8192, K=512{]} 1.15 ms, 235 tflop16/s
{\color{red}\bfseries{}|}\sphinxhyphen{} tma {[}M=8192, N=8192, K=512{]}        1.05 ms, 257 tflop16/s
+\sphinxhyphen{} tma\_persistent {[}…{]}               0.98 ms, 276 tflop16/s


\subsection{Device\sphinxhyphen{}side vs Host\sphinxhyphen{}side Descriptors}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:device-side-vs-host-side-descriptors}}

\subsubsection{Host\sphinxhyphen{}side (TensorDescriptor)}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:host-side-tensordescriptor}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from triton.tools.tensor\_descriptor import TensorDescriptor


\section{Created on CPU, passed to GPU}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:created-on-cpu-passed-to-gpu}}
\sphinxAtStartPar
a\_desc = TensorDescriptor.from\_tensor(a, {[}BLOCK\_M, BLOCK\_K{]})
matmul\_kernel*tma\textasciigrave{}grid \textless{}a\_desc, b\_desc, c\_desc, …\textgreater{}\textasciigrave{}\_

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Pros}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*} \PYG{n}{Simpler}\PYG{p}{,} \PYG{n}{works} \PYG{n}{on} \PYG{n}{Hopper}
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Cons}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*} \PYG{n}{Descriptor} \PYG{n}{creation} \PYG{n}{overhead}\PYG{p}{,} \PYG{n}{limited} \PYG{n}{flexibility}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def kernel(a\_ptr, …):
\begin{quote}

\sphinxAtStartPar
\# Created inside kernel
a\_desc = tl.make\_tensor*descriptor(
\begin{quote}

\sphinxAtStartPar
a\_ptr,
shape={[}M, K{]},
strides={[}K, 1{]},
block\_shape={[}BLOCK\_M, BLOCK\_K{]},
\end{quote}

\sphinxAtStartPar
)
a = a\_desc.load({[}offs\_am, offs\_k{]})
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Pros}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*} \PYG{n}{No} \PYG{n}{host} \PYG{n}{overhead}\PYG{p}{,} \PYG{n}{required} \PYG{k}{for} \PYG{n}{Blackwell} \PYG{n}{warp} \PYG{n}{spec}
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Cons}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*} \PYG{n}{Only} \PYG{n}{available} \PYG{n}{on} \PYG{n}{newer} \PYG{n}{GPUs}
\end{sphinxVerbatim}


\subsection{Warp Specialization Details}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:warp-specialization-details}}

\subsubsection{How It Works}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:how-it-works}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{for ki in tl.range(k\_tiles, warp\_specialize=True):}
\sphinxAtStartPar
\# Hardware may assign:
\# Warps 0\sphinxhyphen{}3: Producer (memory loads)
\# Warps 4\sphinxhyphen{}7: Consumer (compute)
a = a\_desc.load({[}offs\_am, offs\_k{]})  \# Producer
b = b\_desc.load({[}offs\_bn, offs\_k{]})  \# Producer
acc = tl.dot(a, b.T, acc)          \# Consumer

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Without} \PYG{n}{warp} \PYG{n}{specialization}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
All warps do: load A \sphinxhyphen{}\textgreater{} load B \sphinxhyphen{}\textgreater{} compute \sphinxhyphen{}\textgreater{} repeat

\item {} 
\sphinxAtStartPar
Memory and compute are serialized

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{With warp specialization:}
\sphinxhyphen{} Producer warps continuously load data
\sphinxhyphen{} Consumer warps continuously compute
\sphinxhyphen{} Better overlap, higher throughput


\subsubsection{Requirements}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:requirements}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
HAS\_WARP*SPECIALIZE = supports\_ws() and HAS\_TENSOR*DESC
\begin{description}
\sphinxlineitem{def supports\_ws():}
\sphinxAtStartPar
return is\_cuda() and torch.cuda.get\_device*capability(){[}0{]} \textgreater{}= 9

\end{description}


\section{On Hopper: Software pipelining}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:on-hopper-software-pipelining}}

\section{On Blackwell: Hardware warp specialization}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:on-blackwell-hardware-warp-specialization}}

\subsection{Flattening in Persistent Loops}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:flattening-in-persistent-loops}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{for tile\_id in tl.range(start\_pid, num\_tiles, NUM\_SMS,}
\sphinxAtStartPar
flatten=True,
warp\_specialize=WARP\_SPECIALIZE):

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**``flatten=True``:**
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Removes loop\sphinxhyphen{}carried dependencies

\item {} 
\sphinxAtStartPar
Allows better scheduling

\item {} 
\sphinxAtStartPar
Required for software pipelining on Hopper

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}flatten=False\textasciigrave{}\textasciigrave{}:}
\sphinxhyphen{} Keeps loop structure
\sphinxhyphen{} Required for Blackwell hardware warp specialization
\sphinxhyphen{} Better for async warp scheduling

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Choose based on GPU generation}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:choose-based-on-gpu-generation}}
\sphinxAtStartPar
flatten = False if (warp\_specialize and is\_hopper()) else True


\subsection{Performance Comparison}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:performance-comparison}}

\subsubsection{Expected speedups (relative to naive):}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:expected-speedups-relative-to-naive}}
\begin{DUlineblock}{0em}
\item[] Variant | FP16 Speedup | FP8 Speedup | Notes |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}————\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——\sphinxhyphen{}|
| Persistent | 1.05\sphinxhyphen{}1.15x | 1.05\sphinxhyphen{}1.1x | Saves launch overhead |
| TMA | 1.15\sphinxhyphen{}1.25x | 1.2\sphinxhyphen{}1.3x | Better memory access |
| TMA Persistent | 1.2\sphinxhyphen{}1.35x | 1.25\sphinxhyphen{}1.4x | Combined benefits |
| + Warp Spec (Blackwell) | 1.3\sphinxhyphen{}1.5x | 1.4\sphinxhyphen{}1.6x | Hardware overlap |


\subsubsection{When Each Variant Wins}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:when-each-variant-wins}}
\sphinxAtStartPar
\sphinxstylestrong{Naive:}
\sphinxhyphen{} Small matrices (\textless{} 1024x1024)
\sphinxhyphen{} Single matmul
\sphinxhyphen{} Minimal kernel launch overhead needed

\sphinxAtStartPar
\sphinxstylestrong{Persistent:}
\sphinxhyphen{} Medium to large matrices
\sphinxhyphen{} Amortizes setup cost
\sphinxhyphen{} Many tiles to process

\sphinxAtStartPar
\sphinxstylestrong{TMA:}
\sphinxhyphen{} Memory\sphinxhyphen{}bound workloads
\sphinxhyphen{} Hopper+ GPUs
\sphinxhyphen{} Complex memory access patterns

\sphinxAtStartPar
\sphinxstylestrong{TMA Persistent + Warp Spec:}
\sphinxhyphen{} Large matrices (\textgreater{}= 4096x4096)
\sphinxhyphen{} Blackwell GPUs
\sphinxhyphen{} Maximum performance needed


\subsection{Precision Support}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:precision-support}}

\subsubsection{FP16 (Float16)}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:fp16-float16}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
a = torch.randn((M, K), device=’cuda’, dtype=torch.float16)
b = torch.randn((K, N), device=’cuda’, dtype=torch.float16)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}} \PYG{n}{Widely} \PYG{n}{supported}
\PYG{o}{\PYGZhy{}} \PYG{n}{Good} \PYG{n}{balance} \PYG{n}{of} \PYG{n+nb}{range} \PYG{o+ow}{and} \PYG{n}{precision}
\PYG{o}{\PYGZhy{}} \PYG{n}{Standard} \PYG{k}{for} \PYG{n}{training}
\end{sphinxVerbatim}


\subsubsection{FP8 (Float8)}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:fp8-float8}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
dtype = torch.float8\_e4m3fn  \# E4M3 format
a = torch.randn((M, K), dtype=torch.float16).to(dtype)
b = torch.randn((K, N), dtype=torch.float16).to(dtype)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}} \PYG{n}{Requires} \PYG{n}{compute} \PYG{n}{capability} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mf}{9.0}
\PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{2}\PYG{n}{x} \PYG{n}{speedup} \PYG{n}{potential}
\PYG{o}{\PYGZhy{}} \PYG{n}{Lower} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{faster} \PYG{n}{compute}
\PYG{o}{\PYGZhy{}} \PYG{n}{Ideal} \PYG{k}{for} \PYG{n}{inference}
\end{sphinxVerbatim}


\subsection{Auto\sphinxhyphen{}tuning Configuration}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:auto-tuning-configuration}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def matmul\_get*configs():}\begin{description}
\sphinxlineitem{return {[}}\begin{description}
\sphinxlineitem{triton.Config(}\begin{description}
\sphinxlineitem{\{‘BLOCK\_SIZE*M’: BM, ‘BLOCK\_SIZE*N’: BN,}
\sphinxAtStartPar
‘BLOCK\_SIZE*K’: BK, ‘GROUP\_SIZE*M’: 8\},

\end{description}

\sphinxAtStartPar
num\_stages=s, num\_warps=w

\end{description}

\sphinxAtStartPar
)
for BM in {[}128{]}
for BN in {[}128, 256{]}
for BK in {[}64, 128{]}
for s in {[}2, 3, 4{]}
for w in {[}4, 8{]}

\end{description}

\sphinxAtStartPar
{]}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Parameters}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{BLOCK\_SIZE*M/N/K}}: Tile dimensions

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{num\_stages}}: Pipeline depth (2\sphinxhyphen{}4 typical)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{num\_warps}}: Threads per CTA (4 or 8)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{GROUP\_SIZE*M}}: Swizzling factor for L2 locality

\end{itemize}


\subsection{Command\sphinxhyphen{}line Usage}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:command-line-usage}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{FP16 matmul}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:fp16-matmul}}
\sphinxAtStartPar
python 09\sphinxhyphen{}persistent\sphinxhyphen{}matmul.py \textendash{}prec fp16 \textendash{}K\_range 128 1024 \textendash{}K\_step 128


\section{FP8 matmul (requires Hopper+)}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:fp8-matmul-requires-hopper}}
\sphinxAtStartPar
python 09\sphinxhyphen{}persistent\sphinxhyphen{}matmul.py \textendash{}prec fp8 \textendash{}K 512


\section{Profile specific K dimension}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:profile-specific-k-dimension}}
\sphinxAtStartPar
python 09\sphinxhyphen{}persistent\sphinxhyphen{}matmul.py \textendash{}prec fp16 \sphinxhyphen{}K 2048

\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Note:** May fail on GPUs with small shared memory (e.g., RTX 4090). Reduce ``num\PYGZus{}stages`` if needed.
\end{sphinxVerbatim}


\subsection{Common Pitfalls}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:common-pitfalls}}

\subsubsection{1. Wrong B Matrix Layout for TMA}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:wrong-b-matrix-layout-for-tma}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{TMA expects B to be transposed}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:tma-expects-b-to-be-transposed}}
\sphinxAtStartPar
b = b.T.contiguous()  \# Make sure it’s contiguous!

\sphinxAtStartPar
matmul\_tma(a, b, warp\_specialize=False)

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Bad: Using both in same kernel}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:bad-using-both-in-same-kernel}}
\sphinxAtStartPar
a\_desc = TensorDescriptor.from\_tensor(a, …)  \# Host\sphinxhyphen{}side
tl.make\_tensor*descriptor(…)  \# Device\sphinxhyphen{}side


\section{Good: Pick one approach}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:good-pick-one-approach}}\begin{description}
\sphinxlineitem{if HAS\_HOST*TENSOR\_DESC:}
\sphinxAtStartPar
use\_host*descriptors()

\sphinxlineitem{else:}
\sphinxAtStartPar
use\_device*descriptors()

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{if args.prec == ‘fp8’:}\begin{description}
\sphinxlineitem{if not hasattr(torch, ‘float8\_e4m3fn’) or not is\_cuda():}
\sphinxAtStartPar
print(“This example requires CUDA with fp8 support.”)
exit(1)

\end{description}

\end{description}


\subsection{Summary}
\label{\detokenize{gpu-tutorials/09-persistent-matmul:summary}}
\sphinxAtStartPar
\sphinxstylestrong{Persistent matmul} demonstrates advanced GPU programming techniques:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Persistent kernels} reduce launch overhead and improve SM utilization

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{TMA (Tensor Memory Accelerator)} simplifies memory access on Hopper+

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Warp specialization} overlaps memory and compute on Blackwell

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Epilogue subtiling} reduces shared memory pressure

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Proton profiler} provides detailed performance insights

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Kernel variants:}
1. \sphinxstylestrong{Naive}: Baseline, one CTA per tile
2. \sphinxstylestrong{Persistent}: Fixed CTAs, multiple tiles each
3. \sphinxstylestrong{TMA}: Hardware\sphinxhyphen{}accelerated loads (Hopper+)
4. \sphinxstylestrong{TMA Persistent}: Combining persistence and TMA
5. \sphinxstylestrong{Warp Spec}: Producer\sphinxhyphen{}consumer pattern (Blackwell+)

\sphinxAtStartPar
\sphinxstylestrong{Performance tips:}
\sphinxhyphen{} Use TMA on Hopper and newer GPUs
\sphinxhyphen{} Enable warp specialization on Blackwell
\sphinxhyphen{} Profile with Proton to identify bottlenecks
\sphinxhyphen{} Consider epilogue subtiling for memory\sphinxhyphen{}bound kernels
\sphinxhyphen{} Auto\sphinxhyphen{}tune for your specific hardware and problem sizes

\sphinxAtStartPar
This tutorial shows the evolution of matmul optimizations across GPU generations, from simple tiling to sophisticated hardware\sphinxhyphen{}software co\sphinxhyphen{}design!

\sphinxstepscope


\section{Tutorial 10: Block Scaled Matrix Multiplication}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:tutorial-10-block-scaled-matrix-multiplication}}\label{\detokenize{gpu-tutorials/10-block-scaled-matmul::doc}}

\subsection{Overview}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:overview}}
\sphinxAtStartPar
\sphinxstylestrong{Block scaled matmul} enables low\sphinxhyphen{}precision matrix multiplication (FP4/FP8) with per\sphinxhyphen{}block scaling factors. This technique is crucial for:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Quantized inference} \sphinxhyphen{} Running large language models with reduced memory

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Mixed precision training} \sphinxhyphen{} Different precision for different operations

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory bandwidth optimization} \sphinxhyphen{} Fewer bits transferred = faster compute

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hardware acceleration} \sphinxhyphen{} Specialized tensor cores for low\sphinxhyphen{}precision ops

\end{itemize}

\sphinxAtStartPar
This tutorial supports \sphinxstylestrong{four quantization formats}:
1. \sphinxstylestrong{nvfp4} \sphinxhyphen{} NVIDIA’s FP4 format (16 elements per scale, NVIDIA\sphinxhyphen{}only)
2. \sphinxstylestrong{mxfp4} \sphinxhyphen{} Microscaling FP4 (32 elements per scale, OCP standard)
3. \sphinxstylestrong{mxfp8} \sphinxhyphen{} Microscaling FP8 (32 elements per scale)
4. \sphinxstylestrong{mixed} \sphinxhyphen{} FP8 x FP4 mixed precision

\sphinxAtStartPar
\sphinxstylestrong{Hardware requirements:}
\sphinxhyphen{} \sphinxstylestrong{NVIDIA}: Blackwell (compute capability 10.0+) with 5th\sphinxhyphen{}gen Tensor Cores
\sphinxhyphen{} \sphinxstylestrong{AMD}: CDNA4 architecture (MI300 series) with scaled MFMA instructions


\subsection{Key Concepts}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:key-concepts}}

\subsubsection{Block Scaling Fundamentals}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:block-scaling-fundamentals}}
\sphinxAtStartPar
Instead of storing full\sphinxhyphen{}precision values, we store:

\sphinxAtStartPar
Low\sphinxhyphen{}precision value + Scale factor

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Standard} \PYG{n}{matmul}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
C = A @ B

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Block} \PYG{n}{scaled} \PYG{n}{matmul}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\sphinxAtStartPar
C = (A * scale\_a) @ (B * scale\_b)
\begin{description}
\sphinxlineitem{where:}
\sphinxAtStartPar
A, B are low\sphinxhyphen{}precision (fp4/fp8)
scale\_a, scale\_b are per\sphinxhyphen{}block scale factors
C is full precision (fp16/fp32)

\end{description}


\subsubsection{Quantization Formats}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:quantization-formats}}
\begin{DUlineblock}{0em}
\item[] Format | Bits/elem | Vec Size | Hardware | Notes |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\textendash{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\sphinxhyphen{}{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}
| nvfp4 | 4 | 16 | NVIDIA Blackwell | Proprietary, optimized for NVIDIA |
| mxfp4 | 4 | 32 | NVIDIA/AMD | OCP standard, better portability |
| mxfp8 | 8 | 32 | NVIDIA/AMD | Higher precision, still efficient |
| mixed | 4x8 | varies | NVIDIA Blackwell | A in fp8, B in fp4 |

\sphinxAtStartPar
\sphinxstylestrong{Vec Size} = number of elements sharing one scale factor


\subsubsection{Memory Savings}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:memory-savings}}
\sphinxAtStartPar
FP16: 2 bytes per element
FP8:  1 byte per element  \sphinxhyphen{}\textgreater{} 2x memory reduction
FP4:  0.5 bytes per element \sphinxhyphen{}\textgreater{} 4x memory reduction
\begin{description}
\sphinxlineitem{Plus scale factors:}\begin{itemize}
\item {} 
\sphinxAtStartPar
1 scale per 16\sphinxhyphen{}32 elements

\item {} 
\sphinxAtStartPar
Scales typically stored as fp8 or e8m0 (exponent only)

\item {} 
\sphinxAtStartPar
Overhead: \textasciitilde{}3\sphinxhyphen{}6\% of original size

\end{itemize}

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Example} \PYG{k}{for} \PYG{l+m+mi}{8192}\PYG{n}{x8192} \PYG{n}{matrix}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
FP16: 128 MB

\item {} 
\sphinxAtStartPar
FP8 + scales: 64 MB + 2 MB = 66 MB (48\% reduction)

\item {} 
\sphinxAtStartPar
FP4 + scales: 32 MB + 2 MB = 34 MB (73\% reduction)

\end{itemize}


\subsection{Scale Preshuffling (NVIDIA)}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:scale-preshuffling-nvidia}}

\subsubsection{Why Preshuffling?}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:why-preshuffling}}
\sphinxAtStartPar
Tensor cores load scales in specific patterns. To avoid non\sphinxhyphen{}contiguous access, scales must be reorganized:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Original linear layout: {[}M, K // VEC\_SIZE{]}}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:original-linear-layout-m-k-vec-size}}

\section{Each row has K // VEC\_SIZE scales}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:each-row-has-k-vec-size-scales}}

\section{Preshuffled layout: {[}M // 128, K // VEC\_SIZE // 4, 32, 4, 4{]}}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:preshuffled-layout-m-128-k-vec-size-4-32-4-4}}

\section{Organized for 128\sphinxhyphen{}element blocks along M}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:organized-for-128-element-blocks-along-m}}\begin{description}
\sphinxlineitem{Dimension breakdown:}
\sphinxAtStartPar
{[}M // 128{]}         \sphinxhyphen{} Number of 128\sphinxhyphen{}row blocks
{[}K // VEC\_SIZE // 4{]} \sphinxhyphen{} Number of K scale blocks
{[}32{]}               \sphinxhyphen{} 32 rows per sub\sphinxhyphen{}block
{[}4{]}                \sphinxhyphen{} 4 scale groups
{[}4{]}                \sphinxhyphen{} 4 scales per group

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Memory} \PYG{n}{access} \PYG{n}{pattern}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{for each BLOCK\_M x BLOCK\_K tile:}
\sphinxAtStartPar
Load 128 rows x (BLOCK\_K // VEC\_SIZE) scales contiguously
No strided access \sphinxhyphen{}\textgreater{} better memory bandwidth

\end{description}

\sphinxAtStartPar
Inside the kernel:
.. code\sphinxhyphen{}block:: python


\section{Load in 5D preshuffled format}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:load-in-5d-preshuffled-format}}
\sphinxAtStartPar
scale\_a = a\_scale*desc.load({[}0, offs\_scale*m, offs\_scale*k, 0, 0{]})


\section{Reshape to 5D}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:reshape-to-5d}}
\sphinxAtStartPar
scale\_a = scale\_a.reshape(rep\_m, rep\_k, 32, 4, 4)


\section{Transpose to logical 2D layout}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:transpose-to-logical-2d-layout}}
\sphinxAtStartPar
scale\_a = scale\_a.trans(0, 3, 2, 1, 4).reshape(BLOCK\_M, BLOCK\_K // VEC\_SIZE)


\section{Now ready for tl.dot\_scaled}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:now-ready-for-tl-dot-scaled}}

\subsection{Scale Preshuffling (AMD CDNA4)}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:scale-preshuffling-amd-cdna4}}

\subsubsection{MFMA Scale Organization}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:mfma-scale-organization}}
\sphinxAtStartPar
AMD’s MFMA (Matrix Fused Multiply\sphinxhyphen{}Add) instructions require different shuffling:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def shuffle\_scales*cdna4(scales, mfma\_nonkdim):}
\sphinxAtStartPar
“””
Reorganize scales for MFMA instructions.
\begin{description}
\sphinxlineitem{mfma\_nonkdim: 16 or 32}\begin{itemize}
\item {} 
\sphinxAtStartPar
16: mfma\_scaled*16x16x128

\item {} 
\sphinxAtStartPar
32: mfma\_scaled*32x32x64

\end{itemize}

\end{description}

\sphinxAtStartPar
“””
sm, sn = scales.shape
\begin{description}
\sphinxlineitem{if mfma\_nonkdim == 32:}
\sphinxAtStartPar
\# For 32x32 MFMA: pack 4 ops in order 0,1,2,3
scales\_shuffled = scales.view(sm // 32, 32, sn // 8, 4, 2, 1)
scales\_shuffled = scales\_shuffled.permute(0, 2, 4, 1, 3, 5).contiguous()

\sphinxlineitem{elif mfma\_nonkdim == 16:}
\sphinxAtStartPar
\# For 16x16 MFMA: pack 4 ops in order 0,2,1,3
scales\_shuffled = scales.view(sm // 32, 2, 16, sn // 8, 2, 4, 1)
scales\_shuffled = scales\_shuffled.permute(0, 3, 5, 2, 4, 1, 6).contiguous()

\end{description}

\sphinxAtStartPar
return scales\_shuffled.view(sm // 32, sn * 32)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{insight}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*} \PYG{n}{Each} \PYG{n}{thread} \PYG{n}{needs} \PYG{l+m+mi}{4} \PYG{n}{scale} \PYG{n}{values} \PYG{k}{for} \PYG{l+m+mi}{4} \PYG{n}{MFMA} \PYG{n}{operations}\PYG{p}{,} \PYG{n}{packed} \PYG{n}{contiguously}\PYG{o}{.}
\end{sphinxVerbatim}


\subsubsection{Thread\sphinxhyphen{}level Access Pattern}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:thread-level-access-pattern}}\begin{description}
\sphinxlineitem{Without shuffling:}
\sphinxAtStartPar
Thread 0 needs scales at: {[}0, 32, 64, 96{]} \sphinxhyphen{} strided access

\sphinxlineitem{With shuffling:}
\sphinxAtStartPar
Thread 0 needs scales at: {[}0, 1, 2, 3{]}    \sphinxhyphen{} contiguous access
Thread 1 needs scales at: {[}4, 5, 6, 7{]}
…

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Benefits}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Vectorized memory loads (4 bytes at once)

\item {} 
\sphinxAtStartPar
Better global memory coalescing

\item {} 
\sphinxAtStartPar
Lower LDS (shared memory) pressure

\end{itemize}


\subsection{Code Walkthrough}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:code-walkthrough}}

\subsubsection{1. NVIDIA Kernel (TMA\sphinxhyphen{}based)}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:nvidia-kernel-tma-based}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def block\_scaled*matmul\_kernel(
\begin{quote}

\sphinxAtStartPar
a\_desc, b\_desc, c\_desc,
a\_scale*desc, b\_scale*desc,
M, N, K,
output\_type: tl.constexpr,
ELEM\_PER*BYTE\_A: tl.constexpr,  \# 1 for fp8, 2 for fp4
ELEM\_PER*BYTE\_B: tl.constexpr,
VEC\_SIZE: tl.constexpr,          \# 16 for nvfp4, 32 for mxfp4/mxfp8
BLOCK\_M: tl.constexpr,
BLOCK\_N: tl.constexpr,
BLOCK\_K: tl.constexpr,
rep\_m: tl.constexpr,             \# BLOCK\_M // 128
rep\_n: tl.constexpr,             \# BLOCK\_N // 128
rep\_k: tl.constexpr,             \# BLOCK\_K // VEC\_SIZE // 4
NUM\_STAGES: tl.constexpr,
\end{quote}
\begin{description}
\sphinxlineitem{):}
\sphinxAtStartPar
pid = tl.program\_id(axis=0)
num\_pid*m = tl.cdiv(M, BLOCK\_M)
pid\_m = pid \% num\_pid*m
pid\_n = pid // num\_pid*m

\sphinxAtStartPar
offs\_am = pid\_m * BLOCK\_M
offs\_bn = pid\_n * BLOCK\_N
offs\_k*a = 0
offs\_k*b = 0
offs\_scale*m = pid\_m * rep\_m
offs\_scale*n = pid\_n * rep\_n
offs\_scale*k = 0

\sphinxAtStartPar
accumulator = tl.zeros((BLOCK\_M, BLOCK\_N), dtype=tl.float32)

\sphinxAtStartPar
\# Pipelined loop over K
for k in tl.range(0, tl.cdiv(K, BLOCK\_K), num\_stages=NUM\_STAGES):
\begin{quote}

\sphinxAtStartPar
\# Load data blocks
a = a\_desc.load({[}offs\_am, offs\_k*a{]})
b = b\_desc.load({[}offs\_bn, offs\_k*b{]})

\sphinxAtStartPar
\# Load and reshape scales
scale\_a = a\_scale*desc.load({[}0, offs\_scale*m, offs\_scale*k, 0, 0{]})
scale\_b = b\_scale*desc.load({[}0, offs\_scale*n, offs\_scale*k, 0, 0{]})

\sphinxAtStartPar
\# Reshape from 5D to 2D
scale\_a = scale\_a.reshape(rep\_m, rep\_k, 32, 4, 4) 
\begin{quote}

\sphinxAtStartPar
.trans(0, 3, 2, 1, 4) .reshape(BLOCK\_M, BLOCK\_K // VEC\_SIZE)
\end{quote}
\begin{description}
\sphinxlineitem{scale\_b = scale\_b.reshape(rep\_n, rep\_k, 32, 4, 4) }
\sphinxAtStartPar
.trans(0, 3, 2, 1, 4) .reshape(BLOCK\_N, BLOCK\_K // VEC\_SIZE)

\end{description}

\sphinxAtStartPar
\# Perform scaled dot product
if ELEM\_PER*BYTE\_A == 1 and ELEM\_PER*BYTE\_B == 2:
\begin{quote}

\sphinxAtStartPar
\# Mixed precision: A is fp8, B is fp4
accumulator = tl.dot\_scaled(
\begin{quote}

\sphinxAtStartPar
a, scale\_a, “e4m3”,
b.T, scale\_b, “e2m1”,
accumulator
\end{quote}

\sphinxAtStartPar
)
\end{quote}
\begin{description}
\sphinxlineitem{elif ELEM\_PER*BYTE\_A == 2 and ELEM\_PER*BYTE\_B == 2:}
\sphinxAtStartPar
\# Both fp4
accumulator = tl.dot\_scaled(
\begin{quote}

\sphinxAtStartPar
a, scale\_a, “e2m1”,
b.T, scale\_b, “e2m1”,
accumulator
\end{quote}

\sphinxAtStartPar
)

\sphinxlineitem{else:}
\sphinxAtStartPar
\# Both fp8
accumulator = tl.dot\_scaled(
\begin{quote}

\sphinxAtStartPar
a, scale\_a, “e4m3”,
b.T, scale\_b, “e4m3”,
accumulator
\end{quote}

\sphinxAtStartPar
)

\end{description}

\sphinxAtStartPar
\# Advance pointers
offs\_k*a += BLOCK\_K // ELEM\_PER*BYTE\_A
offs\_k*b += BLOCK\_K // ELEM\_PER*BYTE\_B
offs\_scale*k += rep\_k
\end{quote}

\sphinxAtStartPar
\# Store result
c\_desc.store({[}offs\_am, offs\_bn{]}, accumulator.to(output\_dtype))

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Key} \PYG{n}{operations}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.dot\_scaled()}} \sphinxhyphen{} Triton’s scaled matmul intrinsic

\item {} 
\sphinxAtStartPar
Format strings: \sphinxcode{\sphinxupquote{"e4m3"}} (fp8), \sphinxcode{\sphinxupquote{"e2m1"}} (fp4)

\item {} 
\sphinxAtStartPar
Automatic broadcast of scales across VEC\_SIZE elements

\end{itemize}


\subsubsection{2. AMD CDNA4 Kernel}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:amd-cdna4-kernel}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
@triton.jit
def block\_scaled*matmul\_kernel*cdna4(
\begin{quote}

\sphinxAtStartPar
a\_ptr, b\_ptr, c\_ptr,
a\_scales*ptr, b\_scales*ptr,
M, N, K,
stride\_am, stride\_ak, stride\_bk, stride\_bn,
stride\_cm, stride\_cn,
stride\_asm, stride\_ask, stride\_bsn, stride\_bsk,
BLOCK\_M: tl.constexpr,
BLOCK\_N: tl.constexpr,
BLOCK\_K: tl.constexpr,
mfma\_nonkdim: tl.constexpr,  \# 16 or 32
\end{quote}
\begin{description}
\sphinxlineitem{):}
\sphinxAtStartPar
pid = tl.program\_id(axis=0)
num\_pid*n = tl.cdiv(N, BLOCK\_N)
pid\_m = pid // num\_pid*n
pid\_n = pid \% num\_pid*n

\sphinxAtStartPar
\# Packed data: 2 fp4 elements per byte
offs\_k = tl.arange(0, BLOCK\_K // 2)
offs\_am = (pid\_m * BLOCK\_M + tl.arange(0, BLOCK\_M)) \% M
offs\_bn = (pid\_n * BLOCK\_N + tl.arange(0, BLOCK\_N)) \% N

\sphinxAtStartPar
a\_ptrs = a\_ptr + offs\_am{[}:, None{]} * stride\_am + offs\_k{[}None, :{]} * stride\_ak
b\_ptrs = b\_ptr + offs\_k{[}:, None{]} * stride\_bk + offs\_bn{[}None, :{]} * stride\_bn

\sphinxAtStartPar
\# Scale pointers (32 elements per scale)
SCALE\_GROUP*SIZE = 32
offs\_ks = tl.arange(0, BLOCK\_K // SCALE\_GROUP*SIZE * 32)

\sphinxAtStartPar
offs\_asm = (pid\_m * (BLOCK\_M // 32) + tl.arange(0, BLOCK\_M // 32)) \% M
a\_scale*ptrs = a\_scales*ptr + offs\_asm{[}:, None{]} * stride\_asm + 
\begin{quote}

\sphinxAtStartPar
offs\_ks{[}None, :{]} * stride\_ask
\end{quote}

\sphinxAtStartPar
offs\_asn = (pid\_n * (BLOCK\_N // 32) + tl.arange(0, BLOCK\_N // 32)) \% N
b\_scale*ptrs = b\_scales*ptr + offs\_asn{[}:, None{]} * stride\_bsn + 
\begin{quote}

\sphinxAtStartPar
offs\_ks{[}None, :{]} * stride\_bsk
\end{quote}

\sphinxAtStartPar
accumulator = tl.zeros((BLOCK\_M, BLOCK\_N), dtype=tl.float32)

\sphinxAtStartPar
num\_k*iter = tl.cdiv(K, BLOCK\_K // 2)
for k in range(num\_k*iter):
\begin{quote}

\sphinxAtStartPar
\# Load and unshuffle scales
if mfma\_nonkdim == 32:
\begin{quote}
\begin{description}
\sphinxlineitem{a\_scales = tl.load(a\_scale*ptrs) }
\sphinxAtStartPar
.reshape(BLOCK\_M // 32, BLOCK\_K // 32 // 8, 2, 32, 4, 1) .permute(0, 3, 1, 4, 2, 5) .reshape(BLOCK\_M, BLOCK\_K // 32)

\sphinxlineitem{b\_scales = tl.load(b\_scale*ptrs) }
\sphinxAtStartPar
.reshape(BLOCK\_N // 32, BLOCK\_K // 32 // 8, 2, 32, 4, 1) .permute(0, 3, 1, 4, 2, 5) .reshape(BLOCK\_N, BLOCK\_K // 32)

\end{description}
\end{quote}
\begin{description}
\sphinxlineitem{elif mfma\_nonkdim == 16:}\begin{description}
\sphinxlineitem{a\_scales = tl.load(a\_scale*ptrs) }
\sphinxAtStartPar
.reshape(BLOCK\_M // 32, BLOCK\_K // 32 // 8, 4, 16, 2, 2, 1) .permute(0, 5, 3, 1, 4, 2, 6) .reshape(BLOCK\_M, BLOCK\_K // 32)

\end{description}

\sphinxAtStartPar
\# Similar for b\_scales

\end{description}

\sphinxAtStartPar
\# Load packed data
a = tl.load(a\_ptrs)
b = tl.load(b\_ptrs)

\sphinxAtStartPar
\# Scaled matmul
accumulator += tl.dot\_scaled(a, a\_scales, “e2m1”,
\begin{quote}

\sphinxAtStartPar
b, b\_scales, “e2m1”)
\end{quote}

\sphinxAtStartPar
\# Advance pointers
a\_ptrs += (BLOCK\_K // 2) * stride\_ak
b\_ptrs += (BLOCK\_K // 2) * stride\_bk
a\_scale*ptrs += BLOCK\_K * stride\_ask
b\_scale*ptrs += BLOCK\_K * stride\_bsk
\end{quote}

\sphinxAtStartPar
c = accumulator.to(c\_ptr.type.element\_ty)
\# Store with write\sphinxhyphen{}through cache hint
tl.store(c\_ptrs, c, mask=c\_mask, cache\_modifier=”.wt”)

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{AMD}\PYG{o}{\PYGZhy{}}\PYG{n}{specific} \PYG{n}{features}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Explicit unshuffling of scales in kernel

\item {} 
\sphinxAtStartPar
Support for two MFMA shapes (16x16, 32x32)

\item {} 
\sphinxAtStartPar
Write\sphinxhyphen{}through cache modifier for better performance

\item {} 
\sphinxAtStartPar
E8M0 scale format (exponent\sphinxhyphen{}only, 8 bits)

\end{itemize}


\subsection{Initialization and Setup}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:initialization-and-setup}}

\subsubsection{NVIDIA Version}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:nvidia-version}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def initialize\_block*scaled(M, N, K, block\_scale*type=”nvfp4”):}
\sphinxAtStartPar
\# Configuration based on format
BLOCK\_M = 128
BLOCK\_N = 256
BLOCK\_K = 256 if “fp4” in block\_scale*type else 128
VEC\_SIZE = 16 if block\_scale*type == “nvfp4” else 32
ELEM\_PER*BYTE\_A = 2 if “fp4” in block\_scale*type else 1
ELEM\_PER*BYTE\_B = 1 if block\_scale*type == “mxfp8” else 2

\sphinxAtStartPar
\# Generate random data using mxfp helper
from triton.tools.mxfp import MXFP4Tensor

\sphinxAtStartPar
a\_ref = MXFP4Tensor(size=(M, K), device=”cuda”).random()
b\_ref = MXFP4Tensor(size=(N, K), device=”cuda”).random()  \# Transposed

\sphinxAtStartPar
\# Pack for fp4 (2 elements per byte)
if “fp4” in block\_scale*type and block\_scale*type != “mxfp8”:
\begin{quote}

\sphinxAtStartPar
a = a\_ref.to\_packed*tensor(dim=1)
\end{quote}
\begin{description}
\sphinxlineitem{else:}
\sphinxAtStartPar
a = a\_ref.to(torch.float8\_e4m3fn)

\end{description}

\sphinxAtStartPar
\# Create TMA descriptors
a\_desc = TensorDescriptor.from\_tensor(a, {[}BLOCK\_M, BLOCK\_K // ELEM\_PER*BYTE\_A{]})
b\_desc = TensorDescriptor.from\_tensor(b, {[}BLOCK\_N, BLOCK\_K // ELEM\_PER*BYTE\_B{]})

\sphinxAtStartPar
\# Generate scales in 5D preshuffled format
a\_scale*shape = {[}M // 128, K // VEC\_SIZE // 4, 32, 16{]}
b\_scale*shape = {[}N // 128, K // VEC\_SIZE // 4, 32, 16{]}

\sphinxAtStartPar
epsilon = 1e\sphinxhyphen{}8
a\_scale = torch.rand(a\_scale*shape, device=”cuda”) + epsilon
b\_scale = torch.rand(b\_scale*shape, device=”cuda”) + epsilon

\sphinxAtStartPar
\# Reshape to 5D TMA format
a\_scale = a\_scale.reshape(1, a\_scale*shape{[}0{]}, a\_scale.shape{[}1{]}, 2, 256)
b\_scale = b\_scale.reshape(1, b\_scale*shape{[}0{]}, b\_scale.shape{[}1{]}, 2, 256)

\sphinxAtStartPar
a\_scale*desc = TensorDescriptor.from\_tensor(a\_scale, block\_shape={[}1, rep\_m, rep\_k, 2, 256{]})
b\_scale*desc = TensorDescriptor.from\_tensor(b\_scale, block\_shape={[}1, rep\_n, rep\_k, 2, 256{]})

\sphinxAtStartPar
return a\_desc, a\_scale*desc, b\_desc, b\_scale*desc, …

\end{description}


\subsubsection{AMD Version}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:amd-version}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def initialize\_block*scaled\_amd(M, N, K, mfma\_nonkdim):}
\sphinxAtStartPar
BLOCK\_M = 128
BLOCK\_N = 128
BLOCK\_K = 256

\sphinxAtStartPar
x = MXFP4Tensor(size=(M, K), device=”cuda”).random()
w = MXFP4Tensor(size=(N, K), device=”cuda”).random()

\sphinxAtStartPar
\# E8M0 scales (exponent only, 8 bits)
x\_scales = torch.randint(124, 128, (K // 32, M), dtype=torch.uint8, device=”cuda”)
w\_scales = torch.randint(124, 128, (K // 32, N), dtype=torch.uint8, device=”cuda”)

\sphinxAtStartPar
x\_scales = x\_scales.T
w\_scales = w\_scales.T

\sphinxAtStartPar
\# Preshuffle for MFMA access pattern
x\_scales*shuffled = shuffle\_scales*cdna4(x\_scales, mfma\_nonkdim)
w\_scales*shuffled = shuffle\_scales*cdna4(w\_scales, mfma\_nonkdim)

\sphinxAtStartPar
\# Pack 2 fp4 elements per byte
x\_packed = x.to\_packed*tensor(dim=1)
w\_packed = w.to\_packed*tensor(dim=1)

\sphinxAtStartPar
return x\_packed, w\_packed, x\_scales*shuffled, w\_scales*shuffled, …

\end{description}


\subsection{Performance Characteristics}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:performance-characteristics}}

\subsubsection{Theoretical Speedup}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:theoretical-speedup}}
\sphinxAtStartPar
Compute\sphinxhyphen{}bound workload:

\sphinxAtStartPar
FP16 throughput: 312 TFLOPS (H100)
FP8 throughput:  989 TFLOPS (H100) \sphinxhyphen{}\textgreater{} 3.17x faster
FP4 throughput:  1978 TFLOPS (theoretical) \sphinxhyphen{}\textgreater{} 6.3x faster

\sphinxAtStartPar
Memory bandwidth reduction:

\sphinxAtStartPar
FP16: 2 bytes/elem
FP8:  1 byte/elem + scales \sphinxhyphen{}\textgreater{} \textasciitilde{}45\% savings
FP4:  0.5 bytes/elem + scales \sphinxhyphen{}\textgreater{} \textasciitilde{}70\% savings


\subsubsection{Real\sphinxhyphen{}world Performance}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:real-world-performance}}
\sphinxAtStartPar
From benchmarking on H100:
\sphinxhyphen{} \sphinxstylestrong{mxfp8}: 1.8\sphinxhyphen{}2.2x speedup over FP16
\sphinxhyphen{} \sphinxstylestrong{mxfp4}: 2.5\sphinxhyphen{}3.5x speedup over FP16
\sphinxhyphen{} \sphinxstylestrong{nvfp4}: 3.0\sphinxhyphen{}4.0x speedup over FP16 (NVIDIA\sphinxhyphen{}specific optimizations)
\sphinxhyphen{} \sphinxstylestrong{mixed (fp8xfp4)}: 2.2\sphinxhyphen{}3.0x speedup

\sphinxAtStartPar
\sphinxstylestrong{Factors affecting performance:}
\sphinxhyphen{} Matrix size (larger = better amortization)
\sphinxhyphen{} Scale overhead (smaller VEC\_SIZE = more overhead)
\sphinxhyphen{} Memory vs compute bound (FP4 helps more when memory\sphinxhyphen{}bound)


\subsection{Numerical Considerations}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:numerical-considerations}}

\subsubsection{Quantization Error}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:quantization-error}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{FP16 range: +/\sphinxhyphen{}65504, \textasciitilde{}3 decimal digits}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:fp16-range-65504-3-decimal-digits}}

\section{FP8 (E4M3) range: +/\sphinxhyphen{}448, \textasciitilde{}2 decimal digits}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:fp8-e4m3-range-448-2-decimal-digits}}

\section{FP4 (E2M1) range: +/\sphinxhyphen{}6, \textasciitilde{}1 decimal digit}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:fp4-e2m1-range-6-1-decimal-digit}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{*}\PYG{o}{*}\PYG{n}{Per}\PYG{o}{\PYGZhy{}}\PYG{n}{block} \PYG{n}{scaling} \PYG{n}{helps}\PYG{p}{:}\PYG{o}{*}\PYG{o}{*}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Without scaling:}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:without-scaling}}
\sphinxAtStartPar
fp4\_val = quantize\_fp16*to\_fp4(1234.5)  \# Overflow!


\section{With scaling:}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:with-scaling}}
\sphinxAtStartPar
scale = 1234.5 / 6.0  \# \textasciitilde{}205
fp4\_val = quantize\_fp16*to\_fp4(1234.5 / scale)  \# \textasciitilde{} 6
reconstructed = fp4\_val * scale  \# \textasciitilde{} 1234.5

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{def e8m0\_to*f32(x):}
\sphinxAtStartPar
“””Convert 8\sphinxhyphen{}bit exponent\sphinxhyphen{}only to float32.”””
\# No mantissa, only exponent
\# Value = 2\textasciicircum{}(exponent \sphinxhyphen{} 127)
return 2 ** ((x \sphinxhyphen{} 127).to(torch.float32))

\end{description}


\section{Example:}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:example}}

\section{x = 135 \sphinxhyphen{}\textgreater{} 2\textasciicircum{}(135\sphinxhyphen{}127) = 2\textasciicircum{}8 = 256}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:x-135-2-135-127-2-8-256}}

\section{x = 127 \sphinxhyphen{}\textgreater{} 2\textasciicircum{}0 = 1}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:x-127-2-0-1}}

\section{x = 119 \sphinxhyphen{}\textgreater{} 2\textasciicircum{}(\sphinxhyphen{}8) = 0.00390625}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:x-119-2-8-0-00390625}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
**Why exponent\PYGZhy{}only?**
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Scales are typically powers of 2

\item {} 
\sphinxAtStartPar
8 bits gives wide dynamic range

\item {} 
\sphinxAtStartPar
Simpler hardware implementation

\item {} 
\sphinxAtStartPar
Exact representation for power\sphinxhyphen{}of\sphinxhyphen{}2 scales

\end{itemize}


\subsection{Usage Examples}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:usage-examples}}

\subsubsection{Command\sphinxhyphen{}line Benchmarking}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:command-line-benchmarking}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{NVIDIA FP4}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:nvidia-fp4}}
\sphinxAtStartPar
python 10\sphinxhyphen{}block\sphinxhyphen{}scaled\sphinxhyphen{}matmul.py \textendash{}format nvfp4 \textendash{}K\_range 512 8192 \textendash{}bench


\section{NVIDIA FP8}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:nvidia-fp8}}
\sphinxAtStartPar
python 10\sphinxhyphen{}block\sphinxhyphen{}scaled\sphinxhyphen{}matmul.py \textendash{}format mxfp8 \textendash{}K 4096 \textendash{}bench


\section{AMD MXFP4 (automatic detection)}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:amd-mxfp4-automatic-detection}}
\sphinxAtStartPar
python 10\sphinxhyphen{}block\sphinxhyphen{}scaled\sphinxhyphen{}matmul.py \textendash{}format mxfp4 \textendash{}bench


\section{Mixed precision}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:mixed-precision}}
\sphinxAtStartPar
python 10\sphinxhyphen{}block\sphinxhyphen{}scaled\sphinxhyphen{}matmul.py \textendash{}format mixed \textendash{}K\_range 2048 8192 \textendash{}K\_step 2048

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{NVIDIA}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:nvidia}}
\sphinxAtStartPar
validate\_block*scaled(8192, 8192, 8192, block\_scale*type=”nvfp4”)
{[}{[}OK{]}{]} (pass nvfp4)
==============


\section{AMD with both MFMA shapes}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:amd-with-both-mfma-shapes}}
\sphinxAtStartPar
validate\_block*scaled\_amd(8192, 8192, 8192, block\_scale*type=”mxfp4”, mfma\_nonkdim=16)
{[}{[}OK{]}{]} (pass mxfp4, mfma\_nonk*dim 16)
================================

\sphinxAtStartPar
validate\_block*scaled\_amd(8192, 8192, 8192, block\_scale*type=”mxfp4”, mfma\_nonkdim=32)
{[}{[}OK{]}{]} (pass mxfp4, mfma\_nonk*dim 32)
================================


\subsection{Common Pitfalls}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:common-pitfalls}}

\subsubsection{1. Unsupported Hardware}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:unsupported-hardware}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{if not supports\_block*scaling():}
\sphinxAtStartPar
print(“{[}blocked{]} This example requires GPU support for block scaled matmul”)
exit(1)

\sphinxlineitem{def supports\_block*scaling():}\begin{description}
\sphinxlineitem{return (is\_cuda() and torch.cuda.get\_device*capability(){[}0{]} == 10) or }
\sphinxAtStartPar
is\_hip*cdna4()

\end{description}

\end{description}


\subsubsection{2. Format Mismatch}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:format-mismatch}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Bad: Using AMD kernel with NVIDIA formats}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:bad-using-amd-kernel-with-nvidia-formats}}\begin{description}
\sphinxlineitem{if is\_hip*cdna4():}
\sphinxAtStartPar
assert args.format == “mxfp4”, “AMD only supports mxfp4”

\end{description}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\section{Scales must match data dimensions}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:scales-must-match-data-dimensions}}
\sphinxAtStartPar
assert a\_scale.shape == {[}M // 128, K // VEC\_SIZE // 4, 32, 16{]}


\section{After packing to 5D}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:after-packing-to-5d}}
\sphinxAtStartPar
assert a\_scale.shape == {[}1, M // 128, K // VEC\_SIZE // 4, 2, 256{]}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
from typing import Optional
\begin{description}
\sphinxlineitem{def alloc\_fn(size: int, alignment: int, stream: Optional{[}int{]}):}
\sphinxAtStartPar
return torch.empty(size, device=”cuda”, dtype=torch.int8)

\end{description}

\sphinxAtStartPar
triton.set\_allocator(alloc\_fn)


\subsection{Summary}
\label{\detokenize{gpu-tutorials/10-block-scaled-matmul:summary}}
\sphinxAtStartPar
\sphinxstylestrong{Block scaled matmul} enables efficient low\sphinxhyphen{}precision matrix multiplication:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{4\sphinxhyphen{}8x memory reduction} compared to FP16

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{2\sphinxhyphen{}4x compute speedup} on specialized hardware

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Per\sphinxhyphen{}block scaling} maintains numerical accuracy

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hardware acceleration} via 5th\sphinxhyphen{}gen Tensor Cores (NVIDIA) and CDNA4 (AMD)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Supported formats:}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{nvfp4}}: NVIDIA\sphinxhyphen{}optimized FP4 (16 elem/scale)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{mxfp4}}: OCP standard FP4 (32 elem/scale)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{mxfp8}}: OCP standard FP8 (32 elem/scale)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{mixed}}: FP8xFP4 mixed precision

\sphinxAtStartPar
\sphinxstylestrong{Key techniques:}
\sphinxhyphen{} \sphinxstylestrong{Scale preshuffling} for contiguous memory access
\sphinxhyphen{} \sphinxstylestrong{TMA descriptors} for hardware\sphinxhyphen{}accelerated loads
\sphinxhyphen{} \sphinxstylestrong{tl.dot\_scaled} intrinsic for scaled operations
\sphinxhyphen{} \sphinxstylestrong{5D tensor layouts} optimized for tensor cores

\sphinxAtStartPar
\sphinxstylestrong{When to use:}
\sphinxhyphen{} Large language model inference
\sphinxhyphen{} Memory\sphinxhyphen{}constrained workloads
\sphinxhyphen{} Inference on edge devices
\sphinxhyphen{} Training with mixed precision

\sphinxAtStartPar
\sphinxstylestrong{Requirements:}
\sphinxhyphen{} NVIDIA Blackwell (CC 10.0+) or AMD CDNA4
\sphinxhyphen{} Triton with mxfp support
\sphinxhyphen{} Careful attention to scale layout and format

\sphinxAtStartPar
This is the cutting edge of GPU matrix multiplication, enabling the next generation of efficient AI!

\sphinxstepscope


\section{Triton Compiler Architecture Overview}
\label{\detokenize{triton-compiler/01-overview:triton-compiler-architecture-overview}}\label{\detokenize{triton-compiler/01-overview::doc}}
\sphinxAtStartPar
This document provides a comprehensive overview of how the Triton compiler works, from Python decorators to GPU binary code.


\subsection{What is Triton?}
\label{\detokenize{triton-compiler/01-overview:what-is-triton}}
\sphinxAtStartPar
\sphinxstylestrong{Triton} is a language and compiler for writing highly efficient GPU kernels using Python\sphinxhyphen{}like syntax. Unlike CUDA, which requires explicit memory management and complex threading, Triton provides a high\sphinxhyphen{}level block\sphinxhyphen{}based programming model that automatically handles many low\sphinxhyphen{}level details.


\subsubsection{Key Features}
\label{\detokenize{triton-compiler/01-overview:key-features}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Python\sphinxhyphen{}based DSL} \sphinxhyphen{} Write GPU kernels using Python syntax

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Automatic Memory Management} \sphinxhyphen{} No need to manually manage shared memory

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Block\sphinxhyphen{}level Programming} \sphinxhyphen{} Work with blocks of data instead of individual threads

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{JIT Compilation} \sphinxhyphen{} Kernels are compiled at runtime for flexibility

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{MLIR\sphinxhyphen{}based} \sphinxhyphen{} Leverages modern compiler infrastructure

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}backend} \sphinxhyphen{} Supports NVIDIA (CUDA), AMD (ROCm), and potentially others

\end{itemize}


\subsection{Compilation Pipeline Overview}
\label{\detokenize{triton-compiler/01-overview:compilation-pipeline-overview}}
\sphinxAtStartPar
The Triton compiler transforms Python code through several stages:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Python Function (@triton.jit)
       down
Python AST (Abstract Syntax Tree)
       down
Triton IR (TTIR) \PYGZhy{} High\PYGZhy{}level intermediate representation
       down
Triton GPU IR (TTGIR) \PYGZhy{} GPU\PYGZhy{}specific intermediate representation
       down
LLVM IR (LLIR) \PYGZhy{} Low\PYGZhy{}level intermediate representation
       down
PTX / AMDGCN \PYGZhy{} GPU assembly
       down
CUBIN / HSACO \PYGZhy{} GPU binary
\end{sphinxVerbatim}

\sphinxAtStartPar
Each stage performs specific transformations and optimizations.


\subsection{Architecture Components}
\label{\detokenize{triton-compiler/01-overview:architecture-components}}
\sphinxAtStartPar
The Triton compiler consists of several key components:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Frontend (Python)}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{@triton.jit}} decorator

\item {} 
\sphinxAtStartPar
AST parsing and analysis

\item {} 
\sphinxAtStartPar
Type inference

\item {} 
\sphinxAtStartPar
Dependency tracking

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxcode{\sphinxupquote{python/triton/runtime/jit.py}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Code Generator}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Converts Python AST to Triton IR (TTIR)

\item {} 
\sphinxAtStartPar
Handles control flow

\item {} 
\sphinxAtStartPar
Manages value types

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxcode{\sphinxupquote{python/triton/compiler/code\_generator.py}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{MLIR Pipeline}
\begin{itemize}
\item {} 
\sphinxAtStartPar
TTIR \sphinxhyphen{}\textgreater{} TTGIR lowering

\item {} 
\sphinxAtStartPar
GPU\sphinxhyphen{}specific optimizations

\item {} 
\sphinxAtStartPar
Memory coalescing

\item {} 
\sphinxAtStartPar
Shared memory allocation

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxcode{\sphinxupquote{lib/Dialect/}} (C++)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Backend}
\begin{itemize}
\item {} 
\sphinxAtStartPar
LLVM IR generation

\item {} 
\sphinxAtStartPar
Target\sphinxhyphen{}specific code generation

\item {} 
\sphinxAtStartPar
Binary assembly (PTX, AMDGCN)

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxcode{\sphinxupquote{third\_party/nvidia/backend/}} or \sphinxcode{\sphinxupquote{third\_party/amd/backend/}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Runtime}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Kernel caching

\item {} 
\sphinxAtStartPar
Auto\sphinxhyphen{}tuning

\item {} 
\sphinxAtStartPar
Grid computation

\item {} 
\sphinxAtStartPar
Kernel launching

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxcode{\sphinxupquote{python/triton/runtime/}}

\end{enumerate}


\subsection{Compilation Stages in Detail}
\label{\detokenize{triton-compiler/01-overview:compilation-stages-in-detail}}

\subsubsection{Stage 1: Python AST Parsing}
\label{\detokenize{triton-compiler/01-overview:stage-1-python-ast-parsing}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{@triton.jit}} decorator captures the Python function and parses its source code into an Abstract Syntax Tree (AST).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{add\PYGZus{}kernel}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{y\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{output\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{n\PYGZus{}elements}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{pid} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{block\PYGZus{}start} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE}
    \PYG{n}{offsets} \PYG{o}{=} \PYG{n}{block\PYGZus{}start} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}
    \PYG{n}{mask} \PYG{o}{=} \PYG{n}{offsets} \PYG{o}{\PYGZlt{}} \PYG{n}{n\PYGZus{}elements}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{y\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
    \PYG{n}{output} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{output\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{output}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The AST representation captures:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Function signature and parameters

\item {} 
\sphinxAtStartPar
Control flow structures (if/for/while)

\item {} 
\sphinxAtStartPar
Function calls and operations

\item {} 
\sphinxAtStartPar
Constant expressions (\sphinxcode{\sphinxupquote{tl.constexpr}})

\end{itemize}


\subsubsection{Stage 2: Code Generation (TTIR)}
\label{\detokenize{triton-compiler/01-overview:stage-2-code-generation-ttir}}
\sphinxAtStartPar
The code generator walks the Python AST and generates Triton IR (TTIR), a high\sphinxhyphen{}level representation that is backend\sphinxhyphen{}independent.

\sphinxAtStartPar
\sphinxstylestrong{TTIR Example:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
module \PYGZob{}
  tt.func public @add\PYGZus{}kernel(\PYGZpc{}arg0: !tt.ptr\PYGZlt{}f32\PYGZgt{}, \PYGZpc{}arg1: !tt.ptr\PYGZlt{}f32\PYGZgt{},
                              \PYGZpc{}arg2: !tt.ptr\PYGZlt{}f32\PYGZgt{}, \PYGZpc{}arg3: i32) \PYGZob{}
    \PYGZpc{}c0\PYGZus{}i32 = arith.constant 0 : i32
    \PYGZpc{}0 = tt.get\PYGZus{}program\PYGZus{}id x : i32
    \PYGZpc{}1 = arith.muli \PYGZpc{}0, \PYGZpc{}BLOCK\PYGZus{}SIZE : i32
    \PYGZpc{}2 = tt.make\PYGZus{}range \PYGZob{}end = 128 : i32, start = 0 : i32\PYGZcb{} : tensor\PYGZlt{}128xi32\PYGZgt{}
    \PYGZpc{}3 = tt.splat \PYGZpc{}1 : i32 \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128xi32\PYGZgt{}
    \PYGZpc{}4 = arith.addi \PYGZpc{}3, \PYGZpc{}2 : tensor\PYGZlt{}128xi32\PYGZgt{}
    \PYGZpc{}5 = tt.splat \PYGZpc{}arg3 : i32 \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128xi32\PYGZgt{}
    \PYGZpc{}6 = arith.cmpi slt, \PYGZpc{}4, \PYGZpc{}5 : tensor\PYGZlt{}128xi32\PYGZgt{}
    \PYGZpc{}7 = tt.splat \PYGZpc{}arg0 : !tt.ptr\PYGZlt{}f32\PYGZgt{} \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    \PYGZpc{}8 = tt.addptr \PYGZpc{}7, \PYGZpc{}4 : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}, tensor\PYGZlt{}128xi32\PYGZgt{}
    \PYGZpc{}9 = tt.load \PYGZpc{}8, \PYGZpc{}6 : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    // ... more operations ...
    tt.return
  \PYGZcb{}
\PYGZcb{}
\end{sphinxVerbatim}

\sphinxAtStartPar
Key features of TTIR:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Uses MLIR (Multi\sphinxhyphen{}Level Intermediate Representation) infrastructure

\item {} 
\sphinxAtStartPar
Supports tensor types (\sphinxcode{\sphinxupquote{tensor\textless{}128xf32\textgreater{}}})

\item {} 
\sphinxAtStartPar
Pointer arithmetic (\sphinxcode{\sphinxupquote{tt.addptr}})

\item {} 
\sphinxAtStartPar
Block operations (\sphinxcode{\sphinxupquote{tt.load}}, \sphinxcode{\sphinxupquote{tt.store}})

\end{itemize}


\subsubsection{Stage 3: Triton GPU IR (TTGIR)}
\label{\detokenize{triton-compiler/01-overview:stage-3-triton-gpu-ir-ttgir}}
\sphinxAtStartPar
TTIR is lowered to TTGIR, which adds GPU\sphinxhyphen{}specific information:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Thread layout} \sphinxhyphen{} How threads are organized within blocks

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data layout} \sphinxhyphen{} How data is distributed across threads

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory hierarchy} \sphinxhyphen{} Shared memory allocation

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Synchronization} \sphinxhyphen{} Barrier placement

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Transformations:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Memory coalescing optimization

\item {} 
\sphinxAtStartPar
Shared memory insertion

\item {} 
\sphinxAtStartPar
Warp\sphinxhyphen{}level operations

\item {} 
\sphinxAtStartPar
Tensor core utilization (for matmul)

\end{itemize}


\subsubsection{Stage 4: LLVM IR}
\label{\detokenize{triton-compiler/01-overview:stage-4-llvm-ir}}
\sphinxAtStartPar
TTGIR is lowered to LLVM IR, which is closer to assembly:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{define}\PYG{+w}{ }\PYG{k}{void}\PYG{+w}{ }\PYG{n+nv+vg}{@add\PYGZus{}kernel}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}arg0}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}arg1}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}arg2}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}arg3}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{n+nl}{entry:}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}tid}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{call}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv+vg}{@llvm.nvvm.read.ptx.sreg.tid.x}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}bid}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{call}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv+vg}{@llvm.nvvm.read.ptx.sreg.ctaid.x}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}block\PYGZus{}size}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{mul}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}bid}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m}{128}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}offset}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{add}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}block\PYGZus{}size}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}tid}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}in\PYGZus{}bounds}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{icmp}\PYG{+w}{ }\PYG{k}{slt}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}arg3}
\PYG{+w}{  }\PYG{k}{br}\PYG{+w}{ }\PYG{k+kt}{i1}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}in\PYGZus{}bounds}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}load}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}exit}
\PYG{n+nl}{load:}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}ptr}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{getelementptr}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}arg0}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}ptr}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{getelementptr}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}arg1}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}val}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{load}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}ptr}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}val}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{load}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}ptr}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}result}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{fadd}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}val}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}val}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}ptr}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{getelementptr}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}arg2}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}
\PYG{+w}{  }\PYG{k}{store}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}result}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}ptr}
\PYG{+w}{  }\PYG{k}{br}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}exit}
\PYG{n+nl}{exit:}
\PYG{+w}{  }\PYG{k}{ret}\PYG{+w}{ }\PYG{k}{void}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{Stage 5: PTX / AMDGCN}
\label{\detokenize{triton-compiler/01-overview:stage-5-ptx-amdgcn}}
\sphinxAtStartPar
LLVM IR is compiled to GPU assembly:

\sphinxAtStartPar
\sphinxstylestrong{NVIDIA PTX:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kr}{.version}\PYG{+w}{ }\PYG{l+m}{8.0}
\PYG{k+kr}{.target}\PYG{+w}{ }\PYG{n+nv}{sm\PYGZus{}80}
\PYG{k+kr}{.address\PYGZus{}size}\PYG{+w}{ }\PYG{l+m}{64}

\PYG{k+kr}{.visible}\PYG{+w}{ }\PYG{k+kr}{.entry}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel}\PYG{p}{(}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}0}\PYG{o}{,}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}1}\PYG{o}{,}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}2}\PYG{o}{,}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}3}
\PYG{p}{)}
\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.pred}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}p}\PYG{p}{\PYGZlt{}}\PYG{l+m}{2}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f}\PYG{p}{\PYGZlt{}}\PYG{l+m}{3}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.b32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r}\PYG{p}{\PYGZlt{}}\PYG{l+m}{5}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.b64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd}\PYG{p}{\PYGZlt{}}\PYG{l+m}{7}\PYG{p}{\PYGZgt{}}\PYG{p}{;}

\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}0}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd2}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}1}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd3}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}2}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}3}\PYG{p}{]}\PYG{p}{;}

\PYG{+w}{    }\PYG{k}{mov}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}tid.x}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{mov}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r3}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}ctaid.x}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{mul}\PYG{n+nv}{.lo.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r4}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r3}\PYG{o}{,}\PYG{+w}{ }\PYG{l+m}{128}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{add}\PYG{k+kt}{.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r4}\PYG{p}{;}

\PYG{+w}{    }\PYG{k}{setp}\PYG{n+nv}{.ge.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}p1}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r1}\PYG{p}{;}
\PYG{+w}{    }\PYG{err}{@}\PYG{n+nv}{\PYGZpc{}p1}\PYG{+w}{ }\PYG{k}{bra}\PYG{+w}{ }\PYG{n+nv}{EXIT}\PYG{p}{;}

\PYG{+w}{    }\PYG{k}{cvt}\PYG{k+kt}{.s64}\PYG{k+kt}{.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd4}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{shl}\PYG{k+kt}{.b64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd5}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd4}\PYG{o}{,}\PYG{+w}{ }\PYG{l+m}{2}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{add}\PYG{k+kt}{.s64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd6}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd1}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd5}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.global}\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nv}{\PYGZpc{}rd6}\PYG{p}{]}\PYG{p}{;}

\PYG{+w}{    }\PYG{c}{// ... more assembly ...}

\PYG{n+nl}{EXIT:}
\PYG{+w}{    }\PYG{k}{ret}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{Stage 6: Binary (CUBIN / HSACO)}
\label{\detokenize{triton-compiler/01-overview:stage-6-binary-cubin-hsaco}}
\sphinxAtStartPar
PTX is assembled into CUBIN (CUDA binary) using \sphinxcode{\sphinxupquote{ptxas}}, or AMDGCN is assembled into HSACO (HSA Code Object) using AMD’s assembler.

\sphinxAtStartPar
This binary can be executed on the GPU.


\subsection{Key Design Decisions}
\label{\detokenize{triton-compiler/01-overview:key-design-decisions}}

\subsubsection{Block\sphinxhyphen{}based Programming Model}
\label{\detokenize{triton-compiler/01-overview:block-based-programming-model}}
\sphinxAtStartPar
Triton uses \sphinxstylestrong{SPMD (Single Program, Multiple Data)} with blocks:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Instead of thinking about individual threads:}
\PYG{c+c1}{\PYGZsh{} thread\PYGZus{}id = get\PYGZus{}thread\PYGZus{}id()}
\PYG{c+c1}{\PYGZsh{} output[thread\PYGZus{}id] = input[thread\PYGZus{}id] * 2}

\PYG{c+c1}{\PYGZsh{} Triton thinks in blocks:}
\PYG{n}{pid} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Which block am I?}
\PYG{n}{offsets} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Block of indices}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{input\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Load entire block}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{data} \PYG{o}{*} \PYG{l+m+mi}{2}
\PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{output\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{result}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Store entire block}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Benefits:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Easier to write and understand

\item {} 
\sphinxAtStartPar
Compiler handles thread\sphinxhyphen{}level details

\item {} 
\sphinxAtStartPar
Automatic memory coalescing

\item {} 
\sphinxAtStartPar
Better optimization opportunities

\end{itemize}


\subsubsection{JIT Compilation}
\label{\detokenize{triton-compiler/01-overview:jit-compilation}}
\sphinxAtStartPar
Triton compiles kernels \sphinxstylestrong{at runtime} when first called:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{pass}

\PYG{c+c1}{\PYGZsh{} First call: compilation happens here}
\PYG{n}{kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{n}{args}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} \PYGZti{}100ms\PYGZhy{}1s for first compile}

\PYG{c+c1}{\PYGZsh{} Second call: cached, very fast}
\PYG{n}{kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{n}{args}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} \PYGZti{}microseconds}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Benefits:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Can specialize on runtime values (\sphinxcode{\sphinxupquote{constexpr}})

\item {} 
\sphinxAtStartPar
No separate compilation step

\item {} 
\sphinxAtStartPar
Flexible auto\sphinxhyphen{}tuning

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Caching:}

\sphinxAtStartPar
Compiled kernels are cached by:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Source code hash

\item {} 
\sphinxAtStartPar
Compile\sphinxhyphen{}time constants

\item {} 
\sphinxAtStartPar
Compiler options

\item {} 
\sphinxAtStartPar
Environment variables

\end{itemize}


\subsubsection{MLIR Infrastructure}
\label{\detokenize{triton-compiler/01-overview:mlir-infrastructure}}
\sphinxAtStartPar
Triton uses \sphinxstylestrong{MLIR (Multi\sphinxhyphen{}Level Intermediate Representation)} instead of custom IR:

\sphinxAtStartPar
\sphinxstylestrong{Advantages:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reusable passes and transformations

\item {} 
\sphinxAtStartPar
Interoperability with other MLIR\sphinxhyphen{}based compilers

\item {} 
\sphinxAtStartPar
Well\sphinxhyphen{}tested infrastructure

\item {} 
\sphinxAtStartPar
Growing ecosystem

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Dialects used:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt}} \sphinxhyphen{} Triton dialect (TTIR)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{ttg}} \sphinxhyphen{} Triton GPU dialect (TTGIR)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{arith}} \sphinxhyphen{} Arithmetic operations

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{scf}} \sphinxhyphen{} Structured control flow

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{llvm}} \sphinxhyphen{} LLVM dialect

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{nvgpu}} \sphinxhyphen{} NVIDIA GPU specific

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{rocdl}} \sphinxhyphen{} AMD ROCm specific

\end{itemize}


\subsection{Source Code Organization}
\label{\detokenize{triton-compiler/01-overview:source-code-organization}}

\subsubsection{Python Components}
\label{\detokenize{triton-compiler/01-overview:python-components}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{python/triton/}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{runtime/jit.py}} \sphinxhyphen{} \sphinxcode{\sphinxupquote{@triton.jit}} decorator, JITFunction class

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{compiler/compiler.py}} \sphinxhyphen{} Main compilation orchestration

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{compiler/code\_generator.py}} \sphinxhyphen{} AST \sphinxhyphen{}\textgreater{} TTIR conversion

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{backends/compiler.py}} \sphinxhyphen{} Backend abstraction

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{language/}} \sphinxhyphen{} Triton language primitives (tl.load, tl.store, etc.)

\end{itemize}


\subsubsection{C++ Components}
\label{\detokenize{triton-compiler/01-overview:c-components}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{lib/Dialect/}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Triton/}} \sphinxhyphen{} TTIR dialect definition

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPU/}} \sphinxhyphen{} TTGIR dialect and passes

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonNvidiaGPU/}} \sphinxhyphen{} NVIDIA\sphinxhyphen{}specific passes

\end{itemize}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{include/triton/}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
MLIR operation definitions (\sphinxcode{\sphinxupquote{.td}} files)

\item {} 
\sphinxAtStartPar
Pass headers

\end{itemize}


\subsubsection{Backend Components}
\label{\detokenize{triton-compiler/01-overview:backend-components}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{third\_party/nvidia/backend/}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{compiler.py}} \sphinxhyphen{} NVIDIA backend implementation

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{driver.py}} \sphinxhyphen{} CUDA driver interface

\end{itemize}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{third\_party/amd/backend/}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{compiler.py}} \sphinxhyphen{} AMD backend implementation

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{driver.py}} \sphinxhyphen{} ROCm driver interface

\end{itemize}


\subsection{Links to Source Code}
\label{\detokenize{triton-compiler/01-overview:links-to-source-code}}
\sphinxAtStartPar
All links reference Triton v3.5.1:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py}{JIT Decorator}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/compiler/compiler.py}{Compiler}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/compiler/code\_generator.py}{Code Generator}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/backends/compiler.py}{Backend Base}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/third\_party/nvidia/backend/compiler.py}{NVIDIA Backend}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/lib/Dialect/Triton}{Triton Dialect}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/lib/Dialect/TritonGPU}{TritonGPU Dialect}

\end{itemize}


\subsection{Next Steps}
\label{\detokenize{triton-compiler/01-overview:next-steps}}
\sphinxAtStartPar
Continue to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{triton-compiler/02-jit-decorator::doc}]{\sphinxcrossref{\DUrole{doc}{The @triton.jit Decorator}}}} \sphinxhyphen{} Deep dive into \sphinxcode{\sphinxupquote{@triton.jit}}

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{03\sphinxhyphen{}code\sphinxhyphen{}generation}}} \sphinxhyphen{} Python AST to TTIR conversion

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{04\sphinxhyphen{}mlir\sphinxhyphen{}lowering}}} \sphinxhyphen{} MLIR passes and optimizations

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{05\sphinxhyphen{}backend\sphinxhyphen{}compilation}}} \sphinxhyphen{} Backend\sphinxhyphen{}specific compilation

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{06\sphinxhyphen{}kernel\sphinxhyphen{}launch}}} \sphinxhyphen{} Runtime and kernel execution

\end{itemize}

\sphinxstepscope


\section{The @triton.jit Decorator}
\label{\detokenize{triton-compiler/02-jit-decorator:the-triton-jit-decorator}}\label{\detokenize{triton-compiler/02-jit-decorator::doc}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{@triton.jit}} decorator is the entry point to Triton compilation. This document explains how it works, from Python function decoration to compilation triggering.


\subsection{Overview}
\label{\detokenize{triton-compiler/02-jit-decorator:overview}}
\sphinxAtStartPar
When you write a Triton kernel, you decorate it with \sphinxcode{\sphinxupquote{@triton.jit}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{triton}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{triton}\PYG{n+nn}{.}\PYG{n+nn}{language}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{tl}

\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{add\PYGZus{}kernel}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{y\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{out\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{n\PYGZus{}elements}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{pid} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{offsets} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}
    \PYG{n}{mask} \PYG{o}{=} \PYG{n}{offsets} \PYG{o}{\PYGZlt{}} \PYG{n}{n\PYGZus{}elements}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{y\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{out\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{@triton.jit}} decorator transforms the function into a \sphinxcode{\sphinxupquote{JITFunction}} object that handles compilation and execution.


\subsection{How the Decorator Works}
\label{\detokenize{triton-compiler/02-jit-decorator:how-the-decorator-works}}

\subsubsection{Implementation}
\label{\detokenize{triton-compiler/02-jit-decorator:implementation}}
\sphinxAtStartPar
The decorator is defined in \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py}{jit.py}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{jit}\PYG{p}{(}\PYG{n}{fn}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Decorator for triton kernels.}

\PYG{l+s+sd}{    This decorator transforms a Python function into a JITFunction object}
\PYG{l+s+sd}{    that can be compiled and executed on the GPU.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{fn}\PYG{p}{,} \PYG{n}{JITFunction}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{fn}
    \PYG{k}{if} \PYG{o+ow}{not} \PYG{n+nb}{callable}\PYG{p}{(}\PYG{n}{fn}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{raise} \PYG{n+ne}{TypeError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{triton.jit requires a callable argument}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Create JITFunction wrapper}
    \PYG{k}{return} \PYG{n}{JITFunction}\PYG{p}{(}\PYG{n}{fn}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
When you apply \sphinxcode{\sphinxupquote{@triton.jit}}, it creates a \sphinxcode{\sphinxupquote{JITFunction}} instance wrapping your original function.


\subsection{The JITCallable Base Class}
\label{\detokenize{triton-compiler/02-jit-decorator:the-jitcallable-base-class}}
\sphinxAtStartPar
All JIT\sphinxhyphen{}compiled Triton code inherits from \sphinxcode{\sphinxupquote{JITCallable}}, which provides core functionality:


\subsubsection{Source Code Extraction}
\label{\detokenize{triton-compiler/02-jit-decorator:source-code-extraction}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class}\PYG{+w}{ }\PYG{n+nc}{JITCallable}\PYG{p}{:}
    \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{fn}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fn} \PYG{o}{=} \PYG{n}{fn}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{signature} \PYG{o}{=} \PYG{n}{inspect}\PYG{o}{.}\PYG{n}{signature}\PYG{p}{(}\PYG{n}{fn}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Extract source code}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{raw\PYGZus{}src}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{starting\PYGZus{}line\PYGZus{}number} \PYG{o}{=} \PYG{n}{inspect}\PYG{o}{.}\PYG{n}{getsourcelines}\PYG{p}{(}\PYG{n}{fn}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Remove decorators and dedent}
        \PYG{n}{src} \PYG{o}{=} \PYG{n}{textwrap}\PYG{o}{.}\PYG{n}{dedent}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{raw\PYGZus{}src}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{src} \PYG{o}{=} \PYG{n}{src}\PYG{p}{[}\PYG{n}{re}\PYG{o}{.}\PYG{n}{search}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZca{}def}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{s+}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{w+}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{s*}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{src}\PYG{p}{,} \PYG{n}{re}\PYG{o}{.}\PYG{n}{MULTILINE}\PYG{p}{)}\PYG{o}{.}\PYG{n}{start}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}src} \PYG{o}{=} \PYG{n}{src}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py\#L455-L470}{jit.py:455\sphinxhyphen{}470}

\sphinxAtStartPar
\sphinxstylestrong{Why extract source?}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Triton needs the Python AST to compile

\item {} 
\sphinxAtStartPar
Source is hashed for caching

\item {} 
\sphinxAtStartPar
Enables runtime code inspection

\end{itemize}


\subsubsection{AST Parsing}
\label{\detokenize{triton-compiler/02-jit-decorator:ast-parsing}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{parse}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Parse the source code into an AST.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{tree} \PYG{o}{=} \PYG{n}{ast}\PYG{o}{.}\PYG{n}{parse}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}src}\PYG{p}{)}
    \PYG{k}{assert} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{tree}\PYG{p}{,} \PYG{n}{ast}\PYG{o}{.}\PYG{n}{Module}\PYG{p}{)}
    \PYG{k}{assert} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{body}\PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{1}
    \PYG{k}{assert} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{body}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ast}\PYG{o}{.}\PYG{n}{FunctionDef}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{tree}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py\#L527-L532}{jit.py:527\sphinxhyphen{}532}

\sphinxAtStartPar
This converts the source string into a Python AST that the code generator can process.


\subsubsection{Cache Key Generation}
\label{\detokenize{triton-compiler/02-jit-decorator:cache-key-generation}}
\sphinxAtStartPar
Every kernel has a \sphinxstylestrong{cache key} based on its source code and dependencies:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@property}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{cache\PYGZus{}key}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{str}\PYG{p}{:}
    \PYG{k}{with} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}hash\PYGZus{}lock}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash}

        \PYG{c+c1}{\PYGZsh{} Find all dependencies}
        \PYG{n}{nonlocals} \PYG{o}{=} \PYG{n}{inspect}\PYG{o}{.}\PYG{n}{getclosurevars}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fn}\PYG{p}{)}\PYG{o}{.}\PYG{n}{nonlocals}
        \PYG{n}{dependencies\PYGZus{}finder} \PYG{o}{=} \PYG{n}{DependenciesFinder}\PYG{p}{(}
            \PYG{n}{name}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}fn\PYGZus{}name}\PYG{p}{,}
            \PYG{n+nb}{globals}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}globals\PYGZus{}\PYGZus{}}\PYG{p}{,}
            \PYG{n}{nonlocals}\PYG{o}{=}\PYG{n}{nonlocals}\PYG{p}{,}
            \PYG{n}{src}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{src}
        \PYG{p}{)}
        \PYG{n}{dependencies\PYGZus{}finder}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{parse}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Hash = source + dependencies + line number}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash} \PYG{o}{=} \PYG{n}{dependencies\PYGZus{}finder}\PYG{o}{.}\PYG{n}{ret} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{starting\PYGZus{}line\PYGZus{}number}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n+nb}{sorted}\PYG{p}{(}\PYG{n}{dependencies\PYGZus{}finder}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Add constexpr values to hash}
        \PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{triton}\PYG{n+nn}{.}\PYG{n+nn}{language}\PYG{n+nn}{.}\PYG{n+nn}{core}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{constexpr}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash} \PYG{o}{+}\PYG{o}{=} \PYG{n+nb}{str}\PYG{p}{(}\PYG{p}{[}
            \PYG{p}{(}\PYG{n}{name}\PYG{p}{,} \PYG{n}{val}\PYG{p}{)}
            \PYG{k}{for} \PYG{p}{(}\PYG{n}{name}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{n}{val}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}
            \PYG{k}{if} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{val}\PYG{p}{,} \PYG{n}{constexpr}\PYG{p}{)}
        \PYG{p}{]}\PYG{p}{)}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash} \PYG{o}{=} \PYG{n}{hashlib}\PYG{o}{.}\PYG{n}{sha256}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{utf\PYGZhy{}8}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{hexdigest}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py\#L498-L519}{jit.py:498\sphinxhyphen{}519}

\sphinxAtStartPar
\sphinxstylestrong{Cache key includes:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Source code (SHA\sphinxhyphen{}256 hash)

\item {} 
\sphinxAtStartPar
Line number (for duplicate function names)

\item {} 
\sphinxAtStartPar
Global variable values used by the function

\item {} 
\sphinxAtStartPar
Constexpr values

\item {} 
\sphinxAtStartPar
All transitively called functions

\end{itemize}


\subsection{Dependencies Tracking}
\label{\detokenize{triton-compiler/02-jit-decorator:dependencies-tracking}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{DependenciesFinder}} class walks the AST to find all dependencies:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class}\PYG{+w}{ }\PYG{n+nc}{DependenciesFinder}\PYG{p}{(}\PYG{n}{ast}\PYG{o}{.}\PYG{n}{NodeVisitor}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Finds dependencies of a JITFunction.}

\PYG{l+s+sd}{    Tracks:}
\PYG{l+s+sd}{    1. Global variables accessed by the function}
\PYG{l+s+sd}{    2. Other JITCallable functions called}
\PYG{l+s+sd}{    3. Their values at compilation time}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{name}\PYG{p}{,} \PYG{n+nb}{globals}\PYG{p}{,} \PYG{n}{nonlocals}\PYG{p}{,} \PYG{n}{src}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name} \PYG{o}{=} \PYG{n}{name}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hasher} \PYG{o}{=} \PYG{n}{hashlib}\PYG{o}{.}\PYG{n}{sha256}\PYG{p}{(}\PYG{n}{src}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{utf\PYGZhy{}8}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{globals} \PYG{o}{=} \PYG{n+nb}{globals}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{nonlocals} \PYG{o}{=} \PYG{n}{nonlocals}

        \PYG{c+c1}{\PYGZsh{} Map: (var\PYGZus{}name, id(\PYGZus{}\PYGZus{}globals\PYGZus{}\PYGZus{})) \PYGZhy{}\PYGZgt{} (var\PYGZus{}value, \PYGZus{}\PYGZus{}globals\PYGZus{}\PYGZus{})}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py\#L34-L86}{jit.py:34\sphinxhyphen{}86}


\subsubsection{Tracking Global Variables}
\label{\detokenize{triton-compiler/02-jit-decorator:tracking-global-variables}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{visit\PYGZus{}Name}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{node}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Visit a variable name in the AST.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{ctx}\PYG{p}{)} \PYG{o+ow}{is} \PYG{n}{ast}\PYG{o}{.}\PYG{n}{Store}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{node}\PYG{o}{.}\PYG{n}{id}  \PYG{c+c1}{\PYGZsh{} Writing to variable}

    \PYG{k}{if} \PYG{n}{node}\PYG{o}{.}\PYG{n}{id} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{local\PYGZus{}names}\PYG{p}{:}
        \PYG{k}{return} \PYG{k+kc}{None}  \PYG{c+c1}{\PYGZsh{} Local variable shadows global}

    \PYG{c+c1}{\PYGZsh{} Look up in globals/nonlocals}
    \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{name\PYGZus{}lookup}\PYG{p}{(}\PYG{n}{name}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{val} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{globals}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{name}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{val} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{k}{return} \PYG{n}{val}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{globals}
        \PYG{n}{val} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{nonlocals}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{name}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{val} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{k}{return} \PYG{n}{val}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{nonlocals}
        \PYG{k}{return} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}

    \PYG{n}{val}\PYG{p}{,} \PYG{n}{var\PYGZus{}dict} \PYG{o}{=} \PYG{n}{name\PYGZus{}lookup}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{id}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Record the value for cache invalidation}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{record\PYGZus{}reference}\PYG{p}{(}\PYG{n}{val}\PYG{p}{,} \PYG{n}{var\PYGZus{}dict}\PYG{p}{,} \PYG{n}{node}\PYG{o}{.}\PYG{n}{id}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{val}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py\#L156-L178}{jit.py:156\sphinxhyphen{}178}

\sphinxAtStartPar
\sphinxstylestrong{Why track globals?}

\sphinxAtStartPar
If you do this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{CONSTANT} \PYG{o}{=} \PYG{l+m+mi}{42}

\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{CONSTANT}  \PYG{c+c1}{\PYGZsh{} Uses global}

\PYG{n}{kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Compiles with CONSTANT=42}

\PYG{n}{CONSTANT} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} ERROR: CONSTANT changed!}
\end{sphinxVerbatim}

\sphinxAtStartPar
Triton detects the change and raises an error, preventing silent bugs.


\subsubsection{Handling Nested Function Calls}
\label{\detokenize{triton-compiler/02-jit-decorator:handling-nested-function-calls}}
\sphinxAtStartPar
When a kernel calls another JIT function:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{helper}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{x} \PYG{o}{*} \PYG{l+m+mi}{2}

\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{main\PYGZus{}kernel}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{helper}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The dependency finder:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Recognizes \sphinxcode{\sphinxupquote{helper}} as a \sphinxcode{\sphinxupquote{JITCallable}}

\item {} 
\sphinxAtStartPar
Includes \sphinxcode{\sphinxupquote{helper}}’s cache key in \sphinxcode{\sphinxupquote{main\_kernel}}’s hash

\item {} 
\sphinxAtStartPar
Merges \sphinxcode{\sphinxupquote{helper}}’s global variables into \sphinxcode{\sphinxupquote{main\_kernel}}’s

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{\PYGZus{}update\PYGZus{}hash}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{func}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{assert} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{func}\PYG{p}{,} \PYG{n}{JITCallable}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Check for conflicts in global variable values}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{n}{func}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{var\PYGZus{}name}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{k}
        \PYG{n}{v1}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}
        \PYG{n}{v2}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{func}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}
        \PYG{k}{if} \PYG{n}{v1} \PYG{o}{!=} \PYG{n}{v2}\PYG{p}{:}
            \PYG{k}{raise} \PYG{n+ne}{RuntimeError}\PYG{p}{(}
                \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Global variable }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{var\PYGZus{}name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ has value }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{v1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ in }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, }\PYG{l+s+s2}{\PYGZdq{}}
                \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{but }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{func}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ has conflicting value }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{v2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}
            \PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Merge dependencies}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{func}\PYG{o}{.}\PYG{n}{used\PYGZus{}global\PYGZus{}vals}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Update hash with called function\PYGZsq{}s hash}
    \PYG{n}{func\PYGZus{}key} \PYG{o}{=} \PYG{n}{func}\PYG{o}{.}\PYG{n}{cache\PYGZus{}key}
    \PYG{n}{func\PYGZus{}key} \PYG{o}{+}\PYG{o}{=} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n+nb}{getattr}\PYG{p}{(}\PYG{n}{func}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{noinline}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hasher}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{func\PYGZus{}key}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{utf\PYGZhy{}8}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py\#L99-L115}{jit.py:99\sphinxhyphen{}115}


\subsection{The JITFunction Class}
\label{\detokenize{triton-compiler/02-jit-decorator:the-jitfunction-class}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{JITFunction}} extends \sphinxcode{\sphinxupquote{JITCallable}} to add kernel launching:


\subsubsection{Initialization}
\label{\detokenize{triton-compiler/02-jit-decorator:initialization}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class}\PYG{+w}{ }\PYG{n+nc}{JITFunction}\PYG{p}{(}\PYG{n}{JITCallable}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{fn}\PYG{p}{,} \PYG{n}{version}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{do\PYGZus{}not\PYGZus{}specialize}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{do\PYGZus{}not\PYGZus{}specialize\PYGZus{}on\PYGZus{}alignment}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                 \PYG{n}{debug}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{noinline}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n+nb}{repr}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{launch\PYGZus{}metadata}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{fn}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Cache of compiled kernels}
        \PYG{c+c1}{\PYGZsh{} Key: (specialization, options) \PYGZhy{}\PYGZgt{} CompiledKernel}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cache} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{n+nb}{dict}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kernel\PYGZus{}key\PYGZus{}cache} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}

        \PYG{c+c1}{\PYGZsh{} Attributes}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fn}\PYG{o}{.}\PYG{n}{arg\PYGZus{}names} \PYG{o}{=} \PYG{p}{[}\PYG{n}{arg}\PYG{o}{.}\PYG{n}{name} \PYG{k}{for} \PYG{n}{arg} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{signature}\PYG{o}{.}\PYG{n}{parameters}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fn}\PYG{o}{.}\PYG{n}{divisibility} \PYG{o}{=} \PYG{l+m+mi}{16}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{do\PYGZus{}not\PYGZus{}specialize} \PYG{o}{=} \PYG{n}{do\PYGZus{}not\PYGZus{}specialize} \PYG{o+ow}{or} \PYG{p}{[}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{do\PYGZus{}not\PYGZus{}specialize\PYGZus{}on\PYGZus{}alignment} \PYG{o}{=} \PYG{n}{do\PYGZus{}not\PYGZus{}specialize\PYGZus{}on\PYGZus{}alignment} \PYG{o+ow}{or} \PYG{p}{[}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{debug} \PYG{o}{=} \PYG{n}{debug}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{noinline} \PYG{o}{=} \PYG{n}{noinline}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{launch\PYGZus{}metadata} \PYG{o}{=} \PYG{n}{launch\PYGZus{}metadata}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py\#L608-L625}{jit.py:608\sphinxhyphen{}625}

\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{cache}} dictionary stores compiled binaries, avoiding recompilation.


\subsubsection{Kernel Call Handling}
\label{\detokenize{triton-compiler/02-jit-decorator:kernel-call-handling}}
\sphinxAtStartPar
When you call a kernel like \sphinxcode{\sphinxupquote{kernel{[}grid{]}(...)}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}getitem\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{grid}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Returns a callable that can launch the kernel with the given grid.}

\PYG{l+s+sd}{    Usage: kernel[grid](args)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{k}{lambda} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{:} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}
        \PYG{n}{grid}\PYG{o}{=}\PYG{n}{grid}\PYG{p}{,}
        \PYG{o}{*}\PYG{n}{args}\PYG{p}{,}
        \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}
    \PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Then \sphinxcode{\sphinxupquote{run()}} handles:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Argument processing

\item {} 
\sphinxAtStartPar
Specialization (converting runtime values to compile\sphinxhyphen{}time constants)

\item {} 
\sphinxAtStartPar
Cache lookup

\item {} 
\sphinxAtStartPar
Compilation (if needed)

\item {} 
\sphinxAtStartPar
Kernel launch

\end{enumerate}


\subsubsection{Argument Specialization}
\label{\detokenize{triton-compiler/02-jit-decorator:argument-specialization}}
\sphinxAtStartPar
Triton can \sphinxstylestrong{specialize} kernels based on runtime argument values:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Dynamic specialization function}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{\PYGZus{}make\PYGZus{}specialization\PYGZus{}fn}\PYG{p}{(}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{kparams}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{specialization} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

    \PYG{k}{for} \PYG{n}{name}\PYG{p}{,} \PYG{n}{kp} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{sig}\PYG{o}{.}\PYG{n}{parameters}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{kparams}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{kp}\PYG{o}{.}\PYG{n}{is\PYGZus{}constexpr}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Constexpr: always specialized}
            \PYG{n}{specialization}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{(}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{constexpr}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Regular parameter: may specialize based on attributes}
            \PYG{n}{is\PYGZus{}const} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{True}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{kp}\PYG{o}{.}\PYG{n}{is\PYGZus{}const} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{False}\PYG{l+s+s1}{\PYGZsq{}}
            \PYG{n}{specialize} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{False}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{kp}\PYG{o}{.}\PYG{n}{do\PYGZus{}not\PYGZus{}specialize} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{True}\PYG{l+s+s1}{\PYGZsq{}}
            \PYG{n}{align} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{False}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{kp}\PYG{o}{.}\PYG{n}{do\PYGZus{}not\PYGZus{}specialize\PYGZus{}on\PYGZus{}alignment} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{True}\PYG{l+s+s1}{\PYGZsq{}}

            \PYG{n}{ret} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{specialize\PYGZus{}impl(backend, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{is\PYGZus{}const}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{specialize}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{align}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}

            \PYG{k}{if} \PYG{n}{kp}\PYG{o}{.}\PYG{n}{annotation\PYGZus{}type}\PYG{p}{:}
                \PYG{n}{specialization}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{(}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{kp}\PYG{o}{.}\PYG{n}{annotation\PYGZus{}type}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{,) + }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ret}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{[1:]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
            \PYG{k}{else}\PYG{p}{:}
                \PYG{n}{specialization}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ret}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Generate dynamic function}
    \PYG{n}{func\PYGZus{}body} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+s2}{def dynamic\PYGZus{}func(}\PYG{l+s+si}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{, }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{arg\PYGZus{}names}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{):}
\PYG{l+s+s2}{    params = }\PYG{l+s+se}{\PYGZob{}\PYGZob{}}\PYG{l+s+si}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{, }\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{+w}{ }\PYG{k}{for}\PYG{+w}{ }\PYG{n}{name}\PYG{+w}{ }\PYG{o+ow}{in}\PYG{+w}{ }\PYG{n}{sig}\PYG{o}{.}\PYG{n}{parameters}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZcb{}\PYGZcb{}}
\PYG{l+s+s2}{    specialization = [}\PYG{l+s+si}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{specialization}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{]}
\PYG{l+s+s2}{    return params, specialization, options}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{exec}\PYG{p}{(}\PYG{n}{func\PYGZus{}body}\PYG{p}{,} \PYG{n}{func\PYGZus{}namespace}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{func\PYGZus{}namespace}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dynamic\PYGZus{}func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py\#L393-L448}{jit.py:393\sphinxhyphen{}448}

\sphinxAtStartPar
\sphinxstylestrong{What gets specialized?}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.constexpr}} parameters (always)

\item {} 
\sphinxAtStartPar
Integer alignment (if \sphinxcode{\sphinxupquote{data\_ptr() \% 16 == 0}})

\item {} 
\sphinxAtStartPar
Tensor shapes and dtypes

\item {} 
\sphinxAtStartPar
Divisibility by 16

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{n}{ptr}\PYG{p}{,} \PYG{n}{N}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{pass}

\PYG{n}{kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{N}\PYG{o}{=}\PYG{l+m+mi}{1024}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Compiles with N=1024}
\PYG{n}{kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{N}\PYG{o}{=}\PYG{l+m+mi}{2048}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Recompiles with N=2048 (different binary!)}
\PYG{n}{kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{N}\PYG{o}{=}\PYG{l+m+mi}{1024}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Reuses first binary (cached)}
\end{sphinxVerbatim}


\subsection{Compilation Triggering}
\label{\detokenize{triton-compiler/02-jit-decorator:compilation-triggering}}
\sphinxAtStartPar
If no cached kernel exists, \sphinxcode{\sphinxupquote{JITFunction}} triggers compilation:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{run}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{n}{grid}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} ... argument processing ...}

    \PYG{c+c1}{\PYGZsh{} Compute specialization}
    \PYG{n}{params}\PYG{p}{,} \PYG{n}{specialization}\PYG{p}{,} \PYG{n}{options} \PYG{o}{=} \PYG{n}{specialization\PYGZus{}fn}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Compute cache key}
    \PYG{n}{cache\PYGZus{}key} \PYG{o}{=} \PYG{n}{compute\PYGZus{}cache\PYGZus{}key}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kernel\PYGZus{}key\PYGZus{}cache}\PYG{p}{,} \PYG{n}{specialization}\PYG{p}{,} \PYG{n}{options}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Look up in cache}
    \PYG{k}{if} \PYG{n}{cache\PYGZus{}key} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cache}\PYG{p}{:}
        \PYG{n}{kernel} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cache}\PYG{p}{[}\PYG{n}{cache\PYGZus{}key}\PYG{p}{]}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} COMPILE!}
        \PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{.}\PYG{n+nn}{.}\PYG{n+nn}{compiler}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n+nb}{compile}\PYG{p}{,} \PYG{n}{ASTSource}

        \PYG{c+c1}{\PYGZsh{} Create source representation}
        \PYG{n}{src} \PYG{o}{=} \PYG{n}{ASTSource}\PYG{p}{(}
            \PYG{n}{fn}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{p}{,}
            \PYG{n}{signature}\PYG{o}{=}\PYG{n}{specialized\PYGZus{}signature}\PYG{p}{,}
            \PYG{n}{constexprs}\PYG{o}{=}\PYG{n}{constexprs}\PYG{p}{,}
            \PYG{n}{attrs}\PYG{o}{=}\PYG{n}{attrs}
        \PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Trigger compilation}
        \PYG{n}{kernel} \PYG{o}{=} \PYG{n+nb}{compile}\PYG{p}{(}
            \PYG{n}{src}\PYG{o}{=}\PYG{n}{src}\PYG{p}{,}
            \PYG{n}{target}\PYG{o}{=}\PYG{n}{target}\PYG{p}{,}
            \PYG{n}{options}\PYG{o}{=}\PYG{n}{options}
        \PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Cache result}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cache}\PYG{p}{[}\PYG{n}{cache\PYGZus{}key}\PYG{p}{]} \PYG{o}{=} \PYG{n}{kernel}

    \PYG{c+c1}{\PYGZsh{} Launch kernel}
    \PYG{n}{kernel}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{n}{grid}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Compilation creates an \sphinxcode{\sphinxupquote{ASTSource}} object and passes it to the main \sphinxcode{\sphinxupquote{compile()}} function (covered in next sections).


\subsection{Kernel Metadata}
\label{\detokenize{triton-compiler/02-jit-decorator:kernel-metadata}}

\subsubsection{Launch Metadata}
\label{\detokenize{triton-compiler/02-jit-decorator:launch-metadata}}
\sphinxAtStartPar
You can attach metadata to kernels for profiling:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{\PYGZus{}matmul\PYGZus{}launch\PYGZus{}metadata}\PYG{p}{(}\PYG{n}{grid}\PYG{p}{,} \PYG{n}{kernel}\PYG{p}{,} \PYG{n}{args}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{M}\PYG{p}{,} \PYG{n}{N}\PYG{p}{,} \PYG{n}{K} \PYG{o}{=} \PYG{n}{args}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{M}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{args}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{args}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{K}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{k}{return} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{matmul [M=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{M}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, N=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{N}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, K=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{K}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{flops}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{M} \PYG{o}{*} \PYG{n}{N} \PYG{o}{*} \PYG{n}{K}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bytes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{p}{(}\PYG{n}{M\PYGZus{}K} \PYG{o}{+} \PYG{n}{K\PYGZus{}N} \PYG{o}{+} \PYG{n}{M\PYGZus{}N}\PYG{p}{)}
    \PYG{p}{\PYGZcb{}}

\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}\PYG{p}{(}\PYG{n}{launch\PYGZus{}metadata}\PYG{o}{=}\PYG{n}{\PYGZus{}matmul\PYGZus{}launch\PYGZus{}metadata}\PYG{p}{)}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{matmul\PYGZus{}kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{pass}
\end{sphinxVerbatim}

\sphinxAtStartPar
This metadata is passed to profilers like Triton Proton.


\subsubsection{Attributes and Hints}
\label{\detokenize{triton-compiler/02-jit-decorator:attributes-and-hints}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}\PYG{p}{(}
    \PYG{n}{noinline}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Prevent inlining}
    \PYG{n}{debug}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}     \PYG{c+c1}{\PYGZsh{} Enable debug mode}
\PYG{p}{)}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{pass}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{noinline}}: Forces function to be a separate call (not inlined)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{debug}}: Enables debugging features (assertions, bounds checking)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{repr}}: Custom repr() for the function

\end{itemize}


\subsection{Constexpr Parameters}
\label{\detokenize{triton-compiler/02-jit-decorator:constexpr-parameters}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.constexpr}} marks compile\sphinxhyphen{}time constants:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} BLOCK\PYGZus{}SIZE is known at compile time}
    \PYG{c+c1}{\PYGZsh{} Can be used for array bounds, loop limits, etc.}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} Unrolled!}
        \PYG{k}{pass}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Constexpr features:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Must be literal values or constexpr expressions

\item {} 
\sphinxAtStartPar
Can be used in control flow

\item {} 
\sphinxAtStartPar
Can be used for type annotations

\item {} 
\sphinxAtStartPar
Enables loop unrolling and constant folding

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Implementation:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{triton}\PYG{n+nn}{.}\PYG{n+nn}{language}\PYG{n+nn}{.}\PYG{n+nn}{core}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{constexpr}

\PYG{k}{class}\PYG{+w}{ }\PYG{n+nc}{constexpr}\PYG{p}{:}
    \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{value}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value} \PYG{o}{=} \PYG{n}{value}

    \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}repr\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{constexpr[}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{]}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Constexpr values are stored separately from regular arguments and directly embedded in the generated code.


\subsection{Summary}
\label{\detokenize{triton-compiler/02-jit-decorator:summary}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{@triton.jit}} decorator:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Wraps} the function in a \sphinxcode{\sphinxupquote{JITFunction}} object

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Extracts} source code and parses it into AST

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tracks} dependencies (global variables, called functions)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Generates} cache keys for compiled kernels

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Specializes} on runtime values (constexpr, alignment, types)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Triggers} compilation when needed

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Caches} compiled binaries

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Launches} kernels on the GPU

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Key files:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/jit.py}{jit.py} \sphinxhyphen{} Main JIT implementation

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/runtime/cache.py}{cache.py} \sphinxhyphen{} Kernel caching

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/compiler/compiler.py}{compiler.py} \sphinxhyphen{} Compilation orchestration

\end{itemize}

\sphinxAtStartPar
Next: \DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{03\sphinxhyphen{}code\sphinxhyphen{}generation}}} \sphinxhyphen{} How Python AST becomes Triton IR

\sphinxstepscope


\section{Compilation Pipeline: AST to GPU Binary}
\label{\detokenize{triton-compiler/03-compilation-pipeline:compilation-pipeline-ast-to-gpu-binary}}\label{\detokenize{triton-compiler/03-compilation-pipeline::doc}}
\sphinxAtStartPar
This document covers the complete compilation pipeline from Python AST through MLIR transformations to GPU binary.


\subsection{Code Generation: Python AST \sphinxhyphen{}\textgreater{} Triton IR}
\label{\detokenize{triton-compiler/03-compilation-pipeline:code-generation-python-ast-triton-ir}}
\sphinxAtStartPar
The code generator (\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/compiler/code\_generator.py}{code\_generator.py}) converts Python AST to Triton IR (TTIR).


\subsubsection{CodeGenerator Class}
\label{\detokenize{triton-compiler/03-compilation-pipeline:codegenerator-class}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class}\PYG{+w}{ }\PYG{n+nc}{CodeGenerator}\PYG{p}{(}\PYG{n}{ast}\PYG{o}{.}\PYG{n}{NodeVisitor}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{context}\PYG{p}{,} \PYG{n}{prototype}\PYG{p}{,} \PYG{n}{gscope}\PYG{p}{,} \PYG{n}{function\PYGZus{}name}\PYG{p}{,} \PYG{n}{jit\PYGZus{}fn}\PYG{p}{,} \PYG{o}{*}\PYG{p}{,} \PYG{n}{options}\PYG{p}{,} \PYG{n}{codegen\PYGZus{}fns}\PYG{p}{,}
                 \PYG{n}{module\PYGZus{}map}\PYG{p}{,} \PYG{n}{is\PYGZus{}kernel}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{context} \PYG{o}{=} \PYG{n}{context}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder} \PYG{o}{=} \PYG{n}{ir}\PYG{o}{.}\PYG{n}{builder}\PYG{p}{(}\PYG{n}{context}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} MLIR IR builder}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{semantic} \PYG{o}{=} \PYG{n}{TritonSemantic}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Triton semantics}

        \PYG{c+c1}{\PYGZsh{} Scope management}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lscope} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{\PYGZsh{} Local variables}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gscope} \PYG{o}{=} \PYG{n}{gscope}  \PYG{c+c1}{\PYGZsh{} Global variables}

        \PYG{c+c1}{\PYGZsh{} Type inference}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{function\PYGZus{}ret\PYGZus{}types} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{last\PYGZus{}ret\PYGZus{}type} \PYG{o}{=} \PYG{k+kc}{None}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/compiler/code\_generator.py\#L285-L300}{code\_generator.py:285\sphinxhyphen{}300}


\subsubsection{AST Visitor Pattern}
\label{\detokenize{triton-compiler/03-compilation-pipeline:ast-visitor-pattern}}
\sphinxAtStartPar
The generator uses the \sphinxstylestrong{Visitor pattern} to walk the Python AST:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{visit\PYGZus{}FunctionDef}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{node}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Generate IR for function definition.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} Extract arguments}
    \PYG{n}{arg\PYGZus{}types} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{arg} \PYG{o+ow}{in} \PYG{n}{node}\PYG{o}{.}\PYG{n}{args}\PYG{o}{.}\PYG{n}{args}\PYG{p}{:}
        \PYG{n}{arg\PYGZus{}types}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit\PYGZus{}annotation}\PYG{p}{(}\PYG{n}{arg}\PYG{o}{.}\PYG{n}{annotation}\PYG{p}{)}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Generate function signature}
    \PYG{n}{fn\PYGZus{}ty} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder}\PYG{o}{.}\PYG{n}{get\PYGZus{}function\PYGZus{}ty}\PYG{p}{(}\PYG{n}{arg\PYGZus{}types\PYGZus{}ir}\PYG{p}{,} \PYG{n}{ret\PYGZus{}types\PYGZus{}ir}\PYG{p}{)}
    \PYG{n}{fn} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder}\PYG{o}{.}\PYG{n}{create\PYGZus{}function}\PYG{p}{(}\PYG{n}{fn\PYGZus{}ty}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fn\PYGZus{}name}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Generate function body}
    \PYG{k}{for} \PYG{n}{stmt} \PYG{o+ow}{in} \PYG{n}{node}\PYG{o}{.}\PYG{n}{body}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{stmt}\PYG{p}{)}

\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{visit\PYGZus{}Assign}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{node}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Generate IR for assignment: x = expr\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} Visit right\PYGZhy{}hand side}
    \PYG{n}{value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{value}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Store in local scope}
    \PYG{k}{for} \PYG{n}{target} \PYG{o+ow}{in} \PYG{n}{node}\PYG{o}{.}\PYG{n}{targets}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{target}\PYG{p}{,} \PYG{n}{ast}\PYG{o}{.}\PYG{n}{Name}\PYG{p}{)}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lscope}\PYG{p}{[}\PYG{n}{target}\PYG{o}{.}\PYG{n}{id}\PYG{p}{]} \PYG{o}{=} \PYG{n}{value}

\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{visit\PYGZus{}BinOp}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{node}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Generate IR for binary operation: a + b\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{lhs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{left}\PYG{p}{)}
    \PYG{n}{rhs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{right}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Map Python operator to Triton operation}
    \PYG{k}{if} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{op}\PYG{p}{,} \PYG{n}{ast}\PYG{o}{.}\PYG{n}{Add}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder}\PYG{o}{.}\PYG{n}{create\PYGZus{}add}\PYG{p}{(}\PYG{n}{lhs}\PYG{p}{,} \PYG{n}{rhs}\PYG{p}{)}
    \PYG{k}{elif} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{op}\PYG{p}{,} \PYG{n}{ast}\PYG{o}{.}\PYG{n}{Mul}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder}\PYG{o}{.}\PYG{n}{create\PYGZus{}mul}\PYG{p}{(}\PYG{n}{lhs}\PYG{p}{,} \PYG{n}{rhs}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} ... more operators}
\end{sphinxVerbatim}


\subsubsection{Triton Language Primitives}
\label{\detokenize{triton-compiler/03-compilation-pipeline:triton-language-primitives}}
\sphinxAtStartPar
Special handling for \sphinxcode{\sphinxupquote{tl.*}} functions:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{visit\PYGZus{}Call}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{node}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Handle function calls.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{fn} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{func}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Handle tl.load}
    \PYG{k}{if} \PYG{n}{fn} \PYG{o}{==} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{:}
        \PYG{n}{ptr} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{args}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{mask} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{keywords}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mask}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{k}{if} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mask}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{in} \PYG{n}{node}\PYG{o}{.}\PYG{n}{keywords} \PYG{k}{else} \PYG{k+kc}{None}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder}\PYG{o}{.}\PYG{n}{create\PYGZus{}load}\PYG{p}{(}\PYG{n}{ptr}\PYG{p}{,} \PYG{n}{mask}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Handle tl.store}
    \PYG{k}{elif} \PYG{n}{fn} \PYG{o}{==} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{:}
        \PYG{n}{ptr} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{args}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{args}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{mask} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{keywords}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mask}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{k}{if} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mask}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{in} \PYG{n}{node}\PYG{o}{.}\PYG{n}{keywords} \PYG{k}{else} \PYG{k+kc}{None}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder}\PYG{o}{.}\PYG{n}{create\PYGZus{}store}\PYG{p}{(}\PYG{n}{ptr}\PYG{p}{,} \PYG{n}{value}\PYG{p}{,} \PYG{n}{mask}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Handle tl.program\PYGZus{}id}
    \PYG{k}{elif} \PYG{n}{fn} \PYG{o}{==} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{:}
        \PYG{n}{axis} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{args}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder}\PYG{o}{.}\PYG{n}{create\PYGZus{}get\PYGZus{}program\PYGZus{}id}\PYG{p}{(}\PYG{n}{axis}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Mapping table:}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Python
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Triton Operation
&\sphinxstyletheadfamily 
\sphinxAtStartPar
MLIR Op
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.load(ptr)}}
&
\sphinxAtStartPar
Load from memory
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt.load}}
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.store(ptr, val)}}
&
\sphinxAtStartPar
Store to memory
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt.store}}
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.program\_id(0)}}
&
\sphinxAtStartPar
Get block ID
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt.get\_program\_id}}
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.arange(0, N)}}
&
\sphinxAtStartPar
Create range
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt.make\_range}}
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tl.dot(a, b)}}
&
\sphinxAtStartPar
Matrix multiply
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt.dot}}
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{a + b}}
&
\sphinxAtStartPar
Addition
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{arith.addi}}
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{a \textless{} b}}
&
\sphinxAtStartPar
Comparison
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{arith.cmpi}}
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsubsection{Type Inference}
\label{\detokenize{triton-compiler/03-compilation-pipeline:type-inference}}
\sphinxAtStartPar
Triton infers types automatically:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{visit\PYGZus{}BinOp}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{node}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{lhs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{left}\PYG{p}{)}
    \PYG{n}{rhs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{visit}\PYG{p}{(}\PYG{n}{node}\PYG{o}{.}\PYG{n}{right}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Infer result type from operands}
    \PYG{n}{lhs\PYGZus{}ty} \PYG{o}{=} \PYG{n}{lhs}\PYG{o}{.}\PYG{n}{type}
    \PYG{n}{rhs\PYGZus{}ty} \PYG{o}{=} \PYG{n}{rhs}\PYG{o}{.}\PYG{n}{type}

    \PYG{c+c1}{\PYGZsh{} Promote types if needed (e.g., int32 + float32 \PYGZhy{}\PYGZgt{} float32)}
    \PYG{n}{result\PYGZus{}ty} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{promote\PYGZus{}types}\PYG{p}{(}\PYG{n}{lhs\PYGZus{}ty}\PYG{p}{,} \PYG{n}{rhs\PYGZus{}ty}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Cast operands if necessary}
    \PYG{n}{lhs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cast}\PYG{p}{(}\PYG{n}{lhs}\PYG{p}{,} \PYG{n}{result\PYGZus{}ty}\PYG{p}{)}
    \PYG{n}{rhs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cast}\PYG{p}{(}\PYG{n}{rhs}\PYG{p}{,} \PYG{n}{result\PYGZus{}ty}\PYG{p}{)}

    \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{builder}\PYG{o}{.}\PYG{n}{create\PYGZus{}add}\PYG{p}{(}\PYG{n}{lhs}\PYG{p}{,} \PYG{n}{rhs}\PYG{p}{,} \PYG{n}{result\PYGZus{}ty}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Example TTIR Output}
\label{\detokenize{triton-compiler/03-compilation-pipeline:example-ttir-output}}
\sphinxAtStartPar
For this kernel:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{add\PYGZus{}kernel}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{y\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{out\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{N}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{pid} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{offs} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{l+m+mi}{128} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{128}\PYG{p}{)}
    \PYG{n}{mask} \PYG{o}{=} \PYG{n}{offs} \PYG{o}{\PYGZlt{}} \PYG{n}{N}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offs}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{y\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offs}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{out\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offs}\PYG{p}{,} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Generated TTIR:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
module \PYGZob{}
  tt.func @add\PYGZus{}kernel(\PYGZpc{}x\PYGZus{}ptr: !tt.ptr\PYGZlt{}f32\PYGZgt{}, \PYGZpc{}y\PYGZus{}ptr: !tt.ptr\PYGZlt{}f32\PYGZgt{},
                      \PYGZpc{}out\PYGZus{}ptr: !tt.ptr\PYGZlt{}f32\PYGZgt{}) \PYGZob{}
    \PYGZpc{}c128 = arith.constant 128 : i32
    \PYGZpc{}pid = tt.get\PYGZus{}program\PYGZus{}id x : i32
    \PYGZpc{}offset\PYGZus{}base = arith.muli \PYGZpc{}pid, \PYGZpc{}c128 : i32

    \PYGZpc{}range = tt.make\PYGZus{}range \PYGZob{}end = 128 : i32, start = 0 : i32\PYGZcb{} : tensor\PYGZlt{}128xi32\PYGZgt{}
    \PYGZpc{}offset\PYGZus{}base\PYGZus{}splat = tt.splat \PYGZpc{}offset\PYGZus{}base : i32 \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128xi32\PYGZgt{}
    \PYGZpc{}offs = arith.addi \PYGZpc{}offset\PYGZus{}base\PYGZus{}splat, \PYGZpc{}range : tensor\PYGZlt{}128xi32\PYGZgt{}

    \PYGZpc{}N\PYGZus{}splat = tt.splat \PYGZpc{}N : i32 \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128xi32\PYGZgt{}
    \PYGZpc{}mask = arith.cmpi slt, \PYGZpc{}offs, \PYGZpc{}N\PYGZus{}splat : tensor\PYGZlt{}128xi32\PYGZgt{}

    \PYGZpc{}x\PYGZus{}ptr\PYGZus{}splat = tt.splat \PYGZpc{}x\PYGZus{}ptr : !tt.ptr\PYGZlt{}f32\PYGZgt{} \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    \PYGZpc{}x\PYGZus{}ptrs = tt.addptr \PYGZpc{}x\PYGZus{}ptr\PYGZus{}splat, \PYGZpc{}offs : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    \PYGZpc{}x = tt.load \PYGZpc{}x\PYGZus{}ptrs, \PYGZpc{}mask : tensor\PYGZlt{}128xf32\PYGZgt{}

    \PYGZpc{}y\PYGZus{}ptr\PYGZus{}splat = tt.splat \PYGZpc{}y\PYGZus{}ptr : !tt.ptr\PYGZlt{}f32\PYGZgt{} \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    \PYGZpc{}y\PYGZus{}ptrs = tt.addptr \PYGZpc{}y\PYGZus{}ptr\PYGZus{}splat, \PYGZpc{}offs : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    \PYGZpc{}y = tt.load \PYGZpc{}y\PYGZus{}ptrs, \PYGZpc{}mask : tensor\PYGZlt{}128xf32\PYGZgt{}

    \PYGZpc{}result = arith.addf \PYGZpc{}x, \PYGZpc{}y : tensor\PYGZlt{}128xf32\PYGZgt{}

    \PYGZpc{}out\PYGZus{}ptrs = tt.addptr \PYGZpc{}out\PYGZus{}ptr\PYGZus{}splat, \PYGZpc{}offs : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    tt.store \PYGZpc{}out\PYGZus{}ptrs, \PYGZpc{}result, \PYGZpc{}mask : tensor\PYGZlt{}128xf32\PYGZgt{}

    tt.return
  \PYGZcb{}
\PYGZcb{}
\end{sphinxVerbatim}


\subsection{MLIR Lowering: TTIR \sphinxhyphen{}\textgreater{} TTGIR \sphinxhyphen{}\textgreater{} LLIR}
\label{\detokenize{triton-compiler/03-compilation-pipeline:mlir-lowering-ttir-ttgir-llir}}
\sphinxAtStartPar
The MLIR pipeline applies a series of transformation passes.


\subsubsection{Compilation Orchestration}
\label{\detokenize{triton-compiler/03-compilation-pipeline:compilation-orchestration}}
\sphinxAtStartPar
Main compilation function (\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/compiler/compiler.py\#L226}{compiler.py:226}):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{compile}\PYG{p}{(}\PYG{n}{src}\PYG{p}{,} \PYG{n}{target}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{options}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Get backend for target (CUDA, ROCm, etc.)}
    \PYG{n}{backend} \PYG{o}{=} \PYG{n}{make\PYGZus{}backend}\PYG{p}{(}\PYG{n}{target}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Add compilation stages}
    \PYG{n}{stages} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{backend}\PYG{o}{.}\PYG{n}{add\PYGZus{}stages}\PYG{p}{(}\PYG{n}{stages}\PYG{p}{,} \PYG{n}{options}\PYG{p}{,} \PYG{n}{src}\PYG{o}{.}\PYG{n}{language}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Apply each stage in order}
    \PYG{k}{for} \PYG{n}{ext}\PYG{p}{,} \PYG{n}{compile\PYGZus{}ir} \PYG{o+ow}{in} \PYG{n}{stages}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{module} \PYG{o}{=} \PYG{n}{compile\PYGZus{}ir}\PYG{p}{(}\PYG{n}{module}\PYG{p}{,} \PYG{n}{metadata}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Return compiled kernel}
    \PYG{k}{return} \PYG{n}{CompiledKernel}\PYG{p}{(}\PYG{n}{src}\PYG{p}{,} \PYG{n}{metadata\PYGZus{}group}\PYG{p}{,} \PYG{n+nb}{hash}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Stages dictionary:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{stages} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ttir}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k}{lambda} \PYG{n}{module}\PYG{p}{,} \PYG{n}{metadata}\PYG{p}{:} \PYG{n}{module}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Input}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ttgir}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{ttir\PYGZus{}to\PYGZus{}ttgir}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} GPU\PYGZhy{}specific lowering}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{llir}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{ttgir\PYGZus{}to\PYGZus{}llir}\PYG{p}{,}      \PYG{c+c1}{\PYGZsh{} LLVM IR generation}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ptx}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{llir\PYGZus{}to\PYGZus{}ptx}\PYG{p}{,}         \PYG{c+c1}{\PYGZsh{} PTX assembly}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cubin}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{ptx\PYGZus{}to\PYGZus{}cubin}\PYG{p}{,}      \PYG{c+c1}{\PYGZsh{} Binary}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{TTIR \sphinxhyphen{}\textgreater{} TTGIR Transformation}
\label{\detokenize{triton-compiler/03-compilation-pipeline:ttir-ttgir-transformation}}
\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/third\_party/nvidia/backend/compiler.py}{third\_party/nvidia/backend/compiler.py}

\sphinxAtStartPar
\sphinxstylestrong{Key transformations:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Add GPU layout information} \sphinxhyphen{} Specify how data is distributed across threads

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Allocate shared memory} \sphinxhyphen{} Insert \sphinxcode{\sphinxupquote{tt.alloc\_tensor}} operations

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Insert synchronization} \sphinxhyphen{} Add \sphinxcode{\sphinxupquote{tt.barrier}} instructions

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Coalesce memory accesses} \sphinxhyphen{} Optimize memory access patterns

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use tensor cores} \sphinxhyphen{} Lower \sphinxcode{\sphinxupquote{tt.dot}} to tensor core operations

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{ttir\PYGZus{}to\PYGZus{}ttgir}\PYG{p}{(}\PYG{n}{module}\PYG{p}{,} \PYG{n}{metadata}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Apply MLIR passes}
    \PYG{n}{passes}\PYG{o}{.}\PYG{n}{ttir\PYGZus{}to\PYGZus{}ttgir}\PYG{p}{(}\PYG{n}{module}\PYG{p}{,}
                         \PYG{n}{num\PYGZus{}warps}\PYG{o}{=}\PYG{n}{metadata}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{num\PYGZus{}warps}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
                         \PYG{n}{num\PYGZus{}ctas}\PYG{o}{=}\PYG{n}{metadata}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{num\PYGZus{}ctas}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
                         \PYG{n}{capability}\PYG{o}{=}\PYG{n}{metadata}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{capability}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{module}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{C++ passes applied} (in \sphinxcode{\sphinxupquote{lib/Dialect/TritonGPU/}}):
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPUCoalesce}} \sphinxhyphen{} Coalesce memory accesses

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPURemoveLayoutConversions}} \sphinxhyphen{} Optimize layout changes

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPUAccelerateMatmul}} \sphinxhyphen{} Use tensor cores for matmul

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPUPipeline}} \sphinxhyphen{} Software pipelining for memory/compute overlap

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPUPrefetch}} \sphinxhyphen{} Insert prefetch instructions

\end{itemize}


\subsubsection{TTGIR \sphinxhyphen{}\textgreater{} LLVM IR}
\label{\detokenize{triton-compiler/03-compilation-pipeline:ttgir-llvm-ir}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{ttgir\PYGZus{}to\PYGZus{}llir}\PYG{p}{(}\PYG{n}{module}\PYG{p}{,} \PYG{n}{metadata}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Convert to LLVM IR}
    \PYG{n}{passes}\PYG{o}{.}\PYG{n}{convert\PYGZus{}ttgir\PYGZus{}to\PYGZus{}llir}\PYG{p}{(}\PYG{n}{module}\PYG{p}{,}
                                 \PYG{n}{target}\PYG{o}{=}\PYG{n}{metadata}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
                                 \PYG{n}{capability}\PYG{o}{=}\PYG{n}{metadata}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{capability}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{module}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Transformations:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt.load}} \sphinxhyphen{}\textgreater{} LLVM load instructions

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt.store}} \sphinxhyphen{}\textgreater{} LLVM store instructions

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt.dot}} \sphinxhyphen{}\textgreater{} NVVM intrinsics for tensor cores

\item {} 
\sphinxAtStartPar
Thread indexing \sphinxhyphen{}\textgreater{} \sphinxcode{\sphinxupquote{llvm.nvvm.read.ptx.sreg.*}} calls

\item {} 
\sphinxAtStartPar
Barriers \sphinxhyphen{}\textgreater{} \sphinxcode{\sphinxupquote{llvm.nvvm.barrier0()}}

\end{itemize}


\subsection{Backend Compilation}
\label{\detokenize{triton-compiler/03-compilation-pipeline:backend-compilation}}

\subsubsection{LLVM IR \sphinxhyphen{}\textgreater{} PTX}
\label{\detokenize{triton-compiler/03-compilation-pipeline:llvm-ir-ptx}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{llir\PYGZus{}to\PYGZus{}ptx}\PYG{p}{(}\PYG{n}{module}\PYG{p}{,} \PYG{n}{metadata}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Use LLVM backend to generate PTX}
    \PYG{n}{ptx} \PYG{o}{=} \PYG{n}{llvm}\PYG{o}{.}\PYG{n}{translate\PYGZus{}to\PYGZus{}asm}\PYG{p}{(}
        \PYG{n}{module}\PYG{p}{,}
        \PYG{n}{target}\PYG{o}{=}\PYG{n}{metadata}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{features}\PYG{o}{=}\PYG{n}{get\PYGZus{}features}\PYG{p}{(}\PYG{n}{metadata}\PYG{p}{)}
    \PYG{p}{)}
    \PYG{k}{return} \PYG{n}{ptx}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{LLVM generates PTX assembly:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kr}{.version}\PYG{+w}{ }\PYG{l+m}{8.0}
\PYG{k+kr}{.target}\PYG{+w}{ }\PYG{n+nv}{sm\PYGZus{}80}
\PYG{k+kr}{.address\PYGZus{}size}\PYG{+w}{ }\PYG{l+m}{64}

\PYG{k+kr}{.visible}\PYG{+w}{ }\PYG{k+kr}{.entry}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel}\PYG{p}{(}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{x\PYGZus{}ptr}\PYG{o}{,}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{y\PYGZus{}ptr}\PYG{o}{,}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{out\PYGZus{}ptr}
\PYG{p}{)}
\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.pred}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}p}\PYG{p}{\PYGZlt{}}\PYG{l+m}{2}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f}\PYG{p}{\PYGZlt{}}\PYG{l+m}{4}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.b32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r}\PYG{p}{\PYGZlt{}}\PYG{l+m}{8}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.b64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd}\PYG{p}{\PYGZlt{}}\PYG{l+m}{10}\PYG{p}{\PYGZgt{}}\PYG{p}{;}

\PYG{+w}{    }\PYG{k}{mov}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r1}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}tid.x}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{mov}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}ctaid.x}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{mul}\PYG{n+nv}{.lo.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r3}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{l+m}{128}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{add}\PYG{k+kt}{.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r1}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r1}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r3}\PYG{p}{;}

\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nv}{x\PYGZus{}ptr}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{cvt}\PYG{k+kt}{.s64}\PYG{k+kt}{.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r1}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{shl}\PYG{k+kt}{.b64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd3}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd2}\PYG{o}{,}\PYG{+w}{ }\PYG{l+m}{2}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{add}\PYG{k+kt}{.s64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd4}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd1}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd3}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.global}\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nv}{\PYGZpc{}rd4}\PYG{p}{]}\PYG{p}{;}

\PYG{+w}{    }\PYG{c}{// ... more PTX ...}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{PTX \sphinxhyphen{}\textgreater{} CUBIN}
\label{\detokenize{triton-compiler/03-compilation-pipeline:ptx-cubin}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{ptx\PYGZus{}to\PYGZus{}cubin}\PYG{p}{(}\PYG{n}{ptx}\PYG{p}{,} \PYG{n}{metadata}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Call ptxas (NVIDIA\PYGZsq{}s PTX assembler)}
    \PYG{n}{cubin} \PYG{o}{=} \PYG{n}{nvidia}\PYG{o}{.}\PYG{n}{compile\PYGZus{}ptx\PYGZus{}to\PYGZus{}cubin}\PYG{p}{(}
        \PYG{n}{ptx}\PYG{p}{,}
        \PYG{n}{arch}\PYG{o}{=}\PYG{n}{metadata}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{arch}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{options}\PYG{o}{=}\PYG{n}{metadata}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ptx\PYGZus{}options}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{p}{)}
    \PYG{k}{return} \PYG{n}{cubin}
\end{sphinxVerbatim}

\sphinxAtStartPar
Uses \sphinxcode{\sphinxupquote{ptxas}} subprocess:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ptxas\PYG{+w}{ }\PYGZhy{}\PYGZhy{}gpu\PYGZhy{}name\PYG{o}{=}sm\PYGZus{}80\PYG{+w}{ }\PYGZhy{}\PYGZhy{}output\PYGZhy{}file\PYG{+w}{ }kernel.cubin\PYG{+w}{ }kernel.ptx
\end{sphinxVerbatim}

\sphinxAtStartPar
The CUBIN file is a binary executable that can be loaded and run on the GPU.


\subsection{Caching Strategy}
\label{\detokenize{triton-compiler/03-compilation-pipeline:caching-strategy}}
\sphinxAtStartPar
Triton caches compiled kernels to avoid recompilation:


\subsubsection{Cache Key Components}
\label{\detokenize{triton-compiler/03-compilation-pipeline:cache-key-components}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Source hash} \sphinxhyphen{} SHA\sphinxhyphen{}256 of source code

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Specialization} \sphinxhyphen{} Constexpr values, alignments

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Options} \sphinxhyphen{} num\_warps, num\_stages, etc.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Environment} \sphinxhyphen{} CUDA version, PTX version

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Backend} \sphinxhyphen{} CUDA vs ROCm, compute capability

\end{enumerate}


\subsubsection{Cache Directory Structure}
\label{\detokenize{triton-compiler/03-compilation-pipeline:cache-directory-structure}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZti{}/.triton/cache/
|\PYGZhy{}\PYGZhy{} 7a3f2e1b.../         \PYGZsh{} Kernel hash
|   |\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.json   \PYGZsh{} Metadata
|   |\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.ttir   \PYGZsh{} Triton IR
|   |\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.ttgir  \PYGZsh{} Triton GPU IR
|   |\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.llir   \PYGZsh{} LLVM IR
|   |\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.ptx    \PYGZsh{} PTX assembly
|   +\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.cubin  \PYGZsh{} Compiled binary
+\PYGZhy{}\PYGZhy{} 9c4d6a2e.../
    +\PYGZhy{}\PYGZhy{} ...
\end{sphinxVerbatim}


\subsubsection{Cache Lookup}
\label{\detokenize{triton-compiler/03-compilation-pipeline:cache-lookup}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{compile}\PYG{p}{(}\PYG{n}{src}\PYG{p}{,} \PYG{n}{target}\PYG{p}{,} \PYG{n}{options}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Compute cache key}
    \PYG{n}{key} \PYG{o}{=} \PYG{n}{get\PYGZus{}cache\PYGZus{}key}\PYG{p}{(}\PYG{n}{src}\PYG{p}{,} \PYG{n}{backend}\PYG{p}{,} \PYG{n}{options}\PYG{p}{,} \PYG{n}{env\PYGZus{}vars}\PYG{p}{)}
    \PYG{n+nb}{hash} \PYG{o}{=} \PYG{n}{hashlib}\PYG{o}{.}\PYG{n}{sha256}\PYG{p}{(}\PYG{n}{key}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{hexdigest}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Check cache}
    \PYG{n}{fn\PYGZus{}cache\PYGZus{}manager} \PYG{o}{=} \PYG{n}{get\PYGZus{}cache\PYGZus{}manager}\PYG{p}{(}\PYG{n+nb}{hash}\PYG{p}{)}
    \PYG{n}{metadata\PYGZus{}path} \PYG{o}{=} \PYG{n}{fn\PYGZus{}cache\PYGZus{}manager}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{metadata.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{if} \PYG{n}{metadata\PYGZus{}path} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} Cache hit!}
        \PYG{k}{return} \PYG{n}{CompiledKernel}\PYG{p}{(}\PYG{n}{src}\PYG{p}{,} \PYG{n}{metadata\PYGZus{}group}\PYG{p}{,} \PYG{n+nb}{hash}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Cache miss \PYGZhy{} compile}
    \PYG{c+c1}{\PYGZsh{} ...}
\end{sphinxVerbatim}


\subsection{Performance Timeline}
\label{\detokenize{triton-compiler/03-compilation-pipeline:performance-timeline}}
\sphinxAtStartPar
Typical compilation times:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Stage
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Time
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Notes
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
AST Parsing
&
\sphinxAtStartPar
\textless{} 1ms
&
\sphinxAtStartPar
Very fast
\\
\sphinxhline
\sphinxAtStartPar
Code Generation
&
\sphinxAtStartPar
5\sphinxhyphen{}20ms
&
\sphinxAtStartPar
Python AST \sphinxhyphen{}\textgreater{} TTIR
\\
\sphinxhline
\sphinxAtStartPar
MLIR Passes
&
\sphinxAtStartPar
50\sphinxhyphen{}200ms
&
\sphinxAtStartPar
Optimization passes
\\
\sphinxhline
\sphinxAtStartPar
LLVM Backend
&
\sphinxAtStartPar
100\sphinxhyphen{}500ms
&
\sphinxAtStartPar
PTX generation
\\
\sphinxhline
\sphinxAtStartPar
PTX Assembly
&
\sphinxAtStartPar
100\sphinxhyphen{}300ms
&
\sphinxAtStartPar
ptxas invocation
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Total (cold)}
&
\sphinxAtStartPar
\sphinxstylestrong{250\sphinxhyphen{}1020ms}
&
\sphinxAtStartPar
First compilation
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Total (cached)}
&
\sphinxAtStartPar
\sphinxstylestrong{\textless{} 1ms}
&
\sphinxAtStartPar
Subsequent runs
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsection{Summary}
\label{\detokenize{triton-compiler/03-compilation-pipeline:summary}}
\sphinxAtStartPar
The compilation pipeline:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Python AST \sphinxhyphen{}\textgreater{} TTIR} \sphinxhyphen{} Code generator visits AST nodes

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{TTIR \sphinxhyphen{}\textgreater{} TTGIR} \sphinxhyphen{} Add GPU layout and memory hierarchy

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{TTGIR \sphinxhyphen{}\textgreater{} LLIR} \sphinxhyphen{} Lower to LLVM IR with GPU intrinsics

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{LLIR \sphinxhyphen{}\textgreater{} PTX} \sphinxhyphen{} LLVM backend generates assembly

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PTX \sphinxhyphen{}\textgreater{} CUBIN} \sphinxhyphen{} ptxas assembles binary

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Caching} \sphinxhyphen{} Store all intermediate representations

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Key files:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/compiler/code\_generator.py}{code\_generator.py} \sphinxhyphen{} AST \sphinxhyphen{}\textgreater{} TTIR

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/python/triton/compiler/compiler.py}{compiler.py} \sphinxhyphen{} Pipeline orchestration

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/third\_party/nvidia/backend/compiler.py}{third\_party/nvidia/backend/compiler.py} \sphinxhyphen{} NVIDIA backend

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/lib/Dialect/TritonGPU}{lib/Dialect/TritonGPU/} \sphinxhyphen{} MLIR passes (C++)

\end{itemize}

\sphinxAtStartPar
The compiler’s architecture allows for:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Extensibility} \sphinxhyphen{} Easy to add new backends

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Optimization} \sphinxhyphen{} Multiple passes at different levels

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Debugging} \sphinxhyphen{} Can inspect IR at each stage

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Performance} \sphinxhyphen{} Aggressive caching and specialization

\end{itemize}

\sphinxstepscope


\section{Triton vs CUDA C++: Compilation Comparison}
\label{\detokenize{triton-compiler/04-cuda-comparison:triton-vs-cuda-c-compilation-comparison}}\label{\detokenize{triton-compiler/04-cuda-comparison::doc}}
\sphinxAtStartPar
This document explains how Triton’s compilation pipeline relates to traditional CUDA C++ compilation, showing where they differ and where they converge.


\subsection{Overview: Two Paths, Same Destination}
\label{\detokenize{triton-compiler/04-cuda-comparison:overview-two-paths-same-destination}}
\sphinxAtStartPar
Both Triton and CUDA C++ ultimately produce the same binary artifacts that run on NVIDIA GPUs:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PTX (Parallel Thread Execution)} \sphinxhyphen{} GPU assembly language

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{CUBIN (CUDA Binary)} \sphinxhyphen{} GPU machine code

\end{itemize}

\sphinxAtStartPar
However, they take \sphinxstylestrong{completely different paths} to get there:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
CUDA C++ Path:
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
.cu file (C++ with CUDA extensions)
       down
[nvcc frontend] \PYGZhy{} Parse C++ and CUDA syntax
       down
CUDA C++ IR (NVIDIA proprietary)
       down
[cicc compiler] \PYGZhy{} NVIDIA\PYGZsq{}s internal compiler
       down
PTX assembly
       down
[ptxas assembler]
       down
CUBIN binary

Triton Path:
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
Python function (@triton.jit)
       down
Python AST (standard Python parser)
       down
Triton IR (TTIR) \PYGZhy{} MLIR\PYGZhy{}based
       down
Triton GPU IR (TTGIR) \PYGZhy{} MLIR\PYGZhy{}based
       down
LLVM IR \PYGZhy{} Standard LLVM
       down
[LLVM NVPTX backend]
       down
PTX assembly
       down
[ptxas assembler] \PYGZlt{}\PYGZhy{} Same tool as CUDA!
       down
CUBIN binary
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key insight:} Triton and CUDA C++ converge at PTX. From PTX onward, they use the \sphinxstylestrong{same NVIDIA tools} (ptxas).


\subsection{Traditional CUDA C++ Compilation}
\label{\detokenize{triton-compiler/04-cuda-comparison:traditional-cuda-c-compilation}}

\subsubsection{The NVCC Compiler}
\label{\detokenize{triton-compiler/04-cuda-comparison:the-nvcc-compiler}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{nvcc}} is NVIDIA’s proprietary compiler for CUDA C++.

\sphinxAtStartPar
\sphinxstylestrong{Example CUDA C++ kernel:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{// add.cu}
\PYG{k+kr}{\PYGZus{}\PYGZus{}global\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n}{add\PYGZus{}kernel}\PYG{p}{(}\PYG{k+kt}{float}\PYG{o}{*}\PYG{+w}{ }\PYG{n}{x}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{o}{*}\PYG{+w}{ }\PYG{n}{y}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{o}{*}\PYG{+w}{ }\PYG{n}{out}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{idx}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nb}{blockIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n+nb}{blockDim}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n+nb}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{idx}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{        }\PYG{n}{out}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{x}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{y}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Compilation command:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
nvcc\PYG{+w}{ }\PYGZhy{}arch\PYG{o}{=}sm\PYGZus{}80\PYG{+w}{ }add.cu\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }add.o
\end{sphinxVerbatim}


\subsubsection{NVCC Compilation Stages}
\label{\detokenize{triton-compiler/04-cuda-comparison:nvcc-compilation-stages}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{nvcc}} is actually a \sphinxstylestrong{driver} that orchestrates multiple tools:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Preprocessing} \sphinxhyphen{} Handle \sphinxcode{\sphinxupquote{\#include}}, \sphinxcode{\sphinxupquote{\#define}}, etc.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Expand macros and includes}
nvcc\PYG{+w}{ }\PYGZhy{}E\PYG{+w}{ }add.cu\PYG{+w}{ }\PYGZgt{}\PYG{+w}{ }add.i
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Frontend (cudafe++)} \sphinxhyphen{} Parse CUDA C++ syntax
\begin{itemize}
\item {} 
\sphinxAtStartPar
Separates device code (\sphinxcode{\sphinxupquote{\_\_global\_\_}}, \sphinxcode{\sphinxupquote{\_\_device\_\_}}) from host code

\item {} 
\sphinxAtStartPar
Generates CUDA C++ IR (proprietary format)

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Device compiler (cicc)} \sphinxhyphen{} NVIDIA’s internal compiler
\begin{itemize}
\item {} 
\sphinxAtStartPar
Optimizes device code

\item {} 
\sphinxAtStartPar
Generates PTX assembly

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PTX assembler (ptxas)} \sphinxhyphen{} Assembles PTX to CUBIN

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ptxas\PYG{+w}{ }\PYGZhy{}arch\PYG{o}{=}sm\PYGZus{}80\PYG{+w}{ }kernel.ptx\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }kernel.cubin
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Host compiler (g++/cl)} \sphinxhyphen{} Compiles CPU code
\begin{itemize}
\item {} 
\sphinxAtStartPar
Links with CUDA runtime library (\sphinxcode{\sphinxupquote{cudart}})

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Full pipeline visualization:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
add.cu
  down
[nvcc driver]
  |\PYGZhy{}\PYGZhy{}\PYGZgt{} [cudafe++] Device code extraction
  |     down
  |   [cicc] CUDA C++ \PYGZhy{}\PYGZgt{} PTX
  |     down
  |   [ptxas] PTX \PYGZhy{}\PYGZgt{} CUBIN
  |     down
  |   device.cubin
  |
  +\PYGZhy{}\PYGZhy{}\PYGZgt{} [g++/cl] Host code compilation
        down
      host.o
        down
[linker] Combine host + device
        down
    add.exe/add.out
\end{sphinxVerbatim}


\subsubsection{PTX Assembly Output}
\label{\detokenize{triton-compiler/04-cuda-comparison:ptx-assembly-output}}
\sphinxAtStartPar
For our CUDA C++ kernel, \sphinxcode{\sphinxupquote{nvcc}} generates PTX like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kr}{.version}\PYG{+w}{ }\PYG{l+m}{8.0}
\PYG{k+kr}{.target}\PYG{+w}{ }\PYG{n+nv}{sm\PYGZus{}80}
\PYG{k+kr}{.address\PYGZus{}size}\PYG{+w}{ }\PYG{l+m}{64}

\PYG{k+kr}{.visible}\PYG{+w}{ }\PYG{k+kr}{.entry}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel}\PYG{p}{(}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}0}\PYG{o}{,}\PYG{+w}{  }\PYG{c}{// float* x}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}1}\PYG{o}{,}\PYG{+w}{  }\PYG{c}{// float* y}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}2}\PYG{o}{,}\PYG{+w}{  }\PYG{c}{// float* out}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}3}\PYG{+w}{   }\PYG{c}{// int n}
\PYG{p}{)}
\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.pred}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}p}\PYG{p}{\PYGZlt{}}\PYG{l+m}{2}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f}\PYG{p}{\PYGZlt{}}\PYG{l+m}{3}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.b32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r}\PYG{p}{\PYGZlt{}}\PYG{l+m}{5}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.b64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd}\PYG{p}{\PYGZlt{}}\PYG{l+m}{7}\PYG{p}{\PYGZgt{}}\PYG{p}{;}

\PYG{+w}{    }\PYG{c}{// Load parameters}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}0}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd2}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}1}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd3}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}2}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}3}\PYG{p}{]}\PYG{p}{;}

\PYG{+w}{    }\PYG{c}{// Calculate thread index}
\PYG{+w}{    }\PYG{k}{mov}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}tid.x}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{mov}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r3}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}ctaid.x}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{mov}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r4}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}ntid.x}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{mad}\PYG{n+nv}{.lo.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r3}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r4}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{p}{;}\PYG{+w}{  }\PYG{c}{// idx = blockIdx.x * blockDim.x + threadIdx.x}

\PYG{+w}{    }\PYG{c}{// Bounds check}
\PYG{+w}{    }\PYG{k}{setp}\PYG{n+nv}{.ge.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}p1}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r1}\PYG{p}{;}
\PYG{+w}{    }\PYG{err}{@}\PYG{n+nv}{\PYGZpc{}p1}\PYG{+w}{ }\PYG{k}{bra}\PYG{+w}{ }\PYG{n+nv}{EXIT}\PYG{p}{;}

\PYG{+w}{    }\PYG{c}{// Pointer arithmetic and load}
\PYG{+w}{    }\PYG{k}{mul}\PYG{k+kp}{.wide}\PYG{k+kt}{.s32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd4}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{l+m}{4}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{add}\PYG{k+kt}{.s64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd5}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd1}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd4}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.global}\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nv}{\PYGZpc{}rd5}\PYG{p}{]}\PYG{p}{;}\PYG{+w}{  }\PYG{c}{// x[idx]}

\PYG{+w}{    }\PYG{k}{add}\PYG{k+kt}{.s64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd6}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd4}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.global}\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f2}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nv}{\PYGZpc{}rd6}\PYG{p}{]}\PYG{p}{;}\PYG{+w}{  }\PYG{c}{// y[idx]}

\PYG{+w}{    }\PYG{c}{// Addition}
\PYG{+w}{    }\PYG{k}{add}\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f3}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f1}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f2}\PYG{p}{;}

\PYG{+w}{    }\PYG{c}{// Store result}
\PYG{+w}{    }\PYG{k}{add}\PYG{k+kt}{.s64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd7}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd3}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd4}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{st}\PYG{k+kp}{.global}\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nv}{\PYGZpc{}rd7}\PYG{p}{]}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f3}\PYG{p}{;}\PYG{+w}{  }\PYG{c}{// out[idx] = x[idx] + y[idx]}

\PYG{n+nl}{EXIT:}
\PYG{+w}{    }\PYG{k}{ret}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Triton Compilation Revisited}
\label{\detokenize{triton-compiler/04-cuda-comparison:triton-compilation-revisited}}

\subsubsection{The LLVM Path}
\label{\detokenize{triton-compiler/04-cuda-comparison:the-llvm-path}}
\sphinxAtStartPar
Triton uses \sphinxstylestrong{open\sphinxhyphen{}source LLVM} instead of NVIDIA’s proprietary compiler.

\sphinxAtStartPar
\sphinxstylestrong{Equivalent Triton kernel:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{triton}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{triton}\PYG{n+nn}{.}\PYG{n+nn}{language}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{tl}

\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{add\PYGZus{}kernel}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{y\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{out\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{n\PYGZus{}elements}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{pid} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{offs} \PYG{o}{=} \PYG{n}{pid} \PYG{o}{*} \PYG{n}{BLOCK\PYGZus{}SIZE} \PYG{o}{+} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{)}
    \PYG{n}{mask} \PYG{o}{=} \PYG{n}{offs} \PYG{o}{\PYGZlt{}} \PYG{n}{n\PYGZus{}elements}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offs}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{y\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offs}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{out\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offs}\PYG{p}{,} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Compilation stages:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Python AST parsing} \sphinxhyphen{} Standard Python \sphinxcode{\sphinxupquote{ast.parse()}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Code generation} \sphinxhyphen{} Python AST \sphinxhyphen{}\textgreater{} Triton IR (MLIR)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GPU lowering} \sphinxhyphen{} TTIR \sphinxhyphen{}\textgreater{} TTGIR (add layouts, shared memory)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{LLVM lowering} \sphinxhyphen{} TTGIR \sphinxhyphen{}\textgreater{} LLVM IR

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{LLVM backend} \sphinxhyphen{} LLVM IR \sphinxhyphen{}\textgreater{} PTX (using NVPTX backend)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PTX assembler} \sphinxhyphen{} PTX \sphinxhyphen{}\textgreater{} CUBIN (using \sphinxstylestrong{same ptxas} as CUDA!)

\end{enumerate}


\subsubsection{LLVM IR Stage}
\label{\detokenize{triton-compiler/04-cuda-comparison:llvm-ir-stage}}
\sphinxAtStartPar
Before generating PTX, Triton produces LLVM IR:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c}{; LLVM IR for Triton add\PYGZus{}kernel (simplified)}
\PYG{k}{define}\PYG{+w}{ }\PYG{k}{void}\PYG{+w}{ }\PYG{n+nv+vg}{@add\PYGZus{}kernel}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}n}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{n+nl}{entry:}
\PYG{+w}{  }\PYG{c}{; Get thread/block IDs using NVVM intrinsics}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}tid}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{call}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv+vg}{@llvm.nvvm.read.ptx.sreg.tid.x}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}bid}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{call}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv+vg}{@llvm.nvvm.read.ptx.sreg.ctaid.x}\PYG{p}{(}\PYG{p}{)}

\PYG{+w}{  }\PYG{c}{; Calculate offset}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}block\PYGZus{}start}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{mul}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}bid}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m}{128}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}offset}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{add}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}block\PYGZus{}start}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}tid}

\PYG{+w}{  }\PYG{c}{; Bounds check}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}in\PYGZus{}bounds}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{icmp}\PYG{+w}{ }\PYG{k}{slt}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}n}
\PYG{+w}{  }\PYG{k}{br}\PYG{+w}{ }\PYG{k+kt}{i1}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}in\PYGZus{}bounds}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}load}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}exit}

\PYG{n+nl}{load:}
\PYG{+w}{  }\PYG{c}{; Pointer arithmetic}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}gep}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{getelementptr}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}gep}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{getelementptr}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}

\PYG{+w}{  }\PYG{c}{; Load values}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}val}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{load}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}gep}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{align}\PYG{+w}{ }\PYG{l+m}{4}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}val}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{load}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}gep}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{align}\PYG{+w}{ }\PYG{l+m}{4}

\PYG{+w}{  }\PYG{c}{; Compute}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}sum}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{fadd}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}val}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}val}

\PYG{+w}{  }\PYG{c}{; Store}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}gep}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{getelementptr}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}
\PYG{+w}{  }\PYG{k}{store}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}sum}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}gep}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{align}\PYG{+w}{ }\PYG{l+m}{4}
\PYG{+w}{  }\PYG{k}{br}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}exit}

\PYG{n+nl}{exit:}
\PYG{+w}{  }\PYG{k}{ret}\PYG{+w}{ }\PYG{k}{void}
\PYG{p}{\PYGZcb{}}

\PYG{c}{; NVVM intrinsics for GPU built\PYGZhy{}ins}
\PYG{k}{declare}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv+vg}{@llvm.nvvm.read.ptx.sreg.tid.x}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{declare}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv+vg}{@llvm.nvvm.read.ptx.sreg.ctaid.x}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{LLVM NVPTX Backend} \sphinxhyphen{} Converts LLVM IR to PTX

\sphinxAtStartPar
The NVPTX backend is part of open\sphinxhyphen{}source LLVM. It knows how to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Translate LLVM instructions to PTX instructions

\item {} 
\sphinxAtStartPar
Map NVVM intrinsics (\sphinxcode{\sphinxupquote{llvm.nvvm.*}}) to PTX special registers

\item {} 
\sphinxAtStartPar
Generate PTX directives (\sphinxcode{\sphinxupquote{.version}}, \sphinxcode{\sphinxupquote{.target}}, etc.)

\end{itemize}


\subsubsection{PTX Generated by Triton}
\label{\detokenize{triton-compiler/04-cuda-comparison:ptx-generated-by-triton}}
\sphinxAtStartPar
Triton’s LLVM backend generates PTX that looks \sphinxstylestrong{very similar} to CUDA C++:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kr}{.version}\PYG{+w}{ }\PYG{l+m}{8.0}
\PYG{k+kr}{.target}\PYG{+w}{ }\PYG{n+nv}{sm\PYGZus{}80}
\PYG{k+kr}{.address\PYGZus{}size}\PYG{+w}{ }\PYG{l+m}{64}

\PYG{k+kr}{.visible}\PYG{+w}{ }\PYG{k+kr}{.entry}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel}\PYG{p}{(}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}0}\PYG{o}{,}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}1}\PYG{o}{,}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}2}\PYG{o}{,}
\PYG{+w}{    }\PYG{k+kp}{.param}\PYG{+w}{ }\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}3}
\PYG{p}{)}
\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.pred}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}p}\PYG{p}{\PYGZlt{}}\PYG{l+m}{2}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.f32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}f}\PYG{p}{\PYGZlt{}}\PYG{l+m}{3}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.b32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r}\PYG{p}{\PYGZlt{}}\PYG{l+m}{5}\PYG{p}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k+kr}{.reg}\PYG{+w}{ }\PYG{k+kt}{.b64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd}\PYG{p}{\PYGZlt{}}\PYG{l+m}{7}\PYG{p}{\PYGZgt{}}\PYG{p}{;}

\PYG{+w}{    }\PYG{c}{// Nearly identical to CUDA C++ PTX!}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}0}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd2}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}1}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u64}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}rd3}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}2}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{ld}\PYG{k+kp}{.param}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r1}\PYG{o}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{k}{add}\PYG{n+nv}{\PYGZus{}kernel\PYGZus{}param\PYGZus{}3}\PYG{p}{]}\PYG{p}{;}

\PYG{+w}{    }\PYG{k}{mov}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r2}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}tid.x}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{mov}\PYG{k+kt}{.u32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}r3}\PYG{o}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}ctaid.x}\PYG{p}{;}
\PYG{+w}{    }\PYG{c}{// ... rest is similar ...}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Convergence Point: PTX and CUBIN}
\label{\detokenize{triton-compiler/04-cuda-comparison:convergence-point-ptx-and-cubin}}

\subsubsection{Same Tools, Same Artifacts}
\label{\detokenize{triton-compiler/04-cuda-comparison:same-tools-same-artifacts}}
\sphinxAtStartPar
\sphinxstylestrong{PTX Assembler (ptxas)}

\sphinxAtStartPar
Both CUDA C++ and Triton use \sphinxstylestrong{the same tool}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CUDA C++ (called by nvcc)}
ptxas\PYG{+w}{ }\PYGZhy{}arch\PYG{o}{=}sm\PYGZus{}80\PYG{+w}{ }cuda\PYGZus{}kernel.ptx\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }cuda\PYGZus{}kernel.cubin

\PYG{c+c1}{\PYGZsh{} Triton (called by Triton compiler)}
ptxas\PYG{+w}{ }\PYGZhy{}arch\PYG{o}{=}sm\PYGZus{}80\PYG{+w}{ }triton\PYGZus{}kernel.ptx\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }triton\PYGZus{}kernel.cubin
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{ptxas}} is NVIDIA’s proprietary assembler, distributed with CUDA Toolkit.

\sphinxAtStartPar
\sphinxstylestrong{CUBIN Binary Format}

\sphinxAtStartPar
The output \sphinxcode{\sphinxupquote{.cubin}} files have the \sphinxstylestrong{identical format} regardless of source:
\begin{itemize}
\item {} 
\sphinxAtStartPar
ELF binary format

\item {} 
\sphinxAtStartPar
GPU machine code for specific architecture (e.g., sm\_80)

\item {} 
\sphinxAtStartPar
Metadata (register usage, shared memory, etc.)

\item {} 
\sphinxAtStartPar
Relocatable or executable sections

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Binary Equivalence:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Inspect CUBIN from CUDA C++}
cuobjdump\PYG{+w}{ }\PYGZhy{}sass\PYG{+w}{ }cuda\PYGZus{}kernel.cubin

\PYG{c+c1}{\PYGZsh{} Inspect CUBIN from Triton}
cuobjdump\PYG{+w}{ }\PYGZhy{}sass\PYG{+w}{ }triton\PYGZus{}kernel.cubin
\end{sphinxVerbatim}

\sphinxAtStartPar
Both show the same SASS (low\sphinxhyphen{}level GPU assembly).


\subsection{Key Differences}
\label{\detokenize{triton-compiler/04-cuda-comparison:key-differences}}

\subsubsection{Source Language}
\label{\detokenize{triton-compiler/04-cuda-comparison:source-language}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Aspect
&\sphinxstyletheadfamily 
\sphinxAtStartPar
CUDA C++
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Triton
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Language
&
\sphinxAtStartPar
C++ with extensions
&
\sphinxAtStartPar
Python DSL
\\
\sphinxhline
\sphinxAtStartPar
Syntax
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\_\_global\_\_}}, etc.
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{@triton.jit}}
\\
\sphinxhline
\sphinxAtStartPar
Type System
&
\sphinxAtStartPar
C++ static types
&
\sphinxAtStartPar
Python + inference
\\
\sphinxhline
\sphinxAtStartPar
Memory Mgmt
&
\sphinxAtStartPar
Manual (\sphinxcode{\sphinxupquote{\_\_shared\_\_}})
&
\sphinxAtStartPar
Automatic
\\
\sphinxhline
\sphinxAtStartPar
Threading Model
&
\sphinxAtStartPar
SIMT (per\sphinxhyphen{}thread)
&
\sphinxAtStartPar
Block\sphinxhyphen{}level SPMD
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsubsection{Compiler Stack}
\label{\detokenize{triton-compiler/04-cuda-comparison:compiler-stack}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Stage
&\sphinxstyletheadfamily 
\sphinxAtStartPar
CUDA C++
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Triton
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Frontend
&
\sphinxAtStartPar
cudafe++ (proprietary)
&
\sphinxAtStartPar
Python AST (open)
\\
\sphinxhline
\sphinxAtStartPar
Mid\sphinxhyphen{}level IR
&
\sphinxAtStartPar
CUDA IR (proprietary)
&
\sphinxAtStartPar
MLIR (open)
\\
\sphinxhline
\sphinxAtStartPar
Low\sphinxhyphen{}level IR
&
\sphinxAtStartPar
NVIDIA internal
&
\sphinxAtStartPar
LLVM IR (open)
\\
\sphinxhline
\sphinxAtStartPar
Backend
&
\sphinxAtStartPar
cicc (proprietary)
&
\sphinxAtStartPar
LLVM NVPTX (open)
\\
\sphinxhline
\sphinxAtStartPar
Assembler
&
\sphinxAtStartPar
ptxas (NVIDIA)
&
\sphinxAtStartPar
ptxas (NVIDIA)
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Open vs Proprietary:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
CUDA C++: Most of the stack is \sphinxstylestrong{closed\sphinxhyphen{}source}

\item {} 
\sphinxAtStartPar
Triton: Everything up to PTX is \sphinxstylestrong{open\sphinxhyphen{}source} (except ptxas)

\end{itemize}


\subsubsection{Compilation Time}
\label{\detokenize{triton-compiler/04-cuda-comparison:compilation-time}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
CUDA C++ (nvcc):
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\PYGZhy{} First compile: 1\PYGZhy{}5 seconds (C++ parsing overhead)
\PYGZhy{} Incremental: Fast with proper build system
\PYGZhy{} JIT (NVRTC): 100\PYGZhy{}500ms runtime compilation

Triton:
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\PYGZhy{} First compile: 250\PYGZhy{}1000ms (MLIR + LLVM overhead)
\PYGZhy{} Cached: \PYGZlt{} 1ms (hash\PYGZhy{}based cache)
\PYGZhy{} Always JIT: No separate compilation step
\end{sphinxVerbatim}


\subsubsection{Optimization Levels}
\label{\detokenize{triton-compiler/04-cuda-comparison:optimization-levels}}
\sphinxAtStartPar
\sphinxstylestrong{CUDA C++ (nvcc):}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
nvcc\PYG{+w}{ }\PYGZhy{}O0\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} No optimization}
nvcc\PYG{+w}{ }\PYGZhy{}O1\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Basic optimization}
nvcc\PYG{+w}{ }\PYGZhy{}O2\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Default}
nvcc\PYG{+w}{ }\PYGZhy{}O3\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Aggressive (default for device code)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Triton:}

\sphinxAtStartPar
Triton doesn’t expose optimization levels. Instead:
\begin{itemize}
\item {} 
\sphinxAtStartPar
MLIR passes always run (coalescing, layout optimization)

\item {} 
\sphinxAtStartPar
LLVM optimization level is fixed (usually \sphinxhyphen{}O3)

\item {} 
\sphinxAtStartPar
User controls performance through kernel design (block size, etc.)

\end{itemize}


\subsection{Interoperability}
\label{\detokenize{triton-compiler/04-cuda-comparison:interoperability}}

\subsubsection{Can Triton and CUDA C++ Work Together?}
\label{\detokenize{triton-compiler/04-cuda-comparison:can-triton-and-cuda-c-work-together}}
\sphinxAtStartPar
\sphinxstylestrong{Yes!} Because they produce the same artifacts (CUBIN), you can:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Mix kernels in the same application}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} PyTorch example}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{torch}
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{cpp\PYGZus{}extension}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{load}

\PYG{c+c1}{\PYGZsh{} Load CUDA C++ kernel}
\PYG{n}{cuda\PYGZus{}kernel} \PYG{o}{=} \PYG{n}{load}\PYG{p}{(}\PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cuda\PYGZus{}add}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{sources}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{add.cu}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Use Triton kernel}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{triton}

\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{triton\PYGZus{}add}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{pass}

\PYG{c+c1}{\PYGZsh{} Use both in the same program!}
\PYG{n}{cuda\PYGZus{}kernel}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{out}\PYG{p}{)}
\PYG{n}{triton\PYGZus{}add}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{out}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Link compiled objects}

\sphinxAtStartPar
Triton kernels compile to \sphinxcode{\sphinxupquote{.cubin}} files that can be loaded by CUDA runtime:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{// C++ code loading Triton\PYGZhy{}compiled kernel}
\PYG{n}{CUmodule}\PYG{+w}{ }\PYG{k}{module}\PYG{p}{;}
\PYG{n}{cuModuleLoad}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{k}{module}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{triton\PYGZus{}kernel.cubin}\PYG{l+s}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}

\PYG{n}{CUfunction}\PYG{+w}{ }\PYG{n}{kernel}\PYG{p}{;}
\PYG{n}{cuModuleGetFunction}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{kernel}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{module}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{add\PYGZus{}kernel}\PYG{l+s}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}

\PYG{c+c1}{// Launch Triton kernel from C++!}
\PYG{n}{cuLaunchKernel}\PYG{p}{(}\PYG{n}{kernel}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{grid\PYGZus{}x}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{grid\PYGZus{}y}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{grid\PYGZus{}z}\PYG{p}{,}\PYG{+w}{ }\PYG{p}{.}\PYG{p}{.}\PYG{p}{.}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Share memory between kernels}

\sphinxAtStartPar
Both use CUDA unified memory or device pointers:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Allocate with PyTorch}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{1024}\PYG{p}{,} \PYG{n}{device}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cuda}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Use with CUDA C++ kernel}
\PYG{n}{cuda\PYGZus{}kernel}\PYG{o}{.}\PYG{n}{process}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Use with Triton kernel}
\PYG{n}{triton\PYGZus{}kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\end{sphinxVerbatim}

\end{enumerate}


\subsection{Why Triton Chose This Path}
\label{\detokenize{triton-compiler/04-cuda-comparison:why-triton-chose-this-path}}

\subsubsection{Advantages of LLVM Backend}
\label{\detokenize{triton-compiler/04-cuda-comparison:advantages-of-llvm-backend}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Open Source} \sphinxhyphen{} Entire stack (except ptxas) is open and modifiable

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Portable} \sphinxhyphen{} LLVM supports AMD (AMDGCN), Intel, ARM GPUs

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Modern} \sphinxhyphen{} MLIR provides better optimization infrastructure

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ecosystem} \sphinxhyphen{} Reuse LLVM tools (opt, llc, llvm\sphinxhyphen{}dis)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Research\sphinxhyphen{}Friendly} \sphinxhyphen{} Easy to experiment with new passes

\end{enumerate}


\subsubsection{Why Not Use NVCC?}
\label{\detokenize{triton-compiler/04-cuda-comparison:why-not-use-nvcc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
NVCC is \sphinxstylestrong{closed\sphinxhyphen{}source} \sphinxhyphen{} Can’t modify internals

\item {} 
\sphinxAtStartPar
NVCC is \sphinxstylestrong{NVIDIA\sphinxhyphen{}only} \sphinxhyphen{} Can’t target AMD, Intel

\item {} 
\sphinxAtStartPar
NVCC requires \sphinxstylestrong{C++ parsing} \sphinxhyphen{} Complex language frontend

\item {} 
\sphinxAtStartPar
NVCC is \sphinxstylestrong{hard to extend} \sphinxhyphen{} Adding new IR passes is difficult

\end{itemize}


\subsubsection{Trade\sphinxhyphen{}offs}
\label{\detokenize{triton-compiler/04-cuda-comparison:trade-offs}}
\sphinxAtStartPar
\sphinxstylestrong{Advantages of Triton’s approach:}

\sphinxAtStartPar
{[}{[}OK{]}{]} Full control over compilation pipeline
{[}{[}OK{]}{]} Easy to add new optimizations (MLIR passes)
{[}{[}OK{]}{]} Multi\sphinxhyphen{}vendor GPU support (NVIDIA, AMD)
{[}{[}OK{]}{]} Python\sphinxhyphen{}friendly (no C++ build complexity)
{[}{[}OK{]}{]} Reproducible builds (open toolchain)

\sphinxAtStartPar
\sphinxstylestrong{Disadvantages:}

\sphinxAtStartPar
{[}{[}FAIL{]}{]} Dependency on LLVM version
{[}{[}FAIL{]}{]} Can’t use some NVIDIA\sphinxhyphen{}specific optimizations (in cicc)
{[}{[}FAIL{]}{]} Still depends on proprietary ptxas
{[}{[}FAIL{]}{]} Compilation overhead from LLVM


\subsection{Compilation Artifacts Comparison}
\label{\detokenize{triton-compiler/04-cuda-comparison:compilation-artifacts-comparison}}

\subsubsection{File Types}
\label{\detokenize{triton-compiler/04-cuda-comparison:file-types}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Artifact
&\sphinxstyletheadfamily 
\sphinxAtStartPar
CUDA C++
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Triton
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Shared?
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Source
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.cu}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.py}}
&
\sphinxAtStartPar
No
\\
\sphinxhline
\sphinxAtStartPar
IR (high\sphinxhyphen{}level)
&
\sphinxAtStartPar
CUDA IR
&
\sphinxAtStartPar
TTIR/TTGIR (MLIR)
&
\sphinxAtStartPar
No
\\
\sphinxhline
\sphinxAtStartPar
IR (low\sphinxhyphen{}level)
&
\sphinxAtStartPar
NVIDIA internal
&
\sphinxAtStartPar
LLVM IR
&
\sphinxAtStartPar
No
\\
\sphinxhline
\sphinxAtStartPar
Assembly
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.ptx}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.ptx}}
&
\sphinxAtStartPar
\sphinxstylestrong{Yes}
\\
\sphinxhline
\sphinxAtStartPar
Binary
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.cubin}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.cubin}}
&
\sphinxAtStartPar
\sphinxstylestrong{Yes}
\\
\sphinxhline
\sphinxAtStartPar
Fat binary
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.fatbin}}
&
\sphinxAtStartPar
N/A
&
\sphinxAtStartPar
No
\\
\sphinxhline
\sphinxAtStartPar
Object
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.o}}
&
\sphinxAtStartPar
N/A
&
\sphinxAtStartPar
No
\\
\sphinxhline
\sphinxAtStartPar
Executable
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.exe/.out}}
&
\sphinxAtStartPar
N/A (Python runtime)
&
\sphinxAtStartPar
No
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsubsection{Example Directory Structures}
\label{\detokenize{triton-compiler/04-cuda-comparison:example-directory-structures}}
\sphinxAtStartPar
\sphinxstylestrong{CUDA C++ project:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cuda\PYGZus{}project/
|\PYGZhy{}\PYGZhy{} add.cu                    \PYGZsh{} Source code
|\PYGZhy{}\PYGZhy{} add.o                     \PYGZsh{} Compiled object
|\PYGZhy{}\PYGZhy{} add.ptx                   \PYGZsh{} PTX assembly (if \PYGZhy{}ptx flag)
|\PYGZhy{}\PYGZhy{} add.cubin                 \PYGZsh{} GPU binary (if \PYGZhy{}cubin flag)
+\PYGZhy{}\PYGZhy{} add.out                   \PYGZsh{} Final executable
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Triton project:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
triton\PYGZus{}project/
|\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.py             \PYGZsh{} Source code
+\PYGZhy{}\PYGZhy{} \PYGZti{}/.triton/cache/
    +\PYGZhy{}\PYGZhy{} 7a3f2e1b.../          \PYGZsh{} Cache directory (hash\PYGZhy{}based)
        |\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.ttir   \PYGZsh{} Triton IR
        |\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.ttgir  \PYGZsh{} Triton GPU IR
        |\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.llir   \PYGZsh{} LLVM IR
        |\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.ptx    \PYGZsh{} PTX assembly
        +\PYGZhy{}\PYGZhy{} add\PYGZus{}kernel.cubin  \PYGZsh{} GPU binary
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Notice:} Triton caches \sphinxstylestrong{all intermediate representations}, while CUDA C++ only keeps final artifacts by default.


\subsection{Inspecting Compilation Artifacts}
\label{\detokenize{triton-compiler/04-cuda-comparison:inspecting-compilation-artifacts}}

\subsubsection{PTX Inspection}
\label{\detokenize{triton-compiler/04-cuda-comparison:ptx-inspection}}
\sphinxAtStartPar
\sphinxstylestrong{From CUDA C++:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generate PTX}
nvcc\PYG{+w}{ }\PYGZhy{}ptx\PYG{+w}{ }\PYGZhy{}arch\PYG{o}{=}sm\PYGZus{}80\PYG{+w}{ }add.cu\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }add.ptx

\PYG{c+c1}{\PYGZsh{} View PTX}
cat\PYG{+w}{ }add.ptx
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{From Triton:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Find cached PTX}
find\PYG{+w}{ }\PYGZti{}/.triton/cache\PYG{+w}{ }\PYGZhy{}name\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}*.ptx\PYGZdq{}}\PYG{+w}{ }\PYG{p}{|}\PYG{+w}{ }head\PYG{+w}{ }\PYGZhy{}1

\PYG{c+c1}{\PYGZsh{} View PTX}
cat\PYG{+w}{ }\PYGZti{}/.triton/cache/7a3f2e1b.../add\PYGZus{}kernel.ptx
\end{sphinxVerbatim}


\subsubsection{CUBIN Inspection}
\label{\detokenize{triton-compiler/04-cuda-comparison:cubin-inspection}}
\sphinxAtStartPar
\sphinxstylestrong{Disassemble CUBIN to SASS:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Works for both CUDA and Triton!}
cuobjdump\PYG{+w}{ }\PYGZhy{}sass\PYG{+w}{ }kernel.cubin
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{View metadata:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cuobjdump\PYG{+w}{ }\PYGZhy{}elf\PYG{+w}{ }kernel.cubin
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Compare binaries:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generate from both sources}
nvcc\PYG{+w}{ }\PYGZhy{}cubin\PYG{+w}{ }add.cu\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }cuda.cubin
\PYG{c+c1}{\PYGZsh{} (Triton generates triton.cubin)}

\PYG{c+c1}{\PYGZsh{} Compare SASS}
diff\PYG{+w}{ }\PYGZlt{}\PYG{o}{(}cuobjdump\PYG{+w}{ }\PYGZhy{}sass\PYG{+w}{ }cuda.cubin\PYG{o}{)}\PYG{+w}{ }\PYGZlt{}\PYG{o}{(}cuobjdump\PYG{+w}{ }\PYGZhy{}sass\PYG{+w}{ }triton.cubin\PYG{o}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Often, the SASS is \sphinxstylestrong{nearly identical} for simple kernels!


\subsection{Summary}
\label{\detokenize{triton-compiler/04-cuda-comparison:summary}}

\subsubsection{Compilation Paths Compared}
\label{\detokenize{triton-compiler/04-cuda-comparison:compilation-paths-compared}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
CUDA C++:  .cu \PYGZhy{}\PYGZgt{} [nvcc] \PYGZhy{}\PYGZgt{} CUDA IR \PYGZhy{}\PYGZgt{} [cicc] \PYGZhy{}\PYGZgt{} PTX \PYGZhy{}\PYGZgt{} [ptxas] \PYGZhy{}\PYGZgt{} CUBIN
                            up                         up           up
                       Proprietary              Proprietary   Shared

Triton:    .py \PYGZhy{}\PYGZgt{} [AST] \PYGZhy{}\PYGZgt{} TTIR \PYGZhy{}\PYGZgt{} TTGIR \PYGZhy{}\PYGZgt{} LLVM IR \PYGZhy{}\PYGZgt{} [NVPTX] \PYGZhy{}\PYGZgt{} PTX \PYGZhy{}\PYGZgt{} [ptxas] \PYGZhy{}\PYGZgt{} CUBIN
                  up       up       up        up         up        up       up        up
                Open    Open    Open     Open      Open    Shared  Propr.   Shared
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Convergence:} Both paths produce \sphinxstylestrong{identical PTX and CUBIN formats}.


\subsubsection{Key Takeaways}
\label{\detokenize{triton-compiler/04-cuda-comparison:key-takeaways}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Same destination, different routes}
\begin{itemize}
\item {} 
\sphinxAtStartPar
CUDA C++: Proprietary NVIDIA toolchain

\item {} 
\sphinxAtStartPar
Triton: Open\sphinxhyphen{}source LLVM toolchain

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Binary compatibility}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Both produce standard PTX and CUBIN

\item {} 
\sphinxAtStartPar
Can mix kernels from both sources

\item {} 
\sphinxAtStartPar
Use same CUDA runtime APIs

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Trade\sphinxhyphen{}offs}
\begin{itemize}
\item {} 
\sphinxAtStartPar
CUDA C++: Mature, heavily optimized, NVIDIA\sphinxhyphen{}only

\item {} 
\sphinxAtStartPar
Triton: Flexible, portable, easier to extend

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Shared infrastructure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Both use \sphinxcode{\sphinxupquote{ptxas}} for final assembly

\item {} 
\sphinxAtStartPar
Both run on same CUDA driver

\item {} 
\sphinxAtStartPar
Both produce GPU binaries with identical format

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Inspection and debugging}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Same tools work for both (\sphinxcode{\sphinxupquote{cuobjdump}}, \sphinxcode{\sphinxupquote{nvprof}}, \sphinxcode{\sphinxupquote{nsight}})

\item {} 
\sphinxAtStartPar
PTX is human\sphinxhyphen{}readable assembly

\item {} 
\sphinxAtStartPar
CUBIN contains actual machine code

\end{itemize}

\end{enumerate}


\subsection{Further Reading}
\label{\detokenize{triton-compiler/04-cuda-comparison:further-reading}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.nvidia.com/cuda/parallel-thread-execution/}{PTX ISA Reference} \sphinxhyphen{} Official PTX documentation

\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/}{NVCC Documentation} \sphinxhyphen{} NVIDIA’s compiler guide

\item {} 
\sphinxAtStartPar
\sphinxhref{https://llvm.org/docs/NVPTXUsage.html}{LLVM NVPTX Backend} \sphinxhyphen{} LLVM’s PTX generator

\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.nvidia.com/cuda/cuda-binary-utilities/}{CUDA Binary Utilities} \sphinxhyphen{} cuobjdump and friends

\end{itemize}

\sphinxstepscope


\section{MLIR: Core Concepts and Triton Usage}
\label{\detokenize{triton-compiler/05-mlir-concepts:mlir-core-concepts-and-triton-usage}}\label{\detokenize{triton-compiler/05-mlir-concepts::doc}}
\sphinxAtStartPar
This document explains MLIR (Multi\sphinxhyphen{}Level Intermediate Representation), the compiler infrastructure that powers Triton’s compilation pipeline.


\subsection{What is MLIR?}
\label{\detokenize{triton-compiler/05-mlir-concepts:what-is-mlir}}
\sphinxAtStartPar
\sphinxstylestrong{MLIR (Multi\sphinxhyphen{}Level Intermediate Representation)} is a compiler infrastructure project that provides a flexible framework for building optimizing compilers.

\sphinxAtStartPar
\sphinxstylestrong{Created by:} Google (now part of LLVM project)

\sphinxAtStartPar
\sphinxstylestrong{Purpose:} Enable building domain\sphinxhyphen{}specific compilers with reusable components

\sphinxAtStartPar
\sphinxstylestrong{Key idea:} Instead of one “universal” IR, support multiple IRs (dialects) that can coexist and transform between each other.


\subsection{The Problem MLIR Solves}
\label{\detokenize{triton-compiler/05-mlir-concepts:the-problem-mlir-solves}}

\subsubsection{Traditional Compiler Limitations}
\label{\detokenize{triton-compiler/05-mlir-concepts:traditional-compiler-limitations}}
\sphinxAtStartPar
\sphinxstylestrong{LLVM IR} (the traditional choice) has limitations:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
High\PYGZhy{}level code (Python, TensorFlow, etc.)
        down
[MASSIVE LOWERING GAP] \PYGZlt{}\PYGZhy{} Information loss!
        down
LLVM IR (very low\PYGZhy{}level)
        down
Machine code
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Problems:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Information loss} \sphinxhyphen{} High\sphinxhyphen{}level semantics disappear immediately

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Optimization difficulty} \sphinxhyphen{} Hard to optimize after lowering

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Single abstraction level} \sphinxhyphen{} Can’t represent domain\sphinxhyphen{}specific concepts

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Vendor lock\sphinxhyphen{}in} \sphinxhyphen{} Different vendors create incompatible IRs

\end{enumerate}


\subsubsection{Example: Matrix Multiplication}
\label{\detokenize{triton-compiler/05-mlir-concepts:example-matrix-multiplication}}
\sphinxAtStartPar
\sphinxstylestrong{High\sphinxhyphen{}level code:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{C} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{B}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Semantic: \PYGZdq{}matrix multiplication\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Direct lowering to LLVM IR loses meaning:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c}{; LLVM IR \PYGZhy{} just loops and arithmetic!}
\PYG{c}{; Lost information: \PYGZdq{}this is a matrix multiply\PYGZdq{}}
\PYG{err}{f}\PYG{k}{or}\PYG{+w}{ }\PYG{err}{i}\PYG{+w}{ }\PYG{err}{i}\PYG{err}{n}\PYG{+w}{ }\PYG{err}{r}\PYG{err}{o}\PYG{err}{w}\PYG{err}{s}\PYG{p}{(}\PYG{err}{A}\PYG{p}{)}\PYG{err}{:}
\PYG{+w}{    }\PYG{err}{f}\PYG{k}{or}\PYG{+w}{ }\PYG{err}{j}\PYG{+w}{ }\PYG{err}{i}\PYG{err}{n}\PYG{+w}{ }\PYG{err}{c}\PYG{err}{o}\PYG{err}{l}\PYG{err}{s}\PYG{p}{(}\PYG{err}{B}\PYG{p}{)}\PYG{err}{:}
\PYG{+w}{        }\PYG{err}{f}\PYG{k}{or}\PYG{+w}{ }\PYG{err}{k}\PYG{+w}{ }\PYG{err}{i}\PYG{err}{n}\PYG{+w}{ }\PYG{err}{c}\PYG{err}{o}\PYG{err}{l}\PYG{err}{s}\PYG{p}{(}\PYG{err}{A}\PYG{p}{)}\PYG{err}{:}
\PYG{+w}{            }\PYG{err}{C}\PYG{p}{[}\PYG{err}{i}\PYG{p}{]}\PYG{p}{[}\PYG{err}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{err}{+}\PYG{p}{=}\PYG{+w}{ }\PYG{err}{A}\PYG{p}{[}\PYG{err}{i}\PYG{p}{]}\PYG{p}{[}\PYG{err}{k}\PYG{p}{]}\PYG{+w}{ }\PYG{p}{*}\PYG{+w}{ }\PYG{err}{B}\PYG{p}{[}\PYG{err}{k}\PYG{p}{]}\PYG{p}{[}\PYG{err}{j}\PYG{p}{]}\PYG{+w}{  }\PYG{err}{\PYGZsh{}}\PYG{+w}{ }\PYG{err}{G}\PYG{err}{e}\PYG{err}{n}\PYG{err}{e}\PYG{err}{r}\PYG{err}{i}\PYG{k}{c}\PYG{+w}{ }\PYG{err}{l}\PYG{err}{o}\PYG{err}{o}\PYG{err}{p}\PYG{err}{s}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{With MLIR, preserve semantics longer:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
// High\PYGZhy{}level: Linalg dialect preserves \PYGZdq{}matrix multiply\PYGZdq{} operation
linalg.matmul ins(\PYGZpc{}A, \PYGZpc{}B) outs(\PYGZpc{}C)

// Mid\PYGZhy{}level: Affine dialect with loop structure
affine.for \PYGZpc{}i in [0, M):
    affine.for \PYGZpc{}j in [0, N):
        affine.for \PYGZpc{}k in [0, K):
            // ...

// Low\PYGZhy{}level: LLVM dialect
llvm.mul \PYGZpc{}a, \PYGZpc{}b
\end{sphinxVerbatim}

\sphinxAtStartPar
This \sphinxstylestrong{gradual lowering} preserves optimization opportunities at each level.


\subsubsection{MLIR Philosophy}
\label{\detokenize{triton-compiler/05-mlir-concepts:mlir-philosophy}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Traditional Compiler:
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
Source \PYGZhy{}\PYGZgt{} [BIG STEP] \PYGZhy{}\PYGZgt{} LLVM IR \PYGZhy{}\PYGZgt{} Binary
                up
        Loss of information!

MLIR Compiler:
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
Source \PYGZhy{}\PYGZgt{} Dialect1 \PYGZhy{}\PYGZgt{} Dialect2 \PYGZhy{}\PYGZgt{} ... \PYGZhy{}\PYGZgt{} DialectN \PYGZhy{}\PYGZgt{} Binary
         down         down         down         down
      Small, incremental lowering steps
      (Preserve information as long as possible)
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key benefits:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
{[}{[}OK{]}{]} Gradual lowering preserves semantics

\item {} 
\sphinxAtStartPar
{[}{[}OK{]}{]} Multiple abstraction levels coexist

\item {} 
\sphinxAtStartPar
{[}{[}OK{]}{]} Reusable optimization passes

\item {} 
\sphinxAtStartPar
{[}{[}OK{]}{]} Domain\sphinxhyphen{}specific dialects

\end{itemize}


\subsection{Core MLIR Concepts}
\label{\detokenize{triton-compiler/05-mlir-concepts:core-mlir-concepts}}

\subsubsection{1. Dialects}
\label{\detokenize{triton-compiler/05-mlir-concepts:dialects}}
\sphinxAtStartPar
A \sphinxstylestrong{dialect} is a namespace for operations, types, and attributes.

\sphinxAtStartPar
\sphinxstylestrong{Think of dialects as “sublanguages” within MLIR.}


\paragraph{Common Dialects}
\label{\detokenize{triton-compiler/05-mlir-concepts:common-dialects}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Dialect
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Purpose
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Abstraction Level
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{linalg}}
&
\sphinxAtStartPar
Linear algebra operations
&
\sphinxAtStartPar
High\sphinxhyphen{}level
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tensor}}
&
\sphinxAtStartPar
Tensor operations
&
\sphinxAtStartPar
High\sphinxhyphen{}level
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{scf}}
&
\sphinxAtStartPar
Structured control flow (for, while, if)
&
\sphinxAtStartPar
Mid\sphinxhyphen{}level
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{affine}}
&
\sphinxAtStartPar
Polyhedral loop optimizations
&
\sphinxAtStartPar
Mid\sphinxhyphen{}level
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{arith}}
&
\sphinxAtStartPar
Arithmetic operations (add, mul, etc.)
&
\sphinxAtStartPar
Low\sphinxhyphen{}level
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{llvm}}
&
\sphinxAtStartPar
LLVM IR operations
&
\sphinxAtStartPar
Very low\sphinxhyphen{}level
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{gpu}}
&
\sphinxAtStartPar
Generic GPU operations
&
\sphinxAtStartPar
GPU\sphinxhyphen{}specific
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{nvgpu}}
&
\sphinxAtStartPar
NVIDIA GPU\sphinxhyphen{}specific operations
&
\sphinxAtStartPar
NVIDIA\sphinxhyphen{}specific
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\paragraph{Triton’s Custom Dialects}
\label{\detokenize{triton-compiler/05-mlir-concepts:triton-s-custom-dialects}}
\sphinxAtStartPar
Triton defines its own dialects:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Dialect
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Purpose
&\sphinxstyletheadfamily 
\sphinxAtStartPar
File Location
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{tt}}
&
\sphinxAtStartPar
Triton dialect (TTIR)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{lib/Dialect/Triton/}}
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{ttg}}
&
\sphinxAtStartPar
Triton GPU dialect (TTGIR)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{lib/Dialect/TritonGPU/}}
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{ttng}}
&
\sphinxAtStartPar
Triton NVIDIA GPU dialect
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{lib/Dialect/TritonNvidiaGPU/}}
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Example:} Mixing dialects in one function

\begin{sphinxVerbatim}[commandchars=\\\{\}]
func.func @example(\PYGZpc{}arg0: tensor\PYGZlt{}128xf32\PYGZgt{}) \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128xf32\PYGZgt{} \PYGZob{}
    \PYGZpc{}c0 = arith.constant 0 : index        // arith dialect
    \PYGZpc{}c128 = arith.constant 128 : index

    \PYGZpc{}0 = scf.for \PYGZpc{}i = \PYGZpc{}c0 to \PYGZpc{}c128        // scf dialect
        iter\PYGZus{}args(\PYGZpc{}arg = \PYGZpc{}arg0) \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128xf32\PYGZgt{} \PYGZob{}
        \PYGZpc{}1 = linalg.generic \PYGZob{} ... \PYGZcb{}       // linalg dialect
        scf.yield \PYGZpc{}1
    \PYGZcb{}

    return \PYGZpc{}0 : tensor\PYGZlt{}128xf32\PYGZgt{}
\PYGZcb{}
\end{sphinxVerbatim}


\subsubsection{2. Operations}
\label{\detokenize{triton-compiler/05-mlir-concepts:operations}}
\sphinxAtStartPar
\sphinxstylestrong{Operations} (ops) are the fundamental unit of computation in MLIR.


\paragraph{Operation Anatomy}
\label{\detokenize{triton-compiler/05-mlir-concepts:operation-anatomy}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZpc{}result = dialect.operation(\PYGZpc{}operand1, \PYGZpc{}operand2) \PYGZob{}attribute = value\PYGZcb{} : type

// Example:
\PYGZpc{}sum = arith.addi \PYGZpc{}a, \PYGZpc{}b : i32
//  up       up      up   up    up
// result  op   operands  type
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Components:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Opcode:} \sphinxcode{\sphinxupquote{dialect.operation}} (e.g., \sphinxcode{\sphinxupquote{arith.addi}})

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Operands:} Input values (SSA form)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Results:} Output values (SSA form)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Attributes:} Compile\sphinxhyphen{}time constants (metadata)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Types:} Data types of operands and results

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Regions:} Nested code blocks (optional)

\end{itemize}


\paragraph{SSA Form (Static Single Assignment)}
\label{\detokenize{triton-compiler/05-mlir-concepts:ssa-form-static-single-assignment}}
\sphinxAtStartPar
Every value is defined \sphinxstylestrong{exactly once}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
// Valid SSA:
\PYGZpc{}0 = arith.constant 5 : i32
\PYGZpc{}1 = arith.constant 10 : i32
\PYGZpc{}2 = arith.addi \PYGZpc{}0, \PYGZpc{}1 : i32  // \PYGZpc{}2 defined once

// Invalid (not SSA):
\PYGZpc{}x = arith.constant 5 : i32
\PYGZpc{}x = arith.addi \PYGZpc{}x, \PYGZpc{}x : i32  // ERROR: \PYGZpc{}x redefined!
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Benefits:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Simplifies optimization (no aliasing confusion)

\item {} 
\sphinxAtStartPar
Easier data flow analysis

\item {} 
\sphinxAtStartPar
Natural for functional\sphinxhyphen{}style transformations

\end{itemize}


\paragraph{Operation Examples}
\label{\detokenize{triton-compiler/05-mlir-concepts:operation-examples}}
\sphinxAtStartPar
\sphinxstylestrong{Arithmetic:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZpc{}sum = arith.addi \PYGZpc{}a, \PYGZpc{}b : i32           // Integer addition
\PYGZpc{}prod = arith.mulf \PYGZpc{}x, \PYGZpc{}y : f32          // Float multiplication
\PYGZpc{}cmp = arith.cmpi slt, \PYGZpc{}a, \PYGZpc{}b : i32      // Compare: a \PYGZlt{} b
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Triton\sphinxhyphen{}specific:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZpc{}pid = tt.get\PYGZus{}program\PYGZus{}id x : i32         // Get block ID
\PYGZpc{}range = tt.make\PYGZus{}range \PYGZob{}start=0, end=128\PYGZcb{} : tensor\PYGZlt{}128xi32\PYGZgt{}
\PYGZpc{}ptr = tt.addptr \PYGZpc{}base, \PYGZpc{}offset : !tt.ptr\PYGZlt{}f32\PYGZgt{}
\PYGZpc{}data = tt.load \PYGZpc{}ptr, \PYGZpc{}mask : tensor\PYGZlt{}128xf32\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Control flow:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
scf.if \PYGZpc{}condition \PYGZob{}
    // true branch
\PYGZcb{} else \PYGZob{}
    // false branch
\PYGZcb{}

scf.for \PYGZpc{}i = \PYGZpc{}lb to \PYGZpc{}ub step \PYGZpc{}step \PYGZob{}
    // loop body
\PYGZcb{}
\end{sphinxVerbatim}


\subsubsection{3. Regions and Blocks}
\label{\detokenize{triton-compiler/05-mlir-concepts:regions-and-blocks}}

\paragraph{Regions}
\label{\detokenize{triton-compiler/05-mlir-concepts:regions}}
\sphinxAtStartPar
A \sphinxstylestrong{region} is a container for code, similar to a scope.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
scf.if \PYGZpc{}cond \PYGZob{}
    // \PYGZlt{}\PYGZhy{} This is a region
    \PYGZpc{}x = arith.constant 1 : i32
\PYGZcb{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Properties:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Isolated scope (can have local SSA values)

\item {} 
\sphinxAtStartPar
Can contain multiple basic blocks

\item {} 
\sphinxAtStartPar
Used for control flow (if, for, while, functions)

\end{itemize}


\paragraph{Blocks}
\label{\detokenize{triton-compiler/05-mlir-concepts:blocks}}
\sphinxAtStartPar
A \sphinxstylestrong{block} is a sequence of operations with a single entry point.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZca{}bb0(\PYGZpc{}arg0: i32, \PYGZpc{}arg1: i32):  // Block with arguments
    \PYGZpc{}sum = arith.addi \PYGZpc{}arg0, \PYGZpc{}arg1 : i32
    cf.br \PYGZca{}bb1(\PYGZpc{}sum : i32)      // Branch to next block

\PYGZca{}bb1(\PYGZpc{}result: i32):
    return \PYGZpc{}result : i32
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key properties:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Basic block (no control flow within the block)

\item {} 
\sphinxAtStartPar
Can have block arguments (like function parameters)

\item {} 
\sphinxAtStartPar
Ends with a terminator (return, branch, etc.)

\end{itemize}


\paragraph{Example with Regions and Blocks}
\label{\detokenize{triton-compiler/05-mlir-concepts:example-with-regions-and-blocks}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
func.func @example(\PYGZpc{}n: i32) \PYGZhy{}\PYGZgt{} i32 \PYGZob{}
    // Function body is a region with one block

    \PYGZpc{}c0 = arith.constant 0 : i32
    \PYGZpc{}c1 = arith.constant 1 : i32

    \PYGZpc{}result = scf.for \PYGZpc{}i = \PYGZpc{}c0 to \PYGZpc{}n step \PYGZpc{}c1
        iter\PYGZus{}args(\PYGZpc{}acc = \PYGZpc{}c0) \PYGZhy{}\PYGZgt{} i32 \PYGZob{}
        // \PYGZlt{}\PYGZhy{} For loop body is a region

        \PYGZpc{}new\PYGZus{}acc = arith.addi \PYGZpc{}acc, \PYGZpc{}i : i32
        scf.yield \PYGZpc{}new\PYGZus{}acc : i32  // Yield to next iteration
    \PYGZcb{}

    return \PYGZpc{}result : i32
\PYGZcb{}
\end{sphinxVerbatim}


\subsubsection{4. Types}
\label{\detokenize{triton-compiler/05-mlir-concepts:types}}
\sphinxAtStartPar
MLIR has a \sphinxstylestrong{flexible type system} that dialects can extend.


\paragraph{Built\sphinxhyphen{}in Types}
\label{\detokenize{triton-compiler/05-mlir-concepts:built-in-types}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
// Integers
i1                      // 1\PYGZhy{}bit (boolean)
i8, i16, i32, i64       // Signed integers
ui8, ui16, ui32         // Unsigned integers

// Floats
f16, f32, f64           // IEEE floats
bf16                    // bfloat16

// Vectors and tensors
vector\PYGZlt{}4xf32\PYGZgt{}           // 4\PYGZhy{}element float vector
tensor\PYGZlt{}128x128xf32\PYGZgt{}     // 2D tensor
tensor\PYGZlt{}?x?xf32\PYGZgt{}         // Dynamic\PYGZhy{}shape tensor

// Pointers
!llvm.ptr\PYGZlt{}f32\PYGZgt{}          // LLVM pointer to float

// Functions
(i32, i32) \PYGZhy{}\PYGZgt{} i32       // Function type
\end{sphinxVerbatim}


\paragraph{Triton Custom Types}
\label{\detokenize{triton-compiler/05-mlir-concepts:triton-custom-types}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
!tt.ptr\PYGZlt{}f32\PYGZgt{}                    // Triton pointer
!tt.ptr\PYGZlt{}tensor\PYGZlt{}128xf32\PYGZgt{}\PYGZgt{}        // Pointer to tensor
tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}        // Tensor of pointers
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Why custom types?}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Triton pointers have different semantics than LLVM pointers

\item {} 
\sphinxAtStartPar
Support block/tensor operations naturally

\item {} 
\sphinxAtStartPar
Enable Triton\sphinxhyphen{}specific optimizations

\end{itemize}


\paragraph{Type Conversion}
\label{\detokenize{triton-compiler/05-mlir-concepts:type-conversion}}
\sphinxAtStartPar
Types change as you lower between dialects:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
// High\PYGZhy{}level (Triton)
\PYGZpc{}data : tensor\PYGZlt{}128xf32\PYGZgt{}

// Mid\PYGZhy{}level (TritonGPU) \PYGZhy{} add layout
\PYGZpc{}data : tensor\PYGZlt{}128xf32, \PYGZsh{}ttg.blocked\PYGZlt{}\PYGZob{}threads=128\PYGZcb{}\PYGZgt{}\PYGZgt{}

// Low\PYGZhy{}level (LLVM) \PYGZhy{} flatten to individual values
\PYGZpc{}data : !llvm.array\PYGZlt{}128 x f32\PYGZgt{}
\end{sphinxVerbatim}


\subsubsection{5. Attributes}
\label{\detokenize{triton-compiler/05-mlir-concepts:attributes}}
\sphinxAtStartPar
\sphinxstylestrong{Attributes} are compile\sphinxhyphen{}time constants attached to operations.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
// Integer attribute
\PYGZpc{}c = arith.constant 42 : i32
//                   upup
//              attribute value

// Dictionary attribute
tt.make\PYGZus{}range \PYGZob{}start = 0 : i32, end = 128 : i32\PYGZcb{}
//            up                                  up
//            dictionary with start/end fields

// Array attribute
llvm.call @func(\PYGZpc{}arg) \PYGZob{}fastmathFlags = \PYGZsh{}llvm.fastmath\PYGZlt{}fast\PYGZgt{}\PYGZcb{}
\end{sphinxVerbatim}


\paragraph{Common Attribute Types}
\label{\detokenize{triton-compiler/05-mlir-concepts:common-attribute-types}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{IntegerAttr:} \sphinxcode{\sphinxupquote{42 : i32}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{FloatAttr:} \sphinxcode{\sphinxupquote{3.14 : f32}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{StringAttr:} \sphinxcode{\sphinxupquote{"hello"}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ArrayAttr:} \sphinxcode{\sphinxupquote{{[}1, 2, 3{]}}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{DictionaryAttr:} \sphinxcode{\sphinxupquote{\{key1 = val1, key2 = val2\}}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{TypeAttr:} Type as a value

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{UnitAttr:} Presence/absence flag

\end{itemize}


\paragraph{Triton Layout Attributes}
\label{\detokenize{triton-compiler/05-mlir-concepts:triton-layout-attributes}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
// Blocked layout: how data is distributed across threads
\PYGZsh{}ttg.blocked\PYGZlt{}\PYGZob{}
    sizePerThread = [1, 4],
    threadsPerWarp = [2, 16],
    warpsPerCTA = [4, 1],
    order = [1, 0]
\PYGZcb{}\PYGZgt{}

// Shared memory layout
\PYGZsh{}ttg.shared\PYGZlt{}\PYGZob{}
    vec = 8,
    perPhase = 2,
    maxPhase = 4
\PYGZcb{}\PYGZgt{}
\end{sphinxVerbatim}


\subsubsection{6. Passes}
\label{\detokenize{triton-compiler/05-mlir-concepts:passes}}
\sphinxAtStartPar
\sphinxstylestrong{Passes} are transformations that modify MLIR code.


\paragraph{Pass Types}
\label{\detokenize{triton-compiler/05-mlir-concepts:pass-types}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Analysis passes} \sphinxhyphen{} Gather information (don’t modify IR)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Transformation passes} \sphinxhyphen{} Modify IR

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Canonicalization} \sphinxhyphen{} Simplify IR to canonical form

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Lowering passes} \sphinxhyphen{} Convert between dialects

\end{enumerate}


\paragraph{Example Pass Pipeline}
\label{\detokenize{triton-compiler/05-mlir-concepts:example-pass-pipeline}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Triton\PYGZsq{}s pass pipeline (simplified)}
\PYG{n}{pm} \PYG{o}{=} \PYG{n}{PassManager}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} High\PYGZhy{}level optimizations (TTIR)}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{add\PYGZus{}pass}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{triton\PYGZhy{}combine}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}              \PYG{c+c1}{\PYGZsh{} Combine operations}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{add\PYGZus{}pass}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{canonicalize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}                \PYG{c+c1}{\PYGZsh{} Simplify}

\PYG{c+c1}{\PYGZsh{} Lower to GPU IR (TTIR \PYGZhy{}\PYGZgt{} TTGIR)}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{add\PYGZus{}pass}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{triton\PYGZhy{}gpu\PYGZhy{}coalesce}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}         \PYG{c+c1}{\PYGZsh{} Coalesce memory accesses}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{add\PYGZus{}pass}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{triton\PYGZhy{}gpu\PYGZhy{}pipeline}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}         \PYG{c+c1}{\PYGZsh{} Software pipelining}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{add\PYGZus{}pass}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{triton\PYGZhy{}gpu\PYGZhy{}prefetch}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}         \PYG{c+c1}{\PYGZsh{} Insert prefetch}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{add\PYGZus{}pass}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{triton\PYGZhy{}gpu\PYGZhy{}optimize\PYGZhy{}dot}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} Optimize matmul}

\PYG{c+c1}{\PYGZsh{} Lower to LLVM (TTGIR \PYGZhy{}\PYGZgt{} LLVM)}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{add\PYGZus{}pass}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{convert\PYGZhy{}triton\PYGZhy{}gpu\PYGZhy{}to\PYGZhy{}llvm}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{add\PYGZus{}pass}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{convert\PYGZhy{}scf\PYGZhy{}to\PYGZhy{}cf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} Structured \PYGZhy{}\PYGZgt{} unstructured control flow}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{add\PYGZus{}pass}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{convert\PYGZhy{}arith\PYGZhy{}to\PYGZhy{}llvm}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{pm}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{n}{module}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Triton\sphinxhyphen{}Specific Passes}
\label{\detokenize{triton-compiler/05-mlir-concepts:triton-specific-passes}}
\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/lib/Dialect/TritonGPU/Transforms}{lib/Dialect/TritonGPU/Transforms/}

\sphinxAtStartPar
\sphinxstylestrong{Key passes:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPUCoalesce}} \sphinxhyphen{} Optimize memory access patterns

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPUPipeline}} \sphinxhyphen{} Overlap computation and memory transfers

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPUPrefetch}} \sphinxhyphen{} Insert prefetch instructions

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPUAccelerateMatmul}} \sphinxhyphen{} Use tensor cores for matmul

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TritonGPURemoveLayoutConversions}} \sphinxhyphen{} Eliminate redundant layout changes

\end{itemize}


\subsection{MLIR in Triton}
\label{\detokenize{triton-compiler/05-mlir-concepts:mlir-in-triton}}

\subsubsection{Why Triton Uses MLIR}
\label{\detokenize{triton-compiler/05-mlir-concepts:why-triton-uses-mlir}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}level representation}
\begin{itemize}
\item {} 
\sphinxAtStartPar
TTIR: High\sphinxhyphen{}level block operations

\item {} 
\sphinxAtStartPar
TTGIR: GPU\sphinxhyphen{}specific with layouts

\item {} 
\sphinxAtStartPar
LLVM IR: Low\sphinxhyphen{}level machine operations

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reusable infrastructure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Don’t reinvent parsing, printing, pass management

\item {} 
\sphinxAtStartPar
Use existing dialects (arith, scf, llvm)

\item {} 
\sphinxAtStartPar
Benefit from MLIR community improvements

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Extensibility}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Easy to add new operations (TableGen)

\item {} 
\sphinxAtStartPar
Define custom types and attributes

\item {} 
\sphinxAtStartPar
Write dialect\sphinxhyphen{}specific passes

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}backend support}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Same TTIR can target NVIDIA (NVPTX) or AMD (AMDGCN)

\item {} 
\sphinxAtStartPar
Backend\sphinxhyphen{}specific dialects (ttng for NVIDIA, ttag for AMD)

\end{itemize}

\end{enumerate}


\subsubsection{Triton’s MLIR Dialects}
\label{\detokenize{triton-compiler/05-mlir-concepts:triton-s-mlir-dialects}}

\paragraph{Triton Dialect (tt)}
\label{\detokenize{triton-compiler/05-mlir-concepts:triton-dialect-tt}}
\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/lib/Dialect/Triton}{lib/Dialect/Triton/}

\sphinxAtStartPar
\sphinxstylestrong{High\sphinxhyphen{}level operations, backend\sphinxhyphen{}agnostic.}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
// Get block ID
\PYGZpc{}pid = tt.get\PYGZus{}program\PYGZus{}id x : i32

// Create range
\PYGZpc{}range = tt.make\PYGZus{}range \PYGZob{}start = 0 : i32, end = 128 : i32\PYGZcb{}
    : tensor\PYGZlt{}128xi32\PYGZgt{}

// Broadcast scalar to tensor
\PYGZpc{}splat = tt.splat \PYGZpc{}value : i32 \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128xi32\PYGZgt{}

// Pointer arithmetic
\PYGZpc{}ptrs = tt.addptr \PYGZpc{}base\PYGZus{}ptr, \PYGZpc{}offsets
    : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}, tensor\PYGZlt{}128xi32\PYGZgt{}

// Load from memory
\PYGZpc{}data = tt.load \PYGZpc{}ptrs, \PYGZpc{}mask, \PYGZpc{}other
    : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}

// Store to memory
tt.store \PYGZpc{}ptrs, \PYGZpc{}data, \PYGZpc{}mask : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}

// Matrix multiplication (conceptual)
\PYGZpc{}c = tt.dot \PYGZpc{}a, \PYGZpc{}b, \PYGZpc{}acc
    : tensor\PYGZlt{}128x64xf16\PYGZgt{} * tensor\PYGZlt{}64x128xf16\PYGZgt{} \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128x128xf32\PYGZgt{}
\end{sphinxVerbatim}


\paragraph{TritonGPU Dialect (ttg)}
\label{\detokenize{triton-compiler/05-mlir-concepts:tritongpu-dialect-ttg}}
\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/lib/Dialect/TritonGPU}{lib/Dialect/TritonGPU/}

\sphinxAtStartPar
\sphinxstylestrong{GPU\sphinxhyphen{}specific operations with data layout information.}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
// Define layout encoding
\PYGZsh{}blocked = \PYGZsh{}ttg.blocked\PYGZlt{}\PYGZob{}
    sizePerThread = [1, 4],
    threadsPerWarp = [2, 16],
    warpsPerCTA = [4, 1]
\PYGZcb{}\PYGZgt{}

// Tensor with layout
\PYGZpc{}data : tensor\PYGZlt{}128x128xf32, \PYGZsh{}blocked\PYGZgt{}

// Convert between layouts
\PYGZpc{}new\PYGZus{}data = ttg.convert\PYGZus{}layout \PYGZpc{}data
    : tensor\PYGZlt{}128x128xf32, \PYGZsh{}blocked1\PYGZgt{} \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128x128xf32, \PYGZsh{}blocked2\PYGZgt{}

// Allocate shared memory
\PYGZpc{}smem = ttg.alloc\PYGZus{}tensor : tensor\PYGZlt{}128x128xf32, \PYGZsh{}shared\PYGZgt{}

// Insert barrier
ttg.barrier

// Async operations (for pipelining)
\PYGZpc{}token = ttg.async\PYGZus{}commit\PYGZus{}group
ttg.async\PYGZus{}wait \PYGZob{}num = 0 : i32\PYGZcb{}
\end{sphinxVerbatim}


\paragraph{TritonNvidiaGPU Dialect (ttng)}
\label{\detokenize{triton-compiler/05-mlir-concepts:tritonnvidiagpu-dialect-ttng}}
\sphinxAtStartPar
\sphinxstyleemphasis{Location:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/lib/Dialect/TritonNvidiaGPU}{lib/Dialect/TritonNvidiaGPU/}

\sphinxAtStartPar
\sphinxstylestrong{NVIDIA\sphinxhyphen{}specific operations (Hopper+, tensor cores, TMA).}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
// Warp group dot (Hopper tensor cores)
\PYGZpc{}c = ttng.warp\PYGZus{}group\PYGZus{}dot \PYGZpc{}a, \PYGZpc{}b, \PYGZpc{}acc
    : tensor\PYGZlt{}64x64xf16\PYGZgt{} * tensor\PYGZlt{}64x64xf16\PYGZgt{} \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}64x64xf32\PYGZgt{}

// Tensor Memory Accelerator (TMA) load
\PYGZpc{}data = ttng.tma\PYGZus{}load \PYGZpc{}desc, \PYGZpc{}coords
    : !ttng.tma\PYGZus{}descriptor \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128x128xf16\PYGZgt{}

// Distributed shared memory (Hopper)
\PYGZpc{}smem = ttng.alloc\PYGZus{}dsmem : tensor\PYGZlt{}128x128xf32, \PYGZsh{}ttng.dsmem\PYGZgt{}
\end{sphinxVerbatim}


\subsection{Example: Lowering Through Dialects}
\label{\detokenize{triton-compiler/05-mlir-concepts:example-lowering-through-dialects}}
\sphinxAtStartPar
Let’s trace a simple operation through all dialects.


\subsubsection{Python Source}
\label{\detokenize{triton-compiler/05-mlir-concepts:python-source}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{add\PYGZus{}kernel}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{y\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{out\PYGZus{}ptr}\PYG{p}{,} \PYG{n}{N}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{offs} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{128}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offs}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{y\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offs}\PYG{p}{)}
    \PYG{n}{out} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{out\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offs}\PYG{p}{,} \PYG{n}{out}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Stage 1: Triton IR (TTIR)}
\label{\detokenize{triton-compiler/05-mlir-concepts:stage-1-triton-ir-ttir}}
\sphinxAtStartPar
\sphinxstylestrong{Backend\sphinxhyphen{}agnostic, high\sphinxhyphen{}level block operations.}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
module \PYGZob{}
  tt.func @add\PYGZus{}kernel(\PYGZpc{}x\PYGZus{}ptr: !tt.ptr\PYGZlt{}f32\PYGZgt{},
                      \PYGZpc{}y\PYGZus{}ptr: !tt.ptr\PYGZlt{}f32\PYGZgt{},
                      \PYGZpc{}out\PYGZus{}ptr: !tt.ptr\PYGZlt{}f32\PYGZgt{}) \PYGZob{}
    // Create range [0, 128)
    \PYGZpc{}range = tt.make\PYGZus{}range \PYGZob{}start = 0, end = 128\PYGZcb{} : tensor\PYGZlt{}128xi32\PYGZgt{}

    // Broadcast base pointers to tensors
    \PYGZpc{}x\PYGZus{}ptr\PYGZus{}splat = tt.splat \PYGZpc{}x\PYGZus{}ptr : !tt.ptr\PYGZlt{}f32\PYGZgt{} \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    \PYGZpc{}y\PYGZus{}ptr\PYGZus{}splat = tt.splat \PYGZpc{}y\PYGZus{}ptr : !tt.ptr\PYGZlt{}f32\PYGZgt{} \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    \PYGZpc{}out\PYGZus{}ptr\PYGZus{}splat = tt.splat \PYGZpc{}out\PYGZus{}ptr : !tt.ptr\PYGZlt{}f32\PYGZgt{} \PYGZhy{}\PYGZgt{} tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}

    // Compute pointer offsets
    \PYGZpc{}x\PYGZus{}ptrs = tt.addptr \PYGZpc{}x\PYGZus{}ptr\PYGZus{}splat, \PYGZpc{}range : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    \PYGZpc{}y\PYGZus{}ptrs = tt.addptr \PYGZpc{}y\PYGZus{}ptr\PYGZus{}splat, \PYGZpc{}range : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}
    \PYGZpc{}out\PYGZus{}ptrs = tt.addptr \PYGZpc{}out\PYGZus{}ptr\PYGZus{}splat, \PYGZpc{}range : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}\PYGZgt{}

    // Load data
    \PYGZpc{}x = tt.load \PYGZpc{}x\PYGZus{}ptrs : tensor\PYGZlt{}128xf32\PYGZgt{}
    \PYGZpc{}y = tt.load \PYGZpc{}y\PYGZus{}ptrs : tensor\PYGZlt{}128xf32\PYGZgt{}

    // Compute
    \PYGZpc{}out = arith.addf \PYGZpc{}x, \PYGZpc{}y : tensor\PYGZlt{}128xf32\PYGZgt{}

    // Store result
    tt.store \PYGZpc{}out\PYGZus{}ptrs, \PYGZpc{}out : tensor\PYGZlt{}128xf32\PYGZgt{}

    tt.return
  \PYGZcb{}
\PYGZcb{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Notice:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
No layout information yet

\item {} 
\sphinxAtStartPar
No GPU\sphinxhyphen{}specific operations

\item {} 
\sphinxAtStartPar
Pure block\sphinxhyphen{}level semantics

\end{itemize}


\subsubsection{Stage 2: TritonGPU IR (TTGIR)}
\label{\detokenize{triton-compiler/05-mlir-concepts:stage-2-tritongpu-ir-ttgir}}
\sphinxAtStartPar
\sphinxstylestrong{Add GPU layout information.}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{}blocked = \PYGZsh{}ttg.blocked\PYGZlt{}\PYGZob{}
    sizePerThread = [4],
    threadsPerWarp = [32],
    warpsPerCTA = [1]
\PYGZcb{}\PYGZgt{}

module \PYGZob{}
  tt.func @add\PYGZus{}kernel(\PYGZpc{}x\PYGZus{}ptr: !tt.ptr\PYGZlt{}f32\PYGZgt{},
                      \PYGZpc{}y\PYGZus{}ptr: !tt.ptr\PYGZlt{}f32\PYGZgt{},
                      \PYGZpc{}out\PYGZus{}ptr: !tt.ptr\PYGZlt{}f32\PYGZgt{}) \PYGZob{}
    // Range with layout
    \PYGZpc{}range = tt.make\PYGZus{}range \PYGZob{}start = 0, end = 128\PYGZcb{}
        : tensor\PYGZlt{}128xi32, \PYGZsh{}blocked\PYGZgt{}

    // Pointers with layout
    \PYGZpc{}x\PYGZus{}ptrs = tt.addptr \PYGZpc{}x\PYGZus{}ptr\PYGZus{}splat, \PYGZpc{}range
        : tensor\PYGZlt{}128x!tt.ptr\PYGZlt{}f32\PYGZgt{}, \PYGZsh{}blocked\PYGZgt{}

    // Load with layout (coalesced access)
    \PYGZpc{}x = tt.load \PYGZpc{}x\PYGZus{}ptrs
        : tensor\PYGZlt{}128xf32, \PYGZsh{}blocked\PYGZgt{}
    \PYGZpc{}y = tt.load \PYGZpc{}y\PYGZus{}ptrs
        : tensor\PYGZlt{}128xf32, \PYGZsh{}blocked\PYGZgt{}

    // Compute with layout
    \PYGZpc{}out = arith.addf \PYGZpc{}x, \PYGZpc{}y
        : tensor\PYGZlt{}128xf32, \PYGZsh{}blocked\PYGZgt{}

    // Store with layout
    tt.store \PYGZpc{}out\PYGZus{}ptrs, \PYGZpc{}out
        : tensor\PYGZlt{}128xf32, \PYGZsh{}blocked\PYGZgt{}

    tt.return
  \PYGZcb{}
\PYGZcb{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Notice:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Layout attributes added (\sphinxcode{\sphinxupquote{\#blocked}})

\item {} 
\sphinxAtStartPar
Specifies data distribution across threads

\item {} 
\sphinxAtStartPar
Enables memory coalescing optimizations

\end{itemize}


\subsubsection{Stage 3: LLVM Dialect}
\label{\detokenize{triton-compiler/05-mlir-concepts:stage-3-llvm-dialect}}
\sphinxAtStartPar
\sphinxstylestrong{Lowered to LLVM IR (within MLIR).}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
module \PYGZob{}
  llvm.func @add\PYGZus{}kernel(\PYGZpc{}x\PYGZus{}ptr: !llvm.ptr\PYGZlt{}f32\PYGZgt{},
                        \PYGZpc{}y\PYGZus{}ptr: !llvm.ptr\PYGZlt{}f32\PYGZgt{},
                        \PYGZpc{}out\PYGZus{}ptr: !llvm.ptr\PYGZlt{}f32\PYGZgt{}) \PYGZob{}
    // Get thread ID
    \PYGZpc{}tid = llvm.call @llvm.nvvm.read.ptx.sreg.tid.x()
        : () \PYGZhy{}\PYGZgt{} i32

    // Each thread handles 4 elements (sizePerThread = 4)
    \PYGZpc{}c0 = llvm.mlir.constant(0 : i32) : i32
    \PYGZpc{}c4 = llvm.mlir.constant(4 : i32) : i32

    // Loop over thread\PYGZsq{}s elements
    llvm.br \PYGZca{}loop(\PYGZpc{}c0 : i32)

  \PYGZca{}loop(\PYGZpc{}i: i32):
    \PYGZpc{}cond = llvm.icmp \PYGZdq{}slt\PYGZdq{} \PYGZpc{}i, \PYGZpc{}c4 : i32
    llvm.cond\PYGZus{}br \PYGZpc{}cond, \PYGZca{}body, \PYGZca{}exit

  \PYGZca{}body:
    // Calculate global offset
    \PYGZpc{}offset = llvm.add \PYGZpc{}tid, \PYGZpc{}i : i32

    // Load x[offset]
    \PYGZpc{}x\PYGZus{}gep = llvm.getelementptr \PYGZpc{}x\PYGZus{}ptr[\PYGZpc{}offset]
        : (!llvm.ptr\PYGZlt{}f32\PYGZgt{}, i32) \PYGZhy{}\PYGZgt{} !llvm.ptr\PYGZlt{}f32\PYGZgt{}
    \PYGZpc{}x\PYGZus{}val = llvm.load \PYGZpc{}x\PYGZus{}gep : !llvm.ptr\PYGZlt{}f32\PYGZgt{}

    // Load y[offset]
    \PYGZpc{}y\PYGZus{}gep = llvm.getelementptr \PYGZpc{}y\PYGZus{}ptr[\PYGZpc{}offset]
        : (!llvm.ptr\PYGZlt{}f32\PYGZgt{}, i32) \PYGZhy{}\PYGZgt{} !llvm.ptr\PYGZlt{}f32\PYGZgt{}
    \PYGZpc{}y\PYGZus{}val = llvm.load \PYGZpc{}y\PYGZus{}gep : !llvm.ptr\PYGZlt{}f32\PYGZgt{}

    // Compute
    \PYGZpc{}sum = llvm.fadd \PYGZpc{}x\PYGZus{}val, \PYGZpc{}y\PYGZus{}val : f32

    // Store out[offset]
    \PYGZpc{}out\PYGZus{}gep = llvm.getelementptr \PYGZpc{}out\PYGZus{}ptr[\PYGZpc{}offset]
        : (!llvm.ptr\PYGZlt{}f32\PYGZgt{}, i32) \PYGZhy{}\PYGZgt{} !llvm.ptr\PYGZlt{}f32\PYGZgt{}
    llvm.store \PYGZpc{}sum, \PYGZpc{}out\PYGZus{}gep : !llvm.ptr\PYGZlt{}f32\PYGZgt{}

    \PYGZpc{}i\PYGZus{}next = llvm.add \PYGZpc{}i, \PYGZpc{}c1 : i32
    llvm.br \PYGZca{}loop(\PYGZpc{}i\PYGZus{}next : i32)

  \PYGZca{}exit:
    llvm.return
  \PYGZcb{}
\PYGZcb{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Notice:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Explicit thread indexing (\sphinxcode{\sphinxupquote{tid.x}})

\item {} 
\sphinxAtStartPar
Loop over per\sphinxhyphen{}thread elements

\item {} 
\sphinxAtStartPar
Individual memory operations

\item {} 
\sphinxAtStartPar
NVVM intrinsics for GPU built\sphinxhyphen{}ins

\end{itemize}


\subsubsection{Stage 4: LLVM IR (Actual)}
\label{\detokenize{triton-compiler/05-mlir-concepts:stage-4-llvm-ir-actual}}
\sphinxAtStartPar
\sphinxstylestrong{Translated from LLVM dialect to actual LLVM IR.}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{define}\PYG{+w}{ }\PYG{k}{void}\PYG{+w}{ }\PYG{n+nv+vg}{@add\PYGZus{}kernel}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}ptr}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{n+nl}{entry:}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}tid}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{call}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv+vg}{@llvm.nvvm.read.ptx.sreg.tid.x}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{k}{br}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}loop}

\PYG{n+nl}{loop:}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}i}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{phi}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{p}{[}\PYG{+w}{ }\PYG{l+m}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}entry}\PYG{+w}{ }\PYG{p}{]}\PYG{p}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}i.next}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}body}\PYG{+w}{ }\PYG{p}{]}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}cond}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{icmp}\PYG{+w}{ }\PYG{k}{slt}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}i}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m}{4}
\PYG{+w}{  }\PYG{k}{br}\PYG{+w}{ }\PYG{k+kt}{i1}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}cond}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}body}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}exit}

\PYG{n+nl}{body:}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}offset}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{add}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}tid}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}i}

\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}gep}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{getelementptr}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}val}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{load}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}gep}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{align}\PYG{+w}{ }\PYG{l+m}{4}

\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}gep}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{getelementptr}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}
\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}val}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{load}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}gep}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{align}\PYG{+w}{ }\PYG{l+m}{4}

\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}sum}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{fadd}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}x\PYGZus{}val}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}y\PYGZus{}val}

\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}gep}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{getelementptr}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}offset}
\PYG{+w}{  }\PYG{k}{store}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}sum}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{p}{*}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}out\PYGZus{}gep}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{align}\PYG{+w}{ }\PYG{l+m}{4}

\PYG{+w}{  }\PYG{n+nv}{\PYGZpc{}i.next}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{k}{add}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}i}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m}{1}
\PYG{+w}{  }\PYG{k}{br}\PYG{+w}{ }\PYG{k+kt}{label}\PYG{+w}{ }\PYG{n+nv}{\PYGZpc{}loop}

\PYG{n+nl}{exit:}
\PYG{+w}{  }\PYG{k}{ret}\PYG{+w}{ }\PYG{k}{void}
\PYG{p}{\PYGZcb{}}

\PYG{k}{declare}\PYG{+w}{ }\PYG{k+kt}{i32}\PYG{+w}{ }\PYG{n+nv+vg}{@llvm.nvvm.read.ptx.sreg.tid.x}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{This is standard LLVM IR} that the NVPTX backend can compile to PTX.


\subsection{MLIR Tools and Ecosystem}
\label{\detokenize{triton-compiler/05-mlir-concepts:mlir-tools-and-ecosystem}}

\subsubsection{Command\sphinxhyphen{}Line Tools}
\label{\detokenize{triton-compiler/05-mlir-concepts:command-line-tools}}
\sphinxAtStartPar
\sphinxstylestrong{mlir\sphinxhyphen{}opt} \sphinxhyphen{} Optimize and transform MLIR

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Run canonicalization pass}
mlir\PYGZhy{}opt\PYG{+w}{ }\PYGZhy{}\PYGZhy{}canonicalize\PYG{+w}{ }input.mlir\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }output.mlir

\PYG{c+c1}{\PYGZsh{} Run custom pass}
mlir\PYGZhy{}opt\PYG{+w}{ }\PYGZhy{}\PYGZhy{}triton\PYGZhy{}gpu\PYGZhy{}pipeline\PYG{+w}{ }input.mlir

\PYG{c+c1}{\PYGZsh{} Lower to LLVM dialect}
mlir\PYGZhy{}opt\PYG{+w}{ }\PYGZhy{}\PYGZhy{}convert\PYGZhy{}triton\PYGZhy{}to\PYGZhy{}llvm\PYG{+w}{ }input.mlir
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{mlir\sphinxhyphen{}translate} \sphinxhyphen{} Translate between MLIR and other formats

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} MLIR \PYGZhy{}\PYGZgt{} LLVM IR}
mlir\PYGZhy{}translate\PYG{+w}{ }\PYGZhy{}\PYGZhy{}mlir\PYGZhy{}to\PYGZhy{}llvmir\PYG{+w}{ }input.mlir\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }output.ll

\PYG{c+c1}{\PYGZsh{} LLVM IR \PYGZhy{}\PYGZgt{} MLIR}
mlir\PYGZhy{}translate\PYG{+w}{ }\PYGZhy{}\PYGZhy{}import\PYGZhy{}llvm\PYG{+w}{ }input.ll\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }output.mlir
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{mlir\sphinxhyphen{}cpu\sphinxhyphen{}runner} \sphinxhyphen{} JIT execute MLIR on CPU

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mlir\PYGZhy{}cpu\PYGZhy{}runner\PYG{+w}{ }input.mlir\PYG{+w}{ }\PYGZhy{}\PYGZhy{}entry\PYGZhy{}point\PYG{o}{=}main
\end{sphinxVerbatim}


\subsubsection{TableGen for Defining Operations}
\label{\detokenize{triton-compiler/05-mlir-concepts:tablegen-for-defining-operations}}
\sphinxAtStartPar
\sphinxstylestrong{TableGen} is a domain\sphinxhyphen{}specific language for defining MLIR operations.

\sphinxAtStartPar
\sphinxstyleemphasis{Example from Triton:} \sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/include/triton/Dialect/Triton/IR/TritonOps.td}{TritonOps.td}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cSingleLine}{// Define tt.load operation}
\PYG{k}{def}\PYG{+w}{ }\PYG{n}{TT\PYGZus{}LoadOp}\PYG{+w}{ }\PYG{p}{:}\PYG{+w}{ }\PYG{n}{TT\PYGZus{}Op}\PYG{p}{\PYGZlt{}}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{load}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{p}{[}\PYG{n}{MemoryEffects}\PYG{p}{\PYGZlt{}[}\PYG{n}{MemRead}\PYG{p}{]\PYGZgt{}]\PYGZgt{}}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k}{let}\PYG{+w}{ }\PYG{n}{summary}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Load from memory}\PYG{l+s}{\PYGZdq{}}\PYG{p}{;}

\PYG{+w}{  }\PYG{k}{let}\PYG{+w}{ }\PYG{n}{arguments}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{ins}
\PYG{+w}{    }\PYG{n}{TT\PYGZus{}PtrLike}\PYG{p}{:}\PYG{n+nv}{\PYGZdl{}ptr}\PYG{p}{,}\PYG{+w}{        }\PYG{c+cSingleLine}{// Pointer operand}
\PYG{+w}{    }\PYG{n}{Optional}\PYG{p}{\PYGZlt{}}\PYG{n}{I1Tensor}\PYG{p}{\PYGZgt{}:}\PYG{n+nv}{\PYGZdl{}mask}\PYG{p}{,}\PYG{+w}{  }\PYG{c+cSingleLine}{// Optional mask}
\PYG{+w}{    }\PYG{n}{Optional}\PYG{p}{\PYGZlt{}}\PYG{n}{AnyType}\PYG{p}{\PYGZgt{}:}\PYG{n+nv}{\PYGZdl{}other}\PYG{+w}{   }\PYG{c+cSingleLine}{// Optional default value}
\PYG{+w}{  }\PYG{p}{);}

\PYG{+w}{  }\PYG{k}{let}\PYG{+w}{ }\PYG{n}{results}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{outs}
\PYG{+w}{    }\PYG{n}{AnyType}\PYG{p}{:}\PYG{n+nv}{\PYGZdl{}result}\PYG{+w}{           }\PYG{c+cSingleLine}{// Loaded data}
\PYG{+w}{  }\PYG{p}{);}

\PYG{+w}{  }\PYG{k}{let}\PYG{+w}{ }\PYG{n}{assemblyFormat}\PYG{+w}{ }\PYG{p}{=}\PYG{+w}{ }[\PYGZob{}
\PYG{+w}{    }\PYG{n}{\PYGZdl{}ptr}\PYG{+w}{ }\PYG{p}{(}\PYG{err}{`}\PYG{p}{,}\PYG{err}{`}\PYG{+w}{ }\PYG{n}{\PYGZdl{}mask}\PYG{o}{\PYGZca{}}\PYG{+w}{ }\PYG{p}{(}\PYG{err}{`}\PYG{p}{,}\PYG{err}{`}\PYG{+w}{ }\PYG{n}{\PYGZdl{}other}\PYG{o}{\PYGZca{}}\PYG{p}{)}\PYG{o}{?}\PYG{p}{)}\PYG{o}{?}\PYG{+w}{ }\PYG{n}{attr}\PYG{o}{\PYGZhy{}}\PYG{n}{dict}\PYG{+w}{ }\PYG{err}{`}\PYG{o}{:}\PYG{err}{`}\PYG{+w}{ }\PYG{n}{type}\PYG{p}{(}\PYG{n}{\PYGZdl{}result}\PYG{p}{)}
\PYG{+w}{  }\PYGZcb{}]\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{TableGen generates:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
C++ class for the operation

\item {} 
\sphinxAtStartPar
Parsing and printing code

\item {} 
\sphinxAtStartPar
Type inference

\item {} 
\sphinxAtStartPar
Verification

\end{itemize}


\subsubsection{Debugging MLIR}
\label{\detokenize{triton-compiler/05-mlir-concepts:debugging-mlir}}
\sphinxAtStartPar
\sphinxstylestrong{Print IR at each stage:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Set environment variable}
\PYG{n+nb}{export}\PYG{+w}{ }\PYG{n+nv}{MLIR\PYGZus{}ENABLE\PYGZus{}DUMP}\PYG{o}{=}\PYG{l+m}{1}

\PYG{c+c1}{\PYGZsh{} Triton will dump IR at each pass}
python\PYG{+w}{ }kernel.py
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Use \textasciigrave{}\textasciigrave{}\textendash{}debug\textasciigrave{}\textasciigrave{} flag:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mlir\PYGZhy{}opt\PYG{+w}{ }\PYGZhy{}\PYGZhy{}debug\PYG{+w}{ }input.mlir
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Print specific pass output:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} In Python}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{triton}

\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{pass}

\PYG{c+c1}{\PYGZsh{} Compile with debug}
\PYG{n}{kernel}\PYG{p}{[}\PYG{n}{grid}\PYG{p}{]}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{debug}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{MLIR Resources}
\label{\detokenize{triton-compiler/05-mlir-concepts:mlir-resources}}

\subsubsection{Official Documentation}
\label{\detokenize{triton-compiler/05-mlir-concepts:official-documentation}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://mlir.llvm.org/}{MLIR Website} \sphinxhyphen{} Official documentation

\item {} 
\sphinxAtStartPar
\sphinxhref{https://mlir.llvm.org/docs/Dialects/}{MLIR Dialects} \sphinxhyphen{} Built\sphinxhyphen{}in dialects

\item {} 
\sphinxAtStartPar
\sphinxhref{https://mlir.llvm.org/docs/LangRef/}{MLIR Language Reference} \sphinxhyphen{} Syntax and semantics

\item {} 
\sphinxAtStartPar
\sphinxhref{https://mlir.llvm.org/docs/OpDefinitions/}{TableGen Reference} \sphinxhyphen{} Defining operations

\end{itemize}


\subsubsection{Tutorials}
\label{\detokenize{triton-compiler/05-mlir-concepts:tutorials}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://mlir.llvm.org/docs/Tutorials/Toy/}{MLIR Toy Tutorial} \sphinxhyphen{} Build a compiler from scratch

\item {} 
\sphinxAtStartPar
\sphinxhref{https://mlir.llvm.org/talks/}{MLIR Talks} \sphinxhyphen{} Conference presentations

\item {} 
\sphinxAtStartPar
\sphinxhref{https://discourse.llvm.org/c/mlir/}{MLIR Community} \sphinxhyphen{} Discussion forum

\end{itemize}


\subsubsection{Triton\sphinxhyphen{}Specific}
\label{\detokenize{triton-compiler/05-mlir-concepts:triton-specific}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/include/triton/Dialect}{Triton MLIR Dialects} \sphinxhyphen{} Operation definitions

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/lib/Dialect/TritonGPU/Transforms}{Triton Passes} \sphinxhyphen{} Transformation passes

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/triton-lang/triton/tree/v3.5.1/test/Triton}{Triton IR Examples} \sphinxhyphen{} Test cases with IR

\end{itemize}


\subsection{Summary}
\label{\detokenize{triton-compiler/05-mlir-concepts:summary}}

\subsubsection{Key Concepts Recap}
\label{\detokenize{triton-compiler/05-mlir-concepts:key-concepts-recap}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{MLIR = Multi\sphinxhyphen{}Level Intermediate Representation}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Multiple dialects coexist

\item {} 
\sphinxAtStartPar
Gradual lowering preserves semantics

\item {} 
\sphinxAtStartPar
Reusable infrastructure

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dialects} \sphinxhyphen{} Namespaces for operations, types, attributes
\begin{itemize}
\item {} 
\sphinxAtStartPar
Triton has \sphinxcode{\sphinxupquote{tt}}, \sphinxcode{\sphinxupquote{ttg}}, \sphinxcode{\sphinxupquote{ttng}} dialects

\item {} 
\sphinxAtStartPar
Standard dialects: \sphinxcode{\sphinxupquote{arith}}, \sphinxcode{\sphinxupquote{scf}}, \sphinxcode{\sphinxupquote{llvm}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Operations} \sphinxhyphen{} Fundamental computation units
\begin{itemize}
\item {} 
\sphinxAtStartPar
SSA form (single assignment)

\item {} 
\sphinxAtStartPar
Have operands, results, attributes, types

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Types} \sphinxhyphen{} Flexible type system
\begin{itemize}
\item {} 
\sphinxAtStartPar
Built\sphinxhyphen{}in: integers, floats, tensors

\item {} 
\sphinxAtStartPar
Custom: Triton pointers, layouts

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Passes} \sphinxhyphen{} IR transformations
\begin{itemize}
\item {} 
\sphinxAtStartPar
Analysis, optimization, lowering

\item {} 
\sphinxAtStartPar
Triton has GPU\sphinxhyphen{}specific passes

\end{itemize}

\end{enumerate}


\subsubsection{Why MLIR Matters for Triton}
\label{\detokenize{triton-compiler/05-mlir-concepts:why-mlir-matters-for-triton}}
\sphinxAtStartPar
\sphinxstylestrong{Without MLIR:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
{[}{[}FAIL{]}{]} Would need to build entire compiler infrastructure

\item {} 
\sphinxAtStartPar
{[}{[}FAIL{]}{]} Hard to support multiple GPU vendors

\item {} 
\sphinxAtStartPar
{[}{[}FAIL{]}{]} Difficult to add new optimizations

\item {} 
\sphinxAtStartPar
{[}{[}FAIL{]}{]} Can’t reuse existing tools and passes

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{With MLIR:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
{[}{[}OK{]}{]} Reuse robust infrastructure (parsing, printing, pass management)

\item {} 
\sphinxAtStartPar
{[}{[}OK{]}{]} Easy multi\sphinxhyphen{}backend support (NVIDIA, AMD, Intel)

\item {} 
\sphinxAtStartPar
{[}{[}OK{]}{]} Modular, extensible design

\item {} 
\sphinxAtStartPar
{[}{[}OK{]}{]} Benefit from MLIR ecosystem improvements

\item {} 
\sphinxAtStartPar
{[}{[}OK{]}{]} Gradual lowering preserves optimization opportunities

\end{itemize}


\subsubsection{The Big Picture}
\label{\detokenize{triton-compiler/05-mlir-concepts:the-big-picture}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Triton Compiler Pipeline:

Python AST
     down
[Code Generator] \PYGZlt{}\PYGZhy{} Converts AST to MLIR
     down
TTIR (tt dialect) \PYGZlt{}\PYGZhy{} High\PYGZhy{}level block operations
     down
[MLIR Passes] \PYGZlt{}\PYGZhy{} Optimization, coalescing
     down
TTGIR (ttg dialect) \PYGZlt{}\PYGZhy{} Add GPU layouts
     down
[MLIR Passes] \PYGZlt{}\PYGZhy{} Pipelining, prefetch, tensor cores
     down
LLVM Dialect \PYGZlt{}\PYGZhy{} Still MLIR, but LLVM operations
     down
[mlir\PYGZhy{}translate] \PYGZlt{}\PYGZhy{} Convert MLIR \PYGZhy{}\PYGZgt{} LLVM IR
     down
LLVM IR \PYGZlt{}\PYGZhy{} Standard LLVM
     down
[NVPTX Backend] \PYGZlt{}\PYGZhy{} LLVM\PYGZsq{}s PTX generator
     down
PTX Assembly
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{MLIR enables this multi\sphinxhyphen{}stage, optimizing pipeline} with reusable, modular components.

\sphinxstepscope


\section{Learning Paths}
\label{\detokenize{learning-paths:learning-paths}}\label{\detokenize{learning-paths::doc}}
\sphinxAtStartPar
Choose a learning path based on your goals and experience level.


\subsection{Path 1: Fast Track (Essentials)}
\label{\detokenize{learning-paths:path-1-fast-track-essentials}}
\sphinxAtStartPar
\sphinxstylestrong{Goal}: Get productive with Triton quickly

\sphinxAtStartPar
\sphinxstylestrong{Time}: 4\sphinxhyphen{}6 hours

\sphinxAtStartPar
\sphinxstylestrong{Prerequisites}: Basic Python, familiar with PyTorch

\sphinxAtStartPar
\sphinxstylestrong{Sequence}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/gpu-fundamentals::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Fundamentals}}}} (30 min)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Understand SPMD model

\item {} 
\sphinxAtStartPar
Learn GPU hierarchy

\end{itemize}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/01-vector-add::doc}]{\sphinxcrossref{\DUrole{doc}{Vector Addition in Triton}}}} (1 hour)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Write your first kernel

\item {} 
\sphinxAtStartPar
Understand parallelism

\item {} 
\sphinxAtStartPar
Learn memory patterns

\end{itemize}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/02-fused-softmax::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Softmax in Triton}}}} (1.5 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Master kernel fusion

\item {} 
\sphinxAtStartPar
Understand SRAM vs DRAM

\item {} 
\sphinxAtStartPar
Learn reduction operations

\end{itemize}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/03-matrix-multiplication::doc}]{\sphinxcrossref{\DUrole{doc}{Matrix Multiplication in Triton}}}} (2\sphinxhyphen{}3 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Understand tiling

\item {} 
\sphinxAtStartPar
Learn auto\sphinxhyphen{}tuning

\item {} 
\sphinxAtStartPar
Use Tensor Cores

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Outcome}: You can write and optimize basic GPU kernels for common operations.


\subsection{Path 2: Deep Understanding (Comprehensive)}
\label{\detokenize{learning-paths:path-2-deep-understanding-comprehensive}}
\sphinxAtStartPar
\sphinxstylestrong{Goal}: Become a Triton/GPU programming expert

\sphinxAtStartPar
\sphinxstylestrong{Time}: 12\sphinxhyphen{}16 hours

\sphinxAtStartPar
\sphinxstylestrong{Prerequisites}: Path 1 or equivalent knowledge

\sphinxAtStartPar
\sphinxstylestrong{Sequence}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Foundations} (2 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/gpu-fundamentals::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Fundamentals}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/memory-hierarchy::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Memory Hierarchy}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/execution-model::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Execution Model}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Basic Kernels} (3 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/01-vector-add::doc}]{\sphinxcrossref{\DUrole{doc}{Vector Addition in Triton}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/02-fused-softmax::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Softmax in Triton}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compute Optimization} (3 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/03-matrix-multiplication::doc}]{\sphinxcrossref{\DUrole{doc}{Matrix Multiplication in Triton}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/performance-optimization::doc}]{\sphinxcrossref{\DUrole{doc}{Performance Optimization Strategies}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Advanced Memory Techniques} (2 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/04-low-memory-dropout::doc}]{\sphinxcrossref{\DUrole{doc}{Low\sphinxhyphen{}Memory Dropout in Triton}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Training Loop Implementation} (3 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/05-layer-norm::doc}]{\sphinxcrossref{\DUrole{doc}{Layer Normalization in Triton}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{State\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}Art} (3\sphinxhyphen{}4 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/06-fused-attention::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Attention (Flash Attention) in Triton}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Extensions} (1 hour)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/07-extern-functions::doc}]{\sphinxcrossref{\DUrole{doc}{Using External Functions (libdevice) in Triton}}}}

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Outcome}: You can implement complex, production\sphinxhyphen{}ready GPU kernels and optimize them for maximum performance.


\subsection{Path 3: Transformer Focus (For LLM/NLP)}
\label{\detokenize{learning-paths:path-3-transformer-focus-for-llm-nlp}}
\sphinxAtStartPar
\sphinxstylestrong{Goal}: Optimize Transformers and attention mechanisms

\sphinxAtStartPar
\sphinxstylestrong{Time}: 8\sphinxhyphen{}10 hours

\sphinxAtStartPar
\sphinxstylestrong{Prerequisites}: Familiar with Transformers (BERT, GPT, etc.)

\sphinxAtStartPar
\sphinxstylestrong{Sequence}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GPU Basics} (1 hour)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/gpu-fundamentals::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Fundamentals}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/memory-hierarchy::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Memory Hierarchy}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Foundation Kernel} (1 hour)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/01-vector-add::doc}]{\sphinxcrossref{\DUrole{doc}{Vector Addition in Triton}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Attention Building Blocks} (2 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/02-fused-softmax::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Softmax in Triton}}}}

\item {} 
\sphinxAtStartPar
Learn softmax optimization (key for attention)

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Normalization} (2 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/05-layer-norm::doc}]{\sphinxcrossref{\DUrole{doc}{Layer Normalization in Triton}}}}

\item {} 
\sphinxAtStartPar
Essential Transformer component

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Efficient Attention} (3\sphinxhyphen{}4 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/06-fused-attention::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Attention (Flash Attention) in Triton}}}}

\item {} 
\sphinxAtStartPar
Flash Attention for long sequences

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory Efficiency} (1\sphinxhyphen{}2 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/04-low-memory-dropout::doc}]{\sphinxcrossref{\DUrole{doc}{Low\sphinxhyphen{}Memory Dropout in Triton}}}}

\item {} 
\sphinxAtStartPar
Techniques for large models

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Outcome}: You can optimize Transformer models, implement efficient attention, and handle long sequences.


\subsection{Path 4: Computer Vision Focus}
\label{\detokenize{learning-paths:path-4-computer-vision-focus}}
\sphinxAtStartPar
\sphinxstylestrong{Goal}: Optimize CNN and vision models

\sphinxAtStartPar
\sphinxstylestrong{Time}: 8\sphinxhyphen{}10 hours

\sphinxAtStartPar
\sphinxstylestrong{Sequence}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fundamentals} (2 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/gpu-fundamentals::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Fundamentals}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/memory-hierarchy::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Memory Hierarchy}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/01-vector-add::doc}]{\sphinxcrossref{\DUrole{doc}{Vector Addition in Triton}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compute\sphinxhyphen{}Heavy Operations} (3 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/03-matrix-multiplication::doc}]{\sphinxcrossref{\DUrole{doc}{Matrix Multiplication in Triton}}}}

\item {} 
\sphinxAtStartPar
Tiling techniques apply to convolutions

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Activation and Normalization} (2 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/02-fused-softmax::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Softmax in Triton}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/05-layer-norm::doc}]{\sphinxcrossref{\DUrole{doc}{Layer Normalization in Triton}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data Augmentation} (1 hour)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/04-low-memory-dropout::doc}]{\sphinxcrossref{\DUrole{doc}{Low\sphinxhyphen{}Memory Dropout in Triton}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Performance} (2 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/performance-optimization::doc}]{\sphinxcrossref{\DUrole{doc}{Performance Optimization Strategies}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/execution-model::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Execution Model}}}}

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Outcome}: Optimize convolutions, pooling, and other vision\sphinxhyphen{}specific operations.


\subsection{Path 5: Performance Engineering}
\label{\detokenize{learning-paths:path-5-performance-engineering}}
\sphinxAtStartPar
\sphinxstylestrong{Goal}: Maximize GPU utilization and performance

\sphinxAtStartPar
\sphinxstylestrong{Time}: 10\sphinxhyphen{}12 hours

\sphinxAtStartPar
\sphinxstylestrong{Prerequisites}: Comfortable with GPU programming

\sphinxAtStartPar
\sphinxstylestrong{Sequence}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Core Concepts} (3 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
All documents in {\hyperref[\detokenize{gpu-concepts/gpu-fundamentals::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Fundamentals}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/memory-hierarchy::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Memory Hierarchy}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/execution-model::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Execution Model}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/performance-optimization::doc}]{\sphinxcrossref{\DUrole{doc}{Performance Optimization Strategies}}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Practical Optimization} (4 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/02-fused-softmax::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Softmax in Triton}}}} \sphinxhyphen{} Memory optimization

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/03-matrix-multiplication::doc}]{\sphinxcrossref{\DUrole{doc}{Matrix Multiplication in Triton}}}} \sphinxhyphen{} Compute optimization

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/06-fused-attention::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Attention (Flash Attention) in Triton}}}} \sphinxhyphen{} Advanced techniques

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Profiling and Tuning} (2 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{troubleshooting::doc}]{\sphinxcrossref{\DUrole{doc}{Troubleshooting Guide}}}}

\item {} 
\sphinxAtStartPar
Practice with real kernels

\item {} 
\sphinxAtStartPar
Use \sphinxcode{\sphinxupquote{nsys}} and \sphinxcode{\sphinxupquote{ncu}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Case Studies} (2\sphinxhyphen{}3 hours)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Analyze and optimize existing kernels

\item {} 
\sphinxAtStartPar
Compare with PyTorch/cuBLAS

\item {} 
\sphinxAtStartPar
Implement variants

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Outcome}: Expert\sphinxhyphen{}level performance analysis and optimization skills.


\subsection{By Topic}
\label{\detokenize{learning-paths:by-topic}}
\sphinxAtStartPar
If you want to learn specific topics:


\subsubsection{Memory Optimization}
\label{\detokenize{learning-paths:memory-optimization}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/memory-hierarchy::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Memory Hierarchy}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/02-fused-softmax::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Softmax in Triton}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/06-fused-attention::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Attention (Flash Attention) in Triton}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/04-low-memory-dropout::doc}]{\sphinxcrossref{\DUrole{doc}{Low\sphinxhyphen{}Memory Dropout in Triton}}}}

\end{enumerate}


\subsubsection{Compute Optimization}
\label{\detokenize{learning-paths:compute-optimization}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/execution-model::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Execution Model}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/03-matrix-multiplication::doc}]{\sphinxcrossref{\DUrole{doc}{Matrix Multiplication in Triton}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/performance-optimization::doc}]{\sphinxcrossref{\DUrole{doc}{Performance Optimization Strategies}}}}

\end{enumerate}


\subsubsection{Backward Pass / Training}
\label{\detokenize{learning-paths:backward-pass-training}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/05-layer-norm::doc}]{\sphinxcrossref{\DUrole{doc}{Layer Normalization in Triton}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/06-fused-attention::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Attention (Flash Attention) in Triton}}}} (backward)

\end{enumerate}


\subsubsection{Advanced Techniques}
\label{\detokenize{learning-paths:advanced-techniques}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/06-fused-attention::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Attention (Flash Attention) in Triton}}}} \sphinxhyphen{} Online algorithms

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/04-low-memory-dropout::doc}]{\sphinxcrossref{\DUrole{doc}{Low\sphinxhyphen{}Memory Dropout in Triton}}}} \sphinxhyphen{} Recomputation

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/07-extern-functions::doc}]{\sphinxcrossref{\DUrole{doc}{Using External Functions (libdevice) in Triton}}}} \sphinxhyphen{} External libraries

\end{enumerate}


\subsection{Learning Tips}
\label{\detokenize{learning-paths:learning-tips}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Run the Code}

\sphinxAtStartPar
Don’t just read \sphinxhyphen{} execute examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cd} \PYG{n}{triton\PYGZus{}cuda}\PYG{o}{/}\PYG{n}{triton\PYGZus{}practice}
\PYG{n}{python} \PYG{l+m+mi}{01}\PYG{o}{\PYGZhy{}}\PYG{n}{vector}\PYG{o}{\PYGZhy{}}\PYG{n}{add}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Modify and Experiment}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Change \sphinxcode{\sphinxupquote{BLOCK\_SIZE}} values

\item {} 
\sphinxAtStartPar
Try different input sizes

\item {} 
\sphinxAtStartPar
Add print statements

\item {} 
\sphinxAtStartPar
Break things intentionally!

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Profile Your Code}

\sphinxAtStartPar
Use profiling tools:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} NVIDIA}
\PYG{n}{nsys} \PYG{n}{profile} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{set} \PYG{n}{full} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}

\PYG{c+c1}{\PYGZsh{} AMD}
\PYG{n}{rocprof} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compare with PyTorch}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Verify correctness

\item {} 
\sphinxAtStartPar
Measure speedup

\item {} 
\sphinxAtStartPar
Understand trade\sphinxhyphen{}offs

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Join the Community}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton/discussions}{Triton Discussions}

\item {} 
\sphinxAtStartPar
Share your kernels

\item {} 
\sphinxAtStartPar
Ask questions

\end{itemize}

\end{enumerate}


\subsection{Assessment Checkpoints}
\label{\detokenize{learning-paths:assessment-checkpoints}}

\subsubsection{After Path 1 (Fast Track)}
\label{\detokenize{learning-paths:after-path-1-fast-track}}
\sphinxAtStartPar
You should be able to:

\sphinxAtStartPar
{[} {]} Explain SPMD execution model
{[} {]} Write a simple element\sphinxhyphen{}wise kernel
{[} {]} Understand memory coalescing
{[} {]} Implement basic kernel fusion
{[} {]} Use auto\sphinxhyphen{}tuning


\subsubsection{After Path 2 (Comprehensive)}
\label{\detokenize{learning-paths:after-path-2-comprehensive}}
\sphinxAtStartPar
You should be able to:

\sphinxAtStartPar
{[} {]} Implement forward and backward passes
{[} {]} Optimize for both memory and compute
{[} {]} Use Tensor Cores effectively
{[} {]} Write persistent kernels
{[} {]} Achieve 80\%+ of PyTorch performance


\subsubsection{After Path 3 (Transformer)}
\label{\detokenize{learning-paths:after-path-3-transformer}}
\sphinxAtStartPar
You should be able to:

\sphinxAtStartPar
{[} {]} Implement efficient attention mechanisms
{[} {]} Handle long sequences (16K+ tokens)
{[} {]} Optimize layer normalization
{[} {]} Fuse operations in Transformers
{[} {]} Understand Flash Attention algorithm


\subsection{Next Steps After Completing a Path}
\label{\detokenize{learning-paths:next-steps-after-completing-a-path}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Build a Project}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Optimize your own model

\item {} 
\sphinxAtStartPar
Implement a research paper

\item {} 
\sphinxAtStartPar
Contribute to open source

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Advanced Topics}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Multi\sphinxhyphen{}GPU kernels

\item {} 
\sphinxAtStartPar
Quantization (INT8, FP8)

\item {} 
\sphinxAtStartPar
Sparse operations

\item {} 
\sphinxAtStartPar
Custom backward passes

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Contribute}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Share your kernels

\item {} 
\sphinxAtStartPar
Write tutorials

\item {} 
\sphinxAtStartPar
Help others in community

\end{itemize}

\end{enumerate}


\subsection{Resources for Continued Learning}
\label{\detokenize{learning-paths:resources-for-continued-learning}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{references::doc}]{\sphinxcrossref{\DUrole{doc}{References and Resources}}}} \sphinxhyphen{} Papers and documentation

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{troubleshooting::doc}]{\sphinxcrossref{\DUrole{doc}{Troubleshooting Guide}}}} \sphinxhyphen{} Common issues and solutions

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton}{Triton GitHub} \sphinxhyphen{} Latest updates

\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.nvidia.com/cuda/cuda-c-programming-guide/}{CUDA Programming Guide} \sphinxhyphen{} Deep dive

\end{itemize}


\subsection{Choose Your Path}
\label{\detokenize{learning-paths:choose-your-path}}
\sphinxAtStartPar
Ready to start? Pick the path that matches your goals:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Quick start?} \sphinxhyphen{}\textgreater{} {\hyperref[\detokenize{gpu-tutorials/01-vector-add::doc}]{\sphinxcrossref{\DUrole{doc}{Vector Addition in Triton}}}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep learning?} \sphinxhyphen{}\textgreater{} Path 2 (Comprehensive)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Transformers?} \sphinxhyphen{}\textgreater{} Path 3 (Transformer Focus)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Performance?} \sphinxhyphen{}\textgreater{} Path 5 (Performance Engineering)

\end{itemize}

\sphinxAtStartPar
Happy learning! {[}rocket{]}

\sphinxstepscope


\section{Troubleshooting Guide}
\label{\detokenize{troubleshooting:troubleshooting-guide}}\label{\detokenize{troubleshooting::doc}}
\sphinxAtStartPar
Common issues and their solutions when working with Triton and GPU programming.


\subsection{Out of Memory Errors}
\label{\detokenize{troubleshooting:out-of-memory-errors}}

\subsubsection{CUDA Out of Memory}
\label{\detokenize{troubleshooting:cuda-out-of-memory}}
\sphinxAtStartPar
\sphinxstylestrong{Error Message}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+ne}{RuntimeError}\PYG{p}{:} \PYG{n}{CUDA} \PYG{n}{out} \PYG{n}{of} \PYG{n}{memory}\PYG{o}{.} \PYG{n}{Tried} \PYG{n}{to} \PYG{n}{allocate} \PYG{n}{X} \PYG{n}{MB}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Causes}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Batch size too large

\item {} 
\sphinxAtStartPar
Sequence length too long

\item {} 
\sphinxAtStartPar
Too many intermediate tensors

\item {} 
\sphinxAtStartPar
Memory leak

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Solutions}:

\sphinxAtStartPar
\sphinxstylestrong{Reduce batch size}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Before}
\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{64}

\PYG{c+c1}{\PYGZsh{} After}
\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{32}  \PYG{c+c1}{\PYGZsh{} or 16}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Use gradient checkpointing}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Recompute activations instead of storing}
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{checkpoint}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{checkpoint}

\PYG{n}{output} \PYG{o}{=} \PYG{n}{checkpoint}\PYG{p}{(}\PYG{n}{my\PYGZus{}function}\PYG{p}{,} \PYG{n+nb}{input}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Clear cache}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{torch}\PYG{o}{.}\PYG{n}{cuda}\PYG{o}{.}\PYG{n}{empty\PYGZus{}cache}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Check for memory leaks}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Detach tensors when not needed}
\PYG{n}{loss} \PYG{o}{=} \PYG{n}{compute\PYGZus{}loss}\PYG{p}{(}\PYG{n}{output}\PYG{p}{,} \PYG{n}{target}\PYG{p}{)}
\PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{n}{loss}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Convert to Python number}
\PYG{k}{del} \PYG{n}{loss}  \PYG{c+c1}{\PYGZsh{} Free memory}
\end{sphinxVerbatim}


\subsubsection{Out of Shared Memory}
\label{\detokenize{troubleshooting:out-of-shared-memory}}
\sphinxAtStartPar
\sphinxstylestrong{Error Message}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{triton}\PYG{o}{.}\PYG{n}{runtime}\PYG{o}{.}\PYG{n}{errors}\PYG{o}{.}\PYG{n}{OutOfResources}\PYG{p}{:} \PYG{n}{out} \PYG{n}{of} \PYG{n}{resource}\PYG{p}{:} \PYG{n}{shared} \PYG{n}{memory}\PYG{p}{,}
\PYG{n}{Required}\PYG{p}{:} \PYG{l+m+mi}{109568}\PYG{p}{,} \PYG{n}{Hardware} \PYG{n}{limit}\PYG{p}{:} \PYG{l+m+mi}{101376}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Causes}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Block sizes too large

\item {} 
\sphinxAtStartPar
Too many pipeline stages (\sphinxcode{\sphinxupquote{num\_stages}})

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Solutions}:

\sphinxAtStartPar
\sphinxstylestrong{Reduce block sizes}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Before}
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}M}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{128}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}N}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{256}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} After}
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}M}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLOCK\PYGZus{}N}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{128}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Reduce num\_stages}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Before}
\PYG{n}{num\PYGZus{}stages} \PYG{o}{=} \PYG{l+m+mi}{5}

\PYG{c+c1}{\PYGZsh{} After}
\PYG{n}{num\PYGZus{}stages} \PYG{o}{=} \PYG{l+m+mi}{3}  \PYG{c+c1}{\PYGZsh{} or 2}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Adjust auto\sphinxhyphen{}tune configs}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{configs} \PYG{o}{=} \PYG{p}{[}
    \PYG{n}{triton}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}stages}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{num\PYGZus{}warps}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Less SRAM}
    \PYG{c+c1}{\PYGZsh{} Remove configs with large blocks}
\PYG{p}{]}
\end{sphinxVerbatim}


\subsection{Correctness Issues}
\label{\detokenize{troubleshooting:correctness-issues}}

\subsubsection{Wrong Results}
\label{\detokenize{troubleshooting:wrong-results}}
\sphinxAtStartPar
\sphinxstylestrong{Symptoms}: Output doesn’t match PyTorch or expected values

\sphinxAtStartPar
\sphinxstylestrong{Debug Steps}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Check masking}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mask} \PYG{o}{=} \PYG{n}{offsets} \PYG{o}{\PYGZlt{}} \PYG{n}{n\PYGZus{}elements}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{x\PYGZus{}ptr} \PYG{o}{+} \PYG{n}{offsets}\PYG{p}{,} \PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Don\PYGZsq{}t forget mask!}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Verify pointer arithmetic}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check strides are correct}
\PYG{k}{assert} \PYG{n}{a}\PYG{o}{.}\PYG{n}{is\PYGZus{}contiguous}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Print debug info}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Stride: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{a}\PYG{o}{.}\PYG{n}{stride}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use float32 for accumulation}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Bad: FP16 accumulation loses precision}
\PYG{n}{acc} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{M}\PYG{p}{,} \PYG{n}{N}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tl}\PYG{o}{.}\PYG{n}{float16}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Good: FP32 accumulation}
\PYG{n}{acc} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{M}\PYG{p}{,} \PYG{n}{N}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tl}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{acc}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{tl}\PYG{o}{.}\PYG{n}{float16}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Cast at end}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Check numerical stability}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Softmax: always subtract max}
\PYG{n}{x\PYGZus{}max} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{x\PYGZus{}normalized} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{x\PYGZus{}max}  \PYG{c+c1}{\PYGZsh{} Prevents overflow}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Verify boundary conditions}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Test with non\PYGZhy{}power\PYGZhy{}of\PYGZhy{}2 sizes}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{1001}\PYG{p}{,} \PYG{n}{device}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cuda}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Not 1024!}
\end{sphinxVerbatim}

\end{enumerate}


\subsubsection{NaN or Inf Values}
\label{\detokenize{troubleshooting:nan-or-inf-values}}
\sphinxAtStartPar
\sphinxstylestrong{Common causes}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Division by zero}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Add epsilon}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{x} \PYG{o}{/} \PYG{p}{(}\PYG{n}{y} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}8}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Overflow in exp}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Subtract max before exp}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Log of negative/zero}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Clamp before log}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{tl}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{1e\PYGZhy{}10}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Uninitialized memory}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Always initialize}
\PYG{n}{acc} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{M}\PYG{p}{,} \PYG{n}{N}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tl}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Not: acc = tl.empty(...)}
\end{sphinxVerbatim}

\end{enumerate}


\subsection{Performance Issues}
\label{\detokenize{troubleshooting:performance-issues}}

\subsubsection{Slower Than PyTorch}
\label{\detokenize{troubleshooting:slower-than-pytorch}}
\sphinxAtStartPar
\sphinxstylestrong{Diagnosis}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Profile both}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} PyTorch}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{benchmark}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{benchmark}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{benchmark}\PYG{o}{.}\PYG{n}{Timer}\PYG{p}{(}\PYG{n}{stmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{torch.matmul(a, b)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{globals}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{a}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{b}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{t}\PYG{o}{.}\PYG{n}{timeit}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Triton}
\PYG{n}{ms} \PYG{o}{=} \PYG{n}{triton}\PYG{o}{.}\PYG{n}{testing}\PYG{o}{.}\PYG{n}{do\PYGZus{}bench}\PYG{p}{(}\PYG{k}{lambda}\PYG{p}{:} \PYG{n}{triton\PYGZus{}matmul}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Check if PyTorch uses vendor libs}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} PyTorch often uses cuBLAS, cuDNN}
\PYG{c+c1}{\PYGZsh{} These are extremely optimized}
\PYG{c+c1}{\PYGZsh{} Matching them is success!}
\end{sphinxVerbatim}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Common reasons}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Not using Tensor Cores}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Ensure FP16/BF16 inputs

\item {} 
\sphinxAtStartPar
Use \sphinxcode{\sphinxupquote{tl.dot()}} for matmul

\item {} 
\sphinxAtStartPar
Check block sizes are multiples of 16

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Suboptimal configuration}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Need auto\sphinxhyphen{}tuning

\item {} 
\sphinxAtStartPar
Try different block sizes

\item {} 
\sphinxAtStartPar
Adjust num\_warps and num\_stages

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Non\sphinxhyphen{}contiguous tensors}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check contiguity}
\PYG{k}{assert} \PYG{n}{a}\PYG{o}{.}\PYG{n}{is\PYGZus{}contiguous}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Make contiguous if needed}
\PYG{n}{a} \PYG{o}{=} \PYG{n}{a}\PYG{o}{.}\PYG{n}{contiguous}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Missing optimizations}
\begin{itemize}
\item {} 
\sphinxAtStartPar
No kernel fusion

\item {} 
\sphinxAtStartPar
Not using SRAM effectively

\item {} 
\sphinxAtStartPar
Poor memory access patterns

\end{itemize}

\end{enumerate}


\subsubsection{Low GPU Utilization}
\label{\detokenize{troubleshooting:low-gpu-utilization}}
\sphinxAtStartPar
\sphinxstylestrong{Check with}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nvidia}\PYG{o}{\PYGZhy{}}\PYG{n}{smi} \PYG{o}{\PYGZhy{}}\PYG{n}{l} \PYG{l+m+mi}{1}  \PYG{c+c1}{\PYGZsh{} Monitor GPU utilization}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{If low (\textless{}50\%)}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Increase batch size}: More parallel work

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Check occupancy}: May be too low

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Pipeline CPU\sphinxhyphen{}GPU}: Overlap data transfer and compute

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Profile}: Use \sphinxcode{\sphinxupquote{nsys}} to find bottlenecks

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{If high (\textgreater{}90\%) but slow}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Memory\sphinxhyphen{}bound: Optimize memory access

\item {} 
\sphinxAtStartPar
Compute\sphinxhyphen{}bound: Use Tensor Cores, increase arithmetic intensity

\end{itemize}


\subsection{Compilation Issues}
\label{\detokenize{troubleshooting:compilation-issues}}

\subsubsection{Compilation Errors}
\label{\detokenize{troubleshooting:compilation-errors}}
\sphinxAtStartPar
\sphinxstylestrong{Error}: \sphinxcode{\sphinxupquote{TypeError: unsupported operand type(s)}}

\sphinxAtStartPar
\sphinxstylestrong{Cause}: Type mismatch in Triton

\sphinxAtStartPar
\sphinxstylestrong{Solution}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Explicit casting}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{tl}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{tl}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Error}: \sphinxcode{\sphinxupquote{constexpr}} parameter not constant

\sphinxAtStartPar
\sphinxstylestrong{Solution}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Must be compile\PYGZhy{}time constant}
\PYG{n}{BLOCK\PYGZus{}SIZE}\PYG{p}{:} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{constexpr} \PYG{o}{=} \PYG{l+m+mi}{128}  \PYG{c+c1}{\PYGZsh{} Not a variable!}
\end{sphinxVerbatim}


\subsubsection{Slow Compilation}
\label{\detokenize{troubleshooting:slow-compilation}}
\sphinxAtStartPar
\sphinxstylestrong{First compilation is slow} (minutes):
\begin{itemize}
\item {} 
\sphinxAtStartPar
Normal! Triton JIT\sphinxhyphen{}compiles and auto\sphinxhyphen{}tunes

\item {} 
\sphinxAtStartPar
Subsequent runs use cached version

\item {} 
\sphinxAtStartPar
Use \sphinxcode{\sphinxupquote{TRITON\_CACHE\_DIR}} to persist cache

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Every run is slow}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Check if cache is working:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{os}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{environ}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TRITON\PYGZus{}CACHE\PYGZus{}DIR}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Set cache directory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{export} \PYG{n}{TRITON\PYGZus{}CACHE\PYGZus{}DIR}\PYG{o}{=}\PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{cache}
\end{sphinxVerbatim}

\end{itemize}


\subsection{Platform\sphinxhyphen{}Specific Issues}
\label{\detokenize{troubleshooting:platform-specific-issues}}

\subsubsection{NVIDIA\sphinxhyphen{}Specific}
\label{\detokenize{troubleshooting:nvidia-specific}}
\sphinxAtStartPar
\sphinxstylestrong{Compute capability too low}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+ne}{RuntimeError}\PYG{p}{:} \PYG{n}{Triton} \PYG{n}{requires} \PYG{n}{compute} \PYG{n}{capability} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mf}{7.0}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Solution}: Upgrade GPU (Volta or newer required)

\sphinxAtStartPar
\sphinxstylestrong{Driver version mismatch}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{CUDA} \PYG{n}{driver} \PYG{n}{version} \PYG{o+ow}{is} \PYG{n}{insufficient} \PYG{k}{for} \PYG{n}{CUDA} \PYG{n}{runtime} \PYG{n}{version}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Solution}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check versions}
\PYG{n}{nvidia}\PYG{o}{\PYGZhy{}}\PYG{n}{smi}  \PYG{c+c1}{\PYGZsh{} Driver version}
\PYG{n}{python} \PYG{o}{\PYGZhy{}}\PYG{n}{c} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{import torch; print(torch.version.cuda)}\PYG{l+s+s2}{\PYGZdq{}}  \PYG{c+c1}{\PYGZsh{} CUDA version}

\PYG{c+c1}{\PYGZsh{} Update driver if needed}
\end{sphinxVerbatim}


\subsubsection{AMD\sphinxhyphen{}Specific}
\label{\detokenize{troubleshooting:amd-specific}}
\sphinxAtStartPar
\sphinxstylestrong{ROCm not found}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+ne}{ModuleNotFoundError}\PYG{p}{:} \PYG{n}{No} \PYG{n}{module} \PYG{n}{named} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{triton.backends.amd}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Solution}: Install ROCm\sphinxhyphen{}enabled Triton:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{triton} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{index}\PYG{o}{\PYGZhy{}}\PYG{n}{url} \PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{download}\PYG{o}{.}\PYG{n}{pytorch}\PYG{o}{.}\PYG{n}{org}\PYG{o}{/}\PYG{n}{whl}\PYG{o}{/}\PYG{n}{rocm5}\PYG{l+m+mf}{.6}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Kernel launch failures}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Check \sphinxcode{\sphinxupquote{HSA\_OVERRIDE\_GFX\_VERSION}} for older GPUs

\item {} 
\sphinxAtStartPar
Verify ROCm version matches GPU architecture

\end{itemize}


\subsection{Multi\sphinxhyphen{}GPU Issues}
\label{\detokenize{troubleshooting:multi-gpu-issues}}

\subsubsection{Wrong GPU Selected}
\label{\detokenize{troubleshooting:wrong-gpu-selected}}
\sphinxAtStartPar
\sphinxstylestrong{Specify GPU}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Set before any CUDA operations}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{os}
\PYG{n}{os}\PYG{o}{.}\PYG{n}{environ}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CUDA\PYGZus{}VISIBLE\PYGZus{}DEVICES}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{0}\PYG{l+s+s1}{\PYGZsq{}}  \PYG{c+c1}{\PYGZsh{} Use GPU 0}

\PYG{c+c1}{\PYGZsh{} Or in Python}
\PYG{n}{torch}\PYG{o}{.}\PYG{n}{cuda}\PYG{o}{.}\PYG{n}{set\PYGZus{}device}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{In kernel}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{DEVICE} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{device}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cuda:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpu\PYGZus{}id}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{device}\PYG{o}{=}\PYG{n}{DEVICE}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Debugging Techniques}
\label{\detokenize{troubleshooting:debugging-techniques}}

\subsubsection{Print Debugging}
\label{\detokenize{troubleshooting:print-debugging}}
\sphinxAtStartPar
\sphinxstylestrong{In kernel} (limited):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{pid} \PYG{o}{=} \PYG{n}{tl}\PYG{o}{.}\PYG{n}{program\PYGZus{}id}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Only print from first program}
    \PYG{k}{if} \PYG{n}{pid} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{tl}\PYG{o}{.}\PYG{n}{device\PYGZus{}print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{pid:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{pid}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Outside kernel}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Print shapes, dtypes}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Shape: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{shape}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, dtype: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{dtype}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, device: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{device}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Check values}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Min: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Max: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Look for NaN/Inf}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Has NaN: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{isnan}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Has Inf: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{isinf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Profiling}
\label{\detokenize{troubleshooting:profiling}}
\sphinxAtStartPar
\sphinxstylestrong{NVIDIA}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Timeline profiling}
\PYG{n}{nsys} \PYG{n}{profile} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{output} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}

\PYG{c+c1}{\PYGZsh{} Detailed metrics}
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{set} \PYG{n}{full} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{output} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}

\PYG{c+c1}{\PYGZsh{} Specific metrics}
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{metrics} \PYG{n}{dram\PYGZus{}\PYGZus{}throughput}\PYG{o}{.}\PYG{n}{avg}\PYG{o}{.}\PYG{n}{pct\PYGZus{}of\PYGZus{}peak\PYGZus{}sustained\PYGZus{}elapsed} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{AMD}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rocprof} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{stats} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}


\subsubsection{Assertions}
\label{\detokenize{troubleshooting:assertions}}
\sphinxAtStartPar
\sphinxstylestrong{Add runtime checks}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Check bounds}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{static\PYGZus{}assert}\PYG{p}{(}\PYG{n}{BLOCK\PYGZus{}M} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{256}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BLOCK\PYGZus{}M too large}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Runtime assertion}
    \PYG{n}{tl}\PYG{o}{.}\PYG{n}{device\PYGZus{}assert}\PYG{p}{(}\PYG{n}{offset} \PYG{o}{\PYGZlt{}} \PYG{n}{n\PYGZus{}elements}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Out of bounds}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Unit Testing}
\label{\detokenize{troubleshooting:unit-testing}}
\sphinxAtStartPar
\sphinxstylestrong{Test correctness}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{test\PYGZus{}my\PYGZus{}kernel}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{device}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cuda}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Triton result}
    \PYG{n}{y\PYGZus{}triton} \PYG{o}{=} \PYG{n}{my\PYGZus{}kernel}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Reference (PyTorch)}
    \PYG{n}{y\PYGZus{}torch} \PYG{o}{=} \PYG{n}{reference\PYGZus{}implementation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Check}
    \PYG{n}{torch}\PYG{o}{.}\PYG{n}{testing}\PYG{o}{.}\PYG{n}{assert\PYGZus{}close}\PYG{p}{(}\PYG{n}{y\PYGZus{}triton}\PYG{p}{,} \PYG{n}{y\PYGZus{}torch}\PYG{p}{,} \PYG{n}{rtol}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,} \PYG{n}{atol}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Common Error Messages}
\label{\detokenize{troubleshooting:common-error-messages}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{50}{100}\X{50}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Error
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Solution
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{CUDA out of memory}}
&
\sphinxAtStartPar
Reduce batch size, use gradient checkpointing
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{out of resource: shared memory}}
&
\sphinxAtStartPar
Reduce BLOCK\_SIZE, num\_stages
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{out of resource: registers}}
&
\sphinxAtStartPar
Reduce local variables, smaller blocks
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{constexpr parameter must be compile\sphinxhyphen{}time constant}}
&
\sphinxAtStartPar
Use \sphinxcode{\sphinxupquote{tl.constexpr}} type annotation
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Tensor must be contiguous}}
&
\sphinxAtStartPar
Call \sphinxcode{\sphinxupquote{.contiguous()}} on input
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Type mismatch}}
&
\sphinxAtStartPar
Explicit \sphinxcode{\sphinxupquote{to()}} casting
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{AttributeError: \textquotesingle{}Tensor\textquotesingle{} object has no attribute \textquotesingle{}stride\textquotesingle{}}}
&
\sphinxAtStartPar
Pass tensor, not pointer
\\
\sphinxhline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{RuntimeError: unspecified launch failure}}
&
\sphinxAtStartPar
Out of bounds access, check masking
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsection{Getting Help}
\label{\detokenize{troubleshooting:getting-help}}

\subsubsection{When Stuck}
\label{\detokenize{troubleshooting:when-stuck}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Search existing issues}: \sphinxhref{https://github.com/openai/triton/issues}{Triton GitHub Issues}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Minimal reproducible example}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{torch}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{triton}

\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{jit}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{broken\PYGZus{}kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Simplified version that shows the issue}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Provide details}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Error message (full stack trace)

\item {} 
\sphinxAtStartPar
Triton version: \sphinxcode{\sphinxupquote{import triton; print(triton.\_\_version\_\_)}}

\item {} 
\sphinxAtStartPar
PyTorch version: \sphinxcode{\sphinxupquote{import torch; print(torch.\_\_version\_\_)}}

\item {} 
\sphinxAtStartPar
GPU model: \sphinxcode{\sphinxupquote{nvidia\sphinxhyphen{}smi}}

\item {} 
\sphinxAtStartPar
Minimal code to reproduce

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ask in right place}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton/discussions}{Triton Discussions} \sphinxhyphen{} General questions

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton/issues}{Triton Issues} \sphinxhyphen{} Bugs

\item {} 
\sphinxAtStartPar
\sphinxhref{https://discuss.pytorch.org/}{PyTorch Forums} \sphinxhyphen{} PyTorch integration

\end{itemize}

\end{enumerate}


\subsection{Best Practices for Avoiding Issues}
\label{\detokenize{troubleshooting:best-practices-for-avoiding-issues}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Start simple}: Get basic version working before optimizing

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Test incrementally}: Add features one at a time

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Verify correctness}: Always compare with PyTorch

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Profile early}: Understand bottlenecks before optimizing

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use auto\sphinxhyphen{}tuning}: Don’t guess optimal configurations

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Check edge cases}: Non\sphinxhyphen{}power\sphinxhyphen{}of\sphinxhyphen{}2 sizes, empty tensors

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Handle boundaries}: Always use masking for safety

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Maintain precision}: Use float32 for accumulation

\end{enumerate}


\subsection{Prevention Checklist}
\label{\detokenize{troubleshooting:prevention-checklist}}
\sphinxAtStartPar
Before deploying:

\sphinxAtStartPar
{[} {]} Tested with various input sizes
{[} {]} Compared output with PyTorch
{[} {]} Profiled performance
{[} {]} Checked for NaN/Inf
{[} {]} Verified memory usage is reasonable
{[} {]} Tested edge cases (size=1, size=prime number, etc.)
{[} {]} Added assertions for debug builds
{[} {]} Documented any limitations


\subsection{Summary}
\label{\detokenize{troubleshooting:summary}}
\sphinxAtStartPar
Most issues fall into three categories:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory}: OOM, shared memory limits \sphinxhyphen{}\textgreater{} Reduce sizes

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Correctness}: Wrong results, NaN \sphinxhyphen{}\textgreater{} Check masking, precision, stability

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Performance}: Slow \sphinxhyphen{}\textgreater{} Profile, auto\sphinxhyphen{}tune, optimize memory access

\end{enumerate}

\sphinxAtStartPar
When in doubt:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Profile to find the real bottleneck

\item {} 
\sphinxAtStartPar
Compare with PyTorch to verify correctness

\item {} 
\sphinxAtStartPar
Start simple and add complexity incrementally

\end{itemize}

\sphinxAtStartPar
Still stuck? See {\hyperref[\detokenize{references::doc}]{\sphinxcrossref{\DUrole{doc}{References and Resources}}}} for more resources.

\sphinxstepscope


\section{References and Resources}
\label{\detokenize{references:references-and-resources}}\label{\detokenize{references::doc}}
\sphinxAtStartPar
Essential resources for learning more about GPU programming, Triton, and related topics.


\subsection{Official Documentation}
\label{\detokenize{references:official-documentation}}

\subsubsection{Triton}
\label{\detokenize{references:triton}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://triton-lang.org/}{Triton Documentation} \sphinxhyphen{} Official docs

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton}{Triton GitHub Repository} \sphinxhyphen{} Source code and examples

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton/discussions}{Triton Discussions} \sphinxhyphen{} Community Q\&A

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton/issues}{Triton Issues} \sphinxhyphen{} Bug reports and feature requests

\end{itemize}


\subsubsection{CUDA}
\label{\detokenize{references:cuda}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.nvidia.com/cuda/cuda-c-programming-guide/}{CUDA C Programming Guide} \sphinxhyphen{} Comprehensive CUDA reference

\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/}{CUDA C Best Practices Guide} \sphinxhyphen{} Optimization techniques

\item {} 
\sphinxAtStartPar
\sphinxhref{https://developer.nvidia.com/blog}{NVIDIA Developer Blog} \sphinxhyphen{} Latest updates and tutorials

\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.nvidia.com/cuda/cublas/}{cuBLAS Documentation} \sphinxhyphen{} Matrix operations library

\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.nvidia.com/deeplearning/cudnn/}{cuDNN Documentation} \sphinxhyphen{} Deep learning primitives

\end{itemize}


\subsubsection{ROCm (AMD)}
\label{\detokenize{references:rocm-amd}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://rocmdocs.amd.com/}{ROCm Documentation} \sphinxhyphen{} AMD GPU programming

\item {} 
\sphinxAtStartPar
\sphinxhref{https://rocmdocs.amd.com/projects/HIP/}{HIP Programming Guide} \sphinxhyphen{} CUDA\sphinxhyphen{}like programming for AMD

\item {} 
\sphinxAtStartPar
\sphinxhref{https://developer.amd.com/}{AMD Developer Resources}

\end{itemize}


\subsubsection{PyTorch}
\label{\detokenize{references:pytorch}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/}{PyTorch Documentation} \sphinxhyphen{} Deep learning framework

\item {} 
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/tutorials/advanced/cpp\_extension.html}{PyTorch Custom C++/CUDA Extensions}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/blog/}{PyTorch Internals}

\end{itemize}


\subsection{Research Papers}
\label{\detokenize{references:research-papers}}

\subsubsection{Flash Attention}
\label{\detokenize{references:flash-attention}}
\sphinxAtStartPar
\sphinxstylestrong{Flash Attention} \sphinxhyphen{} Dao et al., 2022
\begin{quote}

\sphinxAtStartPar
Seminal paper on O(N) memory attention algorithm
\begin{itemize}
\item {} 
\sphinxAtStartPar
Paper: \sphinxhref{https://arxiv.org/abs/2205.14135}{arXiv:2205.14135}

\item {} 
\sphinxAtStartPar
Key contribution: Online softmax algorithm

\item {} 
\sphinxAtStartPar
Impact: Enables 16K+ token context windows

\end{itemize}
\end{quote}

\sphinxAtStartPar
\sphinxstylestrong{Flash Attention\sphinxhyphen{}2} \sphinxhyphen{} Dao, 2023
\begin{quote}

\sphinxAtStartPar
Improved version with better parallelism
\begin{itemize}
\item {} 
\sphinxAtStartPar
Paper: \sphinxhref{https://arxiv.org/abs/2307.08691}{arXiv:2307.08691}

\item {} 
\sphinxAtStartPar
Improvements: 2x faster than original

\item {} 
\sphinxAtStartPar
Techniques: Warp specialization, better scheduling

\end{itemize}
\end{quote}


\subsubsection{Normalization}
\label{\detokenize{references:normalization}}
\sphinxAtStartPar
\sphinxstylestrong{Layer Normalization} \sphinxhyphen{} Ba et al., 2016
\begin{quote}

\sphinxAtStartPar
Foundation for Transformer architectures
\begin{itemize}
\item {} 
\sphinxAtStartPar
Paper: \sphinxhref{https://arxiv.org/abs/1607.06450}{arXiv:1607.06450}

\item {} 
\sphinxAtStartPar
Key idea: Normalize across features, not batch

\item {} 
\sphinxAtStartPar
Usage: BERT, GPT, all modern Transformers

\end{itemize}
\end{quote}

\sphinxAtStartPar
\sphinxstylestrong{RMSNorm} \sphinxhyphen{} Zhang \& Sennrich, 2019
\begin{quote}

\sphinxAtStartPar
Simplified layer normalization
\begin{itemize}
\item {} 
\sphinxAtStartPar
Paper: \sphinxhref{https://arxiv.org/abs/1910.07467}{arXiv:1910.07467}

\item {} 
\sphinxAtStartPar
Simplification: No mean subtraction

\item {} 
\sphinxAtStartPar
Used in: LLaMA, GPT\sphinxhyphen{}NeoX

\end{itemize}
\end{quote}


\subsubsection{Optimization Techniques}
\label{\detokenize{references:optimization-techniques}}
\sphinxAtStartPar
\sphinxstylestrong{Automatic Differentiation} \sphinxhyphen{} Baydin et al., 2018
\begin{quote}

\sphinxAtStartPar
Survey of autodiff techniques
\begin{itemize}
\item {} 
\sphinxAtStartPar
Paper: \sphinxhref{https://arxiv.org/abs/1502.05767}{arXiv:1502.05767}

\item {} 
\sphinxAtStartPar
Covers forward and reverse mode

\item {} 
\sphinxAtStartPar
Essential for understanding backward passes

\end{itemize}
\end{quote}

\sphinxAtStartPar
\sphinxstylestrong{Mixed Precision Training} \sphinxhyphen{} Micikevicius et al., 2018
\begin{quote}

\sphinxAtStartPar
Training with FP16 for speedup
\begin{itemize}
\item {} 
\sphinxAtStartPar
Paper: \sphinxhref{https://arxiv.org/abs/1710.03740}{arXiv:1710.03740}

\item {} 
\sphinxAtStartPar
Techniques: Loss scaling, master weights

\item {} 
\sphinxAtStartPar
Impact: 2\sphinxhyphen{}3x training speedup

\end{itemize}
\end{quote}


\subsubsection{Triton}
\label{\detokenize{references:id1}}
\sphinxAtStartPar
\sphinxstylestrong{Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations} \sphinxhyphen{} Tillet et al., 2019
\begin{quote}

\sphinxAtStartPar
Original Triton paper
\begin{itemize}
\item {} 
\sphinxAtStartPar
Paper: \sphinxhref{https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf}{ACM MAPL 2019}

\item {} 
\sphinxAtStartPar
Key insight: Block\sphinxhyphen{}level programming model

\item {} 
\sphinxAtStartPar
Impact: Made GPU programming accessible

\end{itemize}
\end{quote}


\subsection{Books}
\label{\detokenize{references:books}}

\subsubsection{GPU Programming}
\label{\detokenize{references:gpu-programming}}
\sphinxAtStartPar
\sphinxstylestrong{Programming Massively Parallel Processors} \sphinxhyphen{} Kirk \& Hwu
\begin{itemize}
\item {} 
\sphinxAtStartPar
Comprehensive introduction to GPU programming

\item {} 
\sphinxAtStartPar
Covers CUDA fundamentals through advanced topics

\item {} 
\sphinxAtStartPar
Excellent for understanding hardware

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{CUDA by Example} \sphinxhyphen{} Sanders \& Kandrot
\begin{itemize}
\item {} 
\sphinxAtStartPar
Practical, example\sphinxhyphen{}driven approach

\item {} 
\sphinxAtStartPar
Good for beginners

\item {} 
\sphinxAtStartPar
Covers basic to intermediate topics

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Professional CUDA C Programming} \sphinxhyphen{} Cheng et al.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Advanced CUDA techniques

\item {} 
\sphinxAtStartPar
Performance optimization

\item {} 
\sphinxAtStartPar
Real\sphinxhyphen{}world case studies

\end{itemize}


\subsubsection{Deep Learning}
\label{\detokenize{references:deep-learning}}
\sphinxAtStartPar
\sphinxstylestrong{Deep Learning} \sphinxhyphen{} Goodfellow, Bengio \& Courville
\begin{itemize}
\item {} 
\sphinxAtStartPar
Comprehensive ML theory

\item {} 
\sphinxAtStartPar
Mathematical foundations

\item {} 
\sphinxAtStartPar
Available free online

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Dive into Deep Learning} \sphinxhyphen{} Zhang et al.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Interactive textbook

\item {} 
\sphinxAtStartPar
Code examples with PyTorch/MXNet

\item {} 
\sphinxAtStartPar
\sphinxhref{https://d2l.ai/}{Available online}

\end{itemize}


\subsection{Tools and Profilers}
\label{\detokenize{references:tools-and-profilers}}

\subsubsection{NVIDIA Tools}
\label{\detokenize{references:nvidia-tools}}
\sphinxAtStartPar
\sphinxstylestrong{Nsight Systems}
\begin{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
System\sphinxhyphen{}wide profiling

\item {} 
\sphinxAtStartPar
Timeline visualization

\item {} 
\sphinxAtStartPar
CPU\sphinxhyphen{}GPU interaction

\end{itemize}

\sphinxAtStartPar
Installation:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Included with CUDA toolkit}
\PYG{n}{nsys} \PYG{n}{profile} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{output} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}
\PYG{n}{nsys}\PYG{o}{\PYGZhy{}}\PYG{n}{ui} \PYG{n}{output}\PYG{o}{.}\PYG{n}{qdrep}  \PYG{c+c1}{\PYGZsh{} View results}
\end{sphinxVerbatim}
\end{quote}

\sphinxAtStartPar
\sphinxstylestrong{Nsight Compute}
\begin{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Kernel\sphinxhyphen{}level profiling

\item {} 
\sphinxAtStartPar
Detailed metrics (memory, compute, occupancy)

\item {} 
\sphinxAtStartPar
Optimization suggestions

\end{itemize}

\sphinxAtStartPar
Usage:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ncu} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{set} \PYG{n}{full} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{output} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}
\PYG{n}{ncu}\PYG{o}{\PYGZhy{}}\PYG{n}{ui} \PYG{n}{output}\PYG{o}{.}\PYG{n}{ncu}\PYG{o}{\PYGZhy{}}\PYG{n}{rep}  \PYG{c+c1}{\PYGZsh{} View results}
\end{sphinxVerbatim}
\end{quote}

\sphinxAtStartPar
\sphinxstylestrong{NVIDIA Visual Profiler (nvvp)}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Legacy tool (being replaced by Nsight)

\item {} 
\sphinxAtStartPar
Still useful for older GPUs

\end{itemize}


\subsubsection{AMD Tools}
\label{\detokenize{references:amd-tools}}
\sphinxAtStartPar
\sphinxstylestrong{ROCProfiler}
\begin{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
AMD’s profiling tool

\item {} 
\sphinxAtStartPar
Similar to NVIDIA tools

\end{itemize}

\sphinxAtStartPar
Usage:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rocprof} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{stats} \PYG{n}{python} \PYG{n}{script}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}
\end{quote}

\sphinxAtStartPar
\sphinxstylestrong{Radeon GPU Profiler}
\begin{itemize}
\item {} 
\sphinxAtStartPar
GUI\sphinxhyphen{}based profiler

\item {} 
\sphinxAtStartPar
Visualization tools

\end{itemize}


\subsubsection{PyTorch Profiler}
\label{\detokenize{references:pytorch-profiler}}
\sphinxAtStartPar
\sphinxstylestrong{torch.profiler}
\begin{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Profile PyTorch operations

\item {} 
\sphinxAtStartPar
Integrated with TensorBoard

\end{itemize}

\sphinxAtStartPar
Example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{profiler}\PYG{o}{.}\PYG{n}{profile}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{prof}\PYG{p}{:}
    \PYG{n}{output} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{n+nb}{input}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{prof}\PYG{o}{.}\PYG{n}{key\PYGZus{}averages}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{table}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}


\subsubsection{Benchmarking}
\label{\detokenize{references:benchmarking}}
\sphinxAtStartPar
\sphinxstylestrong{Triton Built\sphinxhyphen{}in Benchmarking}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@triton}\PYG{o}{.}\PYG{n}{testing}\PYG{o}{.}\PYG{n}{perf\PYGZus{}report}\PYG{p}{(}\PYG{n}{configs}\PYG{p}{)}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{benchmark}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ms} \PYG{o}{=} \PYG{n}{triton}\PYG{o}{.}\PYG{n}{testing}\PYG{o}{.}\PYG{n}{do\PYGZus{}bench}\PYG{p}{(}\PYG{k}{lambda}\PYG{p}{:} \PYG{n}{kernel}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{performance\PYGZus{}metric}\PYG{p}{(}\PYG{n}{ms}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{PyTorch Benchmark}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{benchmark}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{benchmark}

\PYG{n}{t} \PYG{o}{=} \PYG{n}{benchmark}\PYG{o}{.}\PYG{n}{Timer}\PYG{p}{(}\PYG{n}{stmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{operation()}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{globals}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{t}\PYG{o}{.}\PYG{n}{timeit}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Online Resources}
\label{\detokenize{references:online-resources}}

\subsubsection{Tutorials and Courses}
\label{\detokenize{references:tutorials-and-courses}}
\sphinxAtStartPar
\sphinxstylestrong{NVIDIA Deep Learning Institute}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.nvidia.com/en-us/training/}{DLI Courses}

\item {} 
\sphinxAtStartPar
Hands\sphinxhyphen{}on GPU programming courses

\item {} 
\sphinxAtStartPar
Free and paid options

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Coursera \sphinxhyphen{} GPU Programming Specialization}
\begin{itemize}
\item {} 
\sphinxAtStartPar
University courses on GPU programming

\item {} 
\sphinxAtStartPar
Theory and practice

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{YouTube \sphinxhyphen{} NVIDIA Developer Channel}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Conference talks

\item {} 
\sphinxAtStartPar
Tutorial videos

\item {} 
\sphinxAtStartPar
Latest technology updates

\end{itemize}


\subsubsection{Community}
\label{\detokenize{references:community}}
\sphinxAtStartPar
\sphinxstylestrong{Triton Community}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton/discussions}{GitHub Discussions}

\item {} 
\sphinxAtStartPar
Active community

\item {} 
\sphinxAtStartPar
Get help, share kernels

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{PyTorch Forums}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://discuss.pytorch.org/}{discuss.pytorch.org}

\item {} 
\sphinxAtStartPar
Questions on PyTorch + Triton integration

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Reddit}
\begin{itemize}
\item {} 
\sphinxAtStartPar
r/CUDA \sphinxhyphen{} CUDA programming

\item {} 
\sphinxAtStartPar
r/MachineLearning \sphinxhyphen{} ML discussions

\item {} 
\sphinxAtStartPar
r/computergraphics \sphinxhyphen{} GPU graphics

\end{itemize}


\subsubsection{Blogs and Articles}
\label{\detokenize{references:blogs-and-articles}}
\sphinxAtStartPar
\sphinxstylestrong{Lil’Log}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://lilianweng.github.io/}{lilianweng.github.io}

\item {} 
\sphinxAtStartPar
Excellent ML explanations

\item {} 
\sphinxAtStartPar
Flash Attention breakdown

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Jay Alammar’s Blog}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://jalammar.github.io/}{jalammar.github.io}

\item {} 
\sphinxAtStartPar
Visual guides to Transformers

\item {} 
\sphinxAtStartPar
Attention mechanisms explained

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Hugging Face Blog}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://huggingface.co/blog}{huggingface.co/blog}

\item {} 
\sphinxAtStartPar
ML engineering articles

\item {} 
\sphinxAtStartPar
Optimization techniques

\end{itemize}


\subsection{Example Repositories}
\label{\detokenize{references:example-repositories}}

\subsubsection{Triton Examples}
\label{\detokenize{references:triton-examples}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton/tree/main/python/tutorials}{OpenAI Triton Tutorials} \sphinxhyphen{} Official tutorials

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/srush/Triton-Puzzles}{Triton Puzzles} \sphinxhyphen{} Learn by solving puzzles

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/Dao-AILab/awesome-triton}{Awesome Triton} \sphinxhyphen{} Curated list of Triton resources

\end{itemize}


\subsubsection{Production Usage}
\label{\detokenize{references:production-usage}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/vllm-project/vllm}{vLLM} \sphinxhyphen{} LLM inference engine using Triton

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/Dao-AILab/flash-attention}{Flash Attention} \sphinxhyphen{} Official Flash Attention implementation

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/facebookresearch/xformers}{xformers} \sphinxhyphen{} Efficient Transformer components

\end{itemize}


\subsection{Hardware Documentation}
\label{\detokenize{references:hardware-documentation}}

\subsubsection{NVIDIA GPUs}
\label{\detokenize{references:nvidia-gpus}}
\sphinxAtStartPar
\sphinxstylestrong{Architecture Whitepapers}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf}{Volta Architecture}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf}{Ampere Architecture}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://resources.nvidia.com/en-us-tensor-core}{Hopper Architecture}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{GPU Specifications}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.nvidia.com/en-us/data-center/}{NVIDIA Data Center GPUs}

\item {} 
\sphinxAtStartPar
A100, H100, L40S specifications

\item {} 
\sphinxAtStartPar
Compute capabilities reference

\end{itemize}


\subsubsection{AMD GPUs}
\label{\detokenize{references:amd-gpus}}
\sphinxAtStartPar
\sphinxstylestrong{CDNA Architecture}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.amd.com/en/products/accelerators/instinct/mi200.html}{MI200 Architecture}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.amd.com/en/products/accelerators/instinct/mi300.html}{MI300 Series}

\end{itemize}


\subsection{Performance Databases}
\label{\detokenize{references:performance-databases}}
\sphinxAtStartPar
\sphinxstylestrong{MLPerf}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://mlcommons.org/benchmarks/}{mlcommons.org/benchmarks}

\item {} 
\sphinxAtStartPar
Standardized ML benchmarks

\item {} 
\sphinxAtStartPar
Compare different hardware

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Tensor Core Performance}
\begin{itemize}
\item {} 
\sphinxAtStartPar
NVIDIA’s published FLOPS numbers

\item {} 
\sphinxAtStartPar
Vendor benchmarks

\end{itemize}


\subsection{Keeping Up to Date}
\label{\detokenize{references:keeping-up-to-date}}

\subsubsection{Subscribe To}
\label{\detokenize{references:subscribe-to}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/openai/triton/releases}{Triton Releases} \sphinxhyphen{} New features

\item {} 
\sphinxAtStartPar
\sphinxhref{https://developer.nvidia.com/blog}{NVIDIA Developer Blog} \sphinxhyphen{} GPU news

\item {} 
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/blog/}{PyTorch Blog} \sphinxhyphen{} Framework updates

\item {} 
\sphinxAtStartPar
\sphinxhref{https://paperswithcode.com/}{Papers with Code} \sphinxhyphen{} Latest research

\end{itemize}


\subsubsection{Conferences}
\label{\detokenize{references:conferences}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GTC (GPU Technology Conference)} \sphinxhyphen{} NVIDIA’s annual conference

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{NeurIPS} \sphinxhyphen{} Neural Information Processing Systems

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ICML} \sphinxhyphen{} International Conference on Machine Learning

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{MLSys} \sphinxhyphen{} Machine Learning and Systems

\end{itemize}


\subsection{Academic Courses}
\label{\detokenize{references:academic-courses}}
\sphinxAtStartPar
\sphinxstylestrong{Stanford CS149 \sphinxhyphen{} Parallel Computing}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://cs149.stanford.edu/}{cs149.stanford.edu}

\item {} 
\sphinxAtStartPar
GPU programming fundamentals

\item {} 
\sphinxAtStartPar
Assignments and lecture notes

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Stanford CS231n \sphinxhyphen{} CNNs for Visual Recognition}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Covers optimization and efficiency

\item {} 
\sphinxAtStartPar
GPU acceleration topics

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{MIT 6.S965 \sphinxhyphen{} TinyML and Efficient Deep Learning}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Efficiency techniques

\item {} 
\sphinxAtStartPar
Includes GPU optimization

\end{itemize}


\subsection{Contributing}
\label{\detokenize{references:contributing}}
\sphinxAtStartPar
Want to contribute to the ecosystem?
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Triton}: Submit kernels, fix bugs, improve docs

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PyTorch}: Integrate Triton kernels

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Research}: Publish new techniques

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Education}: Write tutorials, create videos

\end{itemize}

\sphinxAtStartPar
See \sphinxhref{https://github.com/openai/triton/blob/main/CONTRIBUTING.md}{Contributing Guidelines}


\subsection{Citation}
\label{\detokenize{references:citation}}
\sphinxAtStartPar
If you use Triton in research, cite:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@inproceedings}\PYG{p}{\PYGZob{}}\PYG{n}{tillet2019triton}\PYG{p}{,}
  \PYG{n}{author}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Tillet}\PYG{p}{,} \PYG{n}{Philippe} \PYG{o+ow}{and} \PYG{n}{Kung}\PYG{p}{,} \PYG{n}{H}\PYG{o}{.} \PYG{n}{T}\PYG{o}{.} \PYG{o+ow}{and} \PYG{n}{Cox}\PYG{p}{,} \PYG{n}{David}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{title}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Triton}\PYG{p}{:} \PYG{n}{An} \PYG{n}{Intermediate} \PYG{n}{Language} \PYG{o+ow}{and} \PYG{n}{Compiler} \PYG{k}{for} \PYG{n}{Tiled} \PYG{n}{Neural} \PYG{n}{Network} \PYG{n}{Computations}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{booktitle}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Proceedings} \PYG{n}{of} \PYG{n}{the} \PYG{l+m+mi}{3}\PYG{n}{rd} \PYG{n}{ACM} \PYG{n}{SIGPLAN} \PYG{n}{International} \PYG{n}{Workshop} \PYG{n}{on} \PYG{n}{Machine} \PYG{n}{Learning} \PYG{o+ow}{and} \PYG{n}{Programming} \PYG{n}{Languages}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{year}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{2019}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{pages}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{10}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{19}\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
For Flash Attention:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@inproceedings}\PYG{p}{\PYGZob{}}\PYG{n}{dao2022flashattention}\PYG{p}{,}
  \PYG{n}{title}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Flash}\PYG{p}{\PYGZob{}}\PYG{n}{A}\PYG{p}{\PYGZcb{}}\PYG{n}{ttention}\PYG{p}{:} \PYG{n}{Fast} \PYG{o+ow}{and} \PYG{n}{Memory}\PYG{o}{\PYGZhy{}}\PYG{n}{Efficient} \PYG{n}{Exact} \PYG{n}{Attention} \PYG{k}{with} \PYG{p}{\PYGZob{}}\PYG{n}{IO}\PYG{p}{\PYGZcb{}}\PYG{o}{\PYGZhy{}}\PYG{n}{Awareness}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{author}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Dao}\PYG{p}{,} \PYG{n}{Tri} \PYG{o+ow}{and} \PYG{n}{Fu}\PYG{p}{,} \PYG{n}{Daniel} \PYG{n}{Y}\PYG{o}{.} \PYG{o+ow}{and} \PYG{n}{Ermon}\PYG{p}{,} \PYG{n}{Stefano} \PYG{o+ow}{and} \PYG{n}{Rudra}\PYG{p}{,} \PYG{n}{Atri} \PYG{o+ow}{and} \PYG{n}{R}\PYG{p}{\PYGZob{}}\PYGZbs{}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e\PYGZcb{}, Christopher\PYGZcb{},}
  \PYG{n}{booktitle}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Advances} \PYG{o+ow}{in} \PYG{n}{Neural} \PYG{n}{Information} \PYG{n}{Processing} \PYG{n}{Systems} \PYG{p}{(}\PYG{n}{NeurIPS}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{year}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{2022}\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Quick Reference}
\label{\detokenize{references:quick-reference}}
\sphinxAtStartPar
\sphinxstylestrong{Most Important}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Triton docs: \sphinxurl{https://triton-lang.org/}

\item {} 
\sphinxAtStartPar
CUDA guide: \sphinxurl{https://docs.nvidia.com/cuda/cuda-c-programming-guide/}

\item {} 
\sphinxAtStartPar
Flash Attention paper: \sphinxurl{https://arxiv.org/abs/2205.14135}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{For Help}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Triton discussions: \sphinxurl{https://github.com/openai/triton/discussions}

\item {} 
\sphinxAtStartPar
PyTorch forums: \sphinxurl{https://discuss.pytorch.org/}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{For Profiling}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
NVIDIA: \sphinxcode{\sphinxupquote{nsys}} and \sphinxcode{\sphinxupquote{ncu}}

\item {} 
\sphinxAtStartPar
AMD: \sphinxcode{\sphinxupquote{rocprof}}

\end{enumerate}

\sphinxAtStartPar
Stay curious and keep optimizing! {[}rocket{]}


\chapter{Quick Navigation}
\label{\detokenize{index:quick-navigation}}
\sphinxAtStartPar
\sphinxstylestrong{CPU Concurrency Quick Start:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/key_concepts::doc}]{\sphinxcrossref{\DUrole{doc}{Key Concepts in Concurrent Programming}}}} \sphinxhyphen{} Understand concurrency fundamentals

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/threading_basics::doc}]{\sphinxcrossref{\DUrole{doc}{Threading Basics: start() and join()}}}} \sphinxhyphen{} Multi\sphinxhyphen{}threaded programming

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{cpu-concurrency/asyncio_event_loop::doc}]{\sphinxcrossref{\DUrole{doc}{Asyncio Event Loop}}}} \sphinxhyphen{} Async I/O programming

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{GPU Programming Quick Start:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-concepts/gpu-fundamentals::doc}]{\sphinxcrossref{\DUrole{doc}{GPU Fundamentals}}}} \sphinxhyphen{} GPU architecture basics

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/01-vector-add::doc}]{\sphinxcrossref{\DUrole{doc}{Vector Addition in Triton}}}} \sphinxhyphen{} Your first GPU kernel

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gpu-tutorials/02-fused-softmax::doc}]{\sphinxcrossref{\DUrole{doc}{Fused Softmax in Triton}}}} \sphinxhyphen{} Kernel optimization

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Common Patterns:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{I/O\sphinxhyphen{}bound tasks} \sphinxhyphen{}\textgreater{} Asyncio or Threading

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{CPU\sphinxhyphen{}bound tasks} \sphinxhyphen{}\textgreater{} Multiprocessing

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Massive parallelism} \sphinxhyphen{}\textgreater{} GPU programming

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep learning} \sphinxhyphen{}\textgreater{} GPU kernels with Triton

\end{itemize}


\chapter{Indices and Tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{genindex}}}

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{search}}}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}