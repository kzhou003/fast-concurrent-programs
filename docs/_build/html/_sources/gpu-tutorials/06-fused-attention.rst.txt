Fused Attention (Flash Attention) in Triton
===========================================

Overview
--------
This implements **Flash Attention v2**, a revolutionary algorithm for computing attention in Transformers. It reduces memory usage from O(NÂ²) to O(N) and achieves 2-4x speedup by using **tiling**, **online softmax**, and **recomputation** strategies. This is one of the most advanced GPU kernels you'll encounter.

What You'll Learn
-----------------
- The **quadratic memory problem** in standard attention
- **Flash Attention algorithm** and its innovations
- **Online softmax**: Computing softmax without storing QK^T matrix
- **Causal masking** for autoregressive models
- **Tensor descriptors** for advanced memory access
- **Shared memory management** for large K/V blocks
- Why attention is the bottleneck in long-sequence Transformers

Background: The Attention Mechanism
-----------------------------------

Standard Attention Formula
~~~~~~~~~~~~~~~~~~~~~~~~~~

::

Attention(Q, K, V) = softmax(QK^T / âˆšd) V
::


Where:
- Q (Query): (batch, heads, seq_len, head_dim)
- K (Key): (batch, heads, seq_len, head_dim)
- V (Value): (batch, heads, seq_len, head_dim)
- Output: (batch, heads, seq_len, head_dim)

Step-by-Step Computation
~~~~~~~~~~~~~~~~~~~~~~~~

1. **Compute similarity scores**:
   ::

   S = QK^T / âˆšd_k
   S shape: (seq_len, seq_len)  # NÃ—N matrix!
   ::


2. **Apply softmax**:
   ::

   P = softmax(S)
   P shape: (seq_len, seq_len)  # Still NÃ—N!
   ::


3. **Weighted sum of values**:
   ::

   O = PV
   O shape: (seq_len, head_dim)
   ::


The Memory Problem
~~~~~~~~~~~~~~~~~~

For sequence length N=2048, head_dim=64, fp16:

**Intermediate matrices**:
- S (scores): 2048 Ã— 2048 Ã— 2 bytes = 8 MB
- P (attention weights): 2048 Ã— 2048 Ã— 2 bytes = 8 MB
- Total per head: 16 MB

**For a Transformer**:
- Batch size: 32
- Heads: 16
- Total: 32 Ã— 16 Ã— 16 MB = **8 GB** just for attention matrices!

**For N=16384** (long sequences):
- Per head: 16384Â² Ã— 2 bytes = 512 MB
- Total: 32 Ã— 16 Ã— 512 MB = **256 GB** ðŸ˜±

**Clearly unsustainable!**

Flash Attention: The Key Insights
---------------------------------

Insight 1: We Don't Need to Store S and P
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Standard approach**:
1. Compute full S = QK^T
2. Store S in HBM (slow)
3. Compute full P = softmax(S)
4. Store P in HBM
5. Compute O = PV

**Flash Attention**:
1. Compute and process S and P in **blocks**
2. Keep blocks in SRAM (fast)
3. Never materialize full S or P in HBM
4. Only store final output O

**Memory**: O(NÂ²) â†’ O(N) ðŸŽ‰

Insight 2: Online Softmax
~~~~~~~~~~~~~~~~~~~~~~~~~

**Challenge**: How to compute softmax without storing full matrix?

**Standard softmax** requires two passes:
.. code-block:: python

Pass 1: Find max
================
max_score = max(S)

Pass 2: Compute softmax
=======================
P = exp(S - max_score) / sum(exp(S - max_score))
::


**Online algorithm**: Update running statistics as we process blocks!

Insight 3: Tiling
~~~~~~~~~~~~~~~~~

Process attention in blocks:
::

For each block of Q (BLOCK_M rows):
    For each block of K,V (BLOCK_N columns):
        Compute block of S
        Update running softmax statistics
        Accumulate into output
::


**Benefit**: Each block fits in SRAM (fast)!

The Online Softmax Algorithm
----------------------------

Standard Softmax
~~~~~~~~~~~~~~~~

For a row S_i = [sâ‚, sâ‚‚, ..., sâ‚™]:

.. code-block:: python

m = max(sâ‚, sâ‚‚, ..., sâ‚™)
numerator = [exp(sâ‚-m), exp(sâ‚‚-m), ..., exp(sâ‚™-m)]
l = sum(numerator)
P_i = numerator / l
::


Block-wise Computation
~~~~~~~~~~~~~~~~~~~~~~

Suppose we process in two blocks: [sâ‚, sâ‚‚] and [sâ‚ƒ, sâ‚„]:

**After block 1**:
::

m_old = max(sâ‚, sâ‚‚)
l_old = exp(sâ‚ - m_old) + exp(sâ‚‚ - m_old)
::


**After block 2**:
::

m_new = max(m_old, max(sâ‚ƒ, sâ‚„))
::


**Problem**: How to update l_old when m changes?

**Solution**: Correction factor!
::

Î± = exp(m_old - m_new)
l_new = Î± * l_old + exp(sâ‚ƒ - m_new) + exp(sâ‚„ - m_new)
::


**Why Î±?**
::

Old contribution: exp(sâ‚ - m_old) + exp(sâ‚‚ - m_old)
With new max:      exp(sâ‚ - m_new) + exp(sâ‚‚ - m_new)
                 = exp(sâ‚ - m_old - (m_new - m_old)) + exp(sâ‚‚ - m_old - (m_new - m_old))
                 = [exp(sâ‚ - m_old) + exp(sâ‚‚ - m_old)] * exp(m_old - m_new)
                 = l_old * Î±
::


Updating the Output
~~~~~~~~~~~~~~~~~~~

Similarly, the accumulated output needs correction:
::

output_old = (1/l_old) * [exp(sâ‚-m_old)*vâ‚ + exp(sâ‚‚-m_old)*vâ‚‚]
::


When we update max:
::

output_new = Î± * output_old + (1/l_new) * [exp(sâ‚ƒ-m_new)*vâ‚ƒ + exp(sâ‚„-m_new)*vâ‚„]
::


**This is the heart of Flash Attention!**

Triton Implementation Details
-----------------------------

The Inner Loop
~~~~~~~~~~~~~~

.. code-block:: python

def *attn_fwd*inner(acc, l_i, m_i, q, desc_k, desc_v, ...):
    for start_n in range(lo, hi, BLOCK_N):
        # Load K block
        k = desc_k.load([offsetk_y, 0]).T

        # Compute QK^T for this block
        qk = tl.dot(q, k)

        # Apply scaling and mask (if causal)
        qk = qk * qk_scale
        if STAGE == 2:  # Causal
            mask = offs_m[:, None] >= (start_n + offs_n[None, :])
            qk = qk + tl.where(mask, 0, -1.0e6)

        # Update max
        m_ij = tl.maximum(m_i, tl.max(qk, 1))

        # Compute softmax probabilities for this block
        p = tl.exp(qk - m_ij[:, None])

        # Correction factor
        alpha = tl.exp(m_i - m_ij)

        # Update sum
        l_ij = tl.sum(p, 1)

        # Update output with correction
        acc = acc * alpha[:, None]

        # Load V block
        v = desc_v.load([offsetv_y, 0])

        # Accumulate
        acc = tl.dot(p, v, acc)

        # Update running statistics
        l_i = l_i * alpha + l_ij
        m_i = m_ij
::


**Key variables**:
- ``m_i``: Current max for each row
- ``l_i``: Current sum of exponentials for each row
- ``acc``: Current weighted sum output
- ``alpha``: Correction factor when max changes

Causal Masking
~~~~~~~~~~~~~~

For autoregressive models (GPT), prevent attending to future tokens:

.. code-block:: python

mask = offs_m[:, None] >= (start_n + offs_n[None, :])
qk = qk + tl.where(mask, 0, -1.0e6)
::


**Visual example** (BLOCK_M=4, BLOCK_N=4):

::

Query rows: [0, 1, 2, 3]
Key columns: [0, 1, 2, 3]

Mask (1 = allowed, 0 = masked):
[[1, 0, 0, 0],   # Query 0 can only attend to Key 0
 [1, 1, 0, 0],   # Query 1 can attend to Keys 0-1
 [1, 1, 1, 0],   # Query 2 can attend to Keys 0-2
 [1, 1, 1, 1]]   # Query 3 can attend to Keys 0-3
::


Setting masked positions to ``-1e6`` â†’ ``exp(-1e6) â‰ˆ 0`` â†’ effectively ignored.

Stages for Causal Attention
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

if STAGE == 1:
    lo, hi = 0, start_m * BLOCK_M  # Before diagonal
elif STAGE == 2:
    lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M  # On diagonal (causal)
else:
    lo, hi = 0, N_CTX  # Full attention (non-causal)
::


**For causal attention**:
- **Stage 1**: Process all blocks before the diagonal (full blocks, no masking needed)
- **Stage 2**: Process diagonal block (needs causal masking)

**Optimization**: Stage 1 can skip masking checks, runs faster!

Memory Optimizations
--------------------

Tensor Descriptors
~~~~~~~~~~~~~~~~~~

Modern GPUs (Hopper, Blackwell) support **tensor descriptors**:

.. code-block:: python

desc_q = TensorDescriptor(q, shape=[y_dim, HEAD_DIM],
                          strides=[HEAD_DIM, 1],
                          block_shape=[BLOCK_M, HEAD_DIM])
::


**Benefits**:
- Hardware-assisted bounds checking
- Automatic address calculation
- Faster than manual pointer arithmetic

**Triton abstracts this**:
.. code-block:: python

q = desc_q.load([offset_y, 0])
::


Looks simple, but uses advanced hardware features!

Warp Specialization
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

warp_specialize = True
::


**Concept**: Different warps in a block do different tasks
- Some warps: Load data from DRAM
- Other warps: Compute on data
- Overlap communication and computation!

**Hopper/Blackwell specific**: These GPUs have dedicated units for loads, so specialization helps.

Reduced Shared Memory
~~~~~~~~~~~~~~~~~~~~~

By using online softmax:
- Don't store full QK^T matrix
- Only need current blocks in SRAM
- Typical usage: ~100 KB vs MBs for naive implementation

FP8 Support
~~~~~~~~~~~

For Blackwell GPUs with FP8 Tensor Cores:

.. code-block:: python

if dtype == tl.float8e5:
    v = desc_v.load([0, offsetv_y]).T  # Transposed layout for FP8
else:
    v = desc_v.load([offsetv_y, 0])
::


**FP8 benefits**:
- 2x memory bandwidth (8 bits vs 16 bits)
- 2x compute throughput (Tensor Cores)
- Trade-off: Slight accuracy loss (acceptable for ML)

Backward Pass
-------------

The backward pass is even more complex!

Preprocess Step
~~~~~~~~~~~~~~~

.. code-block:: python

def *attn_bwd*preprocess(O, DO, Delta, ...):
    # Compute delta = sum(O * DO) for each row
    o = tl.load(O + ...)
    do = tl.load(DO + ...)
    delta = tl.sum(o * do, axis=1)
    tl.store(Delta + ..., delta)
::


**Delta**: Row-wise dot product of output and output gradient
- Used in both dK/dV and dQ computation
- Precompute once, use multiple times

Computing dK and dV
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

def *attn_bwd*dkdv(dk, dv, Q, k, v, DO, M, D, ...):
    for each block of Q:
        # Recompute attention weights
        qk = tl.dot(k, Q)
        p = tl.exp(qk - M)  # M was saved from forward pass

        # Compute dV
        dv += tl.dot(p, DO)

        # Compute dP (gradient of attention weights)
        dp = tl.dot(v, DO.T)

        # Compute dS (before softmax)
        ds = p * (dp - D)  # D is delta from preprocess

        # Compute dK
        dk += tl.dot(ds, Q.T)
::


**Key insight**: Recompute attention weights from Q, K (which we have)
- Don't need to store P from forward pass
- Classic memory-compute trade-off

Computing dQ
~~~~~~~~~~~~

Similar structure, but processes different blocks:

.. code-block:: python

def *attn_bwd*dq(dq, q, K, V, DO, M, D, ...):
    for each block of K,V:
        # Recompute attention
        qk = tl.dot(q, K)
        p = tl.exp(qk - M)

        # Compute dP
        dp = tl.dot(DO, V.T)

        # Compute dS
        ds = p * (dp - D)

        # Compute dQ
        dq += tl.dot(ds, K)
::


Why Recomputation?
~~~~~~~~~~~~~~~~~~

**Traditional approach**:
- Forward: Compute and store P (NÂ² memory)
- Backward: Load P, compute gradients

**Flash Attention**:
- Forward: Don't store P (O(N) memory)
- Backward: Recompute P from Q, K (extra compute, but much less memory)

**Trade-off**:
- 2x compute (recompute in backward)
- But enables 100x longer sequences!

Performance Characteristics
---------------------------

Memory Complexity
~~~~~~~~~~~~~~~~~

**Standard attention**:
- O(NÂ²) for S and P matrices
- Becomes prohibitive for long sequences

**Flash Attention**:
- O(N) for Q, K, V, O
- No intermediate NÂ² matrices
- Enables sequences of 16K, 32K, even 100K+ tokens

Compute Complexity
~~~~~~~~~~~~~~~~~~

Still O(NÂ²) FLOPs (can't change the math), but:
- Better memory access patterns
- Higher arithmetic intensity
- Better cache utilization

**Result**: 2-4x faster despite same FLOPs!

Arithmetic Intensity
~~~~~~~~~~~~~~~~~~~~

With tiling (BLOCK_M=128, BLOCK_N=64):
- Load Q block: 128 Ã— 64 elements
- Load K block: 64 Ã— 64 elements
- Load V block: 64 Ã— 64 elements
- Compute QK: 128 Ã— 64 Ã— 64 = 512K FLOPs
- Compute PV: 128 Ã— 64 Ã— 64 = 512K FLOPs
- Total: ~1M FLOPs per ~20K bytes loaded

Arithmetic Intensity: 1M / 20K â‰ˆ 50 FLOPs/byte

**Compare to naive** (no tiling):
- Each element of S: 1 load Q row, 1 load K row
- Each element of P: 1 load/store
- AI â‰ˆ 2 FLOPs/byte

Flash Attention is **25x more compute-efficient** in memory access!

Auto-Tuning Configurations
--------------------------

.. code-block:: python

configs = [
    triton.Config({'BLOCK_M': BM, 'BLOCK_N': BN}, num_stages=s, num_warps=w)
    for BM in [64, 128]
    for BN in [32, 64, 128]
    for s in [2, 3, 4]
    for w in [4, 8]
]
::


**Block size trade-offs**:
- **Larger blocks**: More reuse, but more SRAM usage
- **Smaller blocks**: Less SRAM, but more overhead

**Typical good configs**:
- BLOCK_M=128, BLOCK_N=64: Balanced
- BLOCK_M=64, BLOCK_N=32: Smaller SRAM GPUs
- More stages: Better for Hopper/Blackwell with more SRAM

Common Pitfalls
---------------

1. SRAM Overflow
~~~~~~~~~~~~~~~~

::

Error: out of resource: shared memory
::


**Cause**: Blocks too large for available SRAM

**Solution**:
- Reduce BLOCK_M or BLOCK_N
- Reduce num_stages
- Adjust auto-tune configs

2. Numerical Instability
~~~~~~~~~~~~~~~~~~~~~~~~

**Problem**: Exp overflow without proper max subtraction

.. code-block:: python

Wrong
=====
p = tl.exp(qk)

Right
=====
m_ij = tl.max(qk, 1)
p = tl.exp(qk - m_ij[:, None])
::


3. Causal Mask Errors
~~~~~~~~~~~~~~~~~~~~~

**Common mistake**: Using ``>`` instead of ``>=``

.. code-block:: python

Wrong for causal (token can't attend to itself!)
================================================
mask = offs_m[:, None] > (start_n + offs_n[None, :])

Right
=====
mask = offs_m[:, None] >= (start_n + offs_n[None, :])
::


4. Forgetting to Update Statistics
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

Must update both m_i and l_i!
=============================
l_i = l_i * alpha + l_ij
m_i = m_ij

And correct accumulator
=======================
acc = acc * alpha[:, None]
::


Forgetting any of these â†’ wrong results.

Extensions and Variants
-----------------------

Multi-Query Attention (MQA)
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use same K, V for all heads:
- K, V shape: (batch, 1, seq_len, head_dim)
- Q shape: (batch, num_heads, seq_len, head_dim)
- Memory savings: num_headsÃ— less for K, V

Grouped Query Attention (GQA)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Groups of heads share K, V:
- Middle ground between MQA and full attention
- Used in Llama 2, Mistral

Sliding Window Attention
~~~~~~~~~~~~~~~~~~~~~~~~

Only attend to nearby tokens:
.. code-block:: python

mask = abs(offs_m[:, None] - offs_n[None, :]) <= window_size
::


Reduces complexity to O(N Ã— window_size).

Flash Attention 3
~~~~~~~~~~~~~~~~~

Latest version (Hopper H100+):
- Warp-specialization optimizations
- Even better overlap of compute and memory
- FP8 support with dynamic scaling
- 1.5-2x faster than Flash Attention 2

Comparison to Standard Attention
--------------------------------

| Aspect | Standard | Flash Attention |
|--------|----------|-----------------|
| Memory | O(NÂ²) | O(N) |
| Speed (2K seq) | 1x | 2-3x faster |
| Speed (16K seq) | OOM | 10x+ faster |
| Max sequence | ~2048 | 100K+ |
| Implementation | Simple | Complex |

Key Takeaways
-------------

1. **Flash Attention solves the O(NÂ²) memory problem**: Enables long sequences
2. **Online softmax is the key innovation**: Compute softmax without full materialization
3. **Tiling + correction factors**: Update statistics as we process blocks
4. **Recomputation in backward pass**: Trade compute for memory
5. **SRAM management is critical**: Blocks must fit in fast memory
6. **Causal masking**: Essential for autoregressive models (GPT, etc.)
7. **Hardware features matter**: Tensor descriptors, warp specialization, FP8
8. **This enables modern LLMs**: GPT-4, Llama, etc. all use variants of Flash Attention

Flash Attention is arguably the most important algorithmic innovation for Transformers in recent years. It enabled the jump from ~2K to 100K+ token context windows!
